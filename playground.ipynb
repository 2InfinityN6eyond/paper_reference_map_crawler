{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install selenium\n",
    "import os\n",
    "import random\n",
    "\n",
    "#from selenium import webdriver\n",
    "#import undetected_chromedriver.v2 as webdriver\n",
    "import undetected_chromedriver as webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Institution :\n",
    "    name :str\n",
    "    google_scholar_url : str\n",
    "    homepage_url : str = None\n",
    "\n",
    "    def toJSON(self) :\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, \n",
    "            sort_keys=True, indent=4)\n",
    "    def toDict(self):\n",
    "        return json.loads(self.toJSON())\n",
    "\n",
    "    def fromDict(self, dic):\n",
    "        self.name = dic['name']\n",
    "        self.google_scholar_url = dic['google_scholar_url']\n",
    "        self.homepage_url = dic['homepage_url']\n",
    "\n",
    "@dataclass\n",
    "class Expertise :\n",
    "    name : str\n",
    "    url : str\n",
    "\n",
    "    def toJSON(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, \n",
    "            sort_keys=True, indent=4)\n",
    "    def toDict(self):\n",
    "        return json.loads(self.toJSON())\n",
    "    def fromDict(self, dic):\n",
    "        self.name = dic['name']\n",
    "        self.url = dic['url']\n",
    "\n",
    "@dataclass\n",
    "class Author :\n",
    "    name : str\n",
    "    google_schorlar_profile_url : str\n",
    "    affiliation : str = None\n",
    "    expertise_list : list[str] = None\n",
    "    homepage_url : str = None\n",
    "    paper_list : list = None\n",
    "    paper_title_list : list = None\n",
    "\n",
    "    def toJSON(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, \n",
    "            sort_keys=True, indent=4)\n",
    "    def toDict(self):\n",
    "        return json.loads(self.toJSON())\n",
    "    def fromDict(self, dic):\n",
    "        self.name = dic['name']\n",
    "        self.google_schorlar_profile_url = dic['google_schorlar_profile_url']\n",
    "        self.affiliation = dic['affiliation']\n",
    "        self.expertise_list = dic['expertise_list']\n",
    "        self.homepage_url = dic['homepage_url']\n",
    "        self.paper_list = dic['paper_list']\n",
    "        self.paper_title_list = dic['paper_title_list']\n",
    "        \n",
    "@dataclass\n",
    "class Paper :\n",
    "    # After search paper title using Google Schorlar,\n",
    "    # fill in basic metadata (abstract) from Google Schorlar\n",
    "    # fill in other metadata from Crossref\n",
    "    DOI : str = None\n",
    "    crossref_json : dict = None\n",
    "    google_schorlar_metadata : dict = None\n",
    "    title : str = None\n",
    "    authors : list = None\n",
    "    abstract : str = None\n",
    "    conference : str = None\n",
    "    journal : str = None\n",
    "    year : int = None\n",
    "    reference_list : list[str] = None\n",
    "    referenced_list : list[str] = None\n",
    "    cite_bibtex : str = None\n",
    "\n",
    "    def toJSON(self):\n",
    "        '''convert to JSON recursively'''\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, \n",
    "            sort_keys=True, indent=4)\n",
    "    def toDict(self):\n",
    "        '''convert to dict recursively'''\n",
    "        return json.loads(self.toJSON())\n",
    "    def fromDict(self, dic) :\n",
    "        '''convert from dict recursively'''\n",
    "        self.DOI = dic['DOI']\n",
    "        self.crossref_json = dic['crossref_json']\n",
    "        self.google_schorlar_metadata = dic['google_schorlar_metadata']\n",
    "        self.title = dic['title']\n",
    "        self.authors = dic['authors']\n",
    "        self.abstract = dic['abstract']\n",
    "        self.conference = dic['conference']\n",
    "        self.journal = dic['journal']\n",
    "        self.year = dic['year']\n",
    "        self.reference_list = dic['reference_list']\n",
    "        self.referenced_list = dic['referenced_list']\n",
    "        self.cite_bibtex = dic['cite_bibtex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "class CrossRefFetcher :\n",
    "    def __init__(self) :\n",
    "        pass\n",
    "\n",
    "    def fetchMetaDatafromTitle(self, paper) :\n",
    "        '''\n",
    "        args :\n",
    "            paper : Paper\n",
    "                expect paper.title\n",
    "        '''\n",
    "        title = urllib.parse.quote(paper.title)\n",
    "        url = f'https://api.crossref.org/works?query.bibliographic={title}&rows=1'\n",
    "        r = requests.get(url)\n",
    "        metadata = r.json()['message']['items'][0]\n",
    "\n",
    "        paper.DOI = metadata['DOI']\n",
    "        paper.crossref_json = metadata\n",
    "        if len(metadata) == 0 :\n",
    "            return None\n",
    "    \n",
    "        reference_list = []\n",
    "        try :\n",
    "            for reference in metadata['reference'] :\n",
    "                if 'DOI' in reference :\n",
    "                    reference_list.append(reference['DOI'])\n",
    "        except :\n",
    "            pass\n",
    "            \n",
    "        paper.reference_list = reference_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = Paper(title = \"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\")\n",
    "\n",
    "crossref_fetcher = CrossRefFetcher()\n",
    "crossref_fetcher.fetchMetaDatafromTitle(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GoogleScharlarSearcher :\n",
    "    BASE_URL = \"https://scholar.google.com\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            institution_dict = None,\n",
    "            expertise_dict = None,\n",
    "        ) :\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        #chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--use_subprocess\")\n",
    "\n",
    "        self.driver = webdriver.Chrome(options=chrome_options)\n",
    "        self.institution_dict = institution_dict\n",
    "        self.expertise_dict = expertise_dict\n",
    "\n",
    "        self.crossref_fetcher = CrossRefFetcher()\n",
    "\n",
    "\n",
    "\n",
    "    def searchPaperByName(self, name) :\n",
    "        self.driver.get(self.BASE_URL)\n",
    "        self.driver.implicitly_wait(10)\n",
    "        # search given paper name\n",
    "        search = self.driver.find_element(by=By.XPATH, value='//*[@id=\"gs_hdr_tsi\"]')\n",
    "        search.send_keys(name)\n",
    "        search.send_keys(Keys.RETURN)\n",
    "        self.driver.implicitly_wait(10)\n",
    "        # click the first paper\n",
    "\n",
    "    def searchAuthorByName(\n",
    "        self,\n",
    "        name,\n",
    "        continue_search = False,\n",
    "        search_width = 1000,\n",
    "        ask_for_continue = False,\n",
    "    ) :\n",
    "        \"\"\"\n",
    "        If continue_search is True, search every co-author until search_width\n",
    "        \"\"\"\n",
    "        self.driver.get(self.BASE_URL)\n",
    "        self.driver.implicitly_wait(10)\n",
    "        # search by author name\n",
    "        searcher = self.driver.find_element(by=By.XPATH, value='//*[@id=\"gs_hdr_tsi\"]')\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        #searcher.send_keys(name)\n",
    "        for chr in name :\n",
    "            searcher.send_keys(chr)\n",
    "            time.sleep(random.randint(1, 10)/200)\n",
    "        searcher.send_keys(Keys.RETURN)\n",
    "        self.driver.implicitly_wait(10)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        if ask_for_continue :\n",
    "            key_input = input(\"Press [n] to stop...\")\n",
    "            if key_input in [\"n\", \"N\", \"no\", \"No\", \"NO\", \"nO\"] :\n",
    "                return [], {}\n",
    "\n",
    "        page_source = self.driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        author_list = soup.find_all(\"h4\", class_=\"gs_rt2\")\n",
    "\n",
    "        author_list = list(map(\n",
    "            lambda author : Author(\n",
    "                name = author.text,\n",
    "                google_schorlar_profile_url = self.BASE_URL +author.find(\"a\")[\"href\"],\n",
    "            ),\n",
    "            author_list\n",
    "        ))\n",
    "\n",
    "        print(f\"authors found : {list(map(lambda author : author.name, author_list))}\")\n",
    "        \n",
    "        auther_href_button_list = self.driver.find_elements(by=By.XPATH, value='//*[@class=\"gs_rt2\"]/a')        \n",
    "        whole_paper_dict = {}\n",
    "        for author, auther_href_button in zip(author_list, auther_href_button_list) :\n",
    "            print(f\"filling google schorlar metadata of papers from {author.name}...\")\n",
    "            auther_href_button.click()\n",
    "            time.sleep(0.2)\n",
    "            author, paper_dict = self.fillAuthor(author, url_already_loaded = True)\n",
    "            whole_paper_dict.update(paper_dict)\n",
    "            self.driver.back()\n",
    "            time.sleep(0.2)\n",
    "\n",
    "            break\n",
    "\n",
    "        #for author in author_list :\n",
    "            #author, paper_dict = self.fillAuthor(author)\n",
    "            #whole_paper_dict.update(paper_dict)\n",
    "\n",
    "\n",
    "        if continue_search and len(author_list) < search_width :\n",
    "            pass\n",
    "\n",
    "\n",
    "        return author_list, whole_paper_dict\n",
    "\n",
    "    def addInstitution(\n",
    "        self,\n",
    "        html_str\n",
    "    ) :\n",
    "        '''\n",
    "        initialize Institution instance and append to\n",
    "        self.instaitution_dict if not exist\n",
    "        args :\n",
    "            institution_html :\n",
    "                expected to have name,\n",
    "                google_schorlar_institution_url field\n",
    "        return :\n",
    "            institution name\n",
    "        '''\n",
    "        #institution_name = html_str.find(\"a\").text\n",
    "        institution_name = html_str.text\n",
    "\n",
    "        if institution_name not in self.institution_dict :\n",
    "            try :\n",
    "                google_schorlar_institution_url = self.BASE_URL + html_str.find(\"a\")[\"href\"]\n",
    "            except Exception as e :\n",
    "                google_schorlar_institution_url = None\n",
    "            homepage_url = None\n",
    "            self.institution_dict[institution_name] = Institution(\n",
    "                name = institution_name,\n",
    "                google_scholar_url = google_schorlar_institution_url,\n",
    "                homepage_url = homepage_url,\n",
    "            )\n",
    "        return institution_name\n",
    "\n",
    "    def addExpertise(\n",
    "        self,\n",
    "        html_str_list\n",
    "    ) :\n",
    "        '''\n",
    "        initialize Expertise instance and append to\n",
    "        self.expertise_dict if not exist\n",
    "        args :\n",
    "            html_str_list :\n",
    "                list of html_str. each elements are html str\n",
    "                expected to have name,\n",
    "                google_schorlar_expertise_url field\n",
    "        return :\n",
    "            expertise name\n",
    "        '''\n",
    "        expertise_name_list = []\n",
    "        for html_str in html_str_list :\n",
    "            expertise_name = html_str.text\n",
    "            if expertise_name not in self.expertise_dict :\n",
    "                google_schorlar_expertise_url = self.BASE_URL + html_str[\"href\"]\n",
    "                self.expertise_dict[expertise_name] = Expertise(\n",
    "                    name = expertise_name,\n",
    "                    url = google_schorlar_expertise_url,\n",
    "                )\n",
    "            expertise_name_list.append(expertise_name)\n",
    "        return expertise_name_list\n",
    "\n",
    "    def fillAuthor(self, author, url_already_loaded = False) :\n",
    "        \"\"\"\n",
    "        fill in author instance\n",
    "        args :\n",
    "            author :\n",
    "                expected to have name, google_schorlar_profile_url field\n",
    "        \"\"\"\n",
    "        # load page html        \n",
    "        if not url_already_loaded :\n",
    "            self.driver.get(author.google_schorlar_profile_url)\n",
    "            self.driver.implicitly_wait(10)\n",
    "        page_source = self.driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # fill in expertise\n",
    "        expertise_html_list = soup.find_all(\"a\", class_=\"gsc_prf_inta\")\n",
    "        expertise_name_list = self.addExpertise(expertise_html_list)\n",
    "        author.expertise_list = expertise_name_list\n",
    "\n",
    "        # fill in institution\n",
    "        institution_html = soup.find(\"div\", class_=\"gsc_prf_il\")\n",
    "        try :\n",
    "            institution_name = self.addInstitution(institution_html)\n",
    "            author.affiliation = institution_name\n",
    "        except Exception as e :\n",
    "            print(e)\n",
    "            print(soup)\n",
    "            raise e\n",
    "\n",
    "        paper_dict = self.makePaperDictFromAuthor(author, url_already_loaded = True)\n",
    "        #DOI_list = list(paper_dict.keys())\n",
    "        #author.paper_list = DOI_list\n",
    "\n",
    "        return author, paper_dict\n",
    "    \n",
    "\n",
    "    def makePaperDictFromAuthor(self, author, url_already_loaded = False, search_width_limit = 20) :\n",
    "        \"\"\"\n",
    "        make paper instance from author instance\n",
    "        args :\n",
    "            author : Author\n",
    "                expected to have name, google_schorlar_profile_url field\n",
    "        return :\n",
    "            paper_list : list[Paper]\n",
    "        \"\"\"\n",
    "\n",
    "        # load page html        \n",
    "        if not url_already_loaded :\n",
    "            self.driver.get(author.google_schorlar_profile_url)\n",
    "            self.driver.implicitly_wait(10)\n",
    "        page_source = self.driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # search papers\n",
    "        # click \"show more\" button until it is disabled\n",
    "        '''\n",
    "        while True :\n",
    "            load_more_button = self.driver.find_element(by=By.XPATH, value='//*[@id=\"gsc_bpf_more\"]')\n",
    "            self.driver.implicitly_wait(10)\n",
    "            load_more_button.click()\n",
    "            time.sleep(2)\n",
    "            if load_more_button.get_property(\"disabled\") :\n",
    "                break\n",
    "        '''\n",
    "        # get papaer html list\n",
    "        page_source = self.driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        paper_html_list = soup.find_all(\"tr\", class_=\"gsc_a_tr\")\n",
    "        paper_html_list = paper_html_list[:search_width_limit]\n",
    "\n",
    "        paper_href_list = self.driver.find_elements(by=By.XPATH, value='//*[@class=\"gsc_a_t\"]/a')\n",
    "\n",
    "\n",
    "        paper_list = []\n",
    "\n",
    "        print(f\"filling google schorlar metadata of papers from {author.name}...\")\n",
    "        with tqdm(total=len(paper_html_list)) as pbar:\n",
    "            #for paper_html in paper_html_list :\n",
    "            for paper_html, paper_href in zip(paper_html_list, paper_href_list) :\n",
    "                google_schorlar_url = self.BASE_URL + paper_html.find(\"a\", class_=\"gsc_a_at\")[\"href\"]\n",
    "                title = paper_html.find(\"a\", class_=\"gsc_a_at\").text\n",
    "                \n",
    "                #self.driver.get(google_schorlar_url)\n",
    "                #self.driver.implicitly_wait(10)\n",
    "                paper_href.click()\n",
    "                time.sleep(0.2)\n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                metadata_list = soup.find_all(\"div\", class_=\"gs_scl\")\n",
    "                html_title = soup.find(\"a\", class_=\"gsc_oci_title_link\")\n",
    "\n",
    "                google_schorlar_metadata = {}\n",
    "                for metadata in metadata_list :\n",
    "                    field = metadata.find(\"div\", class_=\"gsc_oci_field\").text\n",
    "                    value = metadata.find(\"div\", class_=\"gsc_oci_value\").text\n",
    "                    google_schorlar_metadata[field] = value\n",
    "                \n",
    "\n",
    "                paper = Paper(title = title, google_schorlar_metadata = google_schorlar_metadata)\n",
    "                paper_list.append(paper)\n",
    "\n",
    "                pbar.set_postfix_str(title)\n",
    "                pbar.update(1)\n",
    "\n",
    "                self.driver.back()\n",
    "\n",
    "        author.paper_title_list = list(map(lambda paper : paper.title, paper_list))\n",
    "\n",
    "\n",
    "        paper_dict = {}\n",
    "        for paper in paper_list :\n",
    "            paper_dict[paper.title] = paper\n",
    "        return paper_dict\n",
    "\n",
    "        # query_crossref\n",
    "        print(f\"fetching crosserf metadata of papers from {author.name}...\")\n",
    "        for paper in tqdm(paper_list) :\n",
    "            self.crossref_fetcher.fetchMetaDatafromTitle(paper)\n",
    "            paper_dict[paper.DOI] = paper\n",
    "\n",
    "        return paper_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from file if file is available.\n",
    "\n",
    "institution_dict = {}\n",
    "expertise_dict = {}\n",
    "whole_author_list = []\n",
    "whole_paper_dict = {}\n",
    "\n",
    "INSTITUTION_FILE_PATH = \"./institution_dict.json\"\n",
    "if os.path.exists(INSTITUTION_FILE_PATH) :\n",
    "    with open(INSTITUTION_FILE_PATH, \"r\") as f :\n",
    "        institution_dict_raw = json.load(f)\n",
    "    for k, v in institution_dict_raw.items() :\n",
    "        institution_dict[k] = Institution(**v)\n",
    "\n",
    "EXPERTISE_FILE_PATH = \"./expertise_dict.json\"\n",
    "if os.path.exists(EXPERTISE_FILE_PATH) :\n",
    "    with open(EXPERTISE_FILE_PATH, \"r\") as f :\n",
    "        expertise_dict_raw = json.load(f)\n",
    "    for k, v in expertise_dict_raw.items() :\n",
    "        expertise_dict[k] = Expertise(**v)\n",
    "\n",
    "AUTHOR_FILE_PATH = \"./author_list.json\"\n",
    "if os.path.exists(AUTHOR_FILE_PATH) :\n",
    "    with open(AUTHOR_FILE_PATH, \"r\") as f :\n",
    "        author_list_raw = json.load(f)\n",
    "    for author in author_list_raw :\n",
    "        whole_author_list.append(Author(**author))\n",
    "\n",
    "WHOLE_PAPER_FILE_PATH = \"./whole_paper_dict.json\"\n",
    "if os.path.exists(WHOLE_PAPER_FILE_PATH) :\n",
    "    with open(WHOLE_PAPER_FILE_PATH, \"r\") as f :\n",
    "        whole_paper_dict = json.load(f)\n",
    "    for k, v in whole_paper_dict.items() :\n",
    "        whole_paper_dict[k] = Paper(**v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./author_name_list.json\", \"r\") as f :\n",
    "    author_name_list = json.load(f)\n",
    "\n",
    "author_name_to_append_list = author_name_list\n",
    "\n",
    "pre_existing_author_name_list = list(map(lambda author : author.name, whole_author_list))\n",
    "author_name_to_append_list = list(filter(lambda name : name not in pre_existing_author_name_list, author_name_to_append_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "907"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(author_name_list)\n",
    "len(author_name_to_append_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors found : ['Evan Shelhamer']\n",
      "filling google schorlar metadata of papers from Evan Shelhamer...\n",
      "filling google schorlar metadata of papers from Evan Shelhamer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:14<00:00,  1.36it/s, Back to the Source: Diffusion-Driven Adaptation To Test-Time Corruption]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors found : ['Vladlen Koltun']\n",
      "filling google schorlar metadata of papers from Vladlen Koltun...\n",
      "filling google schorlar metadata of papers from Vladlen Koltun...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:12<00:00,  1.54it/s, Tanks and Temples: Benchmarking Large-Scale Scene Reconstruction]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors found : ['Richard Zemel']\n",
      "filling google schorlar metadata of papers from Richard Zemel...\n",
      "filling google schorlar metadata of papers from Richard Zemel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:12<00:00,  1.55it/s, Neural relational inference for interacting systems]                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors found : ['Michael GK Jones', 'Michael Jones', 'Michael D Jones']\n",
      "filling google schorlar metadata of papers from Michael GK Jones...\n",
      "filling google schorlar metadata of papers from Michael GK Jones...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "empty_author_name_list = []\n",
    "\n",
    "gsearch = GoogleScharlarSearcher(institution_dict, expertise_dict)\n",
    "for author_name in author_name_to_append_list :\n",
    "    author_list, paper_dict = gsearch.searchAuthorByName(author_name, ask_for_continue=True)\n",
    "    if len(author_list) == 0 :\n",
    "        empty_author_name_list.append(author_name)\n",
    "        continue\n",
    "    whole_paper_dict.update(paper_dict)\n",
    "    whole_author_list += author_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_paper_dict_dict = {}\n",
    "for key, paper in whole_paper_dict.items() :\n",
    "    whole_paper_dict_dict[key] = paper.toDict()\n",
    "with open('whole_paper_dict.json', 'w') as f:\n",
    "    json.dump(whole_paper_dict_dict, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "whole_author_dict_list = list(map(lambda author : author.toDict(), whole_author_list))\n",
    "with open(\"author_list.json\", 'w') as f :\n",
    "    json.dump(whole_author_dict_list, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "institution_dict = {}\n",
    "for key, institution in gsearch.institution_dict.items() :\n",
    "    institution_dict[key] = institution.toDict()\n",
    "    #institution_dict[key] = json.loads(institution.toJOSN())\n",
    "with open(\"institution_dict.json\", 'w') as f :\n",
    "    json.dump(institution_dict, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "expertise_dict = {}\n",
    "for key, expertise in gsearch.expertise_dict.items() :\n",
    "    expertise_dict[key] = expertise.toDict()\n",
    "with open(\"expertise_dict.json\", 'w') as f :\n",
    "    json.dump(expertise_dict, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "for unvalid_name in empty_author_name_list :\n",
    "    author_name_list.remove(unvalid_name)\n",
    "\n",
    "with open(\"author_name_list.json\", 'w') as f :\n",
    "    json.dump(author_name_list, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "\n",
    "def getMetaData(title) :\n",
    "\n",
    "    # URL encode the title to ensure it's in the correct format for a URL\n",
    "    encoded_title = urllib.parse.quote_plus(title)\n",
    "\n",
    "    # The Crossref API endpoint for works\n",
    "    api_url = f\"https://api.crossref.org/works?query.title={encoded_title}\"\n",
    "\n",
    "    # Make the GET request to the Crossref API\n",
    "    response = requests.get(api_url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the response to JSON\n",
    "        data = response.json()\n",
    "\n",
    "        # Check if there are items in the message\n",
    "        if 'items' in data['message']:\n",
    "            # Loop through the items to find the first one with a DOI\n",
    "            for item in data['message']['items']:\n",
    "                # Print the DOI\n",
    "                print(\"Title:\", item.get('title')[0])\n",
    "                print(\"DOI:\", item.get('DOI'))\n",
    "                break\n",
    "        else:\n",
    "            print(\"No results found for this title.\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Sound-Guided Semantic Image Manipulation\n",
      "DOI: 10.1109/cvpr52688.2022.00337\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import requests\n",
    "\n",
    "# Your paper title\n",
    "title = \"Example Title of Your Academic Paper\"\n",
    "title = \"Visual and Range Data\"\n",
    "title = \"3D Gaussian Splatting for Real-Time Radiance Field Rendering\"\n",
    "title = \"Sound-Guided Semantic Image Manipulation\"\n",
    "\n",
    "# URL encode the title to ensure it's in the correct format for a URL\n",
    "encoded_title = urllib.parse.quote_plus(title)\n",
    "\n",
    "# The Crossref API endpoint for works\n",
    "api_url = f\"https://api.crossref.org/works?query.title={encoded_title}\"\n",
    "\n",
    "# Make the GET request to the Crossref API\n",
    "response = requests.get(api_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the response to JSON\n",
    "    data = response.json()\n",
    "\n",
    "    # Check if there are items in the message\n",
    "    if 'items' in data['message']:\n",
    "        # Loop through the items to find the first one with a DOI\n",
    "        for item in data['message']['items']:\n",
    "            # Print the DOI\n",
    "            print(\"Title:\", item.get('title')[0])\n",
    "            print(\"DOI:\", item.get('DOI'))\n",
    "            break\n",
    "    else:\n",
    "        print(\"No results found for this title.\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOI for 'Sound-Guided Semantic Image Manipulation': 10.1109/cvpr52688.2022.00337\n",
      "References:\n",
      "10.1145/2647868.2655045\n",
      "10.1109/CVPR46437.2021.00232\n",
      "10.1109/CVPR.2019.00244\n",
      "No DOI provided\n",
      "No DOI provided\n",
      "10.1109/CVPR.2019.00772\n",
      "No DOI provided\n",
      "No DOI provided\n",
      "No DOI provided\n",
      "10.1109/ICCV48922.2021.00209\n",
      "No DOI provided\n",
      "10.1109/WACV48630.2021.00313\n",
      "No DOI provided\n",
      "No DOI provided\n",
      "10.1109/ICCV.2019.00453\n",
      "No DOI provided\n",
      "No DOI provided\n",
      "10.1109/ICCV48922.2021.00212\n",
      "No DOI provided\n",
      "10.1109/CVPR42600.2020.00813\n",
      "No DOI provided\n",
      "10.1145/3394171.3413624\n",
      "No DOI provided\n",
      "No DOI provided\n",
      "10.1007/s11633-021-1293-0\n",
      "No DOI provided\n",
      "10.1109/ICCV.2019.01040\n",
      "No DOI provided\n",
      "10.1007/978-90-481-8847-5_10\n",
      "10.1167/16.12.326\n",
      "No DOI provided\n",
      "10.1109/ICASSP.2017.7952261\n",
      "No DOI provided\n",
      "10.1109/IJCNN52387.2021.9533654\n",
      "10.1609/aaai.v32i1.12329\n",
      "No DOI provided\n",
      "No DOI provided\n",
      "No DOI provided\n",
      "10.1109/ICASSP40776.2020.9053174\n",
      "10.1109/CVPR.2015.7298698\n",
      "No DOI provided\n",
      "10.1145/3126686.3126723\n",
      "10.1109/CVPR46437.2021.00229\n",
      "10.1109/CVPR.2019.00482\n",
      "No DOI provided\n",
      "10.1145/3450626.3459838\n",
      "No DOI provided\n",
      "10.1109/ICASSP.2019.8682383\n",
      "No DOI provided\n",
      "10.1109/WACV.2017.58\n",
      "No DOI provided\n",
      "10.1609/aaai.v34i05.6431\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "def get_doi_from_title(title):\n",
    "    encoded_title = urllib.parse.quote_plus(title)\n",
    "    api_url = f\"https://api.crossref.org/works?query.title={encoded_title}\"\n",
    "    response = requests.get(api_url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        items = data['message'].get('items', [])\n",
    "        if items:\n",
    "            # Assuming the first result is the correct one\n",
    "            return items[0].get('DOI')\n",
    "    return None\n",
    "\n",
    "def get_references_from_doi(doi):\n",
    "    api_url = f\"https://api.crossref.org/works/{doi}\"\n",
    "    response = requests.get(api_url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        references = data['message'].get('reference', [])\n",
    "        return references\n",
    "    return []\n",
    "\n",
    "# Example usage:\n",
    "paper_title = \"Example Title of Your Academic Paper\"\n",
    "paper_title = \"3D Gaussian Splatting for Real-Time Radiance Field Rendering\"\n",
    "paper_title = \"Sound-Guided Semantic Image Manipulation\"\n",
    "\n",
    "paper_doi = get_doi_from_title(paper_title)\n",
    "\n",
    "if paper_doi:\n",
    "    print(f\"DOI for '{paper_title}': {paper_doi}\")\n",
    "    references = get_references_from_doi(paper_doi)\n",
    "    if references:\n",
    "        print(\"References:\")\n",
    "        for ref in references:\n",
    "            print(ref.get('DOI', 'No DOI provided'))\n",
    "else:\n",
    "    print(f\"No DOI found for the paper titled '{paper_title}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://scholar.google.com/scholar?hl=ko&as_sdt=0%2C5&q=andrey&btnG=\")\n",
    "driver.implicitly_wait(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‘I'm home for the kids’: contradictory implications for work–life balance of teleworking mothers\n",
      "\n",
      "The situational risks of young drivers: The influence of passengers, time of day and day of week on accident rates\n",
      "\n",
      "Climate change impacts and adaptation: a Canadian perspective\n",
      "\n",
      "Weather as a chronic hazard for road transportation in Canadian cities\n",
      "\n",
      "A temporal analysis of rain-related crash risk\n",
      "\n",
      "Long-term trends in weather-related crash risks\n",
      "\n",
      "Remixing work, family and leisure: teleworkers' experiences of everyday life\n",
      "\n",
      "Relationships between weather and road safety: past and future research directions\n",
      "\n",
      "Weather information and road safety\n",
      "\n",
      "Climate change implications for flexible pavement design and performance in southern Canada\n",
      "\n",
      "Vulnerability index construction: methodological choices and their influence on identifying vulnerable neighbourhoods\n",
      "\n",
      "Mr. Dithers comes to dinner: Telework and the merging of women's work and home domains in Canada\n",
      "\n",
      "The knowledge economy city: Gentrification, studentification and youthification, and their connections to universities\n",
      "\n",
      "Projected implications of climate change for road safety in Greater Vancouver, Canada\n",
      "\n",
      "Climate change and transportation: potential interactions and impacts\n",
      "\n",
      "Residential energy efficiency retrofits: How program design affects participation and outcomes\n",
      "\n",
      "Driver response to rainfall on urban expressways\n",
      "\n",
      "The influence of previous disaster experience and sociodemographics on protective behaviors during two successive tornado events\n",
      "\n",
      "Eco-driver training within the City of Calgary’s municipal fleet: Monitoring the impact\n",
      "\n",
      "Does design matter? The ecological footprint as a planning tool at the local level\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paper_href_list = driver.find_elements(by=By.XPATH, value='//*[@class=\"gsc_a_t\"]/a')\n",
    "for paper_href in paper_href_list :\n",
    "    print(paper_href.text)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
