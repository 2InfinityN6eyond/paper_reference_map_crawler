{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install selenium\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import platform\n",
    "os_name = platform.system()\n",
    "\n",
    "if os_name == \"Darwin\":\n",
    "    import undetected_chromedriver as webdriver\n",
    "    #from selenium import webdriver\n",
    "\n",
    "elif os_name == \"Linux\":\n",
    "    from selenium import webdriver\n",
    "\n",
    "#from selenium import webdriver\n",
    "#import undetected_chromedriver.v2 as webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from containers import Institution, Author, Paper, Expertise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GoogleScharlarSearcher :\n",
    "    BASE_URL = \"https://scholar.google.com\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            institution_dict = None,\n",
    "            expertise_dict = None,\n",
    "            os_name = None\n",
    "        ) :\n",
    "\n",
    "        if False : # os_name == \"Darwin\":\n",
    "            self.driver = webdriver.Safari()\n",
    "        else :\n",
    "            chrome_options = webdriver.ChromeOptions()\n",
    "            #chrome_options.add_argument(\"--headless\")\n",
    "            chrome_options.add_argument(\"--use_subprocess\")\n",
    "\n",
    "            self.driver = webdriver.Chrome(options=chrome_options)\n",
    "        self.institution_dict = institution_dict\n",
    "        self.expertise_dict = expertise_dict\n",
    "\n",
    "    def searchPaperByName(self, name) :\n",
    "        self.driver.get(self.BASE_URL)\n",
    "        self.driver.implicitly_wait(10)\n",
    "        # search given paper name\n",
    "        search = self.driver.find_element(by=By.XPATH, value='//*[@id=\"gs_hdr_tsi\"]')\n",
    "        search.send_keys(name)\n",
    "        search.send_keys(Keys.RETURN)\n",
    "        self.driver.implicitly_wait(10)\n",
    "        # click the first paper\n",
    "\n",
    "    def searchAuthorByName(\n",
    "        self,\n",
    "        name,\n",
    "        continue_search = False,\n",
    "        search_width = 1000,\n",
    "        ask_for_continue = False,\n",
    "    ) :\n",
    "        \"\"\"\n",
    "        If continue_search is True, search every co-author until search_width\n",
    "        \"\"\"\n",
    "        self.driver.get(self.BASE_URL)\n",
    "        self.driver.implicitly_wait(10)\n",
    "\n",
    "        self.checkCaptcha()\n",
    "\n",
    "        # search by author name\n",
    "        searcher = self.driver.find_element(by=By.XPATH, value='//*[@id=\"gs_hdr_tsi\"]')\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        #searcher.send_keys(name)\n",
    "        for chr in name :\n",
    "            searcher.send_keys(chr)\n",
    "            time.sleep(random.randint(1, 10)/200)\n",
    "        searcher.send_keys(Keys.RETURN)\n",
    "        self.driver.implicitly_wait(10)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        self.checkCaptcha()\n",
    "\n",
    "        if ask_for_continue :\n",
    "            key_input = input(\"Press [n] to stop...\")\n",
    "            if key_input in [\"n\", \"N\", \"no\", \"No\", \"NO\", \"nO\"] :\n",
    "                return [], {}\n",
    "\n",
    "        page_source = self.driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        author_list = soup.find_all(\"h4\", class_=\"gs_rt2\")\n",
    "\n",
    "        author_list = list(map(\n",
    "            lambda author : Author(\n",
    "                name = author.text,\n",
    "                google_schorlar_profile_url = self.BASE_URL +author.find(\"a\")[\"href\"],\n",
    "            ),\n",
    "            author_list\n",
    "        ))\n",
    "\n",
    "        print(f\"authors found : {list(map(lambda author : author.name, author_list))}\")\n",
    "        \n",
    "        auther_href_button_list = self.driver.find_elements(by=By.XPATH, value='//*[@class=\"gs_rt2\"]/a')        \n",
    "        whole_paper_dict = {}\n",
    "        for author, auther_href_button in zip(author_list, auther_href_button_list) :\n",
    "            print(f\"filling google schorlar metadata of papers from {author.name}...\")\n",
    "            auther_href_button.click()\n",
    "            time.sleep(0.2)\n",
    "\n",
    "            self.checkCaptcha()\n",
    "\n",
    "            author, paper_dict = self.fillAuthor(author, url_already_loaded = True)\n",
    "            whole_paper_dict.update(paper_dict)\n",
    "            self.driver.back()\n",
    "            time.sleep(0.2)\n",
    "            self.checkCaptcha()\n",
    "\n",
    "            break\n",
    "\n",
    "        #for author in author_list :\n",
    "            #author, paper_dict = self.fillAuthor(author)\n",
    "            #whole_paper_dict.update(paper_dict)\n",
    "\n",
    "\n",
    "        if continue_search and len(author_list) < search_width :\n",
    "            pass\n",
    "\n",
    "\n",
    "        return author_list, whole_paper_dict\n",
    "\n",
    "    def addInstitution(\n",
    "        self,\n",
    "        html_str\n",
    "    ) :\n",
    "        '''\n",
    "        initialize Institution instance and append to\n",
    "        self.instaitution_dict if not exist\n",
    "        args :\n",
    "            institution_html :\n",
    "                expected to have name,\n",
    "                google_schorlar_institution_url field\n",
    "        return :\n",
    "            institution name\n",
    "        '''\n",
    "        #institution_name = html_str.find(\"a\").text\n",
    "        institution_name = html_str.text\n",
    "\n",
    "        if institution_name not in self.institution_dict :\n",
    "            try :\n",
    "                google_schorlar_institution_url = self.BASE_URL + html_str.find(\"a\")[\"href\"]\n",
    "            except Exception as e :\n",
    "                google_schorlar_institution_url = None\n",
    "            homepage_url = None\n",
    "            self.institution_dict[institution_name] = Institution(\n",
    "                name = institution_name,\n",
    "                google_scholar_url = google_schorlar_institution_url,\n",
    "                homepage_url = homepage_url,\n",
    "            )\n",
    "        return institution_name\n",
    "\n",
    "    def addExpertise(\n",
    "        self,\n",
    "        html_str_list\n",
    "    ) :\n",
    "        '''\n",
    "        initialize Expertise instance and append to\n",
    "        self.expertise_dict if not exist\n",
    "        args :\n",
    "            html_str_list :\n",
    "                list of html_str. each elements are html str\n",
    "                expected to have name,\n",
    "                google_schorlar_expertise_url field\n",
    "        return :\n",
    "            expertise name\n",
    "        '''\n",
    "        expertise_name_list = []\n",
    "        for html_str in html_str_list :\n",
    "            expertise_name = html_str.text\n",
    "            if expertise_name not in self.expertise_dict :\n",
    "                google_schorlar_expertise_url = self.BASE_URL + html_str[\"href\"]\n",
    "                self.expertise_dict[expertise_name] = Expertise(\n",
    "                    name = expertise_name,\n",
    "                    url = google_schorlar_expertise_url,\n",
    "                )\n",
    "            expertise_name_list.append(expertise_name)\n",
    "        return expertise_name_list\n",
    "\n",
    "    def fillAuthor(self, author, url_already_loaded = False) :\n",
    "        \"\"\"\n",
    "        fill in author instance\n",
    "        args :\n",
    "            author :\n",
    "                expected to have name, google_schorlar_profile_url field\n",
    "        \"\"\"\n",
    "        # load page html        \n",
    "        if not url_already_loaded :\n",
    "            self.driver.get(author.google_schorlar_profile_url)\n",
    "            self.driver.implicitly_wait(10)\n",
    "        page_source = self.driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # fill in expertise\n",
    "        expertise_html_list = soup.find_all(\"a\", class_=\"gsc_prf_inta\")\n",
    "        expertise_name_list = self.addExpertise(expertise_html_list)\n",
    "        author.expertise_list = expertise_name_list\n",
    "\n",
    "        # fill in institution\n",
    "        institution_html = soup.find(\"div\", class_=\"gsc_prf_il\")\n",
    "        try :\n",
    "            institution_name = self.addInstitution(institution_html)\n",
    "            author.affiliation = institution_name\n",
    "        except Exception as e :\n",
    "            print(e)\n",
    "            print(soup)\n",
    "            raise e\n",
    "\n",
    "        paper_dict = self.makePaperDictFromAuthor(author, url_already_loaded = True)\n",
    "        #DOI_list = list(paper_dict.keys())\n",
    "        #author.paper_list = DOI_list\n",
    "\n",
    "        return author, paper_dict\n",
    "    \n",
    "\n",
    "    def makePaperDictFromAuthor(self, author, url_already_loaded = False, search_width_limit = 20) :\n",
    "        \"\"\"\n",
    "        make paper instance from author instance\n",
    "        args :\n",
    "            author : Author\n",
    "                expected to have name, google_schorlar_profile_url field\n",
    "        return :\n",
    "            paper_list : list[Paper]\n",
    "        \"\"\"\n",
    "\n",
    "        # load page html        \n",
    "        if not url_already_loaded :\n",
    "            self.driver.get(author.google_schorlar_profile_url)\n",
    "            self.driver.implicitly_wait(10)\n",
    "        page_source = self.driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # search papers\n",
    "        # click \"show more\" button until it is disabled\n",
    "        '''\n",
    "        while True :\n",
    "            load_more_button = self.driver.find_element(by=By.XPATH, value='//*[@id=\"gsc_bpf_more\"]')\n",
    "            self.driver.implicitly_wait(10)\n",
    "            load_more_button.click()\n",
    "            time.sleep(2)\n",
    "            if load_more_button.get_property(\"disabled\") :\n",
    "                break\n",
    "        '''\n",
    "        # get papaer html list\n",
    "        page_source = self.driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        paper_html_list = soup.find_all(\"tr\", class_=\"gsc_a_tr\")\n",
    "        paper_html_list = paper_html_list[:search_width_limit]\n",
    "\n",
    "        paper_href_list = self.driver.find_elements(by=By.XPATH, value='//*[@class=\"gsc_a_t\"]/a')\n",
    "\n",
    "\n",
    "        paper_list = []\n",
    "\n",
    "        print(f\"filling google schorlar metadata of papers from {author.name}...\")\n",
    "        with tqdm(total=len(paper_html_list)) as pbar:\n",
    "            #for paper_html in paper_html_list :\n",
    "            for paper_html, paper_href in zip(paper_html_list, paper_href_list) :\n",
    "                google_schorlar_url = self.BASE_URL + paper_html.find(\"a\", class_=\"gsc_a_at\")[\"href\"]\n",
    "                title = paper_html.find(\"a\", class_=\"gsc_a_at\").text\n",
    "                \n",
    "                #self.driver.get(google_schorlar_url)\n",
    "                #self.driver.implicitly_wait(10)\n",
    "                paper_href.click()\n",
    "\n",
    "                self.checkCaptcha()\n",
    "\n",
    "                time.sleep(0.2)\n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                metadata_list = soup.find_all(\"div\", class_=\"gs_scl\")\n",
    "                html_title = soup.find(\"a\", class_=\"gsc_oci_title_link\")\n",
    "\n",
    "                google_schorlar_metadata = {}\n",
    "                for metadata in metadata_list :\n",
    "                    field = metadata.find(\"div\", class_=\"gsc_oci_field\").text\n",
    "                    value = metadata.find(\"div\", class_=\"gsc_oci_value\").text\n",
    "                    google_schorlar_metadata[field] = value\n",
    "                \n",
    "\n",
    "                paper = Paper(title = title, google_schorlar_metadata = google_schorlar_metadata)\n",
    "                paper_list.append(paper)\n",
    "\n",
    "                pbar.set_postfix_str(title)\n",
    "                pbar.update(1)\n",
    "\n",
    "                self.driver.back()\n",
    "                time.sleep(0.2)\n",
    "                self.checkCaptcha()\n",
    "\n",
    "        author.paper_title_list = list(map(lambda paper : paper.title, paper_list))\n",
    "\n",
    "\n",
    "        paper_dict = {}\n",
    "        for paper in paper_list :\n",
    "            paper_dict[paper.title] = paper\n",
    "        return paper_dict\n",
    "\n",
    "        # query_crossref\n",
    "        print(f\"fetching crosserf metadata of papers from {author.name}...\")\n",
    "        for paper in tqdm(paper_list) :\n",
    "            self.crossref_fetcher.fetchMetaDatafromTitle(paper)\n",
    "            paper_dict[paper.DOI] = paper\n",
    "\n",
    "        return paper_dict\n",
    "    \n",
    "    def checkCaptcha(self) :\n",
    "        captcha_found = False\n",
    "        source = self.driver.page_source\n",
    "        if source.find(\"사용자가 로봇이 아니라는 확인이 필요합니다.\") != -1 :\n",
    "            print(\"로봇이 아니라는 확인이 필요합니다 text detected!\")\n",
    "            captcha_found = True\n",
    "        if source.lower().find(\"recaptcha\") != -1 :\n",
    "            print(\"recaptcha detected!\")\n",
    "            captcha_found = True\n",
    "        try:\n",
    "            captcha_image = self.driver.find_element_by_xpath(\"//img[contains(@alt, 'captcha')]\")\n",
    "            if captcha_image:\n",
    "                print(\"captcha image detected!\")\n",
    "                captcha_found = True\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        try:\n",
    "            captcha_text = self.driver.find_element_by_xpath(\"//*[contains(text(), 'prove you are human')]\")\n",
    "            if captcha_text:\n",
    "                print(\"CAPTCHA text detected!\")\n",
    "                captcha_found = True\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        try:\n",
    "            robot_detection = self.driver.find_element_by_xpath(\"//*[contains(text(), 'Google의 시스템이 컴퓨터 네트워크에서 비정상적인 트래픽을 감지했습니다.')]\")\n",
    "            if robot_detection:\n",
    "                print(\"로봇이 아니라는 확인이 필요합니다 detected!\")\n",
    "                captcha_found = True\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        if captcha_found :\n",
    "            key_input = input(\"Press [n] to stop...\")\n",
    "            if key_input in [\"n\", \"N\", \"no\", \"No\", \"NO\", \"nO\"] :\n",
    "                raise Exception(\"captcha detected!\")\n",
    "        # reload page\n",
    "        self.driver.refresh()\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "970\n",
      "568\n",
      "authors found : ['Simon Baker']\n",
      "filling google schorlar metadata of papers from Simon Baker...\n",
      "recaptcha detected!\n",
      "filling google schorlar metadata of papers from Simon Baker...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:35<00:00,  1.80s/it, A layered approach to stereo reconstruction]                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recaptcha detected!\n",
      "authors found : ['Wang, Michael Y', 'Michael Yu Wang', 'Michael Zhuo Wang']\n",
      "filling google schorlar metadata of papers from Wang, Michael Y...\n",
      "filling google schorlar metadata of papers from Wang, Michael Y...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:36<00:00,  1.82s/it, Clinical and radiographic comparison of mini–open transforaminal lumbar interbody fusion with open transforaminal lumbar interbody fusion in 42 patients with long-term follow-up]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors found : ['Stephen L. Johnson', 'Stephen R. Johnson']\n",
      "filling google schorlar metadata of papers from Stephen L. Johnson...\n",
      "filling google schorlar metadata of papers from Stephen L. Johnson...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:35<00:00,  1.77s/it, Genetic variation in the zebrafish]                                                                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors found : ['José A Caballero', 'Jose Ricardo Diaz Caballero', 'Jose Alonso Caballero Márquez']\n",
      "filling google schorlar metadata of papers from José A Caballero...\n",
      "filling google schorlar metadata of papers from José A Caballero...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:35<00:00,  1.78s/it, Mathematical considerations for nonisothermal kinetics in thermal decomposition]                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors found : ['Hengshuang Zhao']\n",
      "filling google schorlar metadata of papers from Hengshuang Zhao...\n",
      "filling google schorlar metadata of papers from Hengshuang Zhao...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:35<00:00,  1.79s/it, LAVT: Language-Aware Vision Transformer for Referring Image Segmentation]                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors found : ['BVK Vijaya Kumar']\n",
      "filling google schorlar metadata of papers from BVK Vijaya Kumar...\n",
      "filling google schorlar metadata of papers from BVK Vijaya Kumar...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:35<00:00,  1.80s/it, Joint disentangling and adaptation for cross-domain person re-identification]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors found : []\n",
      "authors found : ['Subhransu Maji']\n",
      "filling google schorlar metadata of papers from Subhransu Maji...\n",
      "filling google schorlar metadata of papers from Subhransu Maji...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [00:24<00:10,  1.76s/it, 3D shape segmentation with projective convolutional networks]                "
     ]
    }
   ],
   "source": [
    "# read from file if file is available.\n",
    "\n",
    "institution_dict = {}\n",
    "expertise_dict = {}\n",
    "whole_author_list = []\n",
    "whole_paper_dict = {}\n",
    "\n",
    "INSTITUTION_FILE_PATH = \"./institution_dict.json\"\n",
    "if os.path.exists(INSTITUTION_FILE_PATH) :\n",
    "    with open(INSTITUTION_FILE_PATH, \"r\") as f :\n",
    "        institution_dict_raw = json.load(f)\n",
    "    for k, v in institution_dict_raw.items() :\n",
    "        institution_dict[k] = Institution(**v)\n",
    "\n",
    "EXPERTISE_FILE_PATH = \"./expertise_dict.json\"\n",
    "if os.path.exists(EXPERTISE_FILE_PATH) :\n",
    "    with open(EXPERTISE_FILE_PATH, \"r\") as f :\n",
    "        expertise_dict_raw = json.load(f)\n",
    "    for k, v in expertise_dict_raw.items() :\n",
    "        expertise_dict[k] = Expertise(**v)\n",
    "\n",
    "AUTHOR_FILE_PATH = \"./author_list.json\"\n",
    "if os.path.exists(AUTHOR_FILE_PATH) :\n",
    "    with open(AUTHOR_FILE_PATH, \"r\") as f :\n",
    "        author_list_raw = json.load(f)\n",
    "    for author in author_list_raw :\n",
    "        whole_author_list.append(Author(**author))\n",
    "\n",
    "WHOLE_PAPER_FILE_PATH = \"./whole_paper_dict.json\"\n",
    "if os.path.exists(WHOLE_PAPER_FILE_PATH) :\n",
    "    with open(WHOLE_PAPER_FILE_PATH, \"r\") as f :\n",
    "        whole_paper_dict = json.load(f)\n",
    "    for k, v in whole_paper_dict.items() :\n",
    "        whole_paper_dict[k] = Paper(**v)\n",
    "\n",
    "with open(\"./author_name_list.json\", \"r\") as f :\n",
    "    author_name_list = json.load(f)\n",
    "\n",
    "author_name_to_append_list = author_name_list\n",
    "\n",
    "pre_existing_author_name_list = list(map(lambda author : author.name, whole_author_list))\n",
    "author_name_to_append_list = list(filter(lambda name : name not in pre_existing_author_name_list, author_name_to_append_list))\n",
    "\n",
    "print(len(author_name_list))\n",
    "print(len(author_name_to_append_list))\n",
    "\n",
    "empty_author_name_list = []\n",
    "\n",
    "gsearch = GoogleScharlarSearcher(institution_dict, expertise_dict, os_name=os_name)\n",
    "for author_name in author_name_to_append_list :\n",
    "    author_list, paper_dict = gsearch.searchAuthorByName(\n",
    "        author_name,\n",
    "        #ask_for_continue=True\n",
    "    )\n",
    "    if len(author_list) == 0 :\n",
    "        empty_author_name_list.append(author_name)\n",
    "        continue\n",
    "    whole_paper_dict.update(paper_dict)\n",
    "    whole_author_list += author_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for empty_author_name in empty_author_name_list :\n",
    "    author_name_list.remove(empty_author_name)\n",
    "    with open(\"./author_name_list.json\", \"w\") as f :\n",
    "        json.dump(author_name_list, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "whole_paper_dict_dict = {}\n",
    "for key, paper in whole_paper_dict.items() :\n",
    "    whole_paper_dict_dict[key] = paper.toDict()\n",
    "with open('whole_paper_dict.json', 'w') as f:\n",
    "    json.dump(whole_paper_dict_dict, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "whole_author_dict_list = list(map(lambda author : author.toDict(), whole_author_list))\n",
    "with open(\"author_list.json\", 'w') as f :\n",
    "    json.dump(whole_author_dict_list, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "institution_dict = {}\n",
    "for key, institution in gsearch.institution_dict.items() :\n",
    "    institution_dict[key] = institution.toDict()\n",
    "    #institution_dict[key] = json.loads(institution.toJOSN())\n",
    "with open(\"institution_dict.json\", 'w') as f :\n",
    "    json.dump(institution_dict, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "expertise_dict = {}\n",
    "for key, expertise in gsearch.expertise_dict.items() :\n",
    "    expertise_dict[key] = expertise.toDict()\n",
    "with open(\"expertise_dict.json\", 'w') as f :\n",
    "    json.dump(expertise_dict, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for empty_author_name in empty_author_name_list :\n",
    "    author_name_list.remove(empty_author_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_name_list.remove(\"Verena\")\n",
    "ㅠ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./author_name_list.json\", \"w\") as f :\n",
    "    json.dump(author_name_list, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vincent Vanhoucke',\n",
       " 'Vladlen Koltun',\n",
       " 'Verena',\n",
       " 'Victor Lempitsky',\n",
       " 'Vincent Lepetit',\n",
       " 'Valentina Salvatelli',\n",
       " 'Vinod Nair',\n",
       " 'Vishal M. Patel',\n",
       " 'Vittorio Ferrari',\n",
       " 'Vijay Badrinarayanan',\n",
       " 'Visvanathan Ramesh',\n",
       " 'Vittorio Murino',\n",
       " 'Volker Blanz',\n",
       " 'Vaclav Hlavac (Václav Hlaváč)',\n",
       " 'Vincent Rabaud',\n",
       " 'Vicente Ordóñez',\n",
       " 'Vijayan Asari',\n",
       " 'Vibhav Vineet']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda name : name[0] in [\"v\", \"V\"],  author_name_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "whole_paper_dict_dict = {}\n",
    "for key, paper in whole_paper_dict.items() :\n",
    "    whole_paper_dict_dict[key] = paper.toDict()\n",
    "with open('whole_paper_dict.json', 'w') as f:\n",
    "    json.dump(whole_paper_dict_dict, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "whole_author_dict_list = list(map(lambda author : author.toDict(), whole_author_list))\n",
    "with open(\"author_list.json\", 'w') as f :\n",
    "    json.dump(whole_author_dict_list, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "institution_dict = {}\n",
    "for key, institution in gsearch.institution_dict.items() :\n",
    "    institution_dict[key] = institution.toDict()\n",
    "    #institution_dict[key] = json.loads(institution.toJOSN())\n",
    "with open(\"institution_dict.json\", 'w') as f :\n",
    "    json.dump(institution_dict, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "expertise_dict = {}\n",
    "for key, expertise in gsearch.expertise_dict.items() :\n",
    "    expertise_dict[key] = expertise.toDict()\n",
    "with open(\"expertise_dict.json\", 'w') as f :\n",
    "    json.dump(expertise_dict, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Jinyoung Han\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(GoogleScharlarSearcher.BASE_URL)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# search by author name\n",
    "searcher = driver.find_element(by=By.XPATH, value='//*[@id=\"gs_hdr_tsi\"]')\n",
    "time.sleep(0.1)\n",
    "\n",
    "#searcher.send_keys(name)\n",
    "for chr in name :\n",
    "    searcher.send_keys(chr)\n",
    "    time.sleep(random.randint(1, 10)/200)\n",
    "searcher.send_keys(Keys.RETURN)\n",
    "driver.implicitly_wait(10)\n",
    "time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46827"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.find(\"reCAPTCHA\")\n",
    "\n",
    "source.lower().find(\"recaptcha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Institution :\n",
    "    name :str\n",
    "    google_scholar_url : str\n",
    "    homepage_url : str = None\n",
    "\n",
    "    def toJSON(self) :\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, \n",
    "            sort_keys=True, indent=4)\n",
    "    def toDict(self):\n",
    "        return json.loads(self.toJSON())\n",
    "\n",
    "    def fromDict(self, dic):\n",
    "        self.name = dic['name']\n",
    "        self.google_scholar_url = dic['google_scholar_url']\n",
    "        self.homepage_url = dic['homepage_url']\n",
    "\n",
    "@dataclass\n",
    "class Expertise :\n",
    "    name : str\n",
    "    url : str\n",
    "\n",
    "    def toJSON(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, \n",
    "            sort_keys=True, indent=4)\n",
    "    def toDict(self):\n",
    "        return json.loads(self.toJSON())\n",
    "    def fromDict(self, dic):\n",
    "        self.name = dic['name']\n",
    "        self.url = dic['url']\n",
    "\n",
    "@dataclass\n",
    "class Author :\n",
    "    name : str\n",
    "    google_schorlar_profile_url : str\n",
    "    affiliation : str = None\n",
    "    expertise_list : list[str] = None\n",
    "    homepage_url : str = None\n",
    "    paper_list : list = None\n",
    "    paper_title_list : list = None\n",
    "\n",
    "    def toJSON(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, \n",
    "            sort_keys=True, indent=4)\n",
    "    def toDict(self):\n",
    "        return json.loads(self.toJSON())\n",
    "    def fromDict(self, dic):\n",
    "        self.name = dic['name']\n",
    "        self.google_schorlar_profile_url = dic['google_schorlar_profile_url']\n",
    "        self.affiliation = dic['affiliation']\n",
    "        self.expertise_list = dic['expertise_list']\n",
    "        self.homepage_url = dic['homepage_url']\n",
    "        self.paper_list = dic['paper_list']\n",
    "        self.paper_title_list = dic['paper_title_list']\n",
    "        \n",
    "@dataclass\n",
    "class Paper :\n",
    "    # After search paper title using Google Schorlar,\n",
    "    # fill in basic metadata (abstract) from Google Schorlar\n",
    "    # fill in other metadata from Crossref\n",
    "    DOI : str = None\n",
    "    crossref_json : dict = None\n",
    "    google_schorlar_metadata : dict = None\n",
    "    title : str = None\n",
    "    authors : list = None\n",
    "    abstract : str = None\n",
    "    conference : str = None\n",
    "    journal : str = None\n",
    "    year : int = None\n",
    "    reference_list : list[str] = None\n",
    "    referenced_list : list[str] = None\n",
    "    cite_bibtex : str = None\n",
    "\n",
    "    def toJSON(self):\n",
    "        '''convert to JSON recursively'''\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, \n",
    "            sort_keys=True, indent=4)\n",
    "    def toDict(self):\n",
    "        '''convert to dict recursively'''\n",
    "        return json.loads(self.toJSON())\n",
    "    def fromDict(self, dic) :\n",
    "        '''convert from dict recursively'''\n",
    "        self.DOI = dic['DOI']\n",
    "        self.crossref_json = dic['crossref_json']\n",
    "        self.google_schorlar_metadata = dic['google_schorlar_metadata']\n",
    "        self.title = dic['title']\n",
    "        self.authors = dic['authors']\n",
    "        self.abstract = dic['abstract']\n",
    "        self.conference = dic['conference']\n",
    "        self.journal = dic['journal']\n",
    "        self.year = dic['year']\n",
    "        self.reference_list = dic['reference_list']\n",
    "        self.referenced_list = dic['referenced_list']\n",
    "        self.cite_bibtex = dic['cite_bibtex']\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
