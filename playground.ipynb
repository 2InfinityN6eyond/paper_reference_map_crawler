{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mac\n"
     ]
    }
   ],
   "source": [
    "#pip install selenium\n",
    "import os\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import platform\n",
    "os_name = platform.system()\n",
    "\n",
    "if os_name == \"Darwin\":\n",
    "    import undetected_chromedriver as webdriver\n",
    "    #from selenium import webdriver\n",
    "\n",
    "elif os_name == \"Linux\":\n",
    "    from selenium import webdriver\n",
    "\n",
    "#from selenium import webdriver\n",
    "#import undetected_chromedriver.v2 as webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GoogleScharlarSearcher :\n",
    "    BASE_URL = \"https://scholar.google.com\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            institution_dict = None,\n",
    "            expertise_dict = None,\n",
    "            os_name = None\n",
    "        ) :\n",
    "\n",
    "        if False : # os_name == \"Darwin\":\n",
    "            self.driver = webdriver.Safari()\n",
    "        else :\n",
    "            chrome_options = webdriver.ChromeOptions()\n",
    "            #chrome_options.add_argument(\"--headless\")\n",
    "            chrome_options.add_argument(\"--use_subprocess\")\n",
    "\n",
    "            self.driver = webdriver.Chrome(options=chrome_options)\n",
    "        self.institution_dict = institution_dict\n",
    "        self.expertise_dict = expertise_dict\n",
    "\n",
    "    def searchPaperByName(self, name) :\n",
    "        self.driver.get(self.BASE_URL)\n",
    "        self.driver.implicitly_wait(10)\n",
    "        # search given paper name\n",
    "        search = self.driver.find_element(by=By.XPATH, value='//*[@id=\"gs_hdr_tsi\"]')\n",
    "        search.send_keys(name)\n",
    "        search.send_keys(Keys.RETURN)\n",
    "        self.driver.implicitly_wait(10)\n",
    "        # click the first paper\n",
    "\n",
    "    def searchAuthorByName(\n",
    "        self,\n",
    "        name,\n",
    "        continue_search = False,\n",
    "        search_width = 1000,\n",
    "        ask_for_continue = False,\n",
    "    ) :\n",
    "        \"\"\"\n",
    "        If continue_search is True, search every co-author until search_width\n",
    "        \"\"\"\n",
    "        self.driver.get(self.BASE_URL)\n",
    "        self.driver.implicitly_wait(10)\n",
    "\n",
    "        self.checkCaptcha()\n",
    "\n",
    "        # search by author name\n",
    "        searcher = self.driver.find_element(by=By.XPATH, value='//*[@id=\"gs_hdr_tsi\"]')\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        #searcher.send_keys(name)\n",
    "        for chr in name :\n",
    "            searcher.send_keys(chr)\n",
    "            time.sleep(random.randint(1, 10)/200)\n",
    "        searcher.send_keys(Keys.RETURN)\n",
    "        self.driver.implicitly_wait(10)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        self.checkCaptcha()\n",
    "\n",
    "        if ask_for_continue :\n",
    "            key_input = input(\"Press [n] to stop...\")\n",
    "            if key_input in [\"n\", \"N\", \"no\", \"No\", \"NO\", \"nO\"] :\n",
    "                return [], {}\n",
    "\n",
    "        page_source = self.driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        author_list = soup.find_all(\"h4\", class_=\"gs_rt2\")\n",
    "\n",
    "        author_list = list(map(\n",
    "            lambda author : Author(\n",
    "                name = author.text,\n",
    "                google_schorlar_profile_url = self.BASE_URL +author.find(\"a\")[\"href\"],\n",
    "            ),\n",
    "            author_list\n",
    "        ))\n",
    "\n",
    "        print(f\"authors found : {list(map(lambda author : author.name, author_list))}\")\n",
    "        \n",
    "        auther_href_button_list = self.driver.find_elements(by=By.XPATH, value='//*[@class=\"gs_rt2\"]/a')        \n",
    "        whole_paper_dict = {}\n",
    "        for author, auther_href_button in zip(author_list, auther_href_button_list) :\n",
    "            print(f\"filling google schorlar metadata of papers from {author.name}...\")\n",
    "            auther_href_button.click()\n",
    "            time.sleep(0.2)\n",
    "\n",
    "            self.checkCaptcha()\n",
    "\n",
    "            author, paper_dict = self.fillAuthor(author, url_already_loaded = True)\n",
    "            whole_paper_dict.update(paper_dict)\n",
    "            self.driver.back()\n",
    "            time.sleep(0.2)\n",
    "            self.checkCaptcha()\n",
    "\n",
    "            break\n",
    "\n",
    "        #for author in author_list :\n",
    "            #author, paper_dict = self.fillAuthor(author)\n",
    "            #whole_paper_dict.update(paper_dict)\n",
    "\n",
    "\n",
    "        if continue_search and len(author_list) < search_width :\n",
    "            pass\n",
    "\n",
    "\n",
    "        return author_list, whole_paper_dict\n",
    "\n",
    "    def addInstitution(\n",
    "        self,\n",
    "        html_str\n",
    "    ) :\n",
    "        '''\n",
    "        initialize Institution instance and append to\n",
    "        self.instaitution_dict if not exist\n",
    "        args :\n",
    "            institution_html :\n",
    "                expected to have name,\n",
    "                google_schorlar_institution_url field\n",
    "        return :\n",
    "            institution name\n",
    "        '''\n",
    "        #institution_name = html_str.find(\"a\").text\n",
    "        institution_name = html_str.text\n",
    "\n",
    "        if institution_name not in self.institution_dict :\n",
    "            try :\n",
    "                google_schorlar_institution_url = self.BASE_URL + html_str.find(\"a\")[\"href\"]\n",
    "            except Exception as e :\n",
    "                google_schorlar_institution_url = None\n",
    "            homepage_url = None\n",
    "            self.institution_dict[institution_name] = Institution(\n",
    "                name = institution_name,\n",
    "                google_scholar_url = google_schorlar_institution_url,\n",
    "                homepage_url = homepage_url,\n",
    "            )\n",
    "        return institution_name\n",
    "\n",
    "    def addExpertise(\n",
    "        self,\n",
    "        html_str_list\n",
    "    ) :\n",
    "        '''\n",
    "        initialize Expertise instance and append to\n",
    "        self.expertise_dict if not exist\n",
    "        args :\n",
    "            html_str_list :\n",
    "                list of html_str. each elements are html str\n",
    "                expected to have name,\n",
    "                google_schorlar_expertise_url field\n",
    "        return :\n",
    "            expertise name\n",
    "        '''\n",
    "        expertise_name_list = []\n",
    "        for html_str in html_str_list :\n",
    "            expertise_name = html_str.text\n",
    "            if expertise_name not in self.expertise_dict :\n",
    "                google_schorlar_expertise_url = self.BASE_URL + html_str[\"href\"]\n",
    "                self.expertise_dict[expertise_name] = Expertise(\n",
    "                    name = expertise_name,\n",
    "                    url = google_schorlar_expertise_url,\n",
    "                )\n",
    "            expertise_name_list.append(expertise_name)\n",
    "        return expertise_name_list\n",
    "\n",
    "    def fillAuthor(self, author, url_already_loaded = False) :\n",
    "        \"\"\"\n",
    "        fill in author instance\n",
    "        args :\n",
    "            author :\n",
    "                expected to have name, google_schorlar_profile_url field\n",
    "        \"\"\"\n",
    "        # load page html        \n",
    "        if not url_already_loaded :\n",
    "            self.driver.get(author.google_schorlar_profile_url)\n",
    "            self.driver.implicitly_wait(10)\n",
    "        page_source = self.driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # fill in expertise\n",
    "        expertise_html_list = soup.find_all(\"a\", class_=\"gsc_prf_inta\")\n",
    "        expertise_name_list = self.addExpertise(expertise_html_list)\n",
    "        author.expertise_list = expertise_name_list\n",
    "\n",
    "        # fill in institution\n",
    "        institution_html = soup.find(\"div\", class_=\"gsc_prf_il\")\n",
    "        try :\n",
    "            institution_name = self.addInstitution(institution_html)\n",
    "            author.affiliation = institution_name\n",
    "        except Exception as e :\n",
    "            print(e)\n",
    "            print(soup)\n",
    "            raise e\n",
    "\n",
    "        paper_dict = self.makePaperDictFromAuthor(author, url_already_loaded = True)\n",
    "        #DOI_list = list(paper_dict.keys())\n",
    "        #author.paper_list = DOI_list\n",
    "\n",
    "        return author, paper_dict\n",
    "    \n",
    "\n",
    "    def makePaperDictFromAuthor(self, author, url_already_loaded = False, search_width_limit = 20) :\n",
    "        \"\"\"\n",
    "        make paper instance from author instance\n",
    "        args :\n",
    "            author : Author\n",
    "                expected to have name, google_schorlar_profile_url field\n",
    "        return :\n",
    "            paper_list : list[Paper]\n",
    "        \"\"\"\n",
    "\n",
    "        # load page html        \n",
    "        if not url_already_loaded :\n",
    "            self.driver.get(author.google_schorlar_profile_url)\n",
    "            self.driver.implicitly_wait(10)\n",
    "        page_source = self.driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # search papers\n",
    "        # click \"show more\" button until it is disabled\n",
    "        '''\n",
    "        while True :\n",
    "            load_more_button = self.driver.find_element(by=By.XPATH, value='//*[@id=\"gsc_bpf_more\"]')\n",
    "            self.driver.implicitly_wait(10)\n",
    "            load_more_button.click()\n",
    "            time.sleep(2)\n",
    "            if load_more_button.get_property(\"disabled\") :\n",
    "                break\n",
    "        '''\n",
    "        # get papaer html list\n",
    "        page_source = self.driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        paper_html_list = soup.find_all(\"tr\", class_=\"gsc_a_tr\")\n",
    "        paper_html_list = paper_html_list[:search_width_limit]\n",
    "\n",
    "        paper_href_list = self.driver.find_elements(by=By.XPATH, value='//*[@class=\"gsc_a_t\"]/a')\n",
    "\n",
    "\n",
    "        paper_list = []\n",
    "\n",
    "        print(f\"filling google schorlar metadata of papers from {author.name}...\")\n",
    "        with tqdm(total=len(paper_html_list)) as pbar:\n",
    "            #for paper_html in paper_html_list :\n",
    "            for paper_html, paper_href in zip(paper_html_list, paper_href_list) :\n",
    "                google_schorlar_url = self.BASE_URL + paper_html.find(\"a\", class_=\"gsc_a_at\")[\"href\"]\n",
    "                title = paper_html.find(\"a\", class_=\"gsc_a_at\").text\n",
    "                \n",
    "                #self.driver.get(google_schorlar_url)\n",
    "                #self.driver.implicitly_wait(10)\n",
    "                paper_href.click()\n",
    "\n",
    "                self.checkCaptcha()\n",
    "\n",
    "                time.sleep(0.2)\n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                metadata_list = soup.find_all(\"div\", class_=\"gs_scl\")\n",
    "                html_title = soup.find(\"a\", class_=\"gsc_oci_title_link\")\n",
    "\n",
    "                google_schorlar_metadata = {}\n",
    "                for metadata in metadata_list :\n",
    "                    field = metadata.find(\"div\", class_=\"gsc_oci_field\").text\n",
    "                    value = metadata.find(\"div\", class_=\"gsc_oci_value\").text\n",
    "                    google_schorlar_metadata[field] = value\n",
    "                \n",
    "\n",
    "                paper = Paper(title = title, google_schorlar_metadata = google_schorlar_metadata)\n",
    "                paper_list.append(paper)\n",
    "\n",
    "                pbar.set_postfix_str(title)\n",
    "                pbar.update(1)\n",
    "\n",
    "                self.driver.back()\n",
    "                time.sleep(0.2)\n",
    "                self.checkCaptcha()\n",
    "\n",
    "        author.paper_title_list = list(map(lambda paper : paper.title, paper_list))\n",
    "\n",
    "\n",
    "        paper_dict = {}\n",
    "        for paper in paper_list :\n",
    "            paper_dict[paper.title] = paper\n",
    "        return paper_dict\n",
    "\n",
    "        # query_crossref\n",
    "        print(f\"fetching crosserf metadata of papers from {author.name}...\")\n",
    "        for paper in tqdm(paper_list) :\n",
    "            self.crossref_fetcher.fetchMetaDatafromTitle(paper)\n",
    "            paper_dict[paper.DOI] = paper\n",
    "\n",
    "        return paper_dict\n",
    "    \n",
    "    def checkCaptcha(self) :\n",
    "        captcha_found = False\n",
    "        source = self.driver.page_source\n",
    "        if source.find(\"사용자가 로봇이 아니라는 확인이 필요합니다.\") != -1 :\n",
    "            print(\"로봇이 아니라는 확인이 필요합니다 text detected!\")\n",
    "            captcha_found = True\n",
    "        if source.lower().find(\"recaptcha\") != -1 :\n",
    "            print(\"recaptcha detected!\")\n",
    "            captcha_found = True\n",
    "        try:\n",
    "            captcha_image = self.driver.find_element_by_xpath(\"//img[contains(@alt, 'captcha')]\")\n",
    "            if captcha_image:\n",
    "                print(\"captcha image detected!\")\n",
    "                captcha_found = True\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        try:\n",
    "            captcha_text = self.driver.find_element_by_xpath(\"//*[contains(text(), 'prove you are human')]\")\n",
    "            if captcha_text:\n",
    "                print(\"CAPTCHA text detected!\")\n",
    "                captcha_found = True\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        try:\n",
    "            robot_detection = self.driver.find_element_by_xpath(\"//*[contains(text(), 'Google의 시스템이 컴퓨터 네트워크에서 비정상적인 트래픽을 감지했습니다.')]\")\n",
    "            if robot_detection:\n",
    "                print(\"로봇이 아니라는 확인이 필요합니다 detected!\")\n",
    "                captcha_found = True\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        if captcha_found :\n",
    "            key_input = input(\"Press [n] to stop...\")\n",
    "            if key_input in [\"n\", \"N\", \"no\", \"No\", \"NO\", \"nO\"] :\n",
    "                raise Exception(\"captcha detected!\")\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "979\n",
      "611\n",
      "로봇이 아니라는 확인이 필요합니다 text detected!\n",
      "recaptcha detected!\n",
      "authors found : ['Simon Baker']\n",
      "filling google schorlar metadata of papers from Simon Baker...\n",
      "filling google schorlar metadata of papers from Simon Baker...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:40<00:00,  2.00s/it, A layered approach to stereo reconstruction]                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors found : ['Wang, Michael Y', 'Michael Yu Wang', 'Michael Zhuo Wang']\n",
      "filling google schorlar metadata of papers from Wang, Michael Y...\n",
      "filling google schorlar metadata of papers from Wang, Michael Y...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:39<00:00,  1.96s/it, Observation of the Associated Production of a Single Top Quark and a  Boson in  Collisions at ]                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors found : ['Stephen L. Johnson', 'Stephen R. Johnson']\n",
      "filling google schorlar metadata of papers from Stephen L. Johnson...\n",
      "filling google schorlar metadata of papers from Stephen L. Johnson...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:37<00:00,  1.90s/it, Genetic variation in the zebrafish]                                                                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors found : ['Steve Maybank']\n",
      "filling google schorlar metadata of papers from Steve Maybank...\n",
      "filling google schorlar metadata of papers from Steve Maybank...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:38<00:00,  1.93s/it, Gait components and their application to gender recognition]                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors found : ['Peter Corcoran']\n",
      "filling google schorlar metadata of papers from Peter Corcoran...\n",
      "filling google schorlar metadata of papers from Peter Corcoran...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:39<00:00,  1.95s/it, Digital camera with built-in lens calibration table]                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors found : ['Guoying Zhao']\n",
      "filling google schorlar metadata of papers from Guoying Zhao...\n",
      "filling google schorlar metadata of papers from Guoying Zhao...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:39<00:00,  1.95s/it, Aff-wild: Valence and arousal in-the-wild challenge]                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors found : ['Deyu Meng']\n",
      "filling google schorlar metadata of papers from Deyu Meng...\n",
      "filling google schorlar metadata of papers from Deyu Meng...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:39<00:00,  1.95s/it, Hyperspectral image classification with Markov random fields and a convolutional neural network]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recaptcha detected!\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: disconnected: not connected to DevTools\n  (failed to check if window was closed: disconnected: not connected to DevTools)\n  (Session info: chrome=119.0.6045.159)\nStacktrace:\n0   undetected_chromedriver             0x000000010470cd28 undetected_chromedriver + 4795688\n1   undetected_chromedriver             0x00000001047042b3 undetected_chromedriver + 4760243\n2   undetected_chromedriver             0x00000001042dd88d undetected_chromedriver + 407693\n3   undetected_chromedriver             0x00000001042c267e undetected_chromedriver + 296574\n4   undetected_chromedriver             0x00000001042c2573 undetected_chromedriver + 296307\n5   undetected_chromedriver             0x00000001042e0052 undetected_chromedriver + 417874\n6   undetected_chromedriver             0x000000010436f117 undetected_chromedriver + 1003799\n7   undetected_chromedriver             0x0000000104353a73 undetected_chromedriver + 891507\n8   undetected_chromedriver             0x000000010431e143 undetected_chromedriver + 672067\n9   undetected_chromedriver             0x000000010431f31e undetected_chromedriver + 676638\n10  undetected_chromedriver             0x00000001046cd795 undetected_chromedriver + 4536213\n11  undetected_chromedriver             0x00000001046d2853 undetected_chromedriver + 4556883\n12  undetected_chromedriver             0x00000001046b3001 undetected_chromedriver + 4427777\n13  undetected_chromedriver             0x00000001046d359d undetected_chromedriver + 4560285\n14  undetected_chromedriver             0x00000001046a448c undetected_chromedriver + 4367500\n15  undetected_chromedriver             0x00000001046f30e8 undetected_chromedriver + 4690152\n16  undetected_chromedriver             0x00000001046f329e undetected_chromedriver + 4690590\n17  undetected_chromedriver             0x0000000104703eee undetected_chromedriver + 4759278\n18  libsystem_pthread.dylib             0x00007ff8158cd202 _pthread_start + 99\n19  libsystem_pthread.dylib             0x00007ff8158c8bab thread_start + 15\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m gsearch \u001b[38;5;241m=\u001b[39m GoogleScharlarSearcher(institution_dict, expertise_dict, os_name\u001b[38;5;241m=\u001b[39mos_name)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m author_name \u001b[38;5;129;01min\u001b[39;00m author_name_to_append_list :\n\u001b[0;32m---> 51\u001b[0m     author_list, paper_dict \u001b[38;5;241m=\u001b[39m \u001b[43mgsearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearchAuthorByName\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauthor_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#ask_for_continue=True\u001b[39;49;00m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(author_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m :\n\u001b[1;32m     56\u001b[0m         empty_author_name_list\u001b[38;5;241m.\u001b[39mappend(author_name)\n",
      "Cell \u001b[0;32mIn[22], line 48\u001b[0m, in \u001b[0;36mGoogleScharlarSearcher.searchAuthorByName\u001b[0;34m(self, name, continue_search, search_width, ask_for_continue)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckCaptcha()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# search by author name\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m searcher \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m//*[@id=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgs_hdr_tsi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#searcher.send_keys(name)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/first/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:741\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    738\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[1;32m    739\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 741\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/first/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:347\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 347\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/envs/first/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: disconnected: not connected to DevTools\n  (failed to check if window was closed: disconnected: not connected to DevTools)\n  (Session info: chrome=119.0.6045.159)\nStacktrace:\n0   undetected_chromedriver             0x000000010470cd28 undetected_chromedriver + 4795688\n1   undetected_chromedriver             0x00000001047042b3 undetected_chromedriver + 4760243\n2   undetected_chromedriver             0x00000001042dd88d undetected_chromedriver + 407693\n3   undetected_chromedriver             0x00000001042c267e undetected_chromedriver + 296574\n4   undetected_chromedriver             0x00000001042c2573 undetected_chromedriver + 296307\n5   undetected_chromedriver             0x00000001042e0052 undetected_chromedriver + 417874\n6   undetected_chromedriver             0x000000010436f117 undetected_chromedriver + 1003799\n7   undetected_chromedriver             0x0000000104353a73 undetected_chromedriver + 891507\n8   undetected_chromedriver             0x000000010431e143 undetected_chromedriver + 672067\n9   undetected_chromedriver             0x000000010431f31e undetected_chromedriver + 676638\n10  undetected_chromedriver             0x00000001046cd795 undetected_chromedriver + 4536213\n11  undetected_chromedriver             0x00000001046d2853 undetected_chromedriver + 4556883\n12  undetected_chromedriver             0x00000001046b3001 undetected_chromedriver + 4427777\n13  undetected_chromedriver             0x00000001046d359d undetected_chromedriver + 4560285\n14  undetected_chromedriver             0x00000001046a448c undetected_chromedriver + 4367500\n15  undetected_chromedriver             0x00000001046f30e8 undetected_chromedriver + 4690152\n16  undetected_chromedriver             0x00000001046f329e undetected_chromedriver + 4690590\n17  undetected_chromedriver             0x0000000104703eee undetected_chromedriver + 4759278\n18  libsystem_pthread.dylib             0x00007ff8158cd202 _pthread_start + 99\n19  libsystem_pthread.dylib             0x00007ff8158c8bab thread_start + 15\n"
     ]
    }
   ],
   "source": [
    "# read from file if file is available.\n",
    "\n",
    "institution_dict = {}\n",
    "expertise_dict = {}\n",
    "whole_author_list = []\n",
    "whole_paper_dict = {}\n",
    "\n",
    "INSTITUTION_FILE_PATH = \"./institution_dict.json\"\n",
    "if os.path.exists(INSTITUTION_FILE_PATH) :\n",
    "    with open(INSTITUTION_FILE_PATH, \"r\") as f :\n",
    "        institution_dict_raw = json.load(f)\n",
    "    for k, v in institution_dict_raw.items() :\n",
    "        institution_dict[k] = Institution(**v)\n",
    "\n",
    "EXPERTISE_FILE_PATH = \"./expertise_dict.json\"\n",
    "if os.path.exists(EXPERTISE_FILE_PATH) :\n",
    "    with open(EXPERTISE_FILE_PATH, \"r\") as f :\n",
    "        expertise_dict_raw = json.load(f)\n",
    "    for k, v in expertise_dict_raw.items() :\n",
    "        expertise_dict[k] = Expertise(**v)\n",
    "\n",
    "AUTHOR_FILE_PATH = \"./author_list.json\"\n",
    "if os.path.exists(AUTHOR_FILE_PATH) :\n",
    "    with open(AUTHOR_FILE_PATH, \"r\") as f :\n",
    "        author_list_raw = json.load(f)\n",
    "    for author in author_list_raw :\n",
    "        whole_author_list.append(Author(**author))\n",
    "\n",
    "WHOLE_PAPER_FILE_PATH = \"./whole_paper_dict.json\"\n",
    "if os.path.exists(WHOLE_PAPER_FILE_PATH) :\n",
    "    with open(WHOLE_PAPER_FILE_PATH, \"r\") as f :\n",
    "        whole_paper_dict = json.load(f)\n",
    "    for k, v in whole_paper_dict.items() :\n",
    "        whole_paper_dict[k] = Paper(**v)\n",
    "\n",
    "with open(\"./author_name_list.json\", \"r\") as f :\n",
    "    author_name_list = json.load(f)\n",
    "\n",
    "author_name_to_append_list = author_name_list\n",
    "\n",
    "pre_existing_author_name_list = list(map(lambda author : author.name, whole_author_list))\n",
    "author_name_to_append_list = list(filter(lambda name : name not in pre_existing_author_name_list, author_name_to_append_list))\n",
    "\n",
    "print(len(author_name_list))\n",
    "print(len(author_name_to_append_list))\n",
    "\n",
    "empty_author_name_list = []\n",
    "\n",
    "gsearch = GoogleScharlarSearcher(institution_dict, expertise_dict, os_name=os_name)\n",
    "for author_name in author_name_to_append_list :\n",
    "    author_list, paper_dict = gsearch.searchAuthorByName(\n",
    "        author_name,\n",
    "        #ask_for_continue=True\n",
    "    )\n",
    "    if len(author_list) == 0 :\n",
    "        empty_author_name_list.append(author_name)\n",
    "        continue\n",
    "    whole_paper_dict.update(paper_dict)\n",
    "    whole_author_list += author_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for empty_author_name in empty_author_name_list :\n",
    "    author_name_list.remove(empty_author_name)\n",
    "    with open(\"./author_name_list.json\", \"w\") as f :\n",
    "        json.dump(author_name_list, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "whole_paper_dict_dict = {}\n",
    "for key, paper in whole_paper_dict.items() :\n",
    "    whole_paper_dict_dict[key] = paper.toDict()\n",
    "with open('whole_paper_dict.json', 'w') as f:\n",
    "    json.dump(whole_paper_dict_dict, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "whole_author_dict_list = list(map(lambda author : author.toDict(), whole_author_list))\n",
    "with open(\"author_list.json\", 'w') as f :\n",
    "    json.dump(whole_author_dict_list, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "institution_dict = {}\n",
    "for key, institution in gsearch.institution_dict.items() :\n",
    "    institution_dict[key] = institution.toDict()\n",
    "    #institution_dict[key] = json.loads(institution.toJOSN())\n",
    "with open(\"institution_dict.json\", 'w') as f :\n",
    "    json.dump(institution_dict, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "expertise_dict = {}\n",
    "for key, expertise in gsearch.expertise_dict.items() :\n",
    "    expertise_dict[key] = expertise.toDict()\n",
    "with open(\"expertise_dict.json\", 'w') as f :\n",
    "    json.dump(expertise_dict, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for empty_author_name in empty_author_name_list :\n",
    "    author_name_list.remove(empty_author_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_name_list.remove(\"Verena\")\n",
    "ㅠ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./author_name_list.json\", \"w\") as f :\n",
    "    json.dump(author_name_list, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vincent Vanhoucke',\n",
       " 'Vladlen Koltun',\n",
       " 'Verena',\n",
       " 'Victor Lempitsky',\n",
       " 'Vincent Lepetit',\n",
       " 'Valentina Salvatelli',\n",
       " 'Vinod Nair',\n",
       " 'Vishal M. Patel',\n",
       " 'Vittorio Ferrari',\n",
       " 'Vijay Badrinarayanan',\n",
       " 'Visvanathan Ramesh',\n",
       " 'Vittorio Murino',\n",
       " 'Volker Blanz',\n",
       " 'Vaclav Hlavac (Václav Hlaváč)',\n",
       " 'Vincent Rabaud',\n",
       " 'Vicente Ordóñez',\n",
       " 'Vijayan Asari',\n",
       " 'Vibhav Vineet']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda name : name[0] in [\"v\", \"V\"],  author_name_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "whole_paper_dict_dict = {}\n",
    "for key, paper in whole_paper_dict.items() :\n",
    "    whole_paper_dict_dict[key] = paper.toDict()\n",
    "with open('whole_paper_dict.json', 'w') as f:\n",
    "    json.dump(whole_paper_dict_dict, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "whole_author_dict_list = list(map(lambda author : author.toDict(), whole_author_list))\n",
    "with open(\"author_list.json\", 'w') as f :\n",
    "    json.dump(whole_author_dict_list, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "institution_dict = {}\n",
    "for key, institution in gsearch.institution_dict.items() :\n",
    "    institution_dict[key] = institution.toDict()\n",
    "    #institution_dict[key] = json.loads(institution.toJOSN())\n",
    "with open(\"institution_dict.json\", 'w') as f :\n",
    "    json.dump(institution_dict, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "expertise_dict = {}\n",
    "for key, expertise in gsearch.expertise_dict.items() :\n",
    "    expertise_dict[key] = expertise.toDict()\n",
    "with open(\"expertise_dict.json\", 'w') as f :\n",
    "    json.dump(expertise_dict, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Jinyoung Han\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(GoogleScharlarSearcher.BASE_URL)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# search by author name\n",
    "searcher = driver.find_element(by=By.XPATH, value='//*[@id=\"gs_hdr_tsi\"]')\n",
    "time.sleep(0.1)\n",
    "\n",
    "#searcher.send_keys(name)\n",
    "for chr in name :\n",
    "    searcher.send_keys(chr)\n",
    "    time.sleep(random.randint(1, 10)/200)\n",
    "searcher.send_keys(Keys.RETURN)\n",
    "driver.implicitly_wait(10)\n",
    "time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46827"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.find(\"reCAPTCHA\")\n",
    "\n",
    "source.lower().find(\"recaptcha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Institution :\n",
    "    name :str\n",
    "    google_scholar_url : str\n",
    "    homepage_url : str = None\n",
    "\n",
    "    def toJSON(self) :\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, \n",
    "            sort_keys=True, indent=4)\n",
    "    def toDict(self):\n",
    "        return json.loads(self.toJSON())\n",
    "\n",
    "    def fromDict(self, dic):\n",
    "        self.name = dic['name']\n",
    "        self.google_scholar_url = dic['google_scholar_url']\n",
    "        self.homepage_url = dic['homepage_url']\n",
    "\n",
    "@dataclass\n",
    "class Expertise :\n",
    "    name : str\n",
    "    url : str\n",
    "\n",
    "    def toJSON(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, \n",
    "            sort_keys=True, indent=4)\n",
    "    def toDict(self):\n",
    "        return json.loads(self.toJSON())\n",
    "    def fromDict(self, dic):\n",
    "        self.name = dic['name']\n",
    "        self.url = dic['url']\n",
    "\n",
    "@dataclass\n",
    "class Author :\n",
    "    name : str\n",
    "    google_schorlar_profile_url : str\n",
    "    affiliation : str = None\n",
    "    expertise_list : list[str] = None\n",
    "    homepage_url : str = None\n",
    "    paper_list : list = None\n",
    "    paper_title_list : list = None\n",
    "\n",
    "    def toJSON(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, \n",
    "            sort_keys=True, indent=4)\n",
    "    def toDict(self):\n",
    "        return json.loads(self.toJSON())\n",
    "    def fromDict(self, dic):\n",
    "        self.name = dic['name']\n",
    "        self.google_schorlar_profile_url = dic['google_schorlar_profile_url']\n",
    "        self.affiliation = dic['affiliation']\n",
    "        self.expertise_list = dic['expertise_list']\n",
    "        self.homepage_url = dic['homepage_url']\n",
    "        self.paper_list = dic['paper_list']\n",
    "        self.paper_title_list = dic['paper_title_list']\n",
    "        \n",
    "@dataclass\n",
    "class Paper :\n",
    "    # After search paper title using Google Schorlar,\n",
    "    # fill in basic metadata (abstract) from Google Schorlar\n",
    "    # fill in other metadata from Crossref\n",
    "    DOI : str = None\n",
    "    crossref_json : dict = None\n",
    "    google_schorlar_metadata : dict = None\n",
    "    title : str = None\n",
    "    authors : list = None\n",
    "    abstract : str = None\n",
    "    conference : str = None\n",
    "    journal : str = None\n",
    "    year : int = None\n",
    "    reference_list : list[str] = None\n",
    "    referenced_list : list[str] = None\n",
    "    cite_bibtex : str = None\n",
    "\n",
    "    def toJSON(self):\n",
    "        '''convert to JSON recursively'''\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, \n",
    "            sort_keys=True, indent=4)\n",
    "    def toDict(self):\n",
    "        '''convert to dict recursively'''\n",
    "        return json.loads(self.toJSON())\n",
    "    def fromDict(self, dic) :\n",
    "        '''convert from dict recursively'''\n",
    "        self.DOI = dic['DOI']\n",
    "        self.crossref_json = dic['crossref_json']\n",
    "        self.google_schorlar_metadata = dic['google_schorlar_metadata']\n",
    "        self.title = dic['title']\n",
    "        self.authors = dic['authors']\n",
    "        self.abstract = dic['abstract']\n",
    "        self.conference = dic['conference']\n",
    "        self.journal = dic['journal']\n",
    "        self.year = dic['year']\n",
    "        self.reference_list = dic['reference_list']\n",
    "        self.referenced_list = dic['referenced_list']\n",
    "        self.cite_bibtex = dic['cite_bibtex']\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
