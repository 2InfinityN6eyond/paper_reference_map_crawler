{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install selenium\n",
    "import os\n",
    "import random\n",
    "\n",
    "from selenium import webdriver\n",
    "#import undetected_chromedriver.v2 as webdriver\n",
    "#import undetected_chromedriver as webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Institution :\n",
    "    name :str\n",
    "    google_scholar_url : str\n",
    "    homepage_url : str = None\n",
    "\n",
    "    def toJSON(self) :\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, \n",
    "            sort_keys=True, indent=4)\n",
    "    def toDict(self):\n",
    "        return json.loads(self.toJSON())\n",
    "\n",
    "    def fromDict(self, dic):\n",
    "        self.name = dic['name']\n",
    "        self.google_scholar_url = dic['google_scholar_url']\n",
    "        self.homepage_url = dic['homepage_url']\n",
    "\n",
    "@dataclass\n",
    "class Expertise :\n",
    "    name : str\n",
    "    url : str\n",
    "\n",
    "    def toJSON(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, \n",
    "            sort_keys=True, indent=4)\n",
    "    def toDict(self):\n",
    "        return json.loads(self.toJSON())\n",
    "    def fromDict(self, dic):\n",
    "        self.name = dic['name']\n",
    "        self.url = dic['url']\n",
    "\n",
    "@dataclass\n",
    "class Author :\n",
    "    name : str\n",
    "    google_schorlar_profile_url : str\n",
    "    affiliation : str = None\n",
    "    expertise_list : list[str] = None\n",
    "    homepage_url : str = None\n",
    "    paper_list : list = None\n",
    "    paper_title_list : list = None\n",
    "\n",
    "    def toJSON(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, \n",
    "            sort_keys=True, indent=4)\n",
    "    def toDict(self):\n",
    "        return json.loads(self.toJSON())\n",
    "    def fromDict(self, dic):\n",
    "        self.name = dic['name']\n",
    "        self.google_schorlar_profile_url = dic['google_schorlar_profile_url']\n",
    "        self.affiliation = dic['affiliation']\n",
    "        self.expertise_list = dic['expertise_list']\n",
    "        self.homepage_url = dic['homepage_url']\n",
    "        self.paper_list = dic['paper_list']\n",
    "        self.paper_title_list = dic['paper_title_list']\n",
    "        \n",
    "@dataclass\n",
    "class Paper :\n",
    "    # After search paper title using Google Schorlar,\n",
    "    # fill in basic metadata (abstract) from Google Schorlar\n",
    "    # fill in other metadata from Crossref\n",
    "    DOI : str = None\n",
    "    crossref_json : dict = None\n",
    "    google_schorlar_metadata : dict = None\n",
    "    title : str = None\n",
    "    authors : list = None\n",
    "    abstract : str = None\n",
    "    conference : str = None\n",
    "    journal : str = None\n",
    "    year : int = None\n",
    "    reference_list : list[str] = None\n",
    "    referenced_list : list[str] = None\n",
    "    cite_bibtex : str = None\n",
    "\n",
    "    def toJSON(self):\n",
    "        '''convert to JSON recursively'''\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, \n",
    "            sort_keys=True, indent=4)\n",
    "    def toDict(self):\n",
    "        '''convert to dict recursively'''\n",
    "        return json.loads(self.toJSON())\n",
    "    def fromDict(self, dic) :\n",
    "        '''convert from dict recursively'''\n",
    "        self.DOI = dic['DOI']\n",
    "        self.crossref_json = dic['crossref_json']\n",
    "        self.google_schorlar_metadata = dic['google_schorlar_metadata']\n",
    "        self.title = dic['title']\n",
    "        self.authors = dic['authors']\n",
    "        self.abstract = dic['abstract']\n",
    "        self.conference = dic['conference']\n",
    "        self.journal = dic['journal']\n",
    "        self.year = dic['year']\n",
    "        self.reference_list = dic['reference_list']\n",
    "        self.referenced_list = dic['referenced_list']\n",
    "        self.cite_bibtex = dic['cite_bibtex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "class CrossRefFetcher :\n",
    "    def __init__(self) :\n",
    "        pass\n",
    "\n",
    "    def fetchMetaDatafromTitle(self, paper) :\n",
    "        '''\n",
    "        args :\n",
    "            paper : Paper\n",
    "                expect paper.title\n",
    "        '''\n",
    "        title = urllib.parse.quote(paper.title)\n",
    "        url = f'https://api.crossref.org/works?query.bibliographic={title}&rows=1'\n",
    "        r = requests.get(url)\n",
    "        metadata = r.json()['message']['items'][0]\n",
    "\n",
    "        paper.DOI = metadata['DOI']\n",
    "        paper.crossref_json = metadata\n",
    "        if len(metadata) == 0 :\n",
    "            return None\n",
    "    \n",
    "        reference_list = []\n",
    "        try :\n",
    "            for reference in metadata['reference'] :\n",
    "                if 'DOI' in reference :\n",
    "                    reference_list.append(reference['DOI'])\n",
    "        except :\n",
    "            pass\n",
    "            \n",
    "        paper.reference_list = reference_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = Paper(title = \"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\")\n",
    "\n",
    "crossref_fetcher = CrossRefFetcher()\n",
    "crossref_fetcher.fetchMetaDatafromTitle(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GoogleScharlarSearcher :\n",
    "    BASE_URL = \"https://scholar.google.com\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            institution_dict = None,\n",
    "            expertise_dict = None,\n",
    "        ) :\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        #chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--use_subprocess\")\n",
    "\n",
    "        self.driver = webdriver.Chrome(options=chrome_options)\n",
    "        self.institution_dict = institution_dict\n",
    "        self.expertise_dict = expertise_dict\n",
    "\n",
    "        self.crossref_fetcher = CrossRefFetcher()\n",
    "\n",
    "\n",
    "\n",
    "    def searchPaperByName(self, name) :\n",
    "        self.driver.get(self.BASE_URL)\n",
    "        self.driver.implicitly_wait(10)\n",
    "        # search given paper name\n",
    "        search = self.driver.find_element(by=By.XPATH, value='//*[@id=\"gs_hdr_tsi\"]')\n",
    "        search.send_keys(name)\n",
    "        search.send_keys(Keys.RETURN)\n",
    "        self.driver.implicitly_wait(10)\n",
    "        # click the first paper\n",
    "\n",
    "    def searchAuthorByName(self, name, continue_search = False, search_width = 1000) :\n",
    "        \"\"\"\n",
    "        If continue_search is True, search every co-author until search_width\n",
    "        \"\"\"\n",
    "        self.driver.get(self.BASE_URL)\n",
    "        self.driver.implicitly_wait(10)\n",
    "        # search by author name\n",
    "        searcher = self.driver.find_element(by=By.XPATH, value='//*[@id=\"gs_hdr_tsi\"]')\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        #searcher.send_keys(name)\n",
    "\n",
    "        for chr in name :\n",
    "            searcher.send_keys(chr)\n",
    "            time.sleep(random.randint(1, 10)/200)\n",
    "\n",
    "        time.sleep(0.2)\n",
    "        searcher.send_keys(Keys.RETURN)\n",
    "        self.driver.implicitly_wait(10)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        page_source = self.driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        author_list = soup.find_all(\"h4\", class_=\"gs_rt2\")\n",
    "\n",
    "        author_list = list(map(\n",
    "            lambda author : Author(\n",
    "                name = author.text,\n",
    "                google_schorlar_profile_url = self.BASE_URL +author.find(\"a\")[\"href\"],\n",
    "            ),\n",
    "            author_list\n",
    "        ))\n",
    "\n",
    "        print(f\"authors found : {list(map(lambda author : author.name, author_list))}\")\n",
    "        whole_paper_dict = {}\n",
    "        for author in author_list :\n",
    "            author, paper_dict = self.fillAuthor(author)\n",
    "            whole_paper_dict.update(paper_dict)\n",
    "\n",
    "\n",
    "        if continue_search and len(author_list) < search_width :\n",
    "            pass\n",
    "\n",
    "\n",
    "        return author_list, whole_paper_dict\n",
    "\n",
    "    def addInstitution(\n",
    "        self,\n",
    "        html_str\n",
    "    ) :\n",
    "        '''\n",
    "        initialize Institution instance and append to\n",
    "        self.instaitution_dict if not exist\n",
    "        args :\n",
    "            institution_html :\n",
    "                expected to have name,\n",
    "                google_schorlar_institution_url field\n",
    "        return :\n",
    "            institution name\n",
    "        '''\n",
    "        #institution_name = html_str.find(\"a\").text\n",
    "        institution_name = html_str.text\n",
    "\n",
    "        if institution_name not in self.institution_dict :\n",
    "            try :\n",
    "                google_schorlar_institution_url = self.BASE_URL + html_str.find(\"a\")[\"href\"]\n",
    "            except Exception as e :\n",
    "                google_schorlar_institution_url = None\n",
    "            homepage_url = None\n",
    "            self.institution_dict[institution_name] = Institution(\n",
    "                name = institution_name,\n",
    "                google_scholar_url = google_schorlar_institution_url,\n",
    "                homepage_url = homepage_url,\n",
    "            )\n",
    "        return institution_name\n",
    "\n",
    "    def addExpertise(\n",
    "        self,\n",
    "        html_str_list\n",
    "    ) :\n",
    "        '''\n",
    "        initialize Expertise instance and append to\n",
    "        self.expertise_dict if not exist\n",
    "        args :\n",
    "            html_str_list :\n",
    "                list of html_str. each elements are html str\n",
    "                expected to have name,\n",
    "                google_schorlar_expertise_url field\n",
    "        return :\n",
    "            expertise name\n",
    "        '''\n",
    "        expertise_name_list = []\n",
    "        for html_str in html_str_list :\n",
    "            expertise_name = html_str.text\n",
    "            if expertise_name not in self.expertise_dict :\n",
    "                google_schorlar_expertise_url = self.BASE_URL + html_str[\"href\"]\n",
    "                self.expertise_dict[expertise_name] = Expertise(\n",
    "                    name = expertise_name,\n",
    "                    url = google_schorlar_expertise_url,\n",
    "                )\n",
    "            expertise_name_list.append(expertise_name)\n",
    "        return expertise_name_list\n",
    "\n",
    "    def fillAuthor(self, author) :\n",
    "        \"\"\"\n",
    "        fill in author instance\n",
    "        args :\n",
    "            author :\n",
    "                expected to have name, google_schorlar_profile_url field\n",
    "        \"\"\"\n",
    "        # load page html        \n",
    "        self.driver.get(author.google_schorlar_profile_url)\n",
    "        self.driver.implicitly_wait(10)\n",
    "        page_source = self.driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # fill in expertise\n",
    "        expertise_html_list = soup.find_all(\"a\", class_=\"gsc_prf_inta\")\n",
    "        expertise_name_list = self.addExpertise(expertise_html_list)\n",
    "        author.expertise_list = expertise_name_list\n",
    "\n",
    "        # fill in institution\n",
    "        institution_html = soup.find(\"div\", class_=\"gsc_prf_il\")\n",
    "        try :\n",
    "            institution_name = self.addInstitution(institution_html)\n",
    "            author.affiliation = institution_name\n",
    "        except Exception as e :\n",
    "            print(e)\n",
    "            print(soup)\n",
    "            raise e\n",
    "\n",
    "        paper_dict = self.makePaperDictFromAuthor(author)\n",
    "        #DOI_list = list(paper_dict.keys())\n",
    "        #author.paper_list = DOI_list\n",
    "\n",
    "        return author, paper_dict\n",
    "    \n",
    "\n",
    "    def makePaperDictFromAuthor(self, author, search_width_limit = 20) :\n",
    "        \"\"\"\n",
    "        make paper instance from author instance\n",
    "        args :\n",
    "            author : Author\n",
    "                expected to have name, google_schorlar_profile_url field\n",
    "        return :\n",
    "            paper_list : list[Paper]\n",
    "        \"\"\"\n",
    "\n",
    "        # load page html        \n",
    "        self.driver.get(author.google_schorlar_profile_url)\n",
    "        self.driver.implicitly_wait(10)\n",
    "        page_source = self.driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # search papers\n",
    "        # click \"show more\" button until it is disabled\n",
    "        '''\n",
    "        while True :\n",
    "            load_more_button = self.driver.find_element(by=By.XPATH, value='//*[@id=\"gsc_bpf_more\"]')\n",
    "            self.driver.implicitly_wait(10)\n",
    "            load_more_button.click()\n",
    "            time.sleep(2)\n",
    "            if load_more_button.get_property(\"disabled\") :\n",
    "                break\n",
    "        '''\n",
    "        # get papaer html list\n",
    "        page_source = self.driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        paper_html_list = soup.find_all(\"tr\", class_=\"gsc_a_tr\")\n",
    "        paper_html_list = paper_html_list[:search_width_limit]\n",
    "\n",
    "        paper_list = []\n",
    "\n",
    "        print(f\"filling google schorlar metadata of papers from {author.name}...\")\n",
    "        with tqdm(total=len(paper_html_list)) as pbar:\n",
    "            for paper_html in paper_html_list :\n",
    "                google_schorlar_url = self.BASE_URL + paper_html.find(\"a\", class_=\"gsc_a_at\")[\"href\"]\n",
    "                title = paper_html.find(\"a\", class_=\"gsc_a_at\").text\n",
    "                \n",
    "                self.driver.get(google_schorlar_url)\n",
    "                self.driver.implicitly_wait(10)\n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                metadata_list = soup.find_all(\"div\", class_=\"gs_scl\")\n",
    "                \n",
    "                html_title = soup.find(\"a\", class_=\"gsc_oci_title_link\")\n",
    "\n",
    "                google_schorlar_metadata = {}\n",
    "                for metadata in metadata_list :\n",
    "                    field = metadata.find(\"div\", class_=\"gsc_oci_field\").text\n",
    "                    value = metadata.find(\"div\", class_=\"gsc_oci_value\").text\n",
    "                    google_schorlar_metadata[field] = value\n",
    "                \n",
    "\n",
    "                paper = Paper(title = title, google_schorlar_metadata = google_schorlar_metadata)\n",
    "                paper_list.append(paper)\n",
    "\n",
    "                pbar.set_postfix_str(title)\n",
    "                pbar.update(1)\n",
    "\n",
    "        author.paper_title_list = list(map(lambda paper : paper.title, paper_list))\n",
    "\n",
    "\n",
    "        paper_dict = {}\n",
    "\n",
    "        for paper in paper_list :\n",
    "            paper_dict[paper.title] = paper\n",
    "        return paper_dict\n",
    "\n",
    "        # query_crossref\n",
    "        print(f\"fetching crosserf metadata of papers from {author.name}...\")\n",
    "        for paper in tqdm(paper_list) :\n",
    "            self.crossref_fetcher.fetchMetaDatafromTitle(paper)\n",
    "            paper_dict[paper.DOI] = paper\n",
    "\n",
    "        return paper_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from file if file is available.\n",
    "\n",
    "institution_dict = {}\n",
    "expertise_dict = {}\n",
    "whole_author_list = []\n",
    "whole_paper_dict = {}\n",
    "\n",
    "INSTITUTION_FILE_PATH = \"./institution_dict.json\"\n",
    "if os.path.exists(INSTITUTION_FILE_PATH) :\n",
    "    with open(INSTITUTION_FILE_PATH, \"r\") as f :\n",
    "        institution_dict_raw = json.load(f)\n",
    "    for k, v in institution_dict_raw.items() :\n",
    "        institution_dict[k] = Institution(**v)\n",
    "\n",
    "EXPERTISE_FILE_PATH = \"./expertise_dict.json\"\n",
    "if os.path.exists(EXPERTISE_FILE_PATH) :\n",
    "    with open(EXPERTISE_FILE_PATH, \"r\") as f :\n",
    "        expertise_dict_raw = json.load(f)\n",
    "    for k, v in expertise_dict_raw.items() :\n",
    "        expertise_dict[k] = Expertise(**v)\n",
    "\n",
    "AUTHOR_FILE_PATH = \"./author_list.json\"\n",
    "if os.path.exists(AUTHOR_FILE_PATH) :\n",
    "    with open(AUTHOR_FILE_PATH, \"r\") as f :\n",
    "        author_list_raw = json.load(f)\n",
    "    for author in author_list_raw :\n",
    "        whole_author_list.append(Author(**author))\n",
    "\n",
    "WHOLE_PAPER_FILE_PATH = \"./whole_paper_dict.json\"\n",
    "if os.path.exists(WHOLE_PAPER_FILE_PATH) :\n",
    "    with open(WHOLE_PAPER_FILE_PATH, \"r\") as f :\n",
    "        whole_paper_dict = json.load(f)\n",
    "    for k, v in whole_paper_dict.items() :\n",
    "        whole_paper_dict[k] = Paper(**v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./author_name_list.json\", \"r\") as f :\n",
    "    author_name_list = json.load(f)\n",
    "\n",
    "author_name_to_append_list = author_name_list\n",
    "\n",
    "pre_existing_author_name_list = list(map(lambda author : author.name, whole_author_list))\n",
    "author_name_to_append_list = list(filter(lambda name : name not in pre_existing_author_name_list, author_name_to_append_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "997"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(author_name_list)\n",
    "len(author_name_to_append_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors found : ['Jitendra MALIK']\n",
      "filling google schorlar metadata of papers from Jitendra MALIK...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:09<00:00,  2.07it/s, Large displacement optical flow: descriptor matching in variational motion estimation]                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors found : []\n",
      "authors found : []\n",
      "authors found : []\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=118.0.5993.117)\nStacktrace:\n#0 0x5612c7b9cfb3 <unknown>\n#1 0x5612c78704a7 <unknown>\n#2 0x5612c7849204 <unknown>\n#3 0x5612c78decaf <unknown>\n#4 0x5612c78f2756 <unknown>\n#5 0x5612c78d9713 <unknown>\n#6 0x5612c78ac18b <unknown>\n#7 0x5612c78acf7e <unknown>\n#8 0x5612c7b628d8 <unknown>\n#9 0x5612c7b66800 <unknown>\n#10 0x5612c7b70cfc <unknown>\n#11 0x5612c7b67418 <unknown>\n#12 0x5612c7b3442f <unknown>\n#13 0x5612c7b8b4e8 <unknown>\n#14 0x5612c7b8b6b4 <unknown>\n#15 0x5612c7b9c143 <unknown>\n#16 0x7fe5abe94ac3 <unknown>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m gsearch \u001b[38;5;241m=\u001b[39m GoogleScharlarSearcher(institution_dict, expertise_dict)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m author_name \u001b[38;5;129;01min\u001b[39;00m author_name_to_append_list :\n\u001b[0;32m----> 3\u001b[0m     author_list, paper_dict \u001b[38;5;241m=\u001b[39m \u001b[43mgsearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearchAuthorByName\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauthor_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     whole_paper_dict\u001b[38;5;241m.\u001b[39mupdate(paper_dict)\n\u001b[1;32m      5\u001b[0m     whole_author_list \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m author_list\n",
      "Cell \u001b[0;32mIn[20], line 49\u001b[0m, in \u001b[0;36mGoogleScharlarSearcher.searchAuthorByName\u001b[0;34m(self, name, continue_search, search_width)\u001b[0m\n\u001b[1;32m     46\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[1;32m     48\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m \u001b[43msearcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRETURN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver\u001b[38;5;241m.\u001b[39mimplicitly_wait(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     52\u001b[0m page_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver\u001b[38;5;241m.\u001b[39mpage_source\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/selenium/webdriver/remote/webelement.py:230\u001b[0m, in \u001b[0;36mWebElement.send_keys\u001b[0;34m(self, *value)\u001b[0m\n\u001b[1;32m    227\u001b[0m             remote_files\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upload(file))\n\u001b[1;32m    228\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(remote_files)\n\u001b[0;32m--> 230\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSEND_KEYS_TO_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys_to_typing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys_to_typing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/selenium/webdriver/remote/webelement.py:394\u001b[0m, in \u001b[0;36mWebElement._execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    392\u001b[0m     params \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    393\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id\n\u001b[0;32m--> 394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:347\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 347\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.conda/envs/test/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=118.0.5993.117)\nStacktrace:\n#0 0x5612c7b9cfb3 <unknown>\n#1 0x5612c78704a7 <unknown>\n#2 0x5612c7849204 <unknown>\n#3 0x5612c78decaf <unknown>\n#4 0x5612c78f2756 <unknown>\n#5 0x5612c78d9713 <unknown>\n#6 0x5612c78ac18b <unknown>\n#7 0x5612c78acf7e <unknown>\n#8 0x5612c7b628d8 <unknown>\n#9 0x5612c7b66800 <unknown>\n#10 0x5612c7b70cfc <unknown>\n#11 0x5612c7b67418 <unknown>\n#12 0x5612c7b3442f <unknown>\n#13 0x5612c7b8b4e8 <unknown>\n#14 0x5612c7b8b6b4 <unknown>\n#15 0x5612c7b9c143 <unknown>\n#16 0x7fe5abe94ac3 <unknown>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gsearch = GoogleScharlarSearcher(institution_dict, expertise_dict)\n",
    "for author_name in author_name_to_append_list :\n",
    "    author_list, paper_dict = gsearch.searchAuthorByName(author_name)\n",
    "    whole_paper_dict.update(paper_dict)\n",
    "    whole_author_list += author_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_paper_dict_dict = {}\n",
    "for key, paper in whole_paper_dict.items() :\n",
    "    whole_paper_dict_dict[key] = paper.toDict()\n",
    "with open('whole_paper_dict.json', 'w') as f:\n",
    "    json.dump(whole_paper_dict_dict, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "whole_author_dict_list = list(map(lambda author : author.toDict(), whole_author_list))\n",
    "with open(\"author_list.json\", 'w') as f :\n",
    "    json.dump(whole_author_dict_list, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "institution_dict = {}\n",
    "for key, institution in gsearch.institution_dict.items() :\n",
    "    institution_dict[key] = institution.toDict()\n",
    "    #institution_dict[key] = json.loads(institution.toJOSN())\n",
    "with open(\"institution_dict.json\", 'w') as f :\n",
    "    json.dump(institution_dict, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "expertise_dict = {}\n",
    "for key, expertise in gsearch.expertise_dict.items() :\n",
    "    expertise_dict[key] = expertise.toDict()\n",
    "with open(\"expertise_dict.json\", 'w') as f :\n",
    "    json.dump(expertise_dict, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "\n",
    "def getMetaData(title) :\n",
    "\n",
    "    # URL encode the title to ensure it's in the correct format for a URL\n",
    "    encoded_title = urllib.parse.quote_plus(title)\n",
    "\n",
    "    # The Crossref API endpoint for works\n",
    "    api_url = f\"https://api.crossref.org/works?query.title={encoded_title}\"\n",
    "\n",
    "    # Make the GET request to the Crossref API\n",
    "    response = requests.get(api_url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the response to JSON\n",
    "        data = response.json()\n",
    "\n",
    "        # Check if there are items in the message\n",
    "        if 'items' in data['message']:\n",
    "            # Loop through the items to find the first one with a DOI\n",
    "            for item in data['message']['items']:\n",
    "                # Print the DOI\n",
    "                print(\"Title:\", item.get('title')[0])\n",
    "                print(\"DOI:\", item.get('DOI'))\n",
    "                break\n",
    "        else:\n",
    "            print(\"No results found for this title.\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Sound-Guided Semantic Image Manipulation\n",
      "DOI: 10.1109/cvpr52688.2022.00337\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import requests\n",
    "\n",
    "# Your paper title\n",
    "title = \"Example Title of Your Academic Paper\"\n",
    "title = \"Visual and Range Data\"\n",
    "title = \"3D Gaussian Splatting for Real-Time Radiance Field Rendering\"\n",
    "title = \"Sound-Guided Semantic Image Manipulation\"\n",
    "\n",
    "# URL encode the title to ensure it's in the correct format for a URL\n",
    "encoded_title = urllib.parse.quote_plus(title)\n",
    "\n",
    "# The Crossref API endpoint for works\n",
    "api_url = f\"https://api.crossref.org/works?query.title={encoded_title}\"\n",
    "\n",
    "# Make the GET request to the Crossref API\n",
    "response = requests.get(api_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the response to JSON\n",
    "    data = response.json()\n",
    "\n",
    "    # Check if there are items in the message\n",
    "    if 'items' in data['message']:\n",
    "        # Loop through the items to find the first one with a DOI\n",
    "        for item in data['message']['items']:\n",
    "            # Print the DOI\n",
    "            print(\"Title:\", item.get('title')[0])\n",
    "            print(\"DOI:\", item.get('DOI'))\n",
    "            break\n",
    "    else:\n",
    "        print(\"No results found for this title.\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOI for 'Sound-Guided Semantic Image Manipulation': 10.1109/cvpr52688.2022.00337\n",
      "References:\n",
      "10.1145/2647868.2655045\n",
      "10.1109/CVPR46437.2021.00232\n",
      "10.1109/CVPR.2019.00244\n",
      "No DOI provided\n",
      "No DOI provided\n",
      "10.1109/CVPR.2019.00772\n",
      "No DOI provided\n",
      "No DOI provided\n",
      "No DOI provided\n",
      "10.1109/ICCV48922.2021.00209\n",
      "No DOI provided\n",
      "10.1109/WACV48630.2021.00313\n",
      "No DOI provided\n",
      "No DOI provided\n",
      "10.1109/ICCV.2019.00453\n",
      "No DOI provided\n",
      "No DOI provided\n",
      "10.1109/ICCV48922.2021.00212\n",
      "No DOI provided\n",
      "10.1109/CVPR42600.2020.00813\n",
      "No DOI provided\n",
      "10.1145/3394171.3413624\n",
      "No DOI provided\n",
      "No DOI provided\n",
      "10.1007/s11633-021-1293-0\n",
      "No DOI provided\n",
      "10.1109/ICCV.2019.01040\n",
      "No DOI provided\n",
      "10.1007/978-90-481-8847-5_10\n",
      "10.1167/16.12.326\n",
      "No DOI provided\n",
      "10.1109/ICASSP.2017.7952261\n",
      "No DOI provided\n",
      "10.1109/IJCNN52387.2021.9533654\n",
      "10.1609/aaai.v32i1.12329\n",
      "No DOI provided\n",
      "No DOI provided\n",
      "No DOI provided\n",
      "10.1109/ICASSP40776.2020.9053174\n",
      "10.1109/CVPR.2015.7298698\n",
      "No DOI provided\n",
      "10.1145/3126686.3126723\n",
      "10.1109/CVPR46437.2021.00229\n",
      "10.1109/CVPR.2019.00482\n",
      "No DOI provided\n",
      "10.1145/3450626.3459838\n",
      "No DOI provided\n",
      "10.1109/ICASSP.2019.8682383\n",
      "No DOI provided\n",
      "10.1109/WACV.2017.58\n",
      "No DOI provided\n",
      "10.1609/aaai.v34i05.6431\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "def get_doi_from_title(title):\n",
    "    encoded_title = urllib.parse.quote_plus(title)\n",
    "    api_url = f\"https://api.crossref.org/works?query.title={encoded_title}\"\n",
    "    response = requests.get(api_url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        items = data['message'].get('items', [])\n",
    "        if items:\n",
    "            # Assuming the first result is the correct one\n",
    "            return items[0].get('DOI')\n",
    "    return None\n",
    "\n",
    "def get_references_from_doi(doi):\n",
    "    api_url = f\"https://api.crossref.org/works/{doi}\"\n",
    "    response = requests.get(api_url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        references = data['message'].get('reference', [])\n",
    "        return references\n",
    "    return []\n",
    "\n",
    "# Example usage:\n",
    "paper_title = \"Example Title of Your Academic Paper\"\n",
    "paper_title = \"3D Gaussian Splatting for Real-Time Radiance Field Rendering\"\n",
    "paper_title = \"Sound-Guided Semantic Image Manipulation\"\n",
    "\n",
    "paper_doi = get_doi_from_title(paper_title)\n",
    "\n",
    "if paper_doi:\n",
    "    print(f\"DOI for '{paper_title}': {paper_doi}\")\n",
    "    references = get_references_from_doi(paper_doi)\n",
    "    if references:\n",
    "        print(\"References:\")\n",
    "        for ref in references:\n",
    "            print(ref.get('DOI', 'No DOI provided'))\n",
    "else:\n",
    "    print(f\"No DOI found for the paper titled '{paper_title}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
