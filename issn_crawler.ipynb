{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install selenium\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import platform\n",
    "os_name = platform.system()\n",
    "\n",
    "if os_name == \"Darwin\":\n",
    "    #import undetected_chromedriver as webdriver\n",
    "    from selenium import webdriver\n",
    "\n",
    "elif os_name == \"Linux\":\n",
    "    from selenium import webdriver\n",
    "\n",
    "#from selenium import webdriver\n",
    "#import undetected_chromedriver.v2 as webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from containers import Institution, Author, Paper, Expertise\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class JournalConference :\n",
    "    type : str = None\n",
    "    name : str = None\n",
    "    issn : str = None\n",
    "    eissn : str = None\n",
    "    publisher : str = None\n",
    "    url : str = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ISSN_Crawler :\n",
    "    BASE_URL = \"https://portal.issn.org/\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            institution_dict = None,\n",
    "            expertise_dict = None,\n",
    "            os_name = None\n",
    "        ) :\n",
    "\n",
    "        if False : # os_name == \"Darwin\":\n",
    "            self.driver = webdriver.Safari()\n",
    "            self.browser_name = \"safari\"\n",
    "        else :\n",
    "            chrome_options = webdriver.ChromeOptions()\n",
    "            #chrome_options.add_argument(\"--headless\")\n",
    "            chrome_options.add_argument(\"--use_subprocess\")\n",
    "            self.browser_name = \"chrome\"\n",
    "\n",
    "            self.driver = webdriver.Chrome(options=chrome_options)\n",
    "        \n",
    "    def crawl_by_issn\n",
    "\n",
    "\n",
    "    def searchPaperByName(self, name) :\n",
    "        self.driver.get(self.BASE_URL)\n",
    "        self.driver.implicitly_wait(10)\n",
    "        # search given paper name\n",
    "        search = self.driver.find_element(by=By.XPATH, value='//*[@id=\"gs_hdr_tsi\"]')\n",
    "        search.send_keys(name)\n",
    "        search.send_keys(Keys.RETURN)\n",
    "        self.driver.implicitly_wait(10)\n",
    "        # click the first paper\n",
    "\n",
    "    def searchAuthorByName(\n",
    "        self,\n",
    "        name,\n",
    "        continue_search = False,\n",
    "        search_width = 1000,\n",
    "        ask_for_continue = False,\n",
    "    ) :\n",
    "        \"\"\"\n",
    "        If continue_search is True, search every co-author until search_width\n",
    "        \"\"\"\n",
    "        self.driver.get(self.BASE_URL)\n",
    "        self.driver.implicitly_wait(10)\n",
    "\n",
    "        self.checkCaptcha()\n",
    "\n",
    "        # search by author name\n",
    "        searcher = self.driver.find_element(by=By.XPATH, value='//*[@id=\"gs_hdr_tsi\"]')\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        #searcher.send_keys(name)\n",
    "        for chr in name :\n",
    "            searcher.send_keys(chr)\n",
    "            time.sleep(random.randint(1, 10)/200)\n",
    "        searcher.send_keys(Keys.RETURN)\n",
    "        self.driver.implicitly_wait(10)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        self.checkCaptcha()\n",
    "\n",
    "        if ask_for_continue :\n",
    "            key_input = input(\"Press [n] to stop...\")\n",
    "            if key_input in [\"n\", \"N\", \"no\", \"No\", \"NO\", \"nO\"] :\n",
    "                return [], {}\n",
    "\n",
    "        page_source = self.driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        author_list = soup.find_all(\"h4\", class_=\"gs_rt2\")\n",
    "\n",
    "        author_list = list(map(\n",
    "            lambda author : Author(\n",
    "                name = author.text,\n",
    "                google_schorlar_profile_url = self.BASE_URL +author.find(\"a\")[\"href\"],\n",
    "            ),\n",
    "            author_list\n",
    "        ))\n",
    "\n",
    "        print(f\"authors found : {list(map(lambda author : author.name, author_list))}\")\n",
    "        \n",
    "        auther_href_button_list = self.driver.find_elements(by=By.XPATH, value='//*[@class=\"gs_rt2\"]/a')        \n",
    "        print(auther_href_button_list)\n",
    "\n",
    "        print(\"nubmer of authors found : \", len(auther_href_button_list))\n",
    "        whole_paper_dict = {}\n",
    "        for author, auther_href_button in zip(author_list, auther_href_button_list) :\n",
    "            print(f\"filling google schorlar metadata of papers from {author.name}...\")\n",
    "            \n",
    "            if self.browser_name == \"safari\" :\n",
    "                # clik in safari does not work. so use send_keys\n",
    "                auther_href_button.send_keys(Keys.RETURN)\n",
    "            else :\n",
    "                auther_href_button.click()\n",
    "\n",
    "\n",
    "            print(\"clicked\")\n",
    "            self.driver.implicitly_wait(10)\n",
    "\n",
    "            self.checkCaptcha()\n",
    "\n",
    "            author, paper_dict = self.fillAuthor(author, url_already_loaded = True)\n",
    "            whole_paper_dict.update(paper_dict)\n",
    "            self.driver.back()\n",
    "            self.driver.implicitly_wait(10)\n",
    "            self.checkCaptcha()\n",
    "\n",
    "            break\n",
    "\n",
    "        #for author in author_list :\n",
    "            #author, paper_dict = self.fillAuthor(author)\n",
    "            #whole_paper_dict.update(paper_dict)\n",
    "\n",
    "\n",
    "        if continue_search and len(author_list) < search_width :\n",
    "            pass\n",
    "\n",
    "\n",
    "        return author_list, whole_paper_dict\n",
    "\n",
    "    def addInstitution(\n",
    "        self,\n",
    "        html_str\n",
    "    ) :\n",
    "        '''\n",
    "        initialize Institution instance and append to\n",
    "        self.instaitution_dict if not exist\n",
    "        args :\n",
    "            institution_html :\n",
    "                expected to have name,\n",
    "                google_schorlar_institution_url field\n",
    "        return :\n",
    "            institution name\n",
    "        '''\n",
    "        #institution_name = html_str.find(\"a\").text\n",
    "\n",
    "\n",
    "        try :\n",
    "            institution_name = html_str.text\n",
    "        except Exception as e :\n",
    "            print(e)\n",
    "        \n",
    "        if institution_name not in self.institution_dict :\n",
    "            try :\n",
    "                google_schorlar_institution_url = self.BASE_URL + html_str.find(\"a\")[\"href\"]\n",
    "            except Exception as e :\n",
    "                google_schorlar_institution_url = None\n",
    "            homepage_url = None\n",
    "            self.institution_dict[institution_name] = Institution(\n",
    "                name = institution_name,\n",
    "                google_scholar_url = google_schorlar_institution_url,\n",
    "                homepage_url = homepage_url,\n",
    "            )\n",
    "        return institution_name\n",
    "\n",
    "    def addExpertise(\n",
    "        self,\n",
    "        html_str_list\n",
    "    ) :\n",
    "        '''\n",
    "        initialize Expertise instance and append to\n",
    "        self.expertise_dict if not exist\n",
    "        args :\n",
    "            html_str_list :\n",
    "                list of html_str. each elements are html str\n",
    "                expected to have name,\n",
    "                google_schorlar_expertise_url field\n",
    "        return :\n",
    "            expertise name\n",
    "        '''\n",
    "        expertise_name_list = []\n",
    "        for html_str in html_str_list :\n",
    "            expertise_name = html_str.text\n",
    "            if expertise_name not in self.expertise_dict :\n",
    "                google_schorlar_expertise_url = self.BASE_URL + html_str[\"href\"]\n",
    "                self.expertise_dict[expertise_name] = Expertise(\n",
    "                    name = expertise_name,\n",
    "                    url = google_schorlar_expertise_url,\n",
    "                )\n",
    "            expertise_name_list.append(expertise_name)\n",
    "        return expertise_name_list\n",
    "\n",
    "    def fillAuthor(self, author, url_already_loaded = False) :\n",
    "        \"\"\"\n",
    "        fill in author instance\n",
    "        args :\n",
    "            author :\n",
    "                expected to have name, google_schorlar_profile_url field\n",
    "        \"\"\"\n",
    "        # load page html        \n",
    "        if not url_already_loaded :\n",
    "            self.driver.get(author.google_schorlar_profile_url)\n",
    "            self.driver.implicitly_wait(10)\n",
    "        page_source = self.driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # fill in expertise\n",
    "        expertise_html_list = soup.find_all(\"a\", class_=\"gsc_prf_inta\")\n",
    "        expertise_name_list = self.addExpertise(expertise_html_list)\n",
    "        author.expertise_list = expertise_name_list\n",
    "\n",
    "        # fill in institution\n",
    "        institution_html = soup.find(\"div\", class_=\"gsc_prf_il\")\n",
    "        try :\n",
    "            institution_name = self.addInstitution(institution_html)\n",
    "            author.affiliation = institution_name\n",
    "        except Exception as e :\n",
    "            print(e)\n",
    "            raise e\n",
    "\n",
    "        paper_dict = self.makePaperDictFromAuthor(author, url_already_loaded = True)\n",
    "        #DOI_list = list(paper_dict.keys())\n",
    "        #author.paper_list = DOI_list\n",
    "\n",
    "        return author, paper_dict\n",
    "    \n",
    "\n",
    "    def makePaperDictFromAuthor(self, author, url_already_loaded = False, search_width_limit = 20) :\n",
    "        \"\"\"\n",
    "        make paper instance from author instance\n",
    "        args :\n",
    "            author : Author\n",
    "                expected to have name, google_schorlar_profile_url field\n",
    "        return :\n",
    "            paper_list : list[Paper]\n",
    "        \"\"\"\n",
    "\n",
    "        # load page html\n",
    "        if not url_already_loaded :\n",
    "            self.driver.get(author.google_schorlar_profile_url)\n",
    "            self.driver.implicitly_wait(10)\n",
    "        page_source = self.driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # search papers\n",
    "        # click \"show more\" button until it is disabled\n",
    "        '''\n",
    "        while True :\n",
    "            load_more_button = self.driver.find_element(by=By.XPATH, value='//*[@id=\"gsc_bpf_more\"]')\n",
    "            self.driver.implicitly_wait(10)\n",
    "            load_more_button.click()\n",
    "            time.sleep(2)\n",
    "            if load_more_button.get_property(\"disabled\") :\n",
    "                break\n",
    "        '''\n",
    "        # get papaer html list\n",
    "        page_source = self.driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        paper_html_list = soup.find_all(\"tr\", class_=\"gsc_a_tr\")\n",
    "        paper_html_list = paper_html_list[:search_width_limit]\n",
    "\n",
    "        paper_href_list = self.driver.find_elements(by=By.XPATH, value='//*[@class=\"gsc_a_t\"]/a')\n",
    "\n",
    "\n",
    "        paper_list = []\n",
    "\n",
    "        print(f\"filling google schorlar metadata of papers from {author.name}...\")\n",
    "        with tqdm(total=len(paper_html_list)) as pbar:\n",
    "            #for paper_html in paper_html_list :\n",
    "            for paper_html, paper_href in zip(paper_html_list, paper_href_list) :\n",
    "                google_schorlar_url = self.BASE_URL + paper_html.find(\"a\", class_=\"gsc_a_at\")[\"href\"]\n",
    "                title = paper_html.find(\"a\", class_=\"gsc_a_at\").text\n",
    "                \n",
    "                #self.driver.get(google_schorlar_url)\n",
    "                #self.driver.implicitly_wait(10)\n",
    "                if self.browser_name == \"safari\" :\n",
    "                    # clik in safari does not work. so use send_keys\n",
    "                    paper_href.send_keys(Keys.RETURN)\n",
    "                else :\n",
    "                    paper_href.click()\n",
    "\n",
    "                self.checkCaptcha()\n",
    "\n",
    "                time.sleep(0.2)\n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                metadata_list = soup.find_all(\"div\", class_=\"gs_scl\")\n",
    "                html_title = soup.find(\"a\", class_=\"gsc_oci_title_link\")\n",
    "\n",
    "                google_schorlar_metadata = {}\n",
    "                for metadata in metadata_list :\n",
    "                    field = metadata.find(\"div\", class_=\"gsc_oci_field\").text\n",
    "                    value = metadata.find(\"div\", class_=\"gsc_oci_value\").text\n",
    "                    google_schorlar_metadata[field] = value\n",
    "                \n",
    "\n",
    "                paper = Paper(title = title, google_schorlar_metadata = google_schorlar_metadata)\n",
    "                paper_list.append(paper)\n",
    "\n",
    "                pbar.set_postfix_str(title)\n",
    "                pbar.update(1)\n",
    "\n",
    "                self.driver.back()\n",
    "                time.sleep(0.2)\n",
    "                self.checkCaptcha()\n",
    "\n",
    "        author.paper_title_list = list(map(lambda paper : paper.title, paper_list))\n",
    "\n",
    "\n",
    "        paper_dict = {}\n",
    "        for paper in paper_list :\n",
    "            paper_dict[paper.title] = paper\n",
    "        return paper_dict\n",
    "\n",
    "        # query_crossref\n",
    "        print(f\"fetching crosserf metadata of papers from {author.name}...\")\n",
    "        for paper in tqdm(paper_list) :\n",
    "            self.crossref_fetcher.fetchMetaDatafromTitle(paper)\n",
    "            paper_dict[paper.DOI] = paper\n",
    "\n",
    "        return paper_dict\n",
    "    \n",
    "    def checkCaptcha(self) :\n",
    "        captcha_found = False\n",
    "        source = self.driver.page_source\n",
    "        if source.find(\"사용자가 로봇이 아니라는 확인이 필요합니다.\") != -1 :\n",
    "            print(\"로봇이 아니라는 확인이 필요합니다 text detected!\")\n",
    "            captcha_found = True\n",
    "        if source.lower().find(\"recaptcha\") != -1 :\n",
    "            print(\"recaptcha detected!\")\n",
    "            captcha_found = True\n",
    "        try:\n",
    "            captcha_image = self.driver.find_element_by_xpath(\"//img[contains(@alt, 'captcha')]\")\n",
    "            if captcha_image:\n",
    "                print(\"captcha image detected!\")\n",
    "                captcha_found = True\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        try:\n",
    "            captcha_text = self.driver.find_element_by_xpath(\"//*[contains(text(), 'prove you are human')]\")\n",
    "            if captcha_text:\n",
    "                print(\"CAPTCHA text detected!\")\n",
    "                captcha_found = True\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        try:\n",
    "            robot_detection = self.driver.find_element_by_xpath(\"//*[contains(text(), 'Google의 시스템이 컴퓨터 네트워크에서 비정상적인 트래픽을 감지했습니다.')]\")\n",
    "            if robot_detection:\n",
    "                print(\"로봇이 아니라는 확인이 필요합니다 detected!\")\n",
    "                captcha_found = True\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        if captcha_found :\n",
    "            key_input = input(\"Press [n] to stop...\")\n",
    "            if key_input in [\"n\", \"N\", \"no\", \"No\", \"NO\", \"nO\"] :\n",
    "                raise Exception(\"captcha detected!\")\n",
    "        # reload page\n",
    "        #self.driver.refresh()\n",
    "        self.driver.implicitly_wait(10)\n",
    "        time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
