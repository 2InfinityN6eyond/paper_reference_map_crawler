{
    "Deep Residual Learning for Image Recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
            "저자": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
            "전체 인용횟수": "193462회 인용2016201720182019202020212022202312005069119632076428660385664519640487",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2016",
            "학술 문서": "Deep residual learning for image recognitionK He, X Zhang, S Ren, J Sun - Proceedings of the IEEE conference on computer …, 2016189280회 인용 관련 학술자료 전체 76개의 버전 Deep residual learning for image recognition*S Jian, H Kaiming, R Shaoqing, Z Xiangyu - IEEE Conference on Computer Vision & Pattern …, 20165375회 인용 관련 학술자료 전체 2개의 버전 Deep residual learning for image recognition. CoRR abs/1512.03385 (2015)*K He, X Zhang, S Ren, J Sun - 2015470회 인용 관련 학술자료 Deep residual learning for image recognition. arXiv e-prints*K He, X Zhang, S Ren, J Sun - arXiv preprint arXiv:1512.03385, 2015157회 인용 관련 학술자료 Zhang, X.—Ren, S.—Sun, J.: Deep Residual Learning for Image Recognition*K He - Proceedings of the 2016 IEEE Conference on …, 2016131회 인용 관련 학술자료 Deep Learning and Image Recognition*C Li, X Li, M Chen, X Sun - 2023 IEEE 6th International Conference on Electronic …, 2023106회 인용 관련 학술자료 Deep residual learning for image recognition In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 770–778*K He, X Zhang, S Ren, J Sun - IEEE. https://doi. org/10.1109/cvpr, 201698회 인용 관련 학술자료 Deep residual learning for image recognition. CoRR (2015)*K He, X Zhang, S Ren, J Sun - arXiv preprint arXiv:1512.03385, 201680회 인용 관련 학술자료 Deep residual learning for image recognition. 2016 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV*K He, X Zhang, S Ren, J Sun - 201664회 인용 관련 학술자료 Deep Residual Learning for Image Recognition. 2015. doi: 10.48550*K He, X Zhang, S Ren, J Sun - arXiv preprint ARXIV.1512.0338563회 인용 관련 학술자료 Shaoqing Ren and Jian Sun. Deep residual learning for image recognition*K He, X Zhang - 2016 IEEE Confer-ence on Computer Vision and …, 201656회 인용 관련 학술자료 Deep residual learning for image recognition [J](2015)*K He, X Zhang, S Ren, J Sun - arXiv preprint arXiv:1512.03385, 201654회 인용 관련 학술자료 Deep residual learning for image recognition, December 2015*K He, X Zhang, S Ren, J Sun - arXiv preprint arXiv:1512.03385, 201535회 인용 관련 학술자료 Deep Residual Learning for Image Recognition. arXiv e-prints, art*K He, X Zhang, S Ren, J Sun - arXiv preprint arXiv:1512.03385, 201532회 인용 관련 학술자료 Deep residual learning for image recognition. arXiv preprint (2015)*K He, X Zhang, S Ren, J Sun - arXiv preprint arXiv:1512.0338530회 인용 관련 학술자료 S. & Sun, J.(2016) Deep residual learning for image recognition*K He, X Zhang, S Ren - Proceedings of the IEEE international conference on …, 201618회 인용 관련 학술자료 Deep Residual Learning for Image Recognition. dec 2015*K He, X Zhang, S Ren, J Sun - URL http://arxiv. org/abs/1512.0338518회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep Residual Learning for Image Recognition",
        "year": null
    },
    "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/6/4",
            "설명": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP) using 300 proposals per image. Code is available at https://github. com/ShaoqingRen/faster_rcnn.",
            "저자": "Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun",
            "전체 인용횟수": "67246회 인용2016201720182019202020212022202369123324973809610158129101421812056",
            "컨퍼런스": "Neural Information Processing Systems (NIPS), 2015",
            "학술 문서": "Faster r-cnn: Towards real-time object detection with region proposal networksS Ren, K He, R Girshick, J Sun - Advances in neural information processing systems, 201567038회 인용 관련 학술자료 전체 42개의 버전 Faster r-cnn: Towards real-time object detection with region proposal networks. arXiv 2015*S Ren, K He, R Girshick, J Sun - arXiv preprint arXiv:1506.01497, 2015381회 인용 관련 학술자료 Faster R-CNN: towards real-time object detection with region proposal networks. İn: International Conference on Neural Information Processing Systems*SQ Ren, KM He, R Girshick, J Sun - 2018207회 인용 관련 학술자료 Jjitopa Sun 2017 Intelligence M*S Ren, K He, R Girshick - Faster r-cnn: Towards real-time object detection with …59회 인용 관련 학술자료 Faster r-cnn: Towards real-time object detection with region proposal networks [J]S Tren, K He, R Girshick - Advances in neural information processing systems, 20159회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "year": null
    },
    "Mask R-CNN": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/3/20",
            "설명": "We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, eg, allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.",
            "저널": "arXiv preprint arXiv:1703.06870",
            "저자": "Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick",
            "전체 인용횟수": "31086회 인용2017201820192020202120222023165135434765034662775156723",
            "학술 문서": "Mask r-cnnK He, G Gkioxari, P Dollár, R Girshick - Proceedings of the IEEE international conference on …, 201730662회 인용 관련 학술자료 전체 23개의 버전 2017 IEEE International Conference on Computer Vision (ICCV)*K He, G Gkioxari, P Dollár, R Girshick, M R-CNN - IEEE, Venice, Italy, 2017359회 인용 관련 학술자료 Mask r-cnn. arXiv 2017*K He, G Gkioxari, P Dollár, R Girshick - arXiv preprint arXiv:1703.06870, 2020135회 인용 관련 학술자료 Mask r-cnn. arXiv*K He, G Gkioxari, P Dollár, R Girshick - arXiv preprint arXiv:1703.06870, 201772회 인용 관련 학술자료 Mask r-cnn*G Gkioxari, P Dollár, R Girshick - IEEE International conference on Computer Vision …, 201754회 인용 관련 학술자료 Mask r-cnn*K Dollár, R Girshick - Proceedings of the IEEE international conference on …, 201752회 인용 관련 학술자료 전체 7개의 버전 Mask r-cnn*R Mask, K He, G Gkioxari, P Dollár, R Girshick - Proceedings of the IEEE International Conference on …, 20176회 인용 관련 학술자료 전체 2개의 버전 Mask R-CNN. Proc*K He, G Gkioxari, P Dollár, R Girshick - 2017 IEEE International Conference on Computer …, 20184회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Mask R-CNN",
        "year": null
    },
    "Focal Loss for Dense Object Detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/8/7",
            "설명": "The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors.",
            "저자": "Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollár",
            "전체 인용횟수": "25726회 인용20182019202020212022202353718743457553469997122",
            "컨퍼런스": "International Conference on Computer Vision (ICCV), 2017",
            "학술 문서": "Focal loss for dense object detectionTY Lin, P Goyal, R Girshick, K He, P Dollár - Proceedings of the IEEE international conference on …, 201725633회 인용 관련 학술자료 전체 23개의 버전 Focal loss for dense object detection. arXiv 2017*TY Lin, P Goyal, R Girshick, K He, P Dollár - arXiv preprint arXiv:1708.02002, 2002115회 인용 관련 학술자료 Focal loss for dense object detection. arXiv*TY Lin, P Goyal, R Girshick, K He, P Dollár - arXiv preprint arXiv:1708.02002, 201780회 인용 관련 학술자료 Piotr Doll ar. Focal loss for dense object detection*TY Lin, P Goyal, R Girshick, K He - Proceedings of the IEEE International Conference on …, 201761회 인용 관련 학술자료 Focal loss for dense object detection. CoRR abs/1708.02002 (2017)*T Lin, P Goyal, RB Girshick, K He, P Dollár - arXiv preprint arXiv:1708.02002, 201755회 인용 관련 학술자료 Focal Loss for Dense Object Detection (RetinaNet)*TY Lin, P Goyal, R Girshick, K He, P Dollar - Proceedings of the IEEE International Conference on …, 201711회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Focal Loss for Dense Object Detection",
        "year": null
    },
    "Feature Pyramid Networks for Object Detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.",
            "저자": "Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie",
            "전체 인용횟수": "22599회 인용201720182019202020212022202311263720043160483259665735",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2017",
            "학술 문서": "Feature pyramid networks for object detectionTY Lin, P Dollár, R Girshick, K He, B Hariharan… - Proceedings of the IEEE conference on computer …, 201722526회 인용 관련 학술자료 전체 22개의 버전 Feature pyramid networks for object detection. arXiv 2016*TY Lin, P Dollár, R Girshick, K He, B Hariharan… - arXiv preprint arXiv:1612.03144121회 인용 관련 학술자료 Feature pyramid networks for object detection. 2016*TY Lin, P Dollár, R Girshick, K He, B Hariharan… - arXiv preprint arXiv:1612.03144, 201759회 인용 관련 학술자료 Feature pyramid networks for object detection*P Dollar, R Girshick, K He, B Hariharan, S Belongie - CVPR, 201729회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Feature Pyramid Networks for Object Detection",
        "year": null
    },
    "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%) on this dataset.",
            "저자": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
            "전체 인용횟수": "22598회 인용2015201620172018201920202021202220232196521289214529423483379736702990",
            "컨퍼런스": "International Conference on Computer Vision (ICCV), 2015",
            "학술 문서": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classificationK He, X Zhang, S Ren, J Sun - Proceedings of the IEEE international conference on …, 201522483회 인용 관련 학술자료 전체 21개의 버전 IEEE International Conference on Computer Vision*K He, X Zhang, S Ren, J Sun - Delving Deep into Rectifiers: Surpassing Human-Level …, 201565회 인용 관련 학술자료 Delving deep into rectifiers: surpassing human-level performance on ImageNet classification. CoRR abs/1502.01852 (2015)*K He, X Zhang, S Ren, J Sun - arXiv preprint arXiv:1502.01852, 201550회 인용 관련 학술자료 Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. arXiv e-prints*K He, X Zhang, S Ren, J Sun - arXiv preprint arXiv:1502.01852, 201530회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
        "year": null
    },
    "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/6/18",
            "설명": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224   224) input image. This requirement is “artificial” and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, “spatial pyramid pooling”, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image …",
            "저자": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
            "전체 인용횟수": "13825회 인용201420152016201720182019202020212022202357214462772103814431634210725662200",
            "컨퍼런스": "European Conference on Computer Vision (ECCV), 2014",
            "학술 문서": "Spatial pyramid pooling in deep convolutional networks for visual recognitionK He, X Zhang, S Ren, J Sun - IEEE transactions on pattern analysis and machine …, 201513825회 인용 관련 학술자료 전체 26개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition",
        "year": null
    },
    "Learning a Deep Convolutional Network for Image Super-Resolution": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/9/6",
            "게시자": "Springer International Publishing",
            "설명": " We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) [15] that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage.",
            "저자": "Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang",
            "전체 인용횟수": "12913회 인용201520162017201820192020202120222023102348727126617442044225223031999",
            "컨퍼런스": "European Conference on Computer Vision (ECCV), 2014",
            "학술 문서": "Image super-resolution using deep convolutional networks*C Dong, CC Loy, K He, X Tang - IEEE transactions on pattern analysis and machine …, 20158622회 인용 관련 학술자료 전체 13개의 버전 Learning a deep convolutional network for image super-resolutionC Dong, CC Loy, K He, X Tang - Computer Vision–ECCV 2014: 13th European …, 20145583회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning a Deep Convolutional Network for Image Super-Resolution",
        "year": null
    },
    "Aggregated Residual Transformations for Deep Neural Networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call\" cardinality\"(the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.",
            "저자": "Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He",
            "전체 인용횟수": "11016회 인용201720182019202020212022202313456710881684244227302285",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2017",
            "학술 문서": "Aggregated residual transformations for deep neural networksS Xie, R Girshick, P Dollár, Z Tu, K He - Proceedings of the IEEE conference on computer …, 201711007회 인용 관련 학술자료 전체 13개의 버전 Aggregated residual transformations for deep neural networks. arXiv 2016*S Xie, R Girshick, P Dollár, Z Tu, K He - arXiv preprint arXiv:1611.0543157회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Aggregated Residual Transformations for Deep Neural Networks",
        "year": null
    },
    "Identity Mappings in Deep Residual Networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": " Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62 % error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at:                      https://github.com/KaimingHe/resnet-1k-layers                                        .",
            "저자": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
            "전체 인용횟수": "11015회 인용20162017201820192020202120222023120498100314381793212121871739",
            "컨퍼런스": "European Conference on Computer Vision (ECCV), 2016",
            "학술 문서": "Identity mappings in deep residual networksK He, X Zhang, S Ren, J Sun - Computer Vision–ECCV 2016: 14th European …, 201610833회 인용 관련 학술자료 전체 8개의 버전 Identity mappings in deep residual networks*H Kaiming, Z Xiangyu, R Shaoqing, S Jian - European conference on computer vision, 2016240회 인용 관련 학술자료 Identity mappings in deep residual networks (2016)*K He, X Zhang, S Ren, J Sun - arXiv preprint arXiv:1603.05027, 201630회 인용 관련 학술자료 Identity mappings in deep residual networks. CoRR abs/1603.05027 (2016)*K He, X Zhang, S Ren, J Sun - arXiv preprint arXiv:1603.05027, 201621회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Identity Mappings in Deep Residual Networks",
        "year": null
    },
    "Non-local Neural Networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our non-local models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks. Code will be made available.",
            "저자": "Xiaolong Wang, Ross Girshick, Abhinav Gupta, Kaiming He",
            "전체 인용횟수": "9430회 인용2018201920202021202220231106491373227426552315",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2018",
            "학술 문서": "Non-local neural networksX Wang, R Girshick, A Gupta, K He - Proceedings of the IEEE conference on computer …, 20189430회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Non-local Neural Networks",
        "year": null
    },
    "Momentum Contrast for Unsupervised Visual Representation Learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020",
            "설명": "We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.",
            "저자": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick",
            "전체 인용횟수": "9318회 인용2020202120222023455176332463797",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2020",
            "학술 문서": "Momentum contrast for unsupervised visual representation learningK He, H Fan, Y Wu, S Xie, R Girshick - Proceedings of the IEEE/CVF conference on computer …, 20209318회 인용 관련 학술자료 전체 19개의 버전 Girshick Ross B.. 2020*H Kaiming, F Haoqi, W Yuxin, X Saining - … for unsupervised visual representation learning. In …, 202011회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Momentum Contrast for Unsupervised Visual Representation Learning",
        "year": null
    },
    "Single Image Haze Removal using Dark Channel Prior": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/6",
            "게시자": "IEEE",
            "설명": "In this paper, we propose a simple but effective image prior-dark channel prior to remove haze from a single input image. The dark channel prior is a kind of statistics of outdoor haze-free images. It is based on a key observation-most local patches in outdoor haze-free images contain some pixels whose intensity is very low in at least one color channel. Using this prior with the haze imaging model, we can directly estimate the thickness of the haze and recover a high-quality haze-free image. Results on a variety of hazy images demonstrate the power of the proposed prior. Moreover, a high-quality depth map can also be obtained as a byproduct of haze removal.",
            "저자": "Kaiming He, Jian Sun, Xiaoou Tang",
            "전체 인용횟수": "8317회 인용2010201120122013201420152016201720182019202020212022202376111159241344493585582704724872102112001137",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2009",
            "학술 문서": "Single image haze removal using dark channel priorK He, J Sun, X Tang - IEEE transactions on pattern analysis and machine …, 20108272회 인용 관련 학술자료 전체 44개의 버전 Single image haze removal using dark channel priors*J Sun, K He, X Tang - US Patent 8,340,461, 201259회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Single Image Haze Removal using Dark Channel Prior",
        "year": null
    },
    "Guided Image Filtering": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/9/5",
            "게시자": "Springer Berlin Heidelberg",
            "도서": "European Conference on Computer Vision (ECCV), 2010",
            "설명": "In this paper, we propose a novel explicit image filter called guided filter. Derived from a local linear model, the guided filter computes the filtering output by considering the content of a guidance image, which can be the input image itself or another different image. The guided filter can be used as an edge-preserving smoothing operator like the popular bilateral filter [1], but it has better behaviors near edges. The guided filter is also a more generic concept beyond smoothing: It can transfer the structures of the guidance image to the filtering output, enabling new filtering applications like dehazing and guided feathering. Moreover, the guided filter naturally has a fast and nonapproximate linear time algorithm, regardless of the kernel size and the intensity range. Currently, it is one of the fastest edge-preserving filters. Experiments show that the guided filter is both effective and efficient in a great variety of computer …",
            "저자": "Kaiming He, Jian Sun, Xiaoou Tang",
            "전체 인용횟수": "7752회 인용201120122013201420152016201720182019202020212022202343112210355581811798861829830824790644",
            "학술 문서": "Guided image filteringK He, J Sun, X Tang - IEEE transactions on pattern analysis and machine …, 20127752회 인용 관련 학술자료 전체 27개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Guided Image Filtering",
        "year": null
    },
    "R-FCN: Object Detection via Region-based Fully Convolutional Networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets), for object detection. We show competitive results on the PASCAL VOC datasets (eg, 83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20 times faster than the Faster R-CNN counterpart. Code is made publicly available at: https://github. com/daijifeng001/r-fcn.",
            "저자": "Jifeng Dai, Yi Li, Kaiming He, Jian Sun",
            "전체 인용횟수": "6977회 인용20162017201820192020202120222023192917361200128713131208851",
            "컨퍼런스": "Neural Information Processing Systems (NIPS), 2016",
            "학술 문서": "R-fcn: Object detection via region-based fully convolutional networksJ Dai, Y Li, K He, J Sun - Advances in neural information processing systems, 20166959회 인용 관련 학술자료 전체 10개의 버전 R-fcn: Object detection via region-based fully convolutional networks. arXiv 2016*J Dai, Y Li, K He, J Sun - arXiv preprint arXiv:1605.06409, 2016114회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "R-FCN: Object Detection via Region-based Fully Convolutional Networks",
        "year": null
    },
    "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/6/8",
            "설명": "Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves ~90% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.",
            "저널": "arXiv preprint arXiv:1706.02677",
            "저자": "Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, Kaiming He",
            "전체 인용횟수": "3569회 인용201720182019202020212022202355289488603739727647",
            "학술 문서": "Accurate, large minibatch sgd: Training imagenet in 1 hourP Goyal, P Dollár, R Girshick, P Noordhuis… - arXiv preprint arXiv:1706.02677, 20173548회 인용 관련 학술자료 전체 9개의 버전 Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv 2017P Goyal, P Dollár, R Girshick, P Noordhuis… - arXiv preprint arXiv:1706.02677, 201986회 인용 관련 학술자료 Accurate, large minibatch sgd: Training imagenet in 1 hourG Priya, D Piotr, N Pieter, W Lukasz, K Aapo, T Andrew… - arXiv preprint arXiv:1706.02677, 201730회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour",
        "year": null
    },
    "Masked Autoencoders Are Scalable Vision Learners": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2022",
            "설명": "This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, eg, 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: eg, a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.",
            "저자": "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick",
            "전체 인용횟수": "3525회 인용202120222023249932478",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2022",
            "학술 문서": "Masked autoencoders are scalable vision learnersK He, X Chen, S Xie, Y Li, P Dollár, R Girshick - Proceedings of the IEEE/CVF conference on computer …, 20223525회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Masked Autoencoders Are Scalable Vision Learners",
        "year": null
    },
    "Group Normalization": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/3/22",
            "설명": "Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems---BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code.",
            "저자": "Yuxin Wu, Kaiming He",
            "전체 인용횟수": "3421회 인용20182019202020212022202357317599791901734",
            "컨퍼런스": "European Conference on Computer Vision (ECCV), 2018",
            "학술 문서": "Group normalizationY Wu, K He - Proceedings of the European conference on computer …, 20183421회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Group Normalization",
        "year": null
    },
    "Exploring Simple Siamese Representation Learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021",
            "설명": "Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These models maximize the similarity between two augmentations of one image, subject to certain conditions for avoiding collapsing solutions. In this paper, we report surprising empirical results that simple Siamese networks can learn meaningful representations even using none of the following:(i) negative sample pairs,(ii) large batches,(iii) momentum encoders. Our experiments show that collapsing solutions do exist for the loss and structure, but a stop-gradient operation plays an essential role in preventing collapsing. We provide a hypothesis on the implication of stop-gradient, and further show proof-of-concept experiments verifying it. Our\" SimSiam\" method achieves competitive results on ImageNet and downstream tasks. We hope this simple baseline will motivate people to rethink the roles of Siamese architectures for unsupervised representation learning. Code is made available.(https://github. com/facebookresearch/simsiam)",
            "저자": "Xinlei Chen, Kaiming He",
            "전체 인용횟수": "2931회 인용20212022202337111591383",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2021",
            "학술 문서": "Exploring simple siamese representation learningX Chen, K He - Proceedings of the IEEE/CVF conference on computer …, 20212931회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Exploring Simple Siamese Representation Learning",
        "year": null
    },
    "Convolutional Neural Networks at Constrained Time Cost": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "Though recent advanced convolutional neural networks (CNNs) have been improving the image recognition accuracy, the models are getting more complex and time-consuming. For real-world applications in industrial and commercial scenarios, engineers and developers are often faced with the requirement of constrained time budget. In this paper, we investigate the accuracy of CNNs under constrained time cost. Under this constraint, the designs of the network architectures should exhibit as trade-offs among the factors like depth, numbers of filters, filter sizes, etc. With a series of controlled comparisons, we progressively modify a baseline model while preserving its time complexity. This is also helpful for understanding the importance of the factors in network designs. We present an architecture that achieves very competitive accuracy in the ImageNet dataset (11.8% top-5 error, 10-view test), yet is 20% faster than``AlexNet''(16.0% top-5 error, 10-view test).",
            "저자": "Kaiming He, Jian Sun",
            "전체 인용횟수": "2892회 인용19881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202318343533354269548077968010485481412714920252628473733295580159245294310295211",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2015",
            "학술 문서": "Convolutional neural networks at constrained time costK He, J Sun - Proceedings of the IEEE conference on computer …, 20152892회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Convolutional Neural Networks at Constrained Time Cost",
        "year": null
    },
    "SlowFast Networks for Video Recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "We present SlowFast networks for video recognition. Our model involves (i) a Slow pathway, operating at low frame rate, to capture spatial semantics, and (ii) a Fast pathway, operating at high frame rate, to capture motion at fine temporal resolution. The Fast pathway can be made very lightweight by reducing its channel capacity, yet can learn useful temporal information for video recognition. Our models achieve strong performance for both action classification and detection in video, and large improvements are pin-pointed as contributions by our SlowFast concept. We report state-of-the-art accuracy on major video recognition benchmarks, Kinetics, Charades and AVA. Code has been made available at: https://github. com/facebookresearch/SlowFast.",
            "저자": "Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, Kaiming He",
            "전체 인용횟수": "2761회 인용2019202020212022202366295593851942",
            "컨퍼런스": "International Conference on Computer Vision (ICCV), 2019",
            "학술 문서": "Slowfast networks for video recognitionC Feichtenhofer, H Fan, J Malik, K He - Proceedings of the IEEE/CVF international conference …, 20192755회 인용 관련 학술자료 전체 10개의 버전 Slowfast networks for video recognition*F Christoph, F Haoqi, H Kaiming - Proceedings of the IEEE international conference on …, 201926회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "SlowFast Networks for Video Recognition",
        "year": null
    },
    "Improved Baselines with Momentum Contrastive Learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020/3/9",
            "설명": "Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.",
            "저널": "arXiv preprint arXiv:2003.04297",
            "저자": "Xinlei Chen, Haoqi Fan, Ross Girshick, Kaiming He",
            "전체 인용횟수": "2696회 인용202020212022202311151310051055",
            "학술 문서": "Improved baselines with momentum contrastive learningX Chen, H Fan, R Girshick, K He - arXiv preprint arXiv:2003.04297, 20202678회 인용 관련 학술자료 전체 3개의 버전 Improved baselines with momentum contrastive learning. arXiv 2020X Chen, H Fan, R Girshick, K He - arXiv preprint arXiv:2003.04297, 200383회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Improved Baselines with Momentum Contrastive Learning",
        "year": null
    },
    "An Empirical Study of Training Self-Supervised Vision Transformers": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021",
            "저자": "Xinlei Chen, Saining Xie, Kaiming He",
            "전체 인용횟수": "1811회 인용202120222023107738954",
            "컨퍼런스": "International Conference on Computer Vision (ICCV), 2021",
            "학술 문서": "Multiscale vision transformers*H Fan, B Xiong, K Mangalam, Y Li, Z Yan, J Malik… - Proceedings of the IEEE/CVF international conference …, 20211811회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
        "year": null
    },
    "Instance-aware Semantic Segmentation via Multi-task Network Cascades": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "Semantic segmentation research has recently witnessed rapid progress, but many leading methods are unable to identify object instances. In this paper, we present Multi-task Network Cascades for instance-aware semantic segmentation. Our model consists of three networks, respectively differentiating instances, estimating masks, and categorizing objects. These networks form a cascaded structure, and are designed to share their convolutional features. We develop an algorithm for the nontrivial end-to-end training of this causal, cascaded structure. Our solution is a clean, single-step training framework and can be generalized to cascades that have more stages. We demonstrate state-of-the-art instance-aware semantic segmentation accuracy on PASCAL VOC. Meanwhile, our method takes only 360ms testing an image using VGG-16, which is two orders of magnitude faster than previous systems for this challenging problem. As a by product, our method also achieves compelling object detection results which surpass the competitive Fast/Faster R-CNN systems. The method described in this paper is the foundation of our submissions to the MS COCO 2015 segmentation competition, where we won the 1st place.",
            "저자": "Jifeng Dai, Kaiming He, Jian Sun",
            "전체 인용횟수": "1577회 인용201520162017201820192020202120222023554170185267286269183135",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2016",
            "학술 문서": "Instance-aware semantic segmentation via multi-task network cascadesJ Dai, K He, J Sun - Proceedings of the IEEE conference on computer …, 20161577회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Instance-aware Semantic Segmentation via Multi-task Network Cascades",
        "year": null
    },
    "Exploring the Limits of Weakly Supervised Pretraining": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/5/2",
            "설명": "State-of-the-art visual perception models for a wide range of tasks rely on supervised pretraining. ImageNet classification is the de facto pretraining task for these models. Yet, ImageNet is now nearly ten years old and is by modern standards\" small\". Even so, relatively little is known about the behavior of pretraining with datasets that are multiple orders of magnitude larger. The reasons are obvious: such datasets are difficult to collect and annotate. In this paper, we present a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images. Our experiments demonstrate that training for large-scale hashtag prediction leads to excellent results. We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4%(97.6% top-5). We also perform extensive experiments that provide novel empirical data on the relationship between large-scale pretraining and transfer learning performance.",
            "저자": "Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, Laurens van der Maaten",
            "전체 인용횟수": "1379회 인용20182019202020212022202330162258313339270",
            "컨퍼런스": "European Conference on Computer Vision (ECCV), 2018",
            "학술 문서": "Exploring the limits of weakly supervised pretrainingD Mahajan, R Girshick, V Ramanathan, K He, M Paluri… - Proceedings of the European conference on computer …, 20181379회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Exploring the Limits of Weakly Supervised Pretraining",
        "year": null
    },
    "Designing Network Design Spaces": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020",
            "설명": "In this work, we present a new network design paradigm. Our goal is to help advance the understanding of network design and discover design principles that generalize across settings. Instead of focusing on designing individual network instances, we design network design spaces that parametrize populations of networks. The overall process is analogous to classic manual design of networks, but elevated to the design space level. Using our methodology we explore the structure aspect of network design and arrive at a low-dimensional design space consisting of simple, regular networks that we call RegNet. The core insight of the RegNet parametrization is surprisingly simple: widths and depths of good networks can be explained by a quantized linear function. We analyze the RegNet design space and arrive at interesting findings that do not match the current practice of network design. The RegNet design space provides simple and fast networks that work well across a wide range of flop regimes. Under comparable training settings and flops, the RegNet models outperform the popular EfficientNet models while being up to 5x faster on GPUs.",
            "저자": "Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollár",
            "전체 인용횟수": "1346회 인용202020212022202352292486501",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2020",
            "학술 문서": "Designing network design spacesI Radosavovic, RP Kosaraju, R Girshick, K He, P Dollár - Proceedings of the IEEE/CVF conference on computer …, 20201336회 인용 관련 학술자료 전체 11개의 버전 Designing network design spaces*R Ilija, RP Kosaraju, R Girshick, K He, P Dollar - Proceedings of the IEEE/CVF conference on computer …, 202021회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Designing Network Design Spaces",
        "year": null
    },
    "Panoptic Segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "We propose and study a task we name panoptic segmentation (PS). Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more unified view of image segmentation. For more analysis and up-to-date results, please check the arXiv version of the paper:\\smallhttps://arxiv. org/abs/1801.00868.",
            "저자": "Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, Piotr Dollár",
            "전체 인용횟수": "1305회 인용2018201920202021202220231881177282345393",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2019",
            "학술 문서": "Panoptic segmentationA Kirillov, K He, R Girshick, C Rother, P Dollár - Proceedings of the IEEE/CVF conference on computer …, 20191303회 인용 관련 학술자료 전체 8개의 버전 Panoptic segmentation*K Alexander, H Kaiming, R Carsten, D Piotr - Proceedings of the IEEE Conference on Computer …, 201914회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Panoptic Segmentation",
        "year": null
    },
    "Rethinking ImageNet Pre-training": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "We report competitive results on object detection and instance segmentation on the COCO dataset using standard models trained from random initialization. The results are no worse than their ImageNet pre-training counterparts even when using the hyper-parameters of the baseline system (Mask R-CNN) that were optimized for fine-tuning pre-trained models, with the sole exception of increasing the number of training iterations so the randomly initialized models may converge. Training from random initialization is surprisingly robust; our results hold even when:(i) using only 10% of the training data,(ii) for deeper and wider models, and (iii) for multiple tasks and metrics. Experiments show that ImageNet pre-training speeds up convergence early in training, but does not necessarily provide regularization or improve final target task accuracy. To push the envelope we demonstrate 50.9 AP on COCO object detection without using any external data---a result on par with the top COCO 2017 competition results that used ImageNet pre-training. These observations challenge the conventional wisdom of ImageNet pre-training for dependent tasks and we expect these discoveries will encourage people to rethink the current de facto paradigm ofpre-training and fine-tuning'in computer vision.",
            "저자": "Kaiming He, Ross Girshick, Piotr Dollár",
            "전체 인용횟수": "1153회 인용2018201920202021202220236108217280285250",
            "컨퍼런스": "International Conference on Computer Vision (ICCV), 2019",
            "학술 문서": "Rethinking imagenet pre-trainingK He, R Girshick, P Dollár - Proceedings of the IEEE/CVF International Conference …, 20191153회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Rethinking ImageNet Pre-training",
        "year": null
    },
    "BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/3/5",
            "설명": "Recent leading approaches to semantic segmentation rely on deep convolutional networks trained with human-annotated, pixel-level segmentation masks. Such pixel-accurate supervision demands expensive labeling effort and limits the performance of deep networks that usually benefit from more training data. In this paper, we propose a method that achieves competitive accuracy but only requires easily obtained bounding box annotations. The basic idea is to iterate between automatically generating region proposals and training convolutional networks. These two steps gradually recover segmentation masks for improving the networks, and vise versa. Our method, called\" BoxSup\", produces competitive results (eg, 62.0% mAP for validation) supervised by boxes only, on par with strong baselines (eg, 63.8% mAP) fully supervised by masks under the same setting. By leveraging a large amount of bounding boxes, BoxSup further yields state-of-the-art results on PASCAL VOC 2012 and PASCAL-CONTEXT.",
            "저자": "Jifeng Dai, Kaiming He, Jian Sun",
            "전체 인용횟수": "1153회 인용20142015201620172018201920202021202220233216890120149161177197144",
            "컨퍼런스": "International Conference on Computer Vision (ICCV), 2015",
            "학술 문서": "Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentationJ Dai, K He, J Sun - Proceedings of the IEEE international conference on …, 20151153회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation",
        "year": null
    },
    "Panoptic Feature Pyramid Networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/1/8",
            "설명": "The recently introduced panoptic segmentation task has renewed our community's interest in unifying the tasks of instance segmentation (for thing classes) and semantic segmentation (for stuff classes). However, current state-of-the-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation, without performing any shared computation. In this work, we aim to unify these methods at the architectural level, designing a single network for both tasks. Our approach is to endow Mask R-CNN, a popular instance segmentation method, with a semantic segmentation branch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for instance segmentation, but also yields a lightweight, top-performing method for semantic segmentation. In this work, we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and accurate baseline for both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and aid future research in panoptic segmentation.",
            "저자": "Alexander Kirillov, Ross Girshick, Kaiming He, Piotr Dollár",
            "전체 인용횟수": "1116회 인용2019202020212022202341139230337364",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2019",
            "학술 문서": "Panoptic feature pyramid networksA Kirillov, R Girshick, K He, P Dollár - Proceedings of the IEEE/CVF conference on computer …, 20191116회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Panoptic Feature Pyramid Networks",
        "year": null
    },
    "Deep Hough Voting for 3D Object Detection in Point Clouds": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/4/21",
            "설명": "Current 3D object detection methods are heavily influenced by 2D detectors. In order to leverage architectures in 2D detectors, they often convert 3D point clouds to regular grids (ie, to voxel grids or to bird's eye view images), or rely on detection in 2D images to propose 3D boxes. Few works have attempted to directly detect objects in point clouds. In this work, we return to first principles to construct a 3D detection pipeline for point cloud data and as generic as possible. However, due to the sparse nature of the data--samples from 2D manifolds in 3D space--we face a major challenge when directly predicting bounding box parameters from scene points: a 3D object centroid can be far from any surface point thus hard to regress accurately in one step. To address the challenge, we propose VoteNet, an end-to-end 3D object detection network based on a synergy of deep point set networks and Hough voting. Our model achieves state-of-the-art 3D detection on two large datasets of real 3D scans, ScanNet and SUN RGB-D with a simple design, compact model size and high efficiency. Remarkably, VoteNet outperforms previous methods by using purely geometric information without relying on color images.",
            "저자": "Charles R Qi, Or Litany, Kaiming He, Leonidas J Guibas",
            "전체 인용횟수": "1040회 인용2019202020212022202311113257344311",
            "컨퍼런스": "International Conference on Computer Vision (ICCV), 2019",
            "학술 문서": "Deep hough voting for 3d object detection in point cloudsCR Qi, O Litany, K He, LJ Guibas - proceedings of the IEEE/CVF International Conference …, 20191040회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep Hough Voting for 3D Object Detection in Point Clouds",
        "year": null
    },
    "ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "Large-scale data are of crucial importance for learning semantic segmentation models, but annotating per-pixel masks is a tedious and inefficient procedure. We note that for the topic of interactive image segmentation, scribbles are very widely used in academic research and commercial software, and are recognized as one of the most user-friendly ways of interacting. In this paper, we propose to use scribbles to annotate images, and develop an algorithm to train convolutional networks for semantic segmentation supervised by scribbles. Our algorithm is based on a graphical model that jointly propagates information from scribbles to unmarked pixels and learns network parameters. We present competitive object semantic segmentation results on the PASCAL VOC dataset by using scribbles as annotations. Scribbles are also favored for annotating stuff (eg, water, sky, grass) that has no well-defined shape, and our method shows excellent results on the PASCAL-CONTEXT dataset thanks to extra inexpensive scribble annotations.",
            "저자": "Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, Jian Sun",
            "전체 인용횟수": "1036회 인용2016201720182019202020212022202364874119135203224212",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2016",
            "학술 문서": "Scribblesup: Scribble-supervised convolutional networks for semantic segmentationD Lin, J Dai, J Jia, K He, J Sun - Proceedings of the IEEE conference on computer …, 20161036회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation",
        "year": null
    },
    "Is Faster R-CNN Doing Well for Pedestrian Detection?": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": " Detecting pedestrian has been arguably addressed as a special topic beyond general object detection. Although recent deep learning object detectors such as Fast/Faster R-CNN have shown excellent performance for general object detection, they have limited success for detecting pedestrian, and previous leading pedestrian detectors were in general hybrid methods combining hand-crafted and deep convolutional features. In this paper, we investigate issues involving Faster R-CNN for pedestrian detection. We discover that the Region Proposal Network (RPN) in Faster R-CNN indeed performs well as a stand-alone pedestrian detector, but surprisingly, the downstream classifier degrades the results. We argue that two reasons account for the unsatisfactory accuracy: (i) insufficient resolution of feature maps for handling small instances, and (ii) lack of any bootstrapping strategy for mining hard negative …",
            "저자": "Liliang Zhang, Liang Lin, Xiaodan Liang, Kaiming He",
            "전체 인용횟수": "1000회 인용20162017201820192020202120222023510416219714916712178",
            "컨퍼런스": "European Conference on Computer Vision (ECCV), 2016",
            "학술 문서": "Is faster R-CNN doing well for pedestrian detection?L Zhang, L Lin, X Liang, K He - Computer Vision–ECCV 2016: 14th European …, 20161000회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Is Faster R-CNN Doing Well for Pedestrian Detection?",
        "year": null
    },
    "Accelerating Very Deep Convolutional Networks for Classification and Detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "This paper aims to accelerate the test-time computation of convolutional neural networks (CNNs), especially very deep CNNs    that have substantially impacted the computer vision community. Unlike previous methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We develop an effective solution to the resulting nonlinear optimization problem without the need of stochastic gradient descent (SGD). More importantly, while previous methods mainly focus on optimizing one or two layers, our nonlinear method enables an asymmetric reconstruction that reduces the rapidly accumulated error when multiple (e.g.,   10) layers are approximated. For the widely used very deep VGG-16 model    , our method achieves a whole-model speedup of 4   with merely a 0.3 percent increase of top-5 error in ImageNet classification. Our 4    accelerated VGG …",
            "저널": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)",
            "저자": "Xiangyu Zhang, Jianhua Zou, Kaiming He, Jian Sun",
            "전체 인용횟수": "942회 인용2015201620172018201920202021202220237214288132140188193117",
            "학술 문서": "Accelerating very deep convolutional networks for classification and detectionX Zhang, J Zou, K He, J Sun - IEEE transactions on pattern analysis and machine …, 2015942회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Accelerating Very Deep Convolutional Networks for Classification and Detection",
        "year": null
    },
    "Feature Denoising for Improving Adversarial Robustness": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "Adversarial attacks to image classification systems present challenges to convolutional networks and opportunities for understanding them. This study suggests that adversarial perturbations on images lead to noise in the features constructed by these networks. Motivated by this observation, we develop new network architectures that increase adversarial robustness by performing feature denoising. Specifically, our networks contain blocks that denoise the features using non-local means or other filters; the entire networks are trained end-to-end. When combined with adversarial training, our feature denoising networks substantially improve the state-of-the-art in adversarial robustness in both white-box and black-box attack settings. On ImageNet, under 10-iteration PGD white-box attacks where prior art has 27.9% accuracy, our method achieves 55.7%; even under extreme 2000-iteration PGD white-box attacks, our method secures 42.6% accuracy. Our method was ranked first in Competition on Adversarial Attacks and Defenses (CAAD) 2018---it achieved 50.6% classification accuracy on a secret, ImageNet-like test dataset against 48 unknown attackers, surpassing the runner-up approach by 10%. Code is available at https://github. com/facebookresearch/ImageNet-Adversarial-Training.",
            "저자": "Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan Yuille, Kaiming He",
            "전체 인용횟수": "882회 인용201820192020202120222023473196213199193",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2019",
            "학술 문서": "Feature denoising for improving adversarial robustnessC Xie, Y Wu, L Maaten, AL Yuille, K He - Proceedings of the IEEE/CVF conference on computer …, 2019882회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Feature Denoising for Improving Adversarial Robustness",
        "year": null
    },
    "PointRend: Image Segmentation as Rendering": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020",
            "설명": "We present a new method for efficient high-quality image segmentation of objects and scenes. By analogizing classical computer graphics methods for efficient rendering with over-and undersampling challenges faced in pixel labeling tasks, we develop a unique perspective of image segmentation as a rendering problem. From this vantage, we present the PointRend (Point-based Rendering) neural network module: a module that performs point-based segmentation predictions at adaptively selected locations based on an iterative subdivision algorithm. PointRend can be flexibly applied to both instance and semantic segmentation tasks by building on top of existing state-of-the-art models. While many concrete implementations of the general idea are possible, we show that a simple design already achieves excellent results. Qualitatively, PointRend outputs crisp object boundaries in regions that are over-smoothed by previous methods. Quantitatively, PointRend yields significant gains on COCO and Cityscapes, for both instance and semantic segmentation. PointRend's efficiency enables output resolutions that are otherwise impractical in terms of memory or computation compared to existing approaches. Code has been made available at https://github. com/facebookresearch/detectron2/tree/master/projects/PointRend.",
            "저자": "Alexander Kirillov, Yuxin Wu, Kaiming He, Ross Girshick",
            "전체 인용횟수": "767회 인용202020212022202340176247301",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2020",
            "학술 문서": "Pointrend: Image segmentation as renderingA Kirillov, Y Wu, K He, R Girshick - Proceedings of the IEEE/CVF conference on computer …, 2020767회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "PointRend: Image Segmentation as Rendering",
        "year": null
    },
    "Optimized Product Quantization for Approximate Nearest Neighbor Search": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013",
            "설명": "Product quantization is an effective vector quantization approach to compactly encode high-dimensional vectors for fast approximate nearest neighbor (ANN) search. The essence of product quantization is to decompose the original high-dimensional space into the Cartesian product of a finite number of low-dimensional subspaces that are then quantized separately. Optimal space decomposition is important for the performance of ANN search, but still remains unaddressed. In this paper, we optimize product quantization by minimizing quantization distortions wrt the space decomposition and the quantization codebooks. We present two novel methods for optimization: a nonparametric method that alternatively solves two smaller sub-problems, and a parametric method that is guaranteed to achieve the optimal solution if the input data follows some Gaussian distribution. We show by experiments that our optimized approach substantially improves the accuracy of product quantization for ANN search.",
            "저자": "Tiezheng Ge, Kaiming He, Qifa Ke, Jian Sun",
            "전체 인용횟수": "756회 인용20122013201420152016201720182019202020212022202328294975667677768695100",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2013",
            "학술 문서": "Optimized product quantization for approximate nearest neighbor searchT Ge, K He, Q Ke, J Sun - Proceedings of the IEEE conference on computer …, 2013439회 인용 관련 학술자료 전체 14개의 버전 Optimized product quantization*T Ge, K He, Q Ke, J Sun - IEEE transactions on pattern analysis and machine …, 2013354회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Optimized Product Quantization for Approximate Nearest Neighbor Search",
        "year": null
    },
    "Detecting and Recognizing Human-Object Interactions": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "To understand the visual world, a machine must not only recognize individual object instances but also how they interact. Humans are often at the center of such interactions and detecting human-object interactions is an important practical and scientific problem. In this paper, we address the task of detecting (human, verb, object) triplets in challenging everyday photos. We propose a novel model that is driven by a human-centric approach. Our hypothesis is that the appearance of a person--their pose, clothing, action--is a powerful cue for localizing the objects they are interacting with. To exploit this cue, our model learns to predict an action-specific density over target object locations based on the appearance of a detected person. Our model also jointly learns to detect people and objects, and by fusing these predictions it efficiently infers interaction triplets in a clean, jointly trained end-to-end system we call InteractNet. We validate our approach on the recently introduced Verbs in COCO (V-COCO) and HICO-DET datasets, where we show quantitatively compelling results.",
            "저자": "Georgia Gkioxari, Ross Girshick, Piotr Dollár, Kaiming He",
            "전체 인용횟수": "609회 인용201720182019202020212022202363267121128133117",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2018",
            "학술 문서": "Detecting and recognizing human-object interactionsG Gkioxari, R Girshick, P Dollár, K He - Proceedings of the IEEE conference on computer …, 2018606회 인용 관련 학술자료 전체 12개의 버전 Detecting and recognizing human-object interactions*GGRGP Dollár, K He - CVPR, 20184회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Detecting and Recognizing Human-Object Interactions",
        "year": null
    },
    "Convolutional Feature Masking for Joint Object and Stuff Segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "The topic of semantic segmentation has witnessed considerable progress due to the powerful features learned by convolutional neural networks (CNNs). The current leading approaches for semantic segmentation exploit shape information by extracting CNN features from masked image regions. This strategy introduces artificial boundaries on the images and may impact the quality of the extracted features. Besides, the operations on the raw image domain require to compute thousands of networks on a single image, which is time-consuming. In this paper, we propose to exploit shape information via masking convolutional features. The proposal segments (eg, super-pixels) are treated as masks on the convolutional feature maps. The CNN features of segments are directly masked out from these maps and used to train classifiers for recognition. We further propose a joint method to handle objects and\" stuff\"(eg, grass, sky, water) in the same framework. State-of-the-art results are demonstrated on the challenging PASCAL VOC benchmarks, with a compelling computational speed.",
            "저자": "Jifeng Dai, Kaiming He, Jian Sun",
            "전체 인용횟수": "523회 인용20142015201620172018201920202021202220234276590766371443631",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2015",
            "학술 문서": "Convolutional feature masking for joint object and stuff segmentationJ Dai, K He, J Sun - Proceedings of the IEEE conference on computer …, 2015523회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Convolutional Feature Masking for Joint Object and Stuff Segmentation",
        "year": null
    },
    "Detectron": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "권": "6",
            "저자": "Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Dollár, Kaiming He",
            "전체 인용횟수": "520회 인용201820192020202120222023501341171146635",
            "학술 문서": "DetectronR Girshick, I Radosavovic, G Gkioxari, P Dollár, K He - 2018499회 인용 관련 학술자료 Detectron (2018)*R Girshick, I Radosavovic, G Gkioxari, P Dollár, K He - 201151회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Detectron",
        "year": null
    },
    "Instance-sensitive Fully Convolutional Networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": " Fully convolutional networks (FCNs) have been proven very successful for semantic segmentation, but the FCN outputs are unaware of object instances. In this paper, we develop FCNs that are capable of proposing instance-level segment candidates. In contrast to the previous FCN that generates one score map, our FCN is designed to compute a small set of instance-sensitive score maps, each of which is the outcome of a pixel-wise classifier of a relative position to instances. On top of these instance-sensitive score maps, a simple assembling module is able to output instance candidate at each position. In contrast to the recent DeepMask method for segmenting instances, our method does not have any high-dimensional layer related to the mask resolution, but instead exploits image local coherence for estimating instances. We present competitive results of instance segment proposal on both PASCAL …",
            "저자": "Jifeng Dai, Kaiming He, Yi Li, Shaoqing Ren, Jian Sun",
            "전체 인용횟수": "499회 인용20152016201720182019202020212022202329397110375756749",
            "컨퍼런스": "European Conference on Computer Vision (ECCV), 2016",
            "학술 문서": "Instance-sensitive fully convolutional networksJ Dai, K He, Y Li, S Ren, J Sun - Computer Vision–ECCV 2016: 14th European …, 2016499회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Instance-sensitive Fully Convolutional Networks",
        "year": null
    },
    "Long-Term Feature Banks for Detailed Video Understanding": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "To understand the world, we humans constantly need to relate the present to the past, and put events in context. In this paper, we enable existing video models to do the same. We propose a long-term feature bank--supportive information extracted over the entire span of a video--to augment state-of-the-art video models that otherwise would only view short clips of 2-5 seconds. Our experiments demonstrate that augmenting 3D convolutional networks with a long-term feature bank yields state-of-the-art results on three challenging video datasets: AVA, EPIC-Kitchens, and Charades. Code is available online.",
            "저자": "Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming He, Philipp Krähenbühl, Ross Girshick",
            "전체 인용횟수": "494회 인용201920202021202220233183140127110",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2019",
            "학술 문서": "Long-term feature banks for detailed video understandingCY Wu, C Feichtenhofer, H Fan, K He, P Krahenbuhl… - Proceedings of the IEEE/CVF Conference on Computer …, 2019494회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Long-Term Feature Banks for Detailed Video Understanding",
        "year": null
    },
    "Object Detection Networks on Convolutional Feature Maps": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "Most object detectors contain two important components: a feature extractor and an object classifier. The feature extractor has rapidly evolved with significant research efforts leading to better deep convolutional architectures. The object classifier, however, has not received much attention and many recent systems (like SPPnet and Fast/Faster R-CNN) use simple multi-layer perceptrons. This paper demonstrates that carefully designing deep networks for object classification is just as important. We experiment with region-wise classifier networks that use shared, region-independent convolutional features. We call them “Networks on Convolutional feature maps” (NoCs). We discover that aside from deep feature maps, a  deep  and  convolutional  per-region classifier is of particular importance for object detection, whereas latest superior image classification models (such as ResNets and GoogLeNets) do not directly …",
            "저널": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)",
            "저자": "Shaoqing Ren, Kaiming He, Ross Girshick, Xiangyu Zhang, Jian Sun",
            "전체 인용횟수": "491회 인용20152016201720182019202020212022202362335708583795647",
            "학술 문서": "Object detection networks on convolutional feature mapsS Ren, K He, R Girshick, X Zhang, J Sun - IEEE transactions on pattern analysis and machine …, 2016491회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Object Detection Networks on Convolutional Feature Maps",
        "year": null
    },
    "K-means hashing: An affinity-preserving quantization method for learning binary compact codes": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013",
            "설명": "In computer vision there has been increasing interest in learning hashing codes whose Hamming distance approximates the data similarity. The hashing functions play roles in both quantizing the vector space and generating similarity-preserving codes. Most existing hashing methods use hyper-planes (or kernelized hyper-planes) to quantize and encode. In this paper, we present a hashing method adopting the k-means quantization. We propose a novel Affinity-Preserving K-means algorithm which simultaneously performs k-means clustering and learns the binary indices of the quantized cells. The distance between the cells is approximated by the Hamming distance of the cell indices. We further generalize our algorithm to a product space for learning longer codes. Experiments show our method, named as K-means Hashing (KMH), outperforms various state-of-the-art hashing encoding methods.",
            "저자": "Kaiming He, Fang Wen, Jian Sun",
            "전체 인용횟수": "471회 인용20132014201520162017201820192020202120222023436545173605158381816",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2013",
            "학술 문서": "K-means hashing: An affinity-preserving quantization method for learning binary compact codesK He, F Wen, J Sun - Proceedings of the IEEE conference on computer …, 2013471회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "K-means hashing: An affinity-preserving quantization method for learning binary compact codes",
        "year": null
    },
    "Data Distillation: Towards Omni-Supervised Learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "We investigate omni-supervised learning, a special regime of semi-supervised learning in which the learner exploits all available labeled data plus internet-scale sources of unlabeled data. Omni-supervised learning is lower-bounded by performance on existing labeled datasets, offering the potential to surpass state-of-the-art fully supervised methods. To exploit the omni-supervised setting, we propose data distillation, a method that ensembles predictions from multiple transformations of unlabeled data, using a single model, to automatically generate new training annotations. We argue that visual recognition models have recently become accurate enough that it is now possible to apply classic ideas about self-training to challenging real-world data. Our experimental results show that in the cases of human keypoint detection and general object detection, state-of-the-art models trained with data distillation surpass the performance of using labeled data from the COCO dataset alone.",
            "저자": "Ilija Radosavovic, Piotr Dollár, Ross Girshick, Georgia Gkioxari, Kaiming He",
            "전체 인용횟수": "438회 인용201720182019202020212022202322766881187561",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2018",
            "학술 문서": "Data distillation: Towards omni-supervised learningI Radosavovic, P Dollár, R Girshick, G Gkioxari, K He - Proceedings of the IEEE conference on computer …, 2018438회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Data Distillation: Towards Omni-Supervised Learning",
        "year": null
    },
    "Statistics of Patch Offsets for Image Completion": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012",
            "게시자": "Springer Berlin/Heidelberg",
            "설명": " Image completion involves filling missing parts in images. In this paper we address this problem through the statistics of patch offsets. We observe that if we match similar patches in the image and obtain their offsets (relative positions), the statistics of these offsets are sparsely distributed. We further observe that a few dominant offsets provide reliable information for completing the image. With these offsets we fill the missing region by combining a stack of shifted images via optimization. A variety of experiments show that our method yields generally better results and is faster than existing state-of-the-art methods.",
            "저널": "European Conference on Computer Vision (ECCV), 2012",
            "저자": "Kaiming He, Jian Sun",
            "전체 인용횟수": "424회 인용20122013201420152016201720182019202020212022202321436443640465046434020",
            "학술 문서": "Statistics of patch offsets for image completionK He, J Sun - Computer Vision–ECCV 2012: 12th European …, 2012255회 인용 관련 학술자료 전체 8개의 버전 Image completion approaches using the statistics of similar patches*K He, J Sun - IEEE transactions on pattern analysis and machine …, 2014177회 인용 관련 학술자료 전체 7개의 버전 Image completion based on patch offset statistics*K He, J Sun - US Patent App. 14/297,530, 20142회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Statistics of Patch Offsets for Image Completion",
        "year": null
    },
    "Exploring Randomly Wired Neural Networks for Image Recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/4/2",
            "설명": "Neural networks for image recognition have evolved through extensive manual design from simple chain-like models to structures with multiple wiring paths. The success of ResNets and DenseNets is due in large part to their innovative wiring plans. Now, neural architecture search (NAS) studies are exploring the joint optimization of wiring and operation types, however, the space of possible wirings is constrained and still driven by manual design despite being searched. In this paper, we explore a more diverse set of connectivity patterns through the lens of randomly wired neural networks. To do this, we first define the concept of a stochastic network generator that encapsulates the entire network generation process. Encapsulation provides a unified view of NAS and randomly wired networks. Then, we use three classical random graph models to generate randomly wired graphs for networks. The results are surprising: several variants of these random generators yield network instances that have competitive accuracy on the ImageNet benchmark. These results suggest that new efforts focusing on designing better network generators may lead to new breakthroughs by exploring less constrained search spaces with more room for novel design.",
            "저자": "Saining Xie, Alexander Kirillov, Ross Girshick, Kaiming He",
            "전체 인용횟수": "397회 인용20192020202120222023571021117057",
            "컨퍼런스": "International Conference on Computer Vision (ICCV), 2019",
            "학술 문서": "Exploring randomly wired neural networks for image recognitionS Xie, A Kirillov, R Girshick, K He - Proceedings of the IEEE/CVF International Conference …, 2019397회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Exploring Randomly Wired Neural Networks for Image Recognition",
        "year": null
    },
    "A Global Sampling Method for Alpha Matting": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/6/20",
            "게시자": "IEEE",
            "설명": "Alpha matting refers to the problem of softly extracting the foreground from an image. Given a trimap (specifying known foreground/background and unknown pixels), a straightforward way to compute the alpha value is to sample some known foreground and background colors for each unknown pixel. Existing sampling-based matting methods often collect samples near the unknown pixels only. They fail if good samples cannot be found nearby. In this paper, we propose a global sampling method that uses all samples available in the image. Our global sample set avoids missing good samples. A simple but effective cost function is defined to tackle the ambiguity in the sample selection process. To handle the computational complexity introduced by the large number of samples, we pose the sampling task as a correspondence problem. The correspondence search is efficiently achieved by generalizing a randomized …",
            "저자": "Kaiming He, Christoph Rhemann, Carsten Rother, Xiaoou Tang, Jian Sun",
            "전체 인용횟수": "389회 인용2010201120122013201420152016201720182019202020212022202313162119324326184032404743",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2011",
            "학술 문서": "A global sampling method for alpha mattingK He, C Rhemann, C Rother, X Tang, J Sun - CVPR 2011, 2011382회 인용 관련 학술자료 전체 16개의 버전 Opacity measurement using a global pixel set*K He, J Sun, CCE Rother, X Tang - US Patent 8,855,411, 20149회 인용 관련 학술자료 전체 4개의 버전 \" A Global Sampling Method for Alpha Matting\"; Poster: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2011, Colorado Springs; 21.06. 2011-23.06. 2011; in:\" IEEE Computer Vision and Pattern Recognition\",(2011), 8 S.*K He, C Rhemann, C Rother, X Tang, J Sun"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A Global Sampling Method for Alpha Matting",
        "year": null
    },
    "TensorMask: A Foundation for Dense Object Segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/3/28",
            "설명": "Sliding-window object detectors that generate bounding-box object predictions over a dense, regular grid have advanced rapidly and proven popular. In contrast, modern instance segmentation approaches are dominated by methods that first detect object bounding boxes, and then crop and segment these regions, as popularized by Mask R-CNN. In this work, we investigate the paradigm of dense sliding-window instance segmentation, which is surprisingly under-explored. Our core observation is that this task is fundamentally different than other dense prediction tasks such as semantic segmentation or bounding-box object detection, as the output at every spatial location is itself a geometric structure with its own spatial dimensions. To formalize this, we treat dense instance segmentation as a prediction task over 4D tensors and present a general framework called TensorMask that explicitly captures this geometry and enables novel operators on 4D tensors. We demonstrate that the tensor view leads to large gains over baselines that ignore this structure, and leads to results comparable to Mask R-CNN. These promising results suggest that TensorMask can serve as a foundation for novel advances in dense mask prediction and a more complete understanding of the task. Code will be made available.",
            "저자": "Xinlei Chen, Ross Girshick, Kaiming He, Piotr Dollár",
            "전체 인용횟수": "361회 인용2019202020212022202365911310081",
            "컨퍼런스": "International Conference on Computer Vision (ICCV), 2019",
            "학술 문서": "Tensormask: A foundation for dense object segmentationX Chen, R Girshick, K He, P Dollár - Proceedings of the IEEE/CVF international conference …, 2019361회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "TensorMask: A Foundation for Dense Object Segmentation",
        "year": null
    },
    "Constant Time Weighted Median Filtering for Stereo Matching and Beyond": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013",
            "설명": "Despite the continuous advances in local stereo matching for years, most efforts are on developing robust cost computation and aggregation methods. Little attention has been seriously paid to the disparity refinement. In this work, we study weighted median filtering for disparity refinement. We discover that with this refinement, even the simple box filter aggregation achieves comparable accuracy with various sophisticated aggregation methods (with the same refinement). This is due to the nice weighted median filtering properties of removing outlier error while respecting edges/structures. This reveals that the previously overlooked refinement can be at least as crucial as aggregation. We also develop the first constant time algorithm for the previously time-consuming weighted median filter. This makes the simple combination\" box aggregation+ weighted median\" an attractive solution in practice for both speed and accuracy. As a byproduct, the fast weighted median filtering unleashes its potential in other applications that were hampered by high complexities. We show its superiority in various applications such as depth upsampling, clip-art JPEG artifact removal, and image stylization.",
            "저자": "Ziyang Ma, Kaiming He, Yichen Wei, Jian Sun, Enhua Wu",
            "전체 인용횟수": "351회 인용201420152016201720182019202020212022202310466053403434301622",
            "컨퍼런스": "International Conference on Computer Vision (ICCV), 2013",
            "학술 문서": "Constant time weighted median filtering for stereo matching and beyondZ Ma, K He, Y Wei, J Sun, E Wu - Proceedings of the IEEE International Conference on …, 2013351회 인용 관련 학술자료 전체 18개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Constant Time Weighted Median Filtering for Stereo Matching and Beyond",
        "year": null
    },
    "Exploring Plain Vision Transformer Backbones for Object Detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2022",
            "설명": "We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection. This design enables the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for fine-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is sufficient to build a simple feature pyramid from a single-scale feature map (without the common FPN design) and (ii) it is sufficient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain ViT backbones pre-trained as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the previous leading methods that were all based on hierarchical backbones, reaching up to 61.3 AP on the COCO dataset using only ImageNet-1K pre-training. We hope our …",
            "저자": "Yanghao Li, Hanzi Mao, Ross Girshick, Kaiming He",
            "전체 인용횟수": "342회 인용202120222023174263",
            "컨퍼런스": "European Conference on Computer Vision (ECCV), 2022",
            "학술 문서": "Exploring plain vision transformer backbones for object detectionY Li, H Mao, R Girshick, K He - European Conference on Computer Vision, 2022342회 인용 관련 학술자료 전체 5개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Exploring Plain Vision Transformer Backbones for Object Detection",
        "year": null
    },
    "Learning to Segment Every Thing": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "Most methods for object instance segmentation require all training examples to be labeled with segmentation masks. This requirement makes it expensive to annotate new categories and has restricted instance segmentation models to~ 100 well-annotated classes. The goal of this paper is to propose a new partially supervised training paradigm, together with a novel weight transfer function, that enables training instance segmentation models on a large set of categories all of which have box annotations, but only a small fraction of which have mask annotations. These contributions allow us to train Mask R-CNN to detect and segment 3000 visual concepts using box annotations from the Visual Genome dataset and mask annotations from the 80 classes in the COCO dataset. We evaluate our approach in a controlled study on the COCO dataset. This work is a first step towards instance segmentation models that have broad comprehension of the visual world.",
            "저자": "Ronghang Hu, Piotr Dollár, Kaiming He, Trevor Darrell, Ross Girshick",
            "전체 인용횟수": "324회 인용201820192020202120222023256880705128",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2018",
            "학술 문서": "Learning to segment every thingR Hu, P Dollár, K He, T Darrell, R Girshick - Proceedings of the IEEE conference on computer …, 2018324회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning to Segment Every Thing",
        "year": null
    },
    "Efficient and Accurate Approximations of Nonlinear Convolutional Networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "This paper aims to accelerate the test-time computation of deep convolutional neural networks (CNNs). Unlike existing methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We minimize the reconstruction error of the nonlinear responses, subject to a low-rank constraint which helps to reduce the complexity of filters. We develop an effective solution to this constrained nonlinear optimization problem. An algorithm is also presented for reducing the accumulated error when multiple layers are approximated. A whole-model speedup ratio of 4x is demonstrated on a large network trained for ImageNet, while the top-5 error rate is only increased by 0.9%. Our accelerated model has a comparably fast speed as the\" AlexNet\", but is 4.7% more accurate.",
            "저자": "Xiangyu Zhang, Jianhua Zou, Xiang Ming, Kaiming He, Jian Sun",
            "전체 인용횟수": "318회 인용2014201520162017201820192020202120222023172329464556443131",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2015",
            "학술 문서": "Efficient and accurate approximations of nonlinear convolutional networksX Zhang, J Zou, X Ming, K He, J Sun - Proceedings of the IEEE Conference on Computer …, 2015318회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Efficient and Accurate Approximations of Nonlinear Convolutional Networks",
        "year": null
    },
    "Fast Guided Filter": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/5/5",
            "설명": "The guided filter is a technique for edge-aware image filtering. Because of its nice visual quality, fast speed, and ease of implementation, the guided filter has witnessed various applications in real products, such as image editing apps in phones and stereo reconstruction, and has been included in official MATLAB and OpenCV. In this note, we remind that the guided filter can be simply sped up from O(N) time to O(N/s^2) time for a subsampling ratio s. In a variety of applications, this leads to a speedup of >10x with almost no visible degradation. We hope this acceleration will improve performance of current applications and further popularize this filter. Code is released.",
            "저널": "arXiv preprint arXiv:1505.00996",
            "저자": "Kaiming He, Jian Sun",
            "전체 인용횟수": "261회 인용20152016201720182019202020212022202341930244742293232",
            "학술 문서": "Fast guided filterK He, J Sun - arXiv preprint arXiv:1505.00996, 2015261회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Fast Guided Filter",
        "year": null
    },
    "Masked Autoencoders As Spatiotemporal Learners": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2022/5/18",
            "설명": "This paper studies a conceptually simple extension of Masked Autoencoders (MAE) to spatiotemporal representation learning from videos. We randomly mask out spacetime patches in videos and learn an autoencoder to reconstruct them in pixels. Interestingly, we show that our MAE method can learn strong representations with almost no inductive bias on spacetime (only except for patch and positional embeddings), and spacetime-agnostic random masking performs the best. We observe that the optimal masking ratio is as high as 90%(vs. 75% on images), supporting the hypothesis that this ratio is related to information redundancy of the data. A high masking ratio leads to a large speedup, eg,> 4x in wall-clock time or even more. We report competitive results on several challenging video datasets using vanilla Vision Transformers. We observe that MAE can outperform supervised pre-training by large margins. We further report encouraging results of training on real-world, uncurated Instagram data. Our study suggests that the general framework of masked autoencoding (BERT, MAE, etc.) can be a unified methodology for representation learning with minimal domain knowledge.",
            "저자": "Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, Kaiming He",
            "전체 인용횟수": "232회 인용202120222023135195",
            "컨퍼런스": "Neural Information Processing Systems (NeurIPS), 2022",
            "학술 문서": "Masked autoencoders as spatiotemporal learnersC Feichtenhofer, Y Li, K He - Advances in neural information processing systems, 2022232회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Masked Autoencoders As Spatiotemporal Learners",
        "year": null
    },
    "Fast Matting using Large Kernel Matting Laplacian Matrices": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/6/13",
            "게시자": "IEEE",
            "설명": "Image matting is of great importance in both computer vision and graphics applications. Most existing state-of-the-art techniques rely on large sparse matrices such as the matting Laplacian. However, solving these linear systems is often time-consuming, which is unfavored for the user interaction. In this paper, we propose a fast method for high quality matting. We first derive an efficient algorithm to solve a large kernel matting Laplacian. A large kernel propagates information more quickly and may improve the matte quality. To further reduce running time, we also use adaptive kernel sizes by a KD-tree trimap segmentation technique. A variety of experiments show that our algorithm provides high quality results and is 5 to 20 times faster than previous methods.",
            "저자": "Kaiming He, Jian Sun, Xiaoou Tang",
            "전체 인용횟수": "221회 인용2010201120122013201420152016201720182019202020212022202331223202213201581629161311",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2010",
            "학술 문서": "Fast matting using large kernel matting laplacian matricesK He, J Sun, X Tang - 2010 IEEE Computer Society Conference on Computer …, 2010210회 인용 관련 학술자료 전체 11개의 버전 Variable kernel size image matting*J Sun, K He - US Patent 8,625,888, 201412회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Fast Matting using Large Kernel Matting Laplacian Matrices",
        "year": null
    },
    "A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021",
            "설명": "We present a large-scale study on unsupervised spatiotemporal representation learning from videos. With a unified perspective on four recent image-based frameworks, we study a simple objective that can easily generalize all these methods to space-time. Our objective encourages temporally-persistent features in the same video, and in spite of its simplicity, it works surprisingly well across:(i) different unsupervised frameworks,(ii) pre-training datasets,(iii) downstream datasets, and (iv) backbone architectures. We draw a series of intriguing observations from this study, eg, we discover that encouraging long-spanned persistency can be effective even if the timespan is 60 seconds. In addition to state-of-the-art results in multiple benchmarks, we report a few promising cases in which unsupervised pre-training can outperform its supervised counterpart. Code will be made available at https://github. com/facebookresearch/SlowFast.",
            "저자": "Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, Kaiming He",
            "전체 인용횟수": "196회 인용20202021202220232208886",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2021",
            "학술 문서": "A large-scale study on unsupervised spatiotemporal representation learningC Feichtenhofer, H Fan, B Xiong, R Girshick, K He - Proceedings of the IEEE/CVF Conference on Computer …, 2021196회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning",
        "year": null
    },
    "Transitive Invariance for Self-supervised Visual Representation Learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/8/9",
            "설명": "Learning visual representations with self-supervised learning has become popular in computer vision. The idea is to design auxiliary tasks where labels are free to obtain. Most of these tasks end up providing data to learn specific kinds of invariance useful for recognition. In this paper, we propose to exploit different self-supervised approaches to learn representations invariant to (i) inter-instance variations (two objects in the same class should have similar features) and (ii) intra-instance variations (viewpoint, pose, deformations, illumination, etc). Instead of combining two approaches with multi-task learning, we argue to organize and reason the data with multiple variations. Specifically, we propose to generate a graph with millions of objects mined from hundreds of thousands of videos. The objects are connected by two types of edges which correspond to two types of invariance:\" different instances but a similar viewpoint and category\" and\" different viewpoints of the same instance\". By applying simple transitivity on the graph with these edges, we can obtain pairs of images exhibiting richer visual invariance. We use this data to train a Triplet-Siamese network with VGG16 as the base architecture and apply the learned representations to different recognition tasks. For object detection, we achieve 63.2% mAP on PASCAL VOC 2007 using Fast R-CNN (compare to 67.3% with ImageNet pre-training). For the challenging COCO dataset, our method is surprisingly close (23.5%) to the ImageNet-supervised counterpart (24.4%) using the Faster R-CNN framework. We also show that our network can perform significantly better than the ImageNet network …",
            "저자": "Xiaolong Wang, Kaiming He, Abhinav Gupta",
            "전체 인용횟수": "195회 인용20172018201920202021202220231283051293125",
            "컨퍼런스": "International Conference on Computer Vision (ICCV), 2017",
            "학술 문서": "Transitive invariance for self-supervised visual representation learningX Wang, K He, A Gupta - Proceedings of the IEEE international conference on …, 2017195회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Transitive Invariance for Self-supervised Visual Representation Learning",
        "year": null
    },
    "Computing Nearest-Neighbor Fields via Propagation-Assisted KD-Trees": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012",
            "설명": "Matching patches between two images, also known as computing nearest-neighbor fields, has been proven a useful technique in various computer vision/graphics algorithms. But this is a computationally challenging nearest-neighbor search task, because both the query set and the candidate set are of image size. In this paper, we propose Propagation-Assisted KD-Trees to quickly compute an approximate solution. We develop a novel propagation search method for kd-trees. In this method the tree nodes checked by each query are propagated from the nearby queries. This method not only avoids the time-consuming backtracking in traditional tree methods, but is more accurate. Experiments on public data show that our method is 10-20 times faster than the PatchMatch method [4] at the same accuracy, or reduces its error by 70% at the same running time. Our method is also 2-5 times faster and is more accurate …",
            "저자": "Kaiming He, Jian Sun",
            "전체 인용횟수": "174회 인용20122013201420152016201720182019202020212022202311119241919211592261",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2012",
            "학술 문서": "Computing nearest-neighbor fields via propagation-assisted kd-treesK He, J Sun - 2012 IEEE Conference on Computer Vision and …, 2012174회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Computing Nearest-Neighbor Fields via Propagation-Assisted KD-Trees",
        "year": null
    },
    "Rectangling Panoramic Images via Warping": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/7/1",
            "게시자": "ACM",
            "권": "32",
            "설명": "Stitched panoramic images mostly have irregular boundaries. Artists and common users generally prefer rectangular boundaries, which can be obtained through cropping or image completion techniques. In this paper, we present a content-aware warping algorithm that generates rectangular images from stitched panoramic images. Our algorithm consists of two steps. The first local step is mesh-free and preliminarily warps the image into a rectangle. With a grid mesh placed on this rectangle, the second global step optimizes the mesh to preserve shapes and straight lines. In various experiments we demonstrate that the results of our approach are often visually plausible, and the introduced distortion is often unnoticeable.",
            "저널": "ACM Transactions on Graphics (TOG)",
            "저자": "Kaiming He, Huiwen Chang, Jian Sun",
            "전체 인용횟수": "132회 인용201320142015201620172018201920202021202220231213138141720141614",
            "페이지": "79",
            "학술 문서": "Rectangling panoramic images via warpingK He, H Chang, J Sun - ACM Transactions on Graphics (TOG), 201391회 인용 관련 학술자료 전체 5개의 버전 Creation of Rectangular Images from Input Images*K He, H Chang, J Sun - US Patent App. 14/079,310, 201541회 인용 관련 학술자료 전체 2개의 버전 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Rectangling Panoramic Images via Warping",
        "year": null
    },
    "Graph Structure of Neural Networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020/7",
            "설명": "Neural networks are often represented as graphs of connections between neurons. However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance. Here we systematically investigate how does the graph structure of neural networks affect their predictive performance. To this end, we develop a novel graph-based representation of neural networks called relational graph, where layers of neural network computation correspond to rounds of message exchange along the graph structure. Using this representation we show that:(1) a “sweet spot” of relational graphs leads to neural networks with significantly improved predictive performance;(2) neural network’s performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph;(3) our findings are consistent across many different tasks and datasets;(4) the sweet spot can be identified efficiently;(5) top-performing neural networks have graph structure surprisingly similar to those of real biological neural networks. Our work opens new directions for the design of neural architectures and the understanding on neural networks in general.",
            "저자": "Jiaxuan You, Jure Leskovec, Kaiming He, Saining Xie",
            "전체 인용횟수": "130회 인용2019202020212022202319403841",
            "컨퍼런스": "International Conference on Machine Learning (ICML), 2020",
            "학술 문서": "Graph structure of neural networksJ You, J Leskovec, K He, S Xie - International Conference on Machine Learning, 2020130회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Graph Structure of Neural Networks",
        "year": null
    },
    "Benchmarking Detection Transfer Learning with Vision Transformers": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021/11/22",
            "설명": "Object detection is a central downstream task used to test if pre-trained network parameters confer benefits, such as improved accuracy or training speed. The complexity of object detection methods can make this benchmarking non-trivial when new architectures, such as Vision Transformer (ViT) models, arrive. These difficulties (e.g., architectural incompatibility, slow training, high memory consumption, unknown training formulae, etc.) have prevented recent studies from benchmarking detection transfer learning with standard ViT models. In this paper, we present training techniques that overcome these challenges, enabling the use of standard ViT models as the backbone of Mask R-CNN. These tools facilitate the primary goal of our study: we compare five ViT initializations, including recent state-of-the-art self-supervised learning methods, supervised initialization, and a strong random initialization baseline. Our results show that recent masking-based unsupervised learning methods may, for the first time, provide convincing transfer learning improvements on COCO, increasing box AP up to 4% (absolute) over supervised and prior self-supervised pre-training methods. Moreover, these masking-based initializations scale better, with the improvement growing as model size increases.",
            "저널": "arXiv preprint arXiv:2111.11429",
            "저자": "Yanghao Li, Saining Xie, Xinlei Chen, Piotr Dollar, Kaiming He, Ross Girshick",
            "전체 인용횟수": "105회 인용202220234461",
            "학술 문서": "Benchmarking detection transfer learning with vision transformersY Li, S Xie, X Chen, P Dollar, K He, R Girshick - arXiv preprint arXiv:2111.11429, 2021105회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Benchmarking Detection Transfer Learning with Vision Transformers",
        "year": null
    },
    "A Multigrid Method for Efficiently Training Video Models": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020",
            "설명": "Training competitive deep video models is an order of magnitude slower than training their counterpart image models. Slow training causes long research cycles, which hinders progress in video understanding research. Following standard practice for training image models, video model training has used a fixed mini-batch shape: a specific number of clips, frames, and spatial size. However, what is the optimal shape? High resolution models perform well, but train slowly. Low resolution models train faster, but are less accurate. Inspired by multigrid methods in numerical optimization, we propose to use variable mini-batch shapes with different spatial-temporal resolutions that are varied according to a schedule. The different shapes arise from resampling the training data on multiple sampling grids. Training is accelerated by scaling up the mini-batch size and learning rate when shrinking the other dimensions. We empirically demonstrate a general and robust grid schedule that yields a significant out-of-the-box training speedup without a loss in accuracy for different models (I3D, non-local, SlowFast), datasets (Kinetics, Something-Something, Charades), and training settings (with and without pre-training, 128 GPUs or 1 GPU). As an illustrative example, the proposed multigrid method trains a ResNet-50 SlowFast network 4.5 x faster (wall-clock time, same hardware) while also improving accuracy (+ 0.8% absolute) on Kinetics-400 compared to baseline training. Code is available online.",
            "저자": "Chao-Yuan Wu, Ross Girshick, Kaiming He, Christoph Feichtenhofer, Philipp Krähenbühl",
            "전체 인용횟수": "104회 인용20192020202120222023113413019",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2020",
            "학술 문서": "A multigrid method for efficiently training video modelsCY Wu, R Girshick, K He, C Feichtenhofer… - Proceedings of the IEEE/CVF Conference on Computer …, 2020104회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A Multigrid Method for Efficiently Training Video Models",
        "year": null
    },
    "Sparse Projections for High-Dimensional Binary Codes": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "This paper addresses the problem of learning long binary codes from high-dimensional data. We observe that two key challenges arise while learning and using long binary codes:(1) lack of an effective regularizer for the learned high-dimensional mapping and (2) high computational cost for computing long codes. In this paper, we overcome both these problems by introducing a sparsity encouraging regularizer that reduces the effective number of parameters involved in the learned projection operator. This regularizer not only reduces overfitting but, due to the sparse nature of the projection matrix, also leads to a dramatic reduction in the computational cost. To evaluate the effectiveness of our method, we analyze its performance on the problems of nearest neighbour search, image retrieval and image classification. Experiments on a number of challenging datasets show that our method leads to better accuracy than dense projections (ITQ and LSH) with the same code lengths, and meanwhile is over an order of magnitude faster. Furthermore, our method is also more accurate and faster than other recently proposed methods for speeding up high-dimensional binary encoding.",
            "저자": "Yan Xia, Kaiming He, Pushmeet Kohli, Jian Sun",
            "전체 인용횟수": "102회 인용2015201620172018201920202021202220235132811519684",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2015",
            "학술 문서": "Sparse projections for high-dimensional binary codesY Xia, K He, P Kohli, J Sun - Proceedings of the IEEE conference on computer …, 2015102회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Sparse Projections for High-Dimensional Binary Codes",
        "year": null
    },
    "Joint Inverted Indexing": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013",
            "설명": "Inverted indexing is a popular non-exhaustive solution to large scale search. An inverted file is built by a quantizer such as k-means or a tree structure. It has been found that multiple inverted files, obtained by multiple independent random quantizers, are able to achieve practically good recall and speed. Instead of computing the multiple quantizers independently, we present a method that creates them jointly. Our method jointly optimizes all codewords in all quantizers. Then it assigns these codewords to the quantizers. In experiments this method shows significant improvement over various existing methods that use multiple independent quantizers. On the one-billion set of SIFT vectors, our method is faster and more accurate than a recent state-of-the-art inverted indexing method.",
            "저자": "Yan Xia, Kaiming He, Fang Wen, Jian Sun",
            "전체 인용횟수": "87회 인용2014201520162017201820192020202120222023711101411109224",
            "컨퍼런스": "International Conference on Computer Vision (ICCV), 2013",
            "학술 문서": "Joint inverted indexingY Xia, K He, F Wen, J Sun - Proceedings of the IEEE International Conference on …, 201387회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Joint Inverted Indexing",
        "year": null
    },
    "Are Labels Necessary for Neural Architecture Search?": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020/3/26",
            "설명": " Existing neural network architectures in computer vision—whether designed by humans or by machines—were typically found using both images and their associated labels. In this paper, we ask the question: can we find high-quality neural architectures using only images, but no human-annotated labels? To answer this question, we first define a new setup called Unsupervised Neural Architecture Search (UnNAS). We then conduct two sets of experiments. In sample-based experiments, we train a large number (500) of diverse architectures with either supervised or unsupervised objectives, and find that the architecture rankings produced with and without labels are highly correlated. In search-based experiments, we run a well-established NAS algorithm (DARTS) using various unsupervised objectives, and report that the architectures searched without labels can be competitive to their counterparts …",
            "저자": "Chenxi Liu, Piotr Dollár, Kaiming He, Ross Girshick, Alan Yuille, Saining Xie",
            "전체 인용횟수": "79회 인용20202021202220235272918",
            "컨퍼런스": "European Conference on Computer Vision (ECCV), 2020",
            "학술 문서": "Are labels necessary for neural architecture search?C Liu, P Dollár, K He, R Girshick, A Yuille, S Xie - Computer Vision–ECCV 2020: 16th European …, 202079회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Are Labels Necessary for Neural Architecture Search?",
        "year": null
    },
    "Graph Cuts for Supervised Binary Coding": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "설명": " Learning short binary codes is challenged by the inherent discrete nature of the problem. The graph cuts algorithm is a well-studied discrete label assignment solution in computer vision, but has not yet been applied to solve the binary coding problems. This is partially because it was unclear how to use it to learn the encoding (hashing) functions for out-of-sample generalization. In this paper, we formulate supervised binary coding as a single optimization problem that involves both the encoding functions and the binary label assignment. Then we apply the graph cuts algorithm to address the discrete optimization problem involved, with no continuous relaxation. This method, named as Graph Cuts Coding (GCC), shows competitive results in various datasets.",
            "저자": "Tiezheng Ge, Kaiming He, Jian Sun",
            "전체 인용횟수": "75회 인용201520162017201820192020202120222023515161389531",
            "컨퍼런스": "European Conference on Computer Vision (ECCV), 2014",
            "학술 문서": "Graph cuts for supervised binary codingT Ge, K He, J Sun - Computer Vision–ECCV 2014: 13th European …, 201475회 인용 관련 학술자료 전체 5개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Graph Cuts for Supervised Binary Coding",
        "year": null
    },
    "Scaling Language-Image Pre-training via Masking": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2023",
            "설명": "We present Fast Language-Image Pre-training (FLIP), a simple and more efficient method for training CLIP. Our method randomly masks out and removes a large portion of image patches during training. Masking allows us to learn from more image-text pairs given the same wall-clock time and contrast more samples per iteration with similar memory footprint. It leads to a favorable trade-off between accuracy and training time. In our experiments on 400 million image-text pairs, FLIP improves both accuracy and speed over the no-masking baseline. On a large diversity of downstream tasks, FLIP dominantly outperforms the CLIP counterparts trained on the same data. Facilitated by the speedup, we explore the scaling behavior of increasing the model size, data size, or training length, and report encouraging results and comparisons. We hope that our work will foster future research on scaling vision-language learning.",
            "저자": "Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, Kaiming He",
            "전체 인용횟수": "70회 인용20222023169",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2023",
            "학술 문서": "Scaling language-image pre-training via maskingY Li, H Fan, R Hu, C Feichtenhofer, K He - Proceedings of the IEEE/CVF Conference on Computer …, 202370회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Scaling Language-Image Pre-training via Masking",
        "year": null
    },
    "GLoMo: unsupervised learning of transferable relational graphs": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "Modern deep transfer learning approaches have mainly focused on learning generic feature vectors from one task that are transferable to other tasks, such as word embeddings in language and pretrained convolutional features in vision. However, these approaches usually transfer unary features and largely ignore more structured graphical representations. This work explores the possibility of learning generic latent relational graphs that capture dependencies between pairs of data units (eg, words or pixels) from large-scale unlabeled data and transferring the graphs to downstream tasks. Our proposed transfer learning framework improves performance on various tasks including question answering, natural language inference, sentiment analysis, and image classification. We also show that the learned graphs are generic enough to be transferred to different embeddings on which the graphs have not been trained (including GloVe embeddings, ELMo embeddings, and task-specific RNN hidden units), or embedding-free units such as image pixels.",
            "저자": "Zhilin Yang, Jake Zhao, Bhuwan Dhingra, Kaiming He, William W Cohen, Ruslan R Salakhutdinov, Yann LeCun",
            "전체 인용횟수": "66회 인용201820192020202120222023216161596",
            "컨퍼런스": "Neural Information Processing Systems (NIPS), 2018",
            "학술 문서": "Glomo: Unsupervisedly learned relational graphs as transferable representations*Z Yang, J Zhao, B Dhingra, K He, WW Cohen… - arXiv preprint arXiv:1806.05662, 201840회 인용 관련 학술자료 전체 4개의 버전 Glomo: Unsupervised learning of transferable relational graphsZ Yang, J Zhao, B Dhingra, K He, WW Cohen… - Advances in Neural Information Processing Systems, 201826회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "GLoMo: unsupervised learning of transferable relational graphs",
        "year": null
    },
    "Product Sparse Coding": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "설명": "Sparse coding is a widely involved technique in computer vision. However, the expensive computational cost can hamper its applications, typically when the codebook size must be limited due to concerns on running time. In this paper, we study a special case of sparse coding in which the codebook is a Cartesian product of two subcodebooks. We present algorithms to decompose this sparse coding problem into smaller subproblems, which can be separately solved. Our solution, named as Product Sparse Coding (PSC), reduces the time complexity from O (K) to O (sqrt (K)) in the codebook size K. In practice, this can be 20-100x faster than standard sparse coding. In experiments we demonstrate the efficiency and quality of this method on the applications of image classification and image retrieval.",
            "저자": "Tiezheng Ge, Kaiming He, Jian Sun",
            "전체 인용횟수": "29회 인용2014201520162017201820192020202120222023259261112",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2014",
            "학술 문서": "Product sparse codingT Ge, K He, J Sun - Proceedings of the IEEE Conference on Computer …, 201429회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Product Sparse Coding",
        "year": null
    },
    "Content-Aware Rotation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013",
            "설명": "We present an image editing tool called Content-Aware Rotation. Casually shot photos can appear tilted, and are often corrected by rotation and cropping. This trivial solution may remove desired content and hurt image integrity. Instead of doing rigid rotation, we propose a warping method that creates the perception of rotation and avoids cropping. Human vision studies suggest that the perception of rotation is mainly due to horizontal/vertical lines. We design an optimization-based method that preserves the rotation of horizontal/vertical lines, maintains the completeness of the image content, and reduces the warping distortion. An efficient algorithm is developed to address the challenging optimization. We demonstrate our content-aware rotation method on a variety of practical cases.",
            "저자": "Kaiming He, Huiwen Chang, and Jian Sun",
            "전체 인용횟수": "29회 인용20142015201620172018201920202021202220231312832513",
            "컨퍼런스": "International Conference on Computer Vision (ICCV), 2013",
            "학술 문서": "Content-aware rotationK He, H Chang, J Sun - Proceedings of the IEEE International Conference on …, 201326회 인용 관련 학술자료 전체 11개의 버전 Content-aware image rotation*K He, H Chang, J Sun - US Patent 9,466,092, 20163회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Content-Aware Rotation",
        "year": null
    },
    "A Geodesic-Preserving Method for Image Warping": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "The manipulation of panoramic/wide-angle images is usually achieved via image warping. Though various techniques have been developed for preserving shapes and straight lines for warping, these are not sufficient for panoramic/wide-angle images. The image projections will turn the straight lines into curved\" geodesic lines\", and it is fundamentally impossible to keep all these lines straight. In this work, we propose a geodesic-preserving method for content-aware image warping. An energy term is introduced to preserve the geodesic appearance of the geodesic lines, and can be used with shape-preserving terms. Our method is demonstrated in various applications, including rectangling panoramas, resizing panoramic/wide-angle images, and wide-angle image manipulation. An extension to ellipse preservation for general images is also presented.",
            "저자": "Dongping Li, Kaiming He, Jian Sun, Kun Zhou",
            "전체 인용횟수": "28회 인용20152016201720182019202020212022202313244356",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2015",
            "학술 문서": "A geodesic-preserving method for image warpingD Li, K He, J Sun, K Zhou - Proceedings of the IEEE Conference on Computer …, 201528회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A Geodesic-Preserving Method for Image Warping",
        "year": null
    },
    "Faster R-CNN: Towards real-time object detection with region proposal networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP) using 300 proposals per image. Code is available at https://github. com/ShaoqingRen/faster_rcnn.",
            "저자": "Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun",
            "전체 인용횟수": "67089회 인용2016201720182019202020212022202369123324971809410151128971420311938",
            "컨퍼런스": "Advances in neural information processing systems",
            "페이지": "91-99",
            "학술 문서": "Faster r-cnn: Towards real-time object detection with region proposal networksS Ren, K He, R Girshick, J Sun - Advances in neural information processing systems, 201567038회 인용 관련 학술자료 전체 42개의 버전 Faster r-cnn: Towards real-time object detection with region proposal networks. arXiv 2015*S Ren, K He, R Girshick, J Sun - arXiv preprint arXiv:1506.01497, 2015381회 인용 관련 학술자료 Faster r-cnn: Towards real-time object detection with region proposal networks [J]S Tren, K He, R Girshick - Advances in neural information processing systems, 20159회 인용 관련 학술자료 Faster R 鄄 CNN: towards real 鄄 time object detection with region proposal networks*S REN, K HE, R GIRSHICK - Proc of the 28th International Conference on Neural …, 20157회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Faster R-CNN: Towards real-time object detection with region proposal networks",
        "year": null
    },
    "Microsoft coco: Common objects in context": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "게시자": "Springer International Publishing",
            "설명": " We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and …",
            "저자": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, C Lawrence Zitnick",
            "전체 인용횟수": "41249회 인용2015201620172018201920202021202220232495341150225940105464747190959403",
            "컨퍼런스": "Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13",
            "페이지": "740-755",
            "학술 문서": "Microsoft coco: Common objects in contextTY Lin, M Maire, S Belongie, J Hays, P Perona… - Computer Vision–ECCV 2014: 13th European …, 201441161회 인용 관련 학술자료 전체 22개의 버전 & Zitnick, CL (2014, September). Microsoft coco: Common objects in context*TY Lin, M Maire, S Belongie, J Hays, P Perona… - European conference on computer vision128회 인용 관련 학술자료 ar and CL Zitnick “Microsoft COCO: Common objects in context*TY Lin, M Maire, S Belongie, J Hays, P Perona… - Proc. Eur. Conf. Comput. Vis.(ECCV), 201432회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Microsoft coco: Common objects in context",
        "year": null
    },
    "You only look once: Unified, real-time object detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.",
            "저자": "Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi",
            "전체 인용횟수": "41426회 인용20162017201820192020202120222023163986246346336041821198708709",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "779-788",
            "학술 문서": "You only look once: Unified, real-time object detectionJ Redmon, S Divvala, R Girshick, A Farhadi - Proceedings of the IEEE conference on computer …, 201641036회 인용 관련 학술자료 전체 48개의 버전 You only look once: Unified, real-time object detection. arXiv 2015*J Redmon, S Divvala, R Girshick, A Farhadi - arXiv preprint arXiv:1506.02640, 2020231회 인용 관련 학술자료 Yolo: Real-time object detection*J Redmon, A Farhadi - Pjreddie. com, 2018215회 인용 관련 학술자료 You only look once: unified, real-time object detection (2015)*J Redmon, S Divvala, R Girshick, A Farhadi - arXiv preprint arXiv:1506.02640, 2015208회 인용 관련 학술자료 andAli Farhadi,“*J Redmon, S Divvala, R Girshick - You only look once: Unified, real-timeobject detection,” …, 201638회 인용 관련 학술자료 You Only Look Once: Unified, Real-Time Object Detection, University of Washington*J Redmon, S Divvala, R Girshick, A Farhadi - Allen Institute for AI, Facebook AI Research, 201616회 인용 관련 학술자료 YOLO: Real-Time Object Detection. 2018*J Redmon, A Farhadi - URL https://pjreddie. com/darknet/yolov2, 201814회 인용 관련 학술자료 Unified, Real-Time Object Detection Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi University of Washington*YOL Once - Allen Institute for AI, Facebook AI Research, 201610회 인용 관련 학술자료 Ross Girshick a Ali Farhadi*J Redmon, S Divvala - You Only Look Once: Unified, Real-Time Object …, 20167회 인용 관련 학술자료 Joseph and Farhadi,“YOLO: Real-Time Object Detection,” arXiv, 2018*A Redmon3회 인용 관련 학술자료 You only look once: Unified, real-time object detection. to appear in CVPR 2016, abs/1506.02640, 2015*J Redmon, SK Divvala, RB Girshick, A Farhadi - URL http://arxiv. org/abs/1506.026402회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "You only look once: Unified, real-time object detection",
        "year": null
    },
    "Rich feature hierarchies for accurate object detection and semantic segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "설명": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012---achieving a mAP of 53.3%. Our approach combines two key insights:(1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www. cs. berkeley. edu/~ rbg/rcnn.",
            "저자": "Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik",
            "전체 인용횟수": "33858회 인용201420152016201720182019202020212022202323989614782212317142564629537754874466",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "580-587",
            "학술 문서": "Rich feature hierarchies for accurate object detection and semantic segmentationR Girshick, J Donahue, T Darrell, J Malik - Proceedings of the IEEE conference on computer …, 201433858회 인용 관련 학술자료 전체 43개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
        "year": null
    },
    "Fast R-CNN": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++(using Caffe) and is available under the open-source MIT License at https://github. com/rbgirshick/fast-rcnn.",
            "저자": "Ross Girshick",
            "전체 인용횟수": "30819회 인용2015201620172018201920202021202220231046251460264039774619539356204882",
            "컨퍼런스": "Proceedings of the IEEE International Conference on Computer Vision",
            "페이지": "1440-1448",
            "학술 문서": "Fast r-cnnR Girshick - Proceedings of the IEEE international conference on …, 201530789회 인용 관련 학술자료 전체 39개의 버전 Fast r-cnn. arXiv*R Girshick - arXiv preprint arXiv:1504.08083, 2015102회 인용 관련 학술자료 Fast R-CNN. computer science*R Girshick - Proceedings of the 2015 IEEE International …, 201511회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Fast R-CNN",
        "year": null
    },
    "Focal loss for dense object detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors.",
            "저자": "Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollár",
            "전체 인용횟수": "25726회 인용20182019202020212022202353718743457553469997122",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "2980-2988",
            "학술 문서": "Focal loss for dense object detectionTY Lin, P Goyal, R Girshick, K He, P Dollár - Proceedings of the IEEE international conference on …, 201725633회 인용 관련 학술자료 전체 23개의 버전 Focal loss for dense object detection. arXiv 2017*TY Lin, P Goyal, R Girshick, K He, P Dollár - arXiv preprint arXiv:1708.02002, 2002115회 인용 관련 학술자료 Focal loss for dense object detection. arXiv*TY Lin, P Goyal, R Girshick, K He, P Dollár - arXiv preprint arXiv:1708.02002, 201780회 인용 관련 학술자료 Piotr Doll ar. Focal loss for dense object detection*TY Lin, P Goyal, R Girshick, K He - Proceedings of the IEEE International Conference on …, 201761회 인용 관련 학술자료 Focal loss for dense object detection. CoRR abs/1708.02002 (2017)*T Lin, P Goyal, RB Girshick, K He, P Dollár - arXiv preprint arXiv:1708.02002, 201755회 인용 관련 학술자료 Focal Loss for Dense Object Detection (RetinaNet)*TY Lin, P Goyal, R Girshick, K He, P Dollar - Proceedings of the IEEE International Conference on …, 201711회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Focal loss for dense object detection",
        "year": null
    },
    "Feature pyramid networks for object detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.",
            "저자": "Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie",
            "전체 인용횟수": "22675회 인용201720182019202020212022202311263720083170484759945755",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "2117-2125",
            "학술 문서": "Feature pyramid networks for object detectionTY Lin, P Dollár, R Girshick, K He, B Hariharan… - Proceedings of the IEEE conference on computer …, 201722526회 인용 관련 학술자료 전체 22개의 버전 Y, dollár p, girshick r, he k, hariharan b, belongie s. feature pyramid networks for object detection*TY Lin - Proceedings-30th IEEE Conference on Computer …, 2017202회 인용 관련 학술자료 Feature pyramid networks for object detection. arXiv 2016*TY Lin, P Dollár, R Girshick, K He, B Hariharan… - arXiv preprint arXiv:1612.03144121회 인용 관련 학술자료 Feature pyramid networks for object detection. 2016*TY Lin, P Dollár, R Girshick, K He, B Hariharan… - arXiv preprint arXiv:1612.03144, 201759회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Feature pyramid networks for object detection",
        "year": null
    },
    "Caffe: Convolutional architecture for fast feature embedding": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/11/3",
            "도서": "Proceedings of the 22nd ACM international conference on Multimedia",
            "설명": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community …",
            "저자": "Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, Trevor Darrell",
            "전체 인용횟수": "17894회 인용20142015201620172018201920202021202220231841121241934123359265517351204855487",
            "페이지": "675-678",
            "학술 문서": "Caffe: Convolutional architecture for fast feature embeddingY Jia, E Shelhamer, J Donahue, S Karayev, J Long… - Proceedings of the 22nd ACM international conference …, 201417843회 인용 관련 학술자료 전체 22개의 버전 Caffe: An open source convolutional architecture for fast feature embedding (2013)*Y Jia, E Shelhamer - 201374회 인용 관련 학술자료 Caffe: An open source convolutional architecture for fast feature embedding. h ttp*Y Jia - caffe. berkeleyvision. org, 201313회 인용 관련 학술자료 Caffe: convolutional architecture for fast feature embedding (2014). arXiv preprint*Y Jia, E Shelhamer, J Donahue, S Karayev, J Long… - arXiv preprint arXiv:1408.509310회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Caffe: Convolutional architecture for fast feature embedding",
        "year": null
    },
    "Object detection with discriminatively trained part-based models": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/9/22",
            "게시자": "IEEE",
            "권": "32",
            "설명": "We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI--SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Pedro F Felzenszwalb, Ross B Girshick, David McAllester, Deva Ramanan",
            "전체 인용횟수": "12659회 인용20102011201220132014201520162017201820192020202120222023105332500829112113031384132712061093978903764506",
            "페이지": "1627-1645",
            "학술 문서": "Object detection with discriminatively trained part-based modelsPF Felzenszwalb, RB Girshick, D McAllester… - IEEE transactions on pattern analysis and machine …, 200912659회 인용 관련 학술자료 전체 49개의 버전 ",
            "호": "9"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Object detection with discriminatively trained part-based models",
        "year": null
    },
    "Aggregated residual transformations for deep neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call\" cardinality\"(the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.",
            "저자": "Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He",
            "전체 인용횟수": "11007회 인용201720182019202020212022202313456610871684244127262283",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "1492-1500",
            "학술 문서": "Aggregated residual transformations for deep neural networksS Xie, R Girshick, P Dollár, Z Tu, K He - Proceedings of the IEEE conference on computer …, 201711007회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Aggregated residual transformations for deep neural networks",
        "year": null
    },
    "Non-local neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our non-local models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks. Code will be made available.",
            "저자": "Xiaolong Wang, Ross Girshick, Abhinav Gupta, Kaiming He",
            "전체 인용횟수": "9430회 인용2018201920202021202220231106491373227426552315",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "7794-7803",
            "학술 문서": "Non-local neural networksX Wang, R Girshick, A Gupta, K He - Proceedings of the IEEE conference on computer …, 20189430회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Non-local neural networks",
        "year": null
    },
    "Momentum contrast for unsupervised visual representation learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020",
            "설명": "We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.",
            "저자": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick",
            "전체 인용횟수": "9318회 인용2020202120222023455176332463797",
            "컨퍼런스": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "페이지": "9729-9738",
            "학술 문서": "Momentum contrast for unsupervised visual representation learningK He, H Fan, Y Wu, S Xie, R Girshick - Proceedings of the IEEE/CVF conference on computer …, 20209318회 인용 관련 학술자료 전체 19개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Momentum contrast for unsupervised visual representation learning",
        "year": null
    },
    "Accurate, large minibatch sgd: Training imagenet in 1 hour": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/6/8",
            "설명": "Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves ~90% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.",
            "저널": "arXiv preprint arXiv:1706.02677",
            "저자": "Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, Kaiming He",
            "전체 인용횟수": "3558회 인용201720182019202020212022202355288488600736727643",
            "학술 문서": "Accurate, large minibatch sgd: Training imagenet in 1 hourP Goyal, P Dollár, R Girshick, P Noordhuis… - arXiv preprint arXiv:1706.02677, 20173548회 인용 관련 학술자료 전체 9개의 버전 Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv 2017P Goyal, P Dollár, R Girshick, P Noordhuis… - arXiv preprint arXiv:1706.02677, 201986회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Accurate, large minibatch sgd: Training imagenet in 1 hour",
        "year": null
    },
    "Masked autoencoders are scalable vision learners": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2022",
            "설명": "This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, eg, 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: eg, a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.",
            "저자": "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick",
            "전체 인용횟수": "3525회 인용202120222023249932478",
            "컨퍼런스": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "페이지": "16000-16009",
            "학술 문서": "Masked autoencoders are scalable vision learnersK He, X Chen, S Xie, Y Li, P Dollár, R Girshick - Proceedings of the IEEE/CVF conference on computer …, 20223525회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Masked autoencoders are scalable vision learners",
        "year": null
    },
    "Unsupervised deep embedding for clustering analysis": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/6/11",
            "게시자": "PMLR",
            "설명": "Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.",
            "저자": "Junyuan Xie, Ross Girshick, Ali Farhadi",
            "전체 인용횟수": "2864회 인용20162017201820192020202120222023961150289476586654616",
            "컨퍼런스": "International conference on machine learning",
            "페이지": "478-487",
            "학술 문서": "Unsupervised deep embedding for clustering analysisJ Xie, R Girshick, A Farhadi - International conference on machine learning, 20162864회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Unsupervised deep embedding for clustering analysis",
        "year": null
    },
    "Improved baselines with momentum contrastive learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020/3/9",
            "설명": "Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.",
            "저널": "arXiv preprint arXiv:2003.04297",
            "저자": "Xinlei Chen, Haoqi Fan, Ross Girshick, Kaiming He",
            "전체 인용횟수": "2696회 인용202020212022202311151310051055",
            "학술 문서": "Improved baselines with momentum contrastive learningX Chen, H Fan, R Girshick, K He - arXiv preprint arXiv:2003.04297, 20202678회 인용 관련 학술자료 전체 3개의 버전 Improved baselines with momentum contrastive learning. arXiv 2020X Chen, H Fan, R Girshick, K He - arXiv preprint arXiv:2003.04297, 200383회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Improved baselines with momentum contrastive learning",
        "year": null
    },
    "Training region-based object detectors with online hard example mining": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "The field of object detection has made significant advances riding on the wave of region-based ConvNets, but their training procedure still includes many heuristics and hyperparameters that are costly to tune. We present a simple yet surprisingly effective online hard example mining (OHEM) algorithm for training region-based ConvNet detectors. Our motivation is the same as it has always been--detection datasets contain an overwhelming number of easy examples and a small number of hard examples. Automatic selection of these hard examples can make training more effective and efficient. OHEM is a simple and intuitive algorithm that eliminates several heuristics and hyperparameters in common use. But more importantly, it yields consistent and significant boosts in detection performance on benchmarks like PASCAL VOC 2007 and 2012. Its effectiveness increases as datasets become larger and more difficult, as demonstrated by the results on the MS COCO dataset. Moreover, combined with complementary advances in the field, OHEM leads to state-of-the-art results of 78.9% and 76.3% mAP on PASCAL VOC 2007 and 2012 respectively.",
            "저자": "Abhinav Shrivastava, Abhinav Gupta, Ross Girshick",
            "전체 인용횟수": "2561회 인용2016201720182019202020212022202316120266402461448457362",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "761-769",
            "학술 문서": "Training region-based object detectors with online hard example miningA Shrivastava, A Gupta, R Girshick - Proceedings of the IEEE conference on computer …, 20162561회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Training region-based object detectors with online hard example mining",
        "year": null
    },
    "Detectron2": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/10",
            "권": "2",
            "저자": "Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, Ross Girshick",
            "전체 인용횟수": "2345회 인용2020202120222023229607780713",
            "페이지": "2019",
            "학술 문서": "Detectron2Y Wu, A Kirillov, F Massa, WY Lo, R Girshick - 20192218회 인용 관련 학술자료 Detectron2. 2019*Y Wu, A Kirillov, F Massa, WY Lo, R Girshick - 2019299회 인용 관련 학술자료 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Detectron2",
        "year": null
    },
    "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "When building artificial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress and discover short-comings. Existing benchmarks for visual question answer-ing can help, but have strong biases that models can exploit to correctly answer questions without reasoning. They also conflate multiple sources of error, making it hard to pinpoint model weaknesses. We present a diagnostic dataset that tests a range of visual reasoning abilities. It contains minimal biases and has detailed annotations describing the kind of reasoning each question requires. We use this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations.",
            "저자": "Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, Ross Girshick",
            "전체 인용횟수": "2067회 인용201720182019202020212022202351179242318384458424",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "2901-2910",
            "학술 문서": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoningJ Johnson, B Hariharan, L Van Der Maaten, L Fei-Fei… - Proceedings of the IEEE conference on computer …, 20172067회 인용 관련 학술자료 전체 18개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning",
        "year": null
    },
    "Hypercolumns for object segmentation and fine-grained localization": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "Recognition algorithms based on convolutional networks (CNNs) typically use the output of the last layer as feature representation. However, the information in this layer may be too coarse to allow precise localization. On the contrary, earlier layers may be precise in localization but will not capture semantics. To get the best of both worlds, we define the hypercolumn at a pixel as the vector of activations of all CNN units above that pixel. Using hypercolumns as pixel descriptors, we show results on three fine-grained localization tasks: simultaneous detection and segmentation [20], where we improve state-of-the-art from 49.7 mean AP^ r [20] to 59.0, keypoint localization, where we get a 3.3 point boost over [19] and part labeling, where we show a 6.6 point gain over a strong baseline.",
            "저자": "Bharath Hariharan, Pablo Arbeláez, Ross Girshick, Jitendra Malik",
            "전체 인용횟수": "1855회 인용2014201520162017201820192020202120222023653182293290270227192166110",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "447-456",
            "학술 문서": "Hypercolumns for object segmentation and fine-grained localizationB Hariharan, P Arbeláez, R Girshick, J Malik - Proceedings of the IEEE conference on computer …, 20151855회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Hypercolumns for object segmentation and fine-grained localization",
        "year": null
    },
    "Learning rich features from RGB-D images for object detection and segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "게시자": "Springer International Publishing",
            "설명": " In this paper we study the problem of object detection for RGB-D images using semantically rich image and depth features. We propose a new geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity. We demonstrate that this geocentric embedding works better than using raw depth images for learning feature representations with convolutional neural networks. Our final object detection system achieves an average precision of 37.3%, which is a 56% relative improvement over existing methods. We then focus on the task of instance segmentation where we label pixels belonging to object instances found by our detector. For this task, we propose a decision forest approach that classifies pixels in the detection window as foreground or background using a family of unary and binary tests that query shape and …",
            "저자": "Saurabh Gupta, Ross Girshick, Pablo Arbeláez, Jitendra Malik",
            "전체 인용횟수": "1844회 인용20142015201620172018201920202021202220238103188256279237221214170133",
            "컨퍼런스": "Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII 13",
            "페이지": "345-360",
            "학술 문서": "Learning rich features from RGB-D images for object detection and segmentationS Gupta, R Girshick, P Arbeláez, J Malik - Computer Vision–ECCV 2014: 13th European …, 20141844회 인용 관련 학술자료 전체 17개의 버전 Learning Rich Features from RGB-D Images for Object Detection and Segmentation: Supplementary Material*S Gupta, R Girshick, P Arbeláez, J Malik관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning rich features from RGB-D images for object detection and segmentation",
        "year": null
    },
    "Simultaneous detection and segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "게시자": "Springer International Publishing",
            "설명": " We aim to detect all instances of a category in an image and, for each instance, mark the pixels that belong to it. We call this task Simultaneous Detection and Segmentation (SDS). Unlike classical bounding box detection, SDS requires a segmentation and not just a box. Unlike classical semantic segmentation, we require individual object instances. We build on recent work that uses convolutional neural networks to classify category-independent region proposals (R-CNN [16]), introducing a novel architecture tailored for SDS. We then use category-specific, top-down figure-ground predictions to refine our bottom-up proposals. We show a 7 point boost (16% relative) over our baselines on SDS, a 5 point boost (10% relative) over state-of-the-art on semantic segmentation, and state-of-the-art performance in object detection. Finally, we provide diagnostic tools that unpack performance and provide directions …",
            "저자": "Bharath Hariharan, Pablo Arbeláez, Ross Girshick, Jitendra Malik",
            "전체 인용횟수": "1527회 인용201420152016201720182019202020212022202314112155194196188156187157126",
            "컨퍼런스": "Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII 13",
            "페이지": "297-312",
            "학술 문서": "Simultaneous detection and segmentationB Hariharan, P Arbeláez, R Girshick, J Malik - Computer Vision–ECCV 2014: 13th European …, 20141527회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Simultaneous detection and segmentation",
        "year": null
    },
    "Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "It is well known that contextual and multi-scale representations are important for accurate visual recognition. In this paper we present the Inside-Outside Net (ION), an object detector that exploits information both inside and outside the region of interest. Contextual information outside the region of interest is integrated using spatial recurrent neural networks. Inside, we use skip pooling to extract information at multiple scales and levels of abstraction. Through extensive experiments we evaluate the design space and provide readers with an overview of what tricks of the trade are important. ION improves state-of-the-art on PASCAL VOC 2012 object detection from 73.9% to 77.9% mAP. On the new and more challenging MS COCO dataset, we improve state-of-the-art from 19.7% to 33.1% mAP. In the 2015 MS COCO Detection Challenge, our ION model won\" Best Student Entry\" and finished 3rd place overall. As intuition suggests, our detection results provide strong evidence that context and multi-scale representations improve small object detection.",
            "저자": "Sean Bell, C Lawrence Zitnick, Kavita Bala, Ross Girshick",
            "전체 인용횟수": "1491회 인용2016201720182019202020212022202344137192266255233207126",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "2874-2883",
            "학술 문서": "Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networksS Bell, CL Zitnick, K Bala, R Girshick - Proceedings of the IEEE conference on computer …, 20161491회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks",
        "year": null
    },
    "Part-based R-CNNs for fine-grained category detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "게시자": "Springer International Publishing",
            "설명": " Semantic part localization can facilitate fine-grained categorization by explicitly isolating subtle appearance differences associated with specific object parts. Methods for pose-normalized representations have been proposed, but generally presume bounding box annotations at test time due to the difficulty of object detection. We propose a model for fine-grained categorization that overcomes these limitations by leveraging deep convolutional features computed on bottom-up region proposals. Our method learns whole-object and part detectors, enforces learned geometric constraints between them, and predicts a fine-grained category from a pose-normalized representation. Experiments on the Caltech-UCSD bird dataset confirm that our method outperforms state-of-the-art fine-grained categorization methods in an end-to-end evaluation without requiring a bounding box at test time.",
            "저자": "Ning Zhang, Jeff Donahue, Ross Girshick, Trevor Darrell",
            "전체 인용횟수": "1380회 인용20142015201620172018201920202021202220231083131146155162156184182141",
            "컨퍼런스": "Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13",
            "페이지": "834-849",
            "학술 문서": "Part-based R-CNNs for fine-grained category detectionN Zhang, J Donahue, R Girshick, T Darrell - Computer Vision–ECCV 2014: 13th European …, 20141380회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Part-based R-CNNs for fine-grained category detection",
        "year": null
    },
    "Exploring the limits of weakly supervised pretraining": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "State-of-the-art visual perception models for a wide range of tasks rely on supervised pretraining. ImageNet classification is the de facto pretraining task for these models. Yet, ImageNet is now nearly ten years old and is by modern standards\" small\". Even so, relatively little is known about the behavior of pretraining with datasets that are multiple orders of magnitude larger. The reasons are obvious: such datasets are difficult to collect and annotate. In this paper, we present a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images. Our experiments demonstrate that training for large-scale hashtag prediction leads to excellent results. We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4%(97.6% top-5). We also perform extensive experiments that provide novel empirical data on the relationship between large-scale pretraining and transfer learning performance.",
            "저자": "Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, Laurens Van Der Maaten",
            "전체 인용횟수": "1379회 인용20182019202020212022202330162258313339270",
            "컨퍼런스": "Proceedings of the European conference on computer vision (ECCV)",
            "페이지": "181-196",
            "학술 문서": "Exploring the limits of weakly supervised pretrainingD Mahajan, R Girshick, V Ramanathan, K He, M Paluri… - Proceedings of the European conference on computer …, 20181379회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Exploring the limits of weakly supervised pretraining",
        "year": null
    },
    "Designing network design spaces": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020",
            "설명": "In this work, we present a new network design paradigm. Our goal is to help advance the understanding of network design and discover design principles that generalize across settings. Instead of focusing on designing individual network instances, we design network design spaces that parametrize populations of networks. The overall process is analogous to classic manual design of networks, but elevated to the design space level. Using our methodology we explore the structure aspect of network design and arrive at a low-dimensional design space consisting of simple, regular networks that we call RegNet. The core insight of the RegNet parametrization is surprisingly simple: widths and depths of good networks can be explained by a quantized linear function. We analyze the RegNet design space and arrive at interesting findings that do not match the current practice of network design. The RegNet design space provides simple and fast networks that work well across a wide range of flop regimes. Under comparable training settings and flops, the RegNet models outperform the popular EfficientNet models while being up to 5x faster on GPUs.",
            "저자": "Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollár",
            "전체 인용횟수": "1336회 인용202020212022202351292479499",
            "컨퍼런스": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "페이지": "10428-10436",
            "학술 문서": "Designing network design spacesI Radosavovic, RP Kosaraju, R Girshick, K He, P Dollár - Proceedings of the IEEE/CVF conference on computer …, 20201336회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Designing network design spaces",
        "year": null
    },
    "Panoptic segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "We propose and study a task we name panoptic segmentation (PS). Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more unified view of image segmentation. For more analysis and up-to-date results, please check the arXiv version of the paper:\\smallhttps://arxiv. org/abs/1801.00868.",
            "저자": "Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, Piotr Dollár",
            "전체 인용횟수": "1303회 인용2018201920202021202220231881176282345392",
            "컨퍼런스": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "페이지": "9404-9413",
            "학술 문서": "Panoptic segmentationA Kirillov, K He, R Girshick, C Rother, P Dollár - Proceedings of the IEEE/CVF conference on computer …, 20191303회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Panoptic segmentation",
        "year": null
    },
    "Cascade object detection with deformable part models": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/6/13",
            "게시자": "IEEE",
            "설명": "We describe a general method for building cascade classifiers from part-based deformable models such as pictorial structures. We focus primarily on the case of star-structured models and show how a simple algorithm based on partial hypothesis pruning can speed up object detection by more than one order of magnitude without sacrificing detection accuracy. In our algorithm, partial hypotheses are pruned with a sequence of thresholds. In analogy to probably approximately correct (PAC) learning, we introduce the notion of probably approximately admissible (PAA) thresholds. Such thresholds provide theoretical guarantees on the performance of the cascade method and can be computed from a small sample of positive examples. Finally, we outline a cascade detection algorithm for a general class of models defined by a grammar formalism. This class includes not only tree-structured pictorial structures but also …",
            "저자": "Pedro F Felzenszwalb, Ross B Girshick, David McAllester",
            "전체 인용횟수": "1264회 인용201020112012201320142015201620172018201920202021202220231465871151231201161097483771048956",
            "컨퍼런스": "Computer vision and pattern recognition (CVPR), 2010 IEEE conference on",
            "페이지": "2241-2248",
            "학술 문서": "Cascade object detection with deformable part modelsPF Felzenszwalb, RB Girshick, D McAllester - 2010 IEEE Computer society conference on computer …, 20101264회 인용 관련 학술자료 전체 20개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Cascade object detection with deformable part models",
        "year": null
    },
    "Rethinking imagenet pre-training": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "We report competitive results on object detection and instance segmentation on the COCO dataset using standard models trained from random initialization. The results are no worse than their ImageNet pre-training counterparts even when using the hyper-parameters of the baseline system (Mask R-CNN) that were optimized for fine-tuning pre-trained models, with the sole exception of increasing the number of training iterations so the randomly initialized models may converge. Training from random initialization is surprisingly robust; our results hold even when:(i) using only 10% of the training data,(ii) for deeper and wider models, and (iii) for multiple tasks and metrics. Experiments show that ImageNet pre-training speeds up convergence early in training, but does not necessarily provide regularization or improve final target task accuracy. To push the envelope we demonstrate 50.9 AP on COCO object detection without using any external data---a result on par with the top COCO 2017 competition results that used ImageNet pre-training. These observations challenge the conventional wisdom of ImageNet pre-training for dependent tasks and we expect these discoveries will encourage people to rethink the current de facto paradigm ofpre-training and fine-tuning'in computer vision.",
            "저자": "Kaiming He, Ross Girshick, Piotr Dollár",
            "전체 인용횟수": "1153회 인용2018201920202021202220236108217280285250",
            "컨퍼런스": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "페이지": "4918-4927",
            "학술 문서": "Rethinking imagenet pre-trainingK He, R Girshick, P Dollár - Proceedings of the IEEE/CVF International Conference …, 20191153회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Rethinking imagenet pre-training",
        "year": null
    },
    "Segment anything": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2023/4/5",
            "설명": "We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.",
            "저널": "arXiv preprint arXiv:2304.02643",
            "저자": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick",
            "전체 인용횟수": "1130회 인용2022202331112",
            "학술 문서": "Segment anythingA Kirillov, E Mintun, N Ravi, H Mao, C Rolland… - arXiv preprint arXiv:2304.02643, 20231117회 인용 관련 학술자료 전체 3개의 버전 Segment anything. arXiv 2023A Kirillov, E Mintun, N Ravi, H Mao, C Rolland… - arXiv preprint arXiv:2304.0264353회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Segment anything",
        "year": null
    },
    "Panoptic feature pyramid networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "The recently introduced panoptic segmentation task has renewed our community's interest in unifying the tasks of instance segmentation (for thing classes) and semantic segmentation (for stuff classes). However, current state-of-the-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation, without performing any shared computation. In this work, we aim to unify these methods at the architectural level, designing a single network for both tasks. Our approach is to endow Mask R-CNN, a popular instance segmentation method, with a semantic segmentation branch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for instance segmentation, but also yields a lightweight, top-performing method for semantic segmentation. In this work, we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and accurate baseline for both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and aid future research in panoptic segmentation.",
            "저자": "Alexander Kirillov, Ross Girshick, Kaiming He, Piotr Dollár",
            "전체 인용횟수": "1116회 인용2019202020212022202341139230337364",
            "컨퍼런스": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "페이지": "6399-6408",
            "학술 문서": "Panoptic feature pyramid networksA Kirillov, R Girshick, K He, P Dollár - Proceedings of the IEEE/CVF conference on computer …, 20191116회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Panoptic feature pyramid networks",
        "year": null
    },
    "Densenet: Implementing efficient convnet descriptor pyramids": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/4/7",
            "설명": "Convolutional Neural Networks (CNNs) can provide accurate object classification. They can be extended to perform object detection by iterating over dense or selected proposed object regions. However, the runtime of such detectors scales as the total number and/or area of regions to examine per image, and training such detectors may be prohibitively slow. However, for some CNN classifier topologies, it is possible to share significant work among overlapping regions to be classified. This paper presents DenseNet, an open source system that computes dense, multiscale features from the convolutional layers of a CNN based object classifier. Future work will involve training efficient object detectors with DenseNet feature descriptors.",
            "저널": "arXiv preprint arXiv:1404.1869",
            "저자": "Forrest Iandola, Matt Moskewicz, Sergey Karayev, Ross Girshick, Trevor Darrell, Kurt Keutzer",
            "전체 인용횟수": "979회 인용201420152016201720182019202020212022202372119253693124177272201",
            "학술 문서": "Densenet: Implementing efficient convnet descriptor pyramidsF Iandola, M Moskewicz, S Karayev, R Girshick… - arXiv preprint arXiv:1404.1869, 2014961회 인용 관련 학술자료 전체 6개의 버전 Densenet: Implementing efficient convnet descriptor pyramids. arXiv 2014F Iandola, M Moskewicz, S Karayev, R Girshick… - arXiv preprint arXiv:1404.186992회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Densenet: Implementing efficient convnet descriptor pyramids",
        "year": null
    },
    "Lvis: A dataset for large vocabulary instance segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "Progress on object detection is enabled by datasets that focus the research community's attention on open challenges. This process led us from simple images to complex scenes and from bounding boxes to segmentation masks. In this work, we introduce LVIS (pronounced'el-vis'): a new dataset for Large Vocabulary Instance Segmentation. We plan to collect 2.2 million high-quality instance segmentation masks for over 1000 entry-level object categories in 164k images. Due to the Zipfian distribution of categories in natural images, LVIS naturally has a long tail of categories with few training samples. Given that state-of-the-art deep learning methods for object detection perform poorly in the low-sample regime, we believe that our dataset poses an important and exciting new scientific challenge. LVIS is available at http://www. lvisdataset. org.",
            "저자": "Agrim Gupta, Piotr Dollar, Ross Girshick",
            "전체 인용횟수": "891회 인용20192020202120222023977169257376",
            "컨퍼런스": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "페이지": "5356-5364",
            "학술 문서": "Lvis: A dataset for large vocabulary instance segmentationA Gupta, P Dollar, R Girshick - Proceedings of the IEEE/CVF conference on computer …, 2019891회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Lvis: A dataset for large vocabulary instance segmentation",
        "year": null
    },
    "Low-shot Visual Recognition by Shrinking and Hallucinating Features": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/6/9",
            "설명": "Low-shot visual learning--the ability to recognize novel object categories from very few examples--is a hallmark of human visual intelligence. Existing machine learning approaches fail to generalize in the same way. To make progress on this foundational problem, we present a low-shot learning benchmark on complex images that mimics challenges faced by recognition systems in the wild. We then propose (1) representation regularization techniques, and (2) techniques to hallucinate additional training examples for data-starved classes. Together, our methods improve the effectiveness of convolutional networks in low-shot learning, improving the one-shot accuracy on novel classes by 2.3 x on the challenging ImageNet dataset.",
            "저널": "arXiv preprint arXiv:1606.02819",
            "저자": "Bharath Hariharan, Ross Girshick",
            "전체 인용횟수": "853회 인용2016201720182019202020212022202343077118155183149127",
            "학술 문서": "Low-shot visual recognition by shrinking and hallucinating featuresB Hariharan, R Girshick - Proceedings of the IEEE international conference on …, 2017816회 인용 관련 학술자료 전체 7개의 버전 Low-shot visual object recognition*B Hariharan, R Girshick - arXiv preprint arXiv:1606.02819, 201644회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Low-shot Visual Recognition by Shrinking and Hallucinating Features",
        "year": null
    },
    "Low-shot learning from imaginary data": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "Humans can quickly learn new visual concepts, perhaps because they can easily visualize or imagine what novel objects look like from different views. Incorporating this ability to hallucinate novel instances of new concepts might help machine vision systems perform better low-shot learning, ie, learning concepts from few examples. We present a novel approach to low-shot learning that uses this idea. Our approach builds on recent progress in meta-learning (''learning to learn'') by combining a meta-learner with a''hallucinator''that produces additional training examples, and optimizing both models jointly. Our hallucinator can be incorporated into a variety of meta-learners and provides significant gains: up to a 6 point boost in classification accuracy when only a single training example is available, yielding state-of-the-art performance on the challenging ImageNet low-shot classification benchmark.",
            "저자": "Yu-Xiong Wang, Ross Girshick, Martial Hebert, Bharath Hariharan",
            "전체 인용횟수": "768회 인용20182019202020212022202321107153183170131",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "7278-7286",
            "학술 문서": "Low-shot learning from imaginary dataYX Wang, R Girshick, M Hebert, B Hariharan - Proceedings of the IEEE conference on computer …, 2018768회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Low-shot learning from imaginary data",
        "year": null
    },
    "Pointrend: Image segmentation as rendering": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020",
            "설명": "We present a new method for efficient high-quality image segmentation of objects and scenes. By analogizing classical computer graphics methods for efficient rendering with over-and undersampling challenges faced in pixel labeling tasks, we develop a unique perspective of image segmentation as a rendering problem. From this vantage, we present the PointRend (Point-based Rendering) neural network module: a module that performs point-based segmentation predictions at adaptively selected locations based on an iterative subdivision algorithm. PointRend can be flexibly applied to both instance and semantic segmentation tasks by building on top of existing state-of-the-art models. While many concrete implementations of the general idea are possible, we show that a simple design already achieves excellent results. Qualitatively, PointRend outputs crisp object boundaries in regions that are over-smoothed by previous methods. Quantitatively, PointRend yields significant gains on COCO and Cityscapes, for both instance and semantic segmentation. PointRend's efficiency enables output resolutions that are otherwise impractical in terms of memory or computation compared to existing approaches. Code has been made available at https://github. com/facebookresearch/detectron2/tree/master/projects/PointRend.",
            "저자": "Alexander Kirillov, Yuxin Wu, Kaiming He, Ross Girshick",
            "전체 인용횟수": "767회 인용202020212022202340176247301",
            "컨퍼런스": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "페이지": "9799-9808",
            "학술 문서": "Pointrend: Image segmentation as renderingA Kirillov, Y Wu, K He, R Girshick - Proceedings of the IEEE/CVF conference on computer …, 2020767회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pointrend: Image segmentation as rendering",
        "year": null
    },
    "Efficient human pose estimation from single depth images": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/11/10",
            "게시자": "IEEE",
            "권": "35",
            "설명": "We describe two new approaches to human pose estimation. Both can quickly and accurately predict the 3D positions of body joints from a single depth image without using any temporal information. The key to both approaches is the use of a large, realistic, and highly varied synthetic set of training images. This allows us to learn models that are largely invariant to factors such as pose, body shape, field-of-view cropping, and clothing. Our first approach employs an intermediate body parts representation, designed so that an accurate per-pixel classification of the parts will localize the joints of the body. The second approach instead directly regresses the positions of body joints. By using simple depth pixel comparison features and parallelizable decision forests, both approaches can run super-real time on consumer hardware. Our evaluation investigates many aspects of our methods, and compares the approaches …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Jamie Shotton, Ross Girshick, Andrew Fitzgibbon, Toby Sharp, Mat Cook, Mark Finocchio, Richard Moore, Pushmeet Kohli, Antonio Criminisi, Alex Kipman, Andrew Blake",
            "전체 인용횟수": "706회 인용2013201420152016201720182019202020212022202331479097103916061374137",
            "페이지": "2821-2840",
            "학술 문서": "Efficient human pose estimation from single depth imagesJ Shotton, R Girshick, A Fitzgibbon, T Sharp, M Cook… - IEEE transactions on pattern analysis and machine …, 2012706회 인용 관련 학술자료 전체 11개의 버전 ",
            "호": "12"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Efficient human pose estimation from single depth images",
        "year": null
    },
    "Detecting and recognizing human-object interactions": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "To understand the visual world, a machine must not only recognize individual object instances but also how they interact. Humans are often at the center of such interactions and detecting human-object interactions is an important practical and scientific problem. In this paper, we address the task of detecting (human, verb, object) triplets in challenging everyday photos. We propose a novel model that is driven by a human-centric approach. Our hypothesis is that the appearance of a person--their pose, clothing, action--is a powerful cue for localizing the objects they are interacting with. To exploit this cue, our model learns to predict an action-specific density over target object locations based on the appearance of a detected person. Our model also jointly learns to detect people and objects, and by fusing these predictions it efficiently infers interaction triplets in a clean, jointly trained end-to-end system we call InteractNet. We validate our approach on the recently introduced Verbs in COCO (V-COCO) and HICO-DET datasets, where we show quantitatively compelling results.",
            "저자": "Georgia Gkioxari, Ross Girshick, Piotr Dollár, Kaiming He",
            "전체 인용횟수": "606회 인용201720182019202020212022202363267120128132116",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "8359-8367",
            "학술 문서": "Detecting and recognizing human-object interactionsG Gkioxari, R Girshick, P Dollár, K He - Proceedings of the IEEE conference on computer …, 2018606회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Detecting and recognizing human-object interactions",
        "year": null
    },
    "Learning Features by Watching Objects Move": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/7/1",
            "권": "1",
            "설명": "This paper presents a novel yet intuitive approach to unsupervised feature learning. Inspired by the human visual system, we explore whether low-level motion-based grouping cues can be used to learn an effective visual representation. Specifically, we use unsupervised motion-based segmentation on videos to obtain segments, which we use as' pseudo ground truth'to train a convolutional network to segment objects from a single frame. Given the extensive evidence that motion plays a key role in the development of the human visual system, we hope that this straightforward approach to unsupervised learning will be more effective than cleverly designed'pretext'tasks studied in the literature. Indeed, our extensive experiments show that this is the case. When used for transfer learning on object detection, our representation significantly outperforms previous unsupervised approaches across multiple settings, especially when training data for the target task is scarce.",
            "저널": "CVPR",
            "저자": "Deepak Pathak, Ross B Girshick, Piotr Dollár, Trevor Darrell, Bharath Hariharan",
            "전체 인용횟수": "571회 인용2016201720182019202020212022202323383841181186364",
            "페이지": "7",
            "학술 문서": "Learning features by watching objects moveD Pathak, R Girshick, P Dollár, T Darrell, B Hariharan - Proceedings of the IEEE conference on computer …, 2017571회 인용 관련 학술자료 전체 10개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning Features by Watching Objects Move",
        "year": null
    },
    "Inferring and executing programs for visual reasoning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "Existing methods for visual reasoning attempt to directly map inputs to outputs using black-box architectures without explicitly modeling the underlying reasoning processes. As a result, these black-box models often learn to exploit biases in the data rather than learning to perform visual reasoning. Inspired by module networks, this paper proposes a model for visual reasoning that consists of a program generator that constructs an explicit representation of the reasoning process to be performed, and an execution engine that executes the resulting program to produce an answer. Both the program generator and the execution engine are implemented by neural networks, and are trained using a combination of backpropagation and REINFORCE. Using the CLEVR benchmark for visual reasoning, we show that our model significantly outperforms strong baselines and generalizes better in a variety of settings.",
            "저자": "Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zitnick, Ross Girshick",
            "전체 인용횟수": "567회 인용20172018201920202021202220232390115911097857",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "2989-2998",
            "학술 문서": "Inferring and executing programs for visual reasoningJ Johnson, B Hariharan, L Van Der Maaten, J Hoffman… - Proceedings of the IEEE international conference on …, 2017567회 인용 관련 학술자료 전체 16개의 버전 Inferring and executing programs for visual reasoning supplementary material*J Johnson, B Hariharan, L van der Maaten, J Hoffman… - 20171회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Inferring and executing programs for visual reasoning",
        "year": null
    },
    "Analyzing the performance of multilayer neural networks for object recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "게시자": "Springer International Publishing",
            "설명": " In the last two years, convolutional neural networks (CNNs) have achieved an impressive suite of results on standard recognition datasets and tasks. CNN-based features seem poised to quickly replace engineered representations, such as SIFT and HOG. However, compared to SIFT and HOG, we understand much less about the nature of the features learned by large CNNs. In this paper, we experimentally probe several aspects of CNN feature learning in an attempt to help practitioners gain useful, evidence-backed intuitions about how to apply CNNs to computer vision problems.",
            "저자": "Pulkit Agrawal, Ross Girshick, Jitendra Malik",
            "전체 인용횟수": "558회 인용20142015201620172018201920202021202220235626969885765535122",
            "컨퍼런스": "Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII 13",
            "페이지": "329-344",
            "학술 문서": "Analyzing the performance of multilayer neural networks for object recognitionP Agrawal, R Girshick, J Malik - Computer Vision–ECCV 2014: 13th European …, 2014558회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Analyzing the performance of multilayer neural networks for object recognition",
        "year": null
    },
    "Deformable part models are convolutional neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "Deformable part models (DPMs) and convolutional neural networks (CNNs) are two widely used tools for visual recognition. They are typically viewed as distinct approaches: DPMs are graphical models (Markov random fields), while CNNs are\" black-box\" non-linear classifiers. In this paper, we show that a DPM can be formulated as a CNN, thus providing a synthesis of the two ideas. Our construction involves unrolling the DPM inference algorithm and mapping each step to an equivalent CNN layer. From this perspective, it is natural to replace the standard image features used in DPMs with a learned feature extractor. We call the resulting model a DeepPyramid DPM and experimentally validate it on PASCAL VOC object detection. We find that DeepPyramid DPMs significantly outperform DPMs based on histograms of oriented gradients features (HOG) and slightly outperforms a comparable version of the recently introduced R-CNN detection system, while running significantly faster.",
            "저자": "Ross Girshick, Forrest Iandola, Trevor Darrell, Jitendra Malik",
            "전체 인용횟수": "553회 인용20142015201620172018201920202021202220234478190666451534333",
            "컨퍼런스": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition",
            "페이지": "437-446",
            "학술 문서": "Deformable part models are convolutional neural networksR Girshick, F Iandola, T Darrell, J Malik - Proceedings of the IEEE conference on Computer …, 2015553회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deformable part models are convolutional neural networks",
        "year": null
    },
    "Discriminatively Trained Deformable Part Models (DPM) Code": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012",
            "저자": "RB Girshick, PF Felzenszwalb, D McAllester",
            "전체 인용횟수": "544회 인용201020112012201320142015201620172018201920202021202220239445864928068332412111394",
            "출처": "http://www.cs.berkeley.edu/~rbg/latent/",
            "학술 문서": "Discriminatively trained deformable part models, release 5*RB Girshick, PF Felzenszwalb, D McAllester - 2012305회 인용 관련 학술자료 Discriminatively trained deformable part models, release 4*PF Felzenszwalb - http://people. cs. uchicago. edu/pff/latent-release4/, 2010250회 인용 관련 학술자료 전체 2개의 버전 Lsvm-mdpm release 4 notes*R Girshick, P Felzenszwalb, D McAllester - Apr, 20103회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Discriminatively Trained Deformable Part Models (DPM) Code",
        "year": null
    },
    "Efficient Regression of General-Activity Human Poses from Depth Images": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/11",
            "설명": "We present a new approach to general-activity human pose estimation from depth images, building on Hough forests. We extend existing techniques in several ways: real time prediction of multiple 3D joints, explicit learning of voting weights, vote compression to allow larger training sets, and a comparison of several decision-tree training objectives. Key aspects of our work include: regression directly from the raw depth image, without the use of an arbitrary intermediate representation; applicability to general motions (not constrained to particular activities) and the ability to localize occluded as well as visible body joints. Experimental results demonstrate that our method produces state of the art results on several data sets including the challenging MSRC-5000 pose estimation test set, at a speed of about 200 frames per second. Results on silhouettes suggest broader applicability to other imaging modalities.",
            "저자": "Ross Girshick, Jamie Shotton, Pushmeet Kohli, Antonio Criminisi, Andrew Fitzgibbon",
            "전체 인용횟수": "521회 인용2011201220132014201520162017201820192020202120222023249717462714334382815106",
            "컨퍼런스": "ICCV",
            "학술 문서": "Efficient regression of general-activity human poses from depth imagesR Girshick, J Shotton, P Kohli, A Criminisi, A Fitzgibbon - 2011 International Conference on Computer Vision, 2011521회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Efficient Regression of General-Activity Human Poses from Depth Images",
        "year": null
    },
    "Long-term feature banks for detailed video understanding": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "To understand the world, we humans constantly need to relate the present to the past, and put events in context. In this paper, we enable existing video models to do the same. We propose a long-term feature bank--supportive information extracted over the entire span of a video--to augment state-of-the-art video models that otherwise would only view short clips of 2-5 seconds. Our experiments demonstrate that augmenting 3D convolutional networks with a long-term feature bank yields state-of-the-art results on three challenging video datasets: AVA, EPIC-Kitchens, and Charades. Code is available online.",
            "저자": "Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming He, Philipp Krahenbuhl, Ross Girshick",
            "전체 인용횟수": "494회 인용201920202021202220233183140127110",
            "컨퍼런스": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "페이지": "284-293",
            "학술 문서": "Long-term feature banks for detailed video understandingCY Wu, C Feichtenhofer, H Fan, K He, P Krahenbuhl… - Proceedings of the IEEE/CVF Conference on Computer …, 2019494회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Long-term feature banks for detailed video understanding",
        "year": null
    },
    "Early convolutions help transformers see better": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021/12/6",
            "권": "34",
            "설명": "Vision transformer (ViT) models exhibit substandard optimizability. In particular, they are sensitive to the choice of optimizer (AdamW vs. SGD), optimizer hyperparameters, and training schedule length. In comparison, modern convolutional neural networks are easier to optimize. Why is this the case? In this work, we conjecture that the issue lies with the patchify stem of ViT models, which is implemented by a stride-p p× p convolution (p= 16 by default) applied to the input image. This large-kernel plus large-stride convolution runs counter to typical design choices of convolutional layers in neural networks. To test whether this atypical design choice causes an issue, we analyze the optimization behavior of ViT models with their original patchify stem versus a simple counterpart where we replace the ViT stem by a small number of stacked stride-two 3× 3 convolutions. While the vast majority of computation in the two ViT designs is identical, we find that this small change in early visual processing results in markedly different training behavior in terms of the sensitivity to optimization settings as well as the final model accuracy. Using a convolutional stem in ViT dramatically increases optimization stability and also improves peak performance (by∼ 1-2% top-1 accuracy on ImageNet-1k), while maintaining flops and runtime. The improvement can be observed across the wide spectrum of model complexities (from 1G to 36G flops) and dataset scales (from ImageNet-1k to ImageNet-21k). These findings lead us to recommend using a standard, lightweight convolutional stem for ViT models in this regime as a more robust architectural choice compared to the …",
            "저널": "Advances in Neural Information Processing Systems",
            "저자": "Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Dollár, Ross Girshick",
            "전체 인용횟수": "493회 인용20212022202321219251",
            "페이지": "30392-30400",
            "학술 문서": "Early convolutions help transformers see betterT Xiao, M Singh, E Mintun, T Darrell, P Dollár… - Advances in neural information processing systems, 2021493회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Early convolutions help transformers see better",
        "year": null
    },
    "Object detection networks on convolutional feature maps": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/8/17",
            "게시자": "IEEE",
            "권": "39",
            "설명": "Most object detectors contain two important components: a feature extractor and an object classifier. The feature extractor has rapidly evolved with significant research efforts leading to better deep convolutional architectures. The object classifier, however, has not received much attention and many recent systems (like SPPnet and Fast/Faster R-CNN) use simple multi-layer perceptrons. This paper demonstrates that carefully designing deep networks for object classification is just as important. We experiment with region-wise classifier networks that use shared, region-independent convolutional features. We call them “Networks on Convolutional feature maps” (NoCs). We discover that aside from deep feature maps, a  deep  and  convolutional  per-region classifier is of particular importance for object detection, whereas latest superior image classification models (such as ResNets and GoogLeNets) do not directly …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Shaoqing Ren, Kaiming He, Ross Girshick, Xiangyu Zhang, Jian Sun",
            "전체 인용횟수": "491회 인용20152016201720182019202020212022202362335708583795647",
            "페이지": "1476-1481",
            "학술 문서": "Object detection networks on convolutional feature mapsS Ren, K He, R Girshick, X Zhang, J Sun - IEEE transactions on pattern analysis and machine …, 2016491회 인용 관련 학술자료 전체 11개의 버전 ",
            "호": "7"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Object detection networks on convolutional feature maps",
        "year": null
    },
    "Contextual action recognition with r* cnn": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "There are multiple cues in an image which reveal what action a person is performing. For example, a jogger has a pose that is characteristic for jogging, but the scene (eg road, trail) and the presence of other joggers can be an additional source of information. In this work, we exploit the simple observation that actions are accompanied by contextual cues to build a strong action recognition system. We adapt RCNN to use more than one region for classification while still maintaining the ability to localize the action. We call our system R* CNN. The action-specific models and the feature maps are trained jointly, allowing for action specific representations to emerge. R* CNN achieves 90.2% mean AP on the PASAL VOC Action dataset, outperforming all other approaches in the field by a significant margin. Last, we show that R* CNN is not limited to action recognition. In particular, R* CNN can also be used to tackle fine-grained tasks such as attribute classification. We validate this claim by reporting state-of-the-art performance on the Berkeley Attributes of People dataset.",
            "저자": "Georgia Gkioxari, Ross Girshick, Jitendra Malik",
            "전체 인용횟수": "490회 인용20152016201720182019202020212022202333962727977665531",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "1080-1088",
            "학술 문서": "Contextual action recognition with r* cnnG Gkioxari, R Girshick, J Malik - Proceedings of the IEEE international conference on …, 2015490회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Contextual action recognition with r* cnn",
        "year": null
    },
    "Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "게시자": "Springer International Publishing",
            "설명": " As 3D movie viewing becomes mainstream and the Virtual Reality (VR) market emerges, the demand for 3D contents is growing rapidly. Producing 3D videos, however, remains challenging. In this paper we propose to use deep neural networks to automatically convert 2D videos and images to a stereoscopic 3D format. In contrast to previous automatic 2D-to-3D conversion algorithms, which have separate stages and need ground truth depth map as supervision, our approach is trained end-to-end directly on stereo pairs extracted from existing 3D movies. This novel training scheme makes it possible to exploit orders of magnitude more data and significantly increases performance. Indeed, Deep3D outperforms baselines in both quantitative and human subject evaluations.",
            "저자": "Junyuan Xie, Ross Girshick, Ali Farhadi",
            "전체 인용횟수": "475회 인용20162017201820192020202120222023820599986876642",
            "컨퍼런스": "Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14",
            "페이지": "842-857",
            "학술 문서": "Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networksJ Xie, R Girshick, A Farhadi - Computer Vision–ECCV 2016: 14th European …, 2016475회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks",
        "year": null
    },
    "Reducing overfitting in deep networks by decorrelating representations": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/11/19",
            "설명": "One major challenge in training Deep Neural Networks is preventing overfitting. Many techniques such as data augmentation and novel regularizers such as Dropout have been proposed to prevent overfitting without requiring a massive amount of training data. In this work, we propose a new regularizer called DeCov which leads to significantly reduced overfitting (as indicated by the difference between train and val performance), and better generalization. Our regularizer encourages diverse or non-redundant representations in Deep Neural Networks by minimizing the cross-covariance of hidden activations. This simple intuition has been explored in a number of past works but surprisingly has never been applied as a regularizer in supervised learning. Experiments across a range of datasets and network architectures show that this loss always reduces overfitting while almost always maintaining or increasing generalization performance and often improving performance over Dropout.",
            "저널": "arXiv preprint arXiv:1511.06068",
            "저자": "Michael Cogswell, Faruk Ahmed, Ross Girshick, Larry Zitnick, Dhruv Batra",
            "전체 인용횟수": "448회 인용201620172018201920202021202220231634566477765268",
            "학술 문서": "Reducing overfitting in deep networks by decorrelating representationsM Cogswell, F Ahmed, R Girshick, L Zitnick, D Batra - arXiv preprint arXiv:1511.06068, 2015448회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Reducing overfitting in deep networks by decorrelating representations",
        "year": null
    },
    "Data distillation: Towards omni-supervised learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "We investigate omni-supervised learning, a special regime of semi-supervised learning in which the learner exploits all available labeled data plus internet-scale sources of unlabeled data. Omni-supervised learning is lower-bounded by performance on existing labeled datasets, offering the potential to surpass state-of-the-art fully supervised methods. To exploit the omni-supervised setting, we propose data distillation, a method that ensembles predictions from multiple transformations of unlabeled data, using a single model, to automatically generate new training annotations. We argue that visual recognition models have recently become accurate enough that it is now possible to apply classic ideas about self-training to challenging real-world data. Our experimental results show that in the cases of human keypoint detection and general object detection, state-of-the-art models trained with data distillation surpass the performance of using labeled data from the COCO dataset alone.",
            "저자": "Ilija Radosavovic, Piotr Dollár, Ross Girshick, Georgia Gkioxari, Kaiming He",
            "전체 인용횟수": "438회 인용201720182019202020212022202322766881187561",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "4119-4128",
            "학술 문서": "Data distillation: Towards omni-supervised learningI Radosavovic, P Dollár, R Girshick, G Gkioxari, K He - Proceedings of the IEEE conference on computer …, 2018438회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Data distillation: Towards omni-supervised learning",
        "year": null
    },
    "Visual storytelling": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/6",
            "설명": "We introduce the first dataset for sequential vision-to-language, and explore how this data may be used for the task of visual storytelling. The first release of this dataset, SIND1 v. 1, includes 81,743 unique photos in 20,211 sequences, aligned to both descriptive (caption) and story language. We establish several strong baselines for the storytelling task, and motivate an automatic metric to benchmark progress. Modelling concrete description as well as figurative and social language, as provided in this dataset and the storytelling task, has the potential to move artificial intelligence from basic understandings of typical visual scenes towards more and more human-like understanding of grounded event structure and subjective expression.",
            "저자": "Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, C Lawrence Zitnick, Devi Parikh, Lucy Vanderwende, Michel Galley, Margaret Mitchell",
            "전체 인용횟수": "409회 인용20162017201820192020202120222023727486071606766",
            "컨퍼런스": "Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: Human language technologies",
            "페이지": "1233-1239",
            "학술 문서": "Visual storytellingTH Huang, F Ferraro, N Mostafazadeh, I Misra… - Proceedings of the 2016 conference of the North …, 2016406회 인용 관련 학술자료 전체 12개의 버전 Visual storytelling*H Ting-Hao, F Ferraro, N Mostafazadeh, I Misra… - In Proceedings of NAACL, 20166회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Visual storytelling",
        "year": null
    },
    "Exploring randomly wired neural networks for image recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "Neural networks for image recognition have evolved through extensive manual design from simple chain-like models to structures with multiple wiring paths. The success of ResNets and DenseNets is due in large part to their innovative wiring plans. Now, neural architecture search (NAS) studies are exploring the joint optimization of wiring and operation types, however, the space of possible wirings is constrained and still driven by manual design despite being searched. In this paper, we explore a more diverse set of connectivity patterns through the lens of randomly wired neural networks. To do this, we first define the concept of a stochastic network generator that encapsulates the entire network generation process. Encapsulation provides a unified view of NAS and randomly wired networks. Then, we use three classical random graph models to generate randomly wired graphs for networks. The results are surprising: several variants of these random generators yield network instances that have competitive accuracy on the ImageNet benchmark. These results suggest that new efforts focusing on designing better network generators may lead to new breakthroughs by exploring less constrained search spaces with more room for novel design.",
            "저자": "Saining Xie, Alexander Kirillov, Ross Girshick, Kaiming He",
            "전체 인용횟수": "397회 인용20192020202120222023571021117057",
            "컨퍼런스": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "페이지": "1284-1293",
            "학술 문서": "Exploring randomly wired neural networks for image recognitionS Xie, A Kirillov, R Girshick, K He - Proceedings of the IEEE/CVF International Conference …, 2019397회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Exploring randomly wired neural networks for image recognition",
        "year": null
    },
    "LSDA: Large scale detection through adaptation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "권": "27",
            "설명": "A major challenge in scaling object detection is the difficulty of obtaining labeled images for large numbers of categories. Recently, deep convolutional neural networks (CNNs) have emerged as clear winners on object classification benchmarks, in part due to training with 1.2 M+ labeled classification images. Unfortunately, only a small fraction of those labels are available for the detection task. It is much cheaper and easier to collect large quantities of image-level labels from search engines than it is to collect detection data and label it with precise bounding boxes. In this paper, we propose Large Scale Detection through Adaptation (LSDA), an algorithm which learns the difference between the two tasks and transfers this knowledge to classifiers for categories without bounding box annotated data, turning them into detectors. Our method has the potential to enable detection for the tens of thousands of categories that lack bounding box annotations, yet have plenty of classification data. Evaluation on the ImageNet LSVRC-2013 detection challenge demonstrates the efficacy of our approach. This algorithm enables us to produce a> 7.6 K detector by using available classification data from leaf nodes in the ImageNet tree. We additionally demonstrate how to modify our architecture to produce a fast detector (running at 2fps for the 7.6 K detector). Models and software are available at",
            "저널": "Advances in neural information processing systems",
            "저자": "Judy Hoffman, Sergio Guadarrama, Eric S Tzeng, Ronghang Hu, Jeff Donahue, Ross Girshick, Trevor Darrell, Kate Saenko",
            "전체 인용횟수": "364회 인용20142015201620172018201920202021202220234215342475152411922",
            "학술 문서": "LSDA: Large scale detection through adaptationJ Hoffman, S Guadarrama, ES Tzeng, R Hu, J Donahue… - Advances in neural information processing systems, 2014364회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "LSDA: Large scale detection through adaptation",
        "year": null
    },
    "Tensormask: A foundation for dense object segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "Sliding-window object detectors that generate bounding-box object predictions over a dense, regular grid have advanced rapidly and proven popular. In contrast, modern instance segmentation approaches are dominated by methods that first detect object bounding boxes, and then crop and segment these regions, as popularized by Mask R-CNN. In this work, we investigate the paradigm of dense sliding-window instance segmentation, which is surprisingly under-explored. Our core observation is that this task is fundamentally different than other dense prediction tasks such as semantic segmentation or bounding-box object detection, as the output at every spatial location is itself a geometric structure with its own spatial dimensions. To formalize this, we treat dense instance segmentation as a prediction task over 4D tensors and present a general framework called TensorMask that explicitly captures this geometry and enables novel operators on 4D tensors. We demonstrate that the tensor view leads to large gains over baselines that ignore this structure, and leads to results comparable to Mask R-CNN. These promising results suggest that TensorMask can serve as a foundation for novel advances in dense mask prediction and a more complete understanding of the task. Code will be made available.",
            "저자": "Xinlei Chen, Ross Girshick, Kaiming He, Piotr Dollár",
            "전체 인용횟수": "361회 인용2019202020212022202365911310081",
            "컨퍼런스": "Proceedings of the IEEE/CVF international conference on computer vision",
            "페이지": "2061-2069",
            "학술 문서": "Tensormask: A foundation for dense object segmentationX Chen, R Girshick, K He, P Dollár - Proceedings of the IEEE/CVF international conference …, 2019361회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Tensormask: A foundation for dense object segmentation",
        "year": null
    },
    "Object Detection with Grammar Models": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011",
            "설명": "Compositional models provide an elegant formalism for representing the visual appearance of highly variable objects. While such models are appealing from a theoretical point of view, it has been difficult to demonstrate that they lead to performance advantages on challenging datasets. Here we develop a grammar model for person detection and show that it outperforms previous high-performance systems on the PASCAL benchmark. Our model represents people using a hierarchy of deformable parts, variable structure and an explicit model of occlusion for partially visible objects. To train the model, we introduce a new discriminative framework for learning structured prediction models from weakly-labeled data.",
            "저자": "Ross B Girshick, Pedro F Felzenszwalb, David McAllester",
            "전체 인용횟수": "356회 인용2010201120122013201420152016201720182019202020212022202311193857444423211521252612",
            "컨퍼런스": "NIPS 2011",
            "학술 문서": "Object detection with grammar modelsR Girshick, P Felzenszwalb, D McAllester - Advances in neural information processing systems, 2011356회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Object Detection with Grammar Models",
        "year": null
    },
    "Exploring plain vision transformer backbones for object detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2022/10/23",
            "게시자": "Springer Nature Switzerland",
            "도서": "European Conference on Computer Vision",
            "설명": "We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection. This design enables the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for fine-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is sufficient to build a simple feature pyramid from a single-scale feature map (without the common FPN design) and (ii) it is sufficient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain ViT backbones pre-trained as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the previous leading methods that were all based on hierarchical backbones, reaching up to 61.3 AP on the COCO dataset using only ImageNet-1K pre-training. We hope our …",
            "저자": "Yanghao Li, Hanzi Mao, Ross Girshick, Kaiming He",
            "전체 인용횟수": "342회 인용202120222023174263",
            "페이지": "280-296",
            "학술 문서": "Exploring plain vision transformer backbones for object detectionY Li, H Mao, R Girshick, K He - European Conference on Computer Vision, 2022342회 인용 관련 학술자료 전체 5개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Exploring plain vision transformer backbones for object detection",
        "year": null
    },
    "Learning to segment every thing": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "Most methods for object instance segmentation require all training examples to be labeled with segmentation masks. This requirement makes it expensive to annotate new categories and has restricted instance segmentation models to~ 100 well-annotated classes. The goal of this paper is to propose a new partially supervised training paradigm, together with a novel weight transfer function, that enables training instance segmentation models on a large set of categories all of which have box annotations, but only a small fraction of which have mask annotations. These contributions allow us to train Mask R-CNN to detect and segment 3000 visual concepts using box annotations from the Visual Genome dataset and mask annotations from the 80 classes in the COCO dataset. We evaluate our approach in a controlled study on the COCO dataset. This work is a first step towards instance segmentation models that have broad comprehension of the visual world.",
            "저자": "Ronghang Hu, Piotr Dollár, Kaiming He, Trevor Darrell, Ross Girshick",
            "전체 인용횟수": "324회 인용201820192020202120222023256880705128",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "4233-4241",
            "학술 문서": "Learning to segment every thingR Hu, P Dollár, K He, T Darrell, R Girshick - Proceedings of the IEEE conference on computer …, 2018324회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning to segment every thing",
        "year": null
    },
    "Aligning 3D models to RGB-D images of cluttered scenes": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "The goal of this work is to represent objects in an RGB-D scene with corresponding 3D models from a library. We approach this problem by first detecting and segmenting object instances in the scene and then using a convolutional neural network (CNN) to predict the pose of the object. This CNN is trained using pixel surface normals in images containing renderings of synthetic objects. When tested on real data, our method outperforms alternative algorithms trained on real data. We then use this coarse pose estimate along with the inferred pixel support to align a small number of prototypical models to the data, and place into the scene the model that fits best. We observe a 48% relative improvement in performance at the task of 3D detection over the current state-of-the-art, while being an order of magnitude faster.",
            "저자": "Saurabh Gupta, Pablo Arbeláez, Ross Girshick, Jitendra Malik",
            "전체 인용횟수": "292회 인용2014201520162017201820192020202120222023163345454640302112",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "4731-4740",
            "학술 문서": "Aligning 3D models to RGB-D images of cluttered scenesS Gupta, P Arbeláez, R Girshick, J Malik - Proceedings of the IEEE conference on computer …, 2015292회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Aligning 3D models to RGB-D images of cluttered scenes",
        "year": null
    },
    "On learning to localize objects with minimal supervision": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/6/18",
            "게시자": "PMLR",
            "설명": "Learning to localize objects with minimal supervision is an important problem in computer vision, since large fully annotated datasets are extremely costly to obtain. In this paper, we propose a new method that achieves this goal with only image-level labels of whether the objects are present or not. Our approach combines a discriminative submodular cover problem for automatically discovering a set of positive object windows with a smoothed latent SVM formulation. The latter allows us to leverage efficient quasi-Newton optimization techniques. Our experiments demonstrate that the proposed approach provides a 50% relative improvement in mean average precision over the current state-of-the-art on PASCAL VOC 2007 detection.",
            "저자": "Hyun Oh Song, Ross Girshick, Stefanie Jegelka, Julien Mairal, Zaid Harchaoui, Trevor Darrell",
            "전체 인용횟수": "285회 인용2014201520162017201820192020202120222023927484641391817166",
            "컨퍼런스": "International Conference on Machine Learning",
            "페이지": "1611-1619",
            "학술 문서": "On learning to localize objects with minimal supervisionHO Song, R Girshick, S Jegelka, J Mairal, Z Harchaoui… - International Conference on Machine Learning, 2014285회 인용 관련 학술자료 전체 18개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "On learning to localize objects with minimal supervision",
        "year": null
    },
    "Indoor scene understanding with rgb-d images: Bottom-up segmentation, object detection and semantic segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/4",
            "게시자": "Springer US",
            "권": "112",
            "설명": " In this paper, we address the problems of contour detection, bottom-up grouping, object detection and semantic segmentation on RGB-D data. We focus on the challenging setting of cluttered indoor scenes, and evaluate our approach on the recently introduced NYU-Depth V2 (NYUD2) dataset (Silberman et al., ECCV, 2012). We propose algorithms for object boundary detection and hierarchical segmentation that generalize the  approach of Arbelaez et al. (TPAMI, 2011) by making effective use of depth information. We show that our system can label each contour with its type (depth, normal or albedo). We also propose a generic method for long-range amodal completion of surfaces and show its effectiveness in grouping. We train RGB-D object detectors by analyzing and computing histogram of oriented gradients on the depth image and using them with deformable part models (Felzenszwalb et …",
            "저널": "International Journal of Computer Vision",
            "저자": "Saurabh Gupta, Pablo Arbeláez, Ross Girshick, Jitendra Malik",
            "전체 인용횟수": "279회 인용20152016201720182019202020212022202316283946463636189",
            "페이지": "133-149",
            "학술 문서": "Indoor scene understanding with rgb-d images: Bottom-up segmentation, object detection and semantic segmentationS Gupta, P Arbeláez, R Girshick, J Malik - International Journal of Computer Vision, 2015276회 인용 관련 학술자료 전체 10개의 버전 I1. 4: Invited Paper: Indoor Scene Understanding from RGB‐D Images*S Gupta, R Girshick, P Arbeláez, J Malik - SID Symposium Digest of Technical Papers, 20153회 인용 관련 학술자료 Indoor scene understanding from RGB-D images*S Gupta, P Arbeláez, R Girshick, J Malik - Digest of Technical Papers-SID International …, 2015"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Indoor scene understanding with rgb-d images: Bottom-up segmentation, object detection and semantic segmentation",
        "year": null
    },
    "maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/4/29",
            "저자": "Francisco Massa, Ross Girshick",
            "전체 인용횟수": "255회 인용20182019202020212022202325684722714",
            "학술 문서": "maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorchF Massa, R Girshick - 2018255회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch",
        "year": null
    },
    "Seeing through the Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/6",
            "설명": "When human annotators are given a choice about what to label in an image, they apply their own subjective judgments on what to ignore and what to mention. We refer to these noisy\" human-centric\" annotations as exhibiting human reporting bias. Examples of such annotations include image tags and keywords found on photo sharing sites, or in datasets containing image captions. In this paper, we use these noisy annotations for learning visually correct image classifiers. Such annotations do not use consistent vocabulary, and miss a significant amount of the information present in an image; however, we demonstrate that the noise in these annotations exhibits structure and can be modeled. We propose an algorithm to decouple the human reporting bias from the correct visually grounded labels. Our results are highly interpretable for reporting\" what's in the image\" versus\" what's worth saying.\" We demonstrate the algorithm's efficacy along a variety of metrics and datasets, including MS COCO and Yahoo Flickr 100M. We show significant improvements over traditional algorithms for both image classification and image captioning, doubling the performance of existing methods in some cases.",
            "저널": "CVPR",
            "저자": "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell, Ross Girshick",
            "전체 인용횟수": "233회 인용20162017201820192020202120222023520293438363728",
            "학술 문서": "Seeing through the human reporting bias: Visual classifiers from noisy human-centric labelsI Misra, C Lawrence Zitnick, M Mitchell, R Girshick - Proceedings of the IEEE conference on computer …, 2016230회 인용 관련 학술자료 전체 11개의 버전 Learning visual classifiers using human-centric annotations*I Misra, CL Zitnick, M Mitchell, RB Girshick - CoRR, abs/1512.06974, 20153회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Seeing through the Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels",
        "year": null
    },
    "Exploring nearest neighbor approaches for image captioning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/5/17",
            "설명": "We explore a variety of nearest neighbor baseline approaches for image captioning. These approaches find a set of nearest neighbor images in the training set from which a caption may be borrowed for the query image. We select a caption for the query image by finding the caption that best represents the \"consensus\" of the set of candidate captions gathered from the nearest neighbor images. When measured by automatic evaluation metrics on the MS COCO caption evaluation server, these approaches perform as well as many recent approaches that generate novel captions. However, human studies show that a method that generates novel captions is still preferred over the nearest neighbor approach.",
            "저널": "arXiv preprint arXiv:1505.04467",
            "저자": "Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C Lawrence Zitnick",
            "전체 인용횟수": "214회 인용20142015201620172018201920202021202220231112332352625182017",
            "학술 문서": "Exploring nearest neighbor approaches for image captioningJ Devlin, S Gupta, R Girshick, M Mitchell, CL Zitnick - arXiv preprint arXiv:1505.04467, 2015214회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Exploring nearest neighbor approaches for image captioning",
        "year": null
    },
    "Using k-poselets for detecting people and localizing their keypoints": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "설명": "A k-poselet is a deformable part model (DPM) with k parts, where each of the parts is a poselet, aligned to a specific configuration of keypoints based on ground-truth annotations. A separate template is used to learn the appearance of each part. The parts are allowed to move with respect to each other with a deformation cost that is learned at training time. This model is richer than both the traditional version of poselets and DPMs. It enables a unified approach to person detection and keypoint prediction which, barring contemporaneous approaches based on CNN features, achieves state-of-the-art keypoint prediction while maintaining competitive detection performance.",
            "저자": "Georgia Gkioxari, Bharath Hariharan, Ross Girshick, Jitendra Malik",
            "전체 인용횟수": "210회 인용20142015201620172018201920202021202220234172127222522222715",
            "컨퍼런스": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "페이지": "3582-3589",
            "학술 문서": "Using k-poselets for detecting people and localizing their keypointsG Gkioxari, B Hariharan, R Girshick, J Malik - Proceedings of the IEEE Conference on Computer …, 2014210회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Using k-poselets for detecting people and localizing their keypoints",
        "year": null
    },
    "A large-scale study on unsupervised spatiotemporal representation learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021",
            "설명": "We present a large-scale study on unsupervised spatiotemporal representation learning from videos. With a unified perspective on four recent image-based frameworks, we study a simple objective that can easily generalize all these methods to space-time. Our objective encourages temporally-persistent features in the same video, and in spite of its simplicity, it works surprisingly well across:(i) different unsupervised frameworks,(ii) pre-training datasets,(iii) downstream datasets, and (iv) backbone architectures. We draw a series of intriguing observations from this study, eg, we discover that encouraging long-spanned persistency can be effective even if the timespan is 60 seconds. In addition to state-of-the-art results in multiple benchmarks, we report a few promising cases in which unsupervised pre-training can outperform its supervised counterpart. Code will be made available at https://github. com/facebookresearch/SlowFast.",
            "저자": "Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, Kaiming He",
            "전체 인용횟수": "196회 인용20202021202220232208886",
            "컨퍼런스": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "페이지": "3299-3309",
            "학술 문서": "A large-scale study on unsupervised spatiotemporal representation learningC Feichtenhofer, H Fan, B Xiong, R Girshick, K He - Proceedings of the IEEE/CVF Conference on Computer …, 2021196회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A large-scale study on unsupervised spatiotemporal representation learning",
        "year": null
    },
    "Actions and attributes from wholes and parts": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "We investigate the importance of parts for the tasks of action and attribute classification. We develop a part-based approach by leveraging convolutional network features inspired by recent advances in computer vision. Our part detectors are a deep version of poselets and capture parts of the human body under a distinct set of poses. For the tasks of action and attribute classification, we train holistic convolutional neural networks and show that adding parts leads to top-performing results for both tasks. We observe that for deeper networks parts are less significant. In addition, we demonstrate the effectiveness of our approach when we replace an oracle person detector, as is the default in the current evaluation protocol for both tasks, with a state-of-the-art person detection system.",
            "저자": "Georgia Gkioxari, Ross Girshick, Jitendra Malik",
            "전체 인용횟수": "183회 인용2014201520162017201820192020202120222023131820281826192014",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "2470-2478",
            "학술 문서": "Actions and attributes from wholes and partsG Gkioxari, R Girshick, J Malik - Proceedings of the IEEE international conference on …, 2015183회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Actions and attributes from wholes and parts",
        "year": null
    },
    "R-CNNs for pose estimation and action detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/6/19",
            "설명": "We present convolutional neural networks for the tasks of keypoint (pose) prediction and action classification of people in unconstrained images. Our approach involves training an R-CNN detector with loss functions depending on the task being tackled. We evaluate our method on the challenging PASCAL VOC dataset and compare it to previous leading approaches. Our method gives state-of-the-art results for keypoint and action prediction. Additionally, we introduce a new dataset for action detection, the task of simultaneously localizing people and classifying their actions, and present results using our approach.",
            "저널": "arXiv preprint arXiv:1406.5212",
            "저자": "Georgia Gkioxari, Bharath Hariharan, Ross Girshick, Jitendra Malik",
            "전체 인용횟수": "177회 인용20152016201720182019202020212022202320202123252322138",
            "학술 문서": "R-cnns for pose estimation and action detectionG Gkioxari, B Hariharan, R Girshick, J Malik - arXiv preprint arXiv:1406.5212, 2014177회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "R-CNNs for pose estimation and action detection",
        "year": null
    },
    "Boundary iou: Improving object-centric image segmentation evaluation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021",
            "설명": "We present Boundary IoU (Intersection-over-Union), a new segmentation evaluation measure focused on boundary quality. We perform an extensive analysis across different error types and object sizes and show that Boundary IoU is significantly more sensitive than the standard Mask IoU measure to boundary errors for large objects and does not over-penalize errors on smaller objects. The new quality measure displays several desirable characteristics like symmetry wrt prediction/ground truth pairs and balanced responsiveness across scales, which makes it more suitable for segmentation evaluation than other boundary-focused measures like Trimap IoU and F-measure. Based on Boundary IoU, we update the standard evaluation protocols for instance and panoptic segmentation tasks by proposing the Boundary AP (Average Precision) and Boundary PQ (Panoptic Quality) metrics, respectively. Our experiments show that the new evaluation metrics track boundary quality improvements that are generally overlooked by current Mask IoU-based evaluation metrics. We hope that the adoption of the new boundary-sensitive evaluation metrics will lead to rapid progress in segmentation methods that improve boundary quality.",
            "저자": "Bowen Cheng, Ross Girshick, Piotr Dollár, Alexander C Berg, Alexander Kirillov",
            "전체 인용횟수": "174회 인용20202021202220232207078",
            "컨퍼런스": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "페이지": "15334-15342",
            "학술 문서": "Boundary IoU: Improving object-centric image segmentation evaluationB Cheng, R Girshick, P Dollár, AC Berg, A Kirillov - Proceedings of the IEEE/CVF Conference on Computer …, 2021174회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Boundary iou: Improving object-centric image segmentation evaluation",
        "year": null
    },
    "Object detection and classification in images": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/1/2",
            "발명자": "Jian Sun, Ross Girshick, Shaoqing Ren, Kaiming He",
            "설명": "Systems, methods, and computer-readable media for providing fast and accurate object detection and classification in images are described herein. In some examples, a computing device can receive an input image. The computing device can process the image, and generate a convolutional feature map. In some configurations, the convolutional feature map can be processed through a Region Proposal Network (RPN) to generate proposals for candidate objects in the image. In various examples, the computing device can process the convolutional feature map with the proposals through a Fast Region-Based Convolutional Neural Network (FRCN) proposal classifier to determine a class of each object in the image and a confidence score associated therewith. The computing device can then provide a requestor with an output including the object classification and/or confidence score.",
            "전체 인용횟수": "130회 인용20182019202020212022202312824283316",
            "출원번호": "15001417",
            "특허 번호": "9858496",
            "특허청": "US",
            "학술 문서": "Object detection and classification in imagesJ Sun, R Girshick, S Ren, K He - US Patent 9,858,496, 2018130회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Object detection and classification in images",
        "year": null
    },
    "Understanding objects in detail with fine-grained attributes": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "설명": "We study the problem of understanding objects in detail, intended as recognizing a wide array of fine-grained object attributes. To this end, we introduce a dataset of 7,413 airplanes annotated in detail with parts and their attributes, leveraging images donated by airplane spotters and crowdsourcing both the design and collection of the detailed annotations. We provide a number of insights that should help researchers interested in designing fine-grained datasets for other basic level categories. We show that the collected data can be used to study the relation between part detection and attribute prediction by diagnosing the performance of classifiers that pool information from different parts of an object. We note that the prediction of certain attributes can benefit substantially from accurate part detection. We also show that, differently from previous results in object detection, employing a large number of part templates can improve detection accuracy at the expenses of detection speed. We finally propose a coarse-to-fine approach to speed up detection through a hierarchical cascade algorithm.",
            "저자": "Andrea Vedaldi, Siddharth Mahendran, Stavros Tsogkas, Subhransu Maji, Ross Girshick, Juho Kannala, Esa Rahtu, Iasonas Kokkinos, Matthew B Blaschko, David Weiss, Ben Taskar, Karen Simonyan, Naomi Saphra, Sammy Mohamed",
            "전체 인용횟수": "125회 인용2014201520162017201820192020202120222023316272517118665",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "3622-3629",
            "학술 문서": "Understanding objects in detail with fine-grained attributesA Vedaldi, S Mahendran, S Tsogkas, S Maji, R Girshick… - Proceedings of the IEEE conference on computer …, 2014125회 인용 관련 학술자료 전체 32개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Understanding objects in detail with fine-grained attributes",
        "year": null
    },
    "Benchmarking detection transfer learning with vision transformers": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021/11/22",
            "설명": "Object detection is a central downstream task used to test if pre-trained network parameters confer benefits, such as improved accuracy or training speed. The complexity of object detection methods can make this benchmarking non-trivial when new architectures, such as Vision Transformer (ViT) models, arrive. These difficulties (e.g., architectural incompatibility, slow training, high memory consumption, unknown training formulae, etc.) have prevented recent studies from benchmarking detection transfer learning with standard ViT models. In this paper, we present training techniques that overcome these challenges, enabling the use of standard ViT models as the backbone of Mask R-CNN. These tools facilitate the primary goal of our study: we compare five ViT initializations, including recent state-of-the-art self-supervised learning methods, supervised initialization, and a strong random initialization baseline. Our results show that recent masking-based unsupervised learning methods may, for the first time, provide convincing transfer learning improvements on COCO, increasing box AP up to 4% (absolute) over supervised and prior self-supervised pre-training methods. Moreover, these masking-based initializations scale better, with the improvement growing as model size increases.",
            "저널": "arXiv preprint arXiv:2111.11429",
            "저자": "Yanghao Li, Saining Xie, Xinlei Chen, Piotr Dollar, Kaiming He, Ross Girshick",
            "전체 인용횟수": "105회 인용202220234461",
            "학술 문서": "Benchmarking detection transfer learning with vision transformersY Li, S Xie, X Chen, P Dollar, K He, R Girshick - arXiv preprint arXiv:2111.11429, 2021105회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Benchmarking detection transfer learning with vision transformers",
        "year": null
    },
    "A multigrid method for efficiently training video models": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020",
            "설명": "Training competitive deep video models is an order of magnitude slower than training their counterpart image models. Slow training causes long research cycles, which hinders progress in video understanding research. Following standard practice for training image models, video model training has used a fixed mini-batch shape: a specific number of clips, frames, and spatial size. However, what is the optimal shape? High resolution models perform well, but train slowly. Low resolution models train faster, but are less accurate. Inspired by multigrid methods in numerical optimization, we propose to use variable mini-batch shapes with different spatial-temporal resolutions that are varied according to a schedule. The different shapes arise from resampling the training data on multiple sampling grids. Training is accelerated by scaling up the mini-batch size and learning rate when shrinking the other dimensions. We empirically demonstrate a general and robust grid schedule that yields a significant out-of-the-box training speedup without a loss in accuracy for different models (I3D, non-local, SlowFast), datasets (Kinetics, Something-Something, Charades), and training settings (with and without pre-training, 128 GPUs or 1 GPU). As an illustrative example, the proposed multigrid method trains a ResNet-50 SlowFast network 4.5 x faster (wall-clock time, same hardware) while also improving accuracy (+ 0.8% absolute) on Kinetics-400 compared to baseline training. Code is available online.",
            "저자": "Chao-Yuan Wu, Ross Girshick, Kaiming He, Christoph Feichtenhofer, Philipp Krahenbuhl",
            "전체 인용횟수": "104회 인용20192020202120222023113413019",
            "컨퍼런스": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "페이지": "153-162",
            "학술 문서": "A multigrid method for efficiently training video modelsCY Wu, R Girshick, K He, C Feichtenhofer… - Proceedings of the IEEE/CVF Conference on Computer …, 2020104회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A multigrid method for efficiently training video models",
        "year": null
    },
    "Object detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021/10/13",
            "게시자": "Springer International Publishing",
            "도서": "Computer Vision: A Reference Guide",
            "설명": "BackgroundThe goal of object detection is to detect all instances of objects from one or several known classes, such as people, cars, or faces in an image. Typically only a small number of objects are present in the image, but there is a very large number of possible locations and scales at which they can occur and that need to somehow be explored.Each detection is reported with some form of pose information. This could be as simple as the location of the object, a location and scale, a bounding box, or a segmentation mask. In other situations the pose information is more detailed and contains the parameters of a linear or nonlinear transformation. For example a face detector may compute the locations of the eyes, nose, and mouth, in addition to the bounding box of the face. An example of a bicycle detection that specifies the locations of certain parts is shown in Fig. 1. The pose could also be defined by a three …",
            "저자": "Yali Amit, Pedro Felzenszwalb, Ross Girshick",
            "전체 인용횟수": "103회 인용201420152016201720182019202020212022202311525117102436",
            "페이지": "875-883",
            "학술 문서": "Object detectionY Amit, P Felzenszwalb, R Girshick - Computer Vision: A Reference Guide, 2021103회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Object detection",
        "year": null
    },
    "Phyre: A new benchmark for physical reasoning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "권": "32",
            "설명": "Understanding and reasoning about physics is an important ability of intelligent agents. We develop the PHYRE benchmark for physical reasoning that contains a set of simple classical mechanics puzzles in a 2D physical environment. The benchmark is designed to encourage the development of learning algorithms that are sample-efficient and generalize well across puzzles. We test several modern learning algorithms on PHYRE and find that these algorithms fall short in solving the puzzles efficiently. We expect that PHYRE will encourage the development of novel sample-efficient agents that learn efficient but useful models of physics. For code and to play PHYRE for yourself, please visit https://player. phyre. ai.",
            "저널": "Advances in Neural Information Processing Systems",
            "저자": "Anton Bakhtin, Laurens van der Maaten, Justin Johnson, Laura Gustafson, Ross Girshick",
            "전체 인용횟수": "103회 인용20192020202120222023521332123",
            "학술 문서": "Phyre: A new benchmark for physical reasoningA Bakhtin, L van der Maaten, J Johnson, L Gustafson… - Advances in Neural Information Processing Systems, 2019103회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Phyre: A new benchmark for physical reasoning",
        "year": null
    },
    "Sparselet models for efficient multiclass object detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012",
            "게시자": "Springer Berlin Heidelberg",
            "설명": " We develop an intermediate representation for deformable part models and show that this representation has favorable performance characteristics for multi-class problems when the number of classes is high. Our model uses sparse coding of part filters to represent each filter as a sparse linear combination of shared dictionary elements. This leads to a universal set of parts that are shared among all object classes. Reconstruction of the original part filter responses via sparse matrix-vector product reduces computation relative to conventional part filter convolutions. Our model is well suited to a parallel implementation, and we report a new GPU DPM implementation that takes advantage of sparse coding of part filters. The speed-up offered by our intermediate representation and parallel computation enable real-time DPM detection of 20 different object classes on a laptop computer.",
            "저자": "Hyun Oh Song, Stefan Zickler, Tim Althoff, Ross Girshick, Mario Fritz, Christopher Geyer, Pedro Felzenszwalb, Trevor Darrell",
            "전체 인용횟수": "103회 인용201220132014201520162017201820192020202120222023121222210473111",
            "컨퍼런스": "Computer Vision–ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part II 12",
            "페이지": "802-815",
            "학술 문서": "Sparselet models for efficient multiclass object detectionHO Song, S Zickler, T Althoff, R Girshick, M Fritz… - Computer Vision–ECCV 2012: 12th European …, 2012103회 인용 관련 학술자료 전체 22개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Sparselet models for efficient multiclass object detection",
        "year": null
    },
    "From rigid templates to grammars: Object detection with structured models": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/1/1",
            "기관": "University of Chicago",
            "설명": "We develop models for localizing instances of a generic object category, such as cars or people, in images. We define these models using a grammar formalism. In this formalism compositional rules are used to encode models that can range in complexity from simple rigid templates to rich deformable part models with variable structure. A central contribution of this dissertation is an exploration along this axis, wherein we gradually enrich our object category representations. We demonstrate that these richer models lead to improved object detection performance on challenging datasets such as the PASCAL VOC Challenges.",
            "저자": "Ross Brook Girshick",
            "전체 인용횟수": "89회 인용20122013201420152016201720182019202020212022202313748135211111211",
            "학술 문서": "From rigid templates to grammars: Object detection with structured modelsRB Girshick - 201289회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "From rigid templates to grammars: Object detection with structured models",
        "year": null
    },
    "Visual object detection with deformable part models": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/9/1",
            "게시자": "ACM",
            "권": "56",
            "설명": "We describe a state-of-the-art system for finding objects in cluttered images. Our system is based on deformable models that represent objects using local part templates and geometric constraints on the locations of parts. We reduce object detection to classification with latent variables. The latent variables introduce invariances that make it possible to detect objects with highly variable appearance. We use a generalization of support vector machines to incorporate latent information during training. This has led to a general framework for discriminative training of classifiers with latent variables. Discriminative training benefits from large training datasets. In practice we use an iterative algorithm that alternates between estimating latent values for positive examples and solving a large convex optimization problem. Practical optimization of this large convex problem can be done using active set techniques for adaptive …",
            "저널": "Communications of the ACM",
            "저자": "Pedro Felzenszwalb, Ross Girshick, David McAllester, Deva Ramanan",
            "전체 인용횟수": "83회 인용2014201520162017201820192020202120222023586781313995",
            "페이지": "97-105",
            "학술 문서": "Visual object detection with deformable part modelsP Felzenszwalb, R Girshick, D McAllester, D Ramanan - Communications of the ACM, 201383회 인용 관련 학술자료 전체 2개의 버전 ",
            "호": "9"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Visual object detection with deformable part models",
        "year": null
    },
    "Fast and accurate model scaling": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021",
            "설명": "In this work we analyze strategies for convolutional neural network scaling; that is, the process of scaling a base convolutional network to endow it with greater computational complexity and consequently representational power. Example scaling strategies may include increasing model width, depth, resolution, etc. While various scaling strategies exist, their tradeoffs are not fully understood. Existing analysis typically focuses on the interplay of accuracy and flops (floating point operations). Yet, as we demonstrate, various scaling strategies affect model parameters, activations, and consequently actual runtime quite differently. In our experiments we show the surprising result that numerous scaling strategies yield networks with similar accuracy but with widely varying properties. This leads us to propose a simple fast compound scaling strategy that encourages primarily scaling model width, while scaling depth and resolution to a lesser extent. Unlike currently popular scaling strategies, which result in about O (s) increase in model activation wrt scaling flops by a factor of s, the proposed fast compound scaling results in close to O (sqrt s) increase in activations, while achieving excellent accuracy. Fewer activations leads to speedups on modern memory-bandwidth limited hardware (eg, GPUs). More generally, we hope this work provides a framework for analyzing scaling strategies under various computational constraints.",
            "저자": "Piotr Dollár, Mannat Singh, Ross Girshick",
            "전체 인용횟수": "82회 인용202120222023112446",
            "컨퍼런스": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "페이지": "924-932",
            "학술 문서": "Fast and accurate model scalingP Dollár, M Singh, R Girshick - Proceedings of the IEEE/CVF Conference on Computer …, 202182회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Fast and accurate model scaling",
        "year": null
    },
    "Are labels necessary for neural architecture search?": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020",
            "게시자": "Springer International Publishing",
            "설명": " Existing neural network architectures in computer vision—whether designed by humans or by machines—were typically found using both images and their associated labels. In this paper, we ask the question: can we find high-quality neural architectures using only images, but no human-annotated labels? To answer this question, we first define a new setup called Unsupervised Neural Architecture Search (UnNAS). We then conduct two sets of experiments. In sample-based experiments, we train a large number (500) of diverse architectures with either supervised or unsupervised objectives, and find that the architecture rankings produced with and without labels are highly correlated. In search-based experiments, we run a well-established NAS algorithm (DARTS) using various unsupervised objectives, and report that the architectures searched without labels can be competitive to their counterparts …",
            "저자": "Chenxi Liu, Piotr Dollár, Kaiming He, Ross Girshick, Alan Yuille, Saining Xie",
            "전체 인용횟수": "79회 인용20202021202220235272918",
            "컨퍼런스": "Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IV 16",
            "페이지": "798-813",
            "학술 문서": "Are labels necessary for neural architecture search?C Liu, P Dollár, K He, R Girshick, A Yuille, S Xie - Computer Vision–ECCV 2020: 16th European …, 202079회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Are labels necessary for neural architecture search?",
        "year": null
    },
    "Learning by asking questions": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "We introduce an interactive learning framework for the development and testing of intelligent visual systems, called learning-by-asking (LBA). We explore LBA in context of the Visual Question Answering (VQA) task. LBA differs from standard VQA training in that most questions are not observed during training time, and the learner must ask questions it wants answers to. Thus, LBA more closely mimics natural learning and has the potential to be more data-efficient than the traditional VQA setting. We present a model that performs LBA on the CLEVR dataset, and show that it automatically discovers an easy-to-hard curriculum when learning interactively from an oracle. Our LBA generated data consistently matches or outperforms the CLEVR train data and is more sample efficient. We also show that our model asks questions that generalize to state-of-the-art VQA models and to novel test time distributions.",
            "저자": "Ishan Misra, Ross Girshick, Rob Fergus, Martial Hebert, Abhinav Gupta, Laurens Van Der Maaten",
            "전체 인용횟수": "79회 인용20182019202020212022202361915171110",
            "컨퍼런스": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "페이지": "11-20",
            "학술 문서": "Learning by asking questionsI Misra, R Girshick, R Fergus, M Hebert, A Gupta… - Proceedings of the IEEE Conference on Computer …, 201879회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning by asking questions",
        "year": null
    },
    "Human body pose estimation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/1/28",
            "발명자": "Jamie Daniel Joseph Shotton, Shahram Izadi, Otmar Hilliges, David Kim, David Geoffrey Molyneaux, Matthew Darius Cook, Pushmeet Kohli, Antonio Criminisi, Ross Brook Girshick, Andrew William Fitzgibbon",
            "설명": "Techniques for human body pose estimation are disclosed herein. Images such as depth images, silhouette images, or volumetric images may be generated and pixels or voxels of the images may be identified. The techniques may process the pixels or voxels to determine a probability that each pixel or voxel is associated with a segment of a body captured in the image or to determine a three-dimensional representation for each pixel or voxel that is associated with a location on a canonical body. These probabilities or three-dimensional representations may then be utilized along with the images to construct a posed model of the body captured in the image.",
            "전체 인용횟수": "74회 인용201320142015201620172018201920202021202220237413876731081",
            "출원번호": "13040205",
            "특허 번호": "8638985",
            "특허청": "US",
            "학술 문서": "Human body pose estimationJDJ Shotton, S Izadi, O Hilliges, D Kim, DG Molyneaux… - US Patent 8,638,985, 201474회 인용 관련 학술자료 전체 4개의 버전 Journal Publications (Peer Reviewed) 1. J. Shotton, R. Girshick, A. Fitzgibbon, T. Sharp, M. Cook, M. Finocchio, R. Moore, P. Kohli, A.*RB Girshick - US Patent App, 2012관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Human body pose estimation",
        "year": null
    },
    "Inferring 3d object pose in RGB-D images": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/2/16",
            "설명": "The goal of this work is to replace objects in an RGB-D scene with corresponding 3D models from a library. We approach this problem by first detecting and segmenting object instances in the scene using the approach from Gupta et al. [13]. We use a convolutional neural network (CNN) to predict the pose of the object. This CNN is trained using pixel normals in images containing rendered synthetic objects. When tested on real data, it outperforms alternative algorithms trained on real data. We then use this coarse pose estimate along with the inferred pixel support to align a small number of prototypical models to the data, and place the model that fits the best into the scene. We observe a 48% relative improvement in performance at the task of 3D detection over the current state-of-the-art [33], while being an order of magnitude faster at the same time.",
            "저널": "arXiv preprint arXiv:1502.04652",
            "저자": "Saurabh Gupta, Pablo Arbeláez, Ross Girshick, Jitendra Malik",
            "전체 인용횟수": "56회 인용201520162017201820192020202120222023375987942",
            "학술 문서": "Inferring 3d object pose in RGB-D imagesS Gupta, P Arbeláez, R Girshick, J Malik - arXiv preprint arXiv:1502.04652, 201556회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Inferring 3d object pose in RGB-D images",
        "year": null
    },
    "Impact of data on generalization of AI for surgical intelligence applications": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020/12/17",
            "게시자": "Nature Publishing Group UK",
            "권": "10",
            "설명": "AI is becoming ubiquitous, revolutionizing many aspects of our lives. In surgery, it is still a promise. AI has the potential to improve surgeon performance and impact patient care, from post-operative debrief to real-time decision support. But, how much data is needed by an AI-based system to learn surgical context with high fidelity? To answer this question, we leveraged a large-scale, diverse, cholecystectomy video dataset. We assessed surgical workflow recognition and report a deep learning system, that not only detects surgical phases, but does so with high accuracy and is able to generalize to new settings and unseen medical centers. Our findings provide a solid foundation for translating AI applications from research to practice, ushering in a new era of surgical intelligence.",
            "저널": "Scientific reports",
            "저자": "Omri Bar, Daniel Neimark, Maya Zohar, Gregory D Hager, Ross Girshick, Gerald M Fried, Tamir Wolf, Dotan Asselmann",
            "전체 인용횟수": "52회 인용202120222023121623",
            "페이지": "22208",
            "학술 문서": "Impact of data on generalization of AI for surgical intelligence applicationsO Bar, D Neimark, M Zohar, GD Hager, R Girshick… - Scientific reports, 202052회 인용 관련 학술자료 전체 10개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Impact of data on generalization of AI for surgical intelligence applications",
        "year": null
    },
    "The three R’s of computer vision: Recognition, reconstruction and reorganization": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/3/1",
            "게시자": "North-Holland",
            "권": "72",
            "설명": "We argue for the importance of the interaction between recognition, reconstruction and re-organization, and propose that as a unifying framework for computer vision. In this view, recognition of objects is reciprocally linked to re-organization, with bottom-up grouping processes generating candidates, which can be classified using top down knowledge, following which the segmentations can be refined again. Recognition of 3D objects could benefit from a reconstruction of 3D structure, and 3D reconstruction can benefit from object category-specific priors. We also show that reconstruction of 3D structure from video data goes hand in hand with the reorganization of the scene. We demonstrate pipelined versions of two systems, one for RGB-D images, and another for RGB images, which produce rich 3D scene interpretations in this framework.",
            "저널": "Pattern Recognition Letters",
            "저자": "Jitendra Malik, Pablo Arbeláez, Joao Carreira, Katerina Fragkiadaki, Ross Girshick, Georgia Gkioxari, Saurabh Gupta, Bharath Hariharan, Abhishek Kar, Shubham Tulsiani",
            "전체 인용횟수": "50회 인용20162017201820192020202120222023461064568",
            "페이지": "4-14",
            "학술 문서": "The three R’s of computer vision: Recognition, reconstruction and reorganizationJ Malik, P Arbeláez, J Carreira, K Fragkiadaki… - Pattern Recognition Letters, 201650회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The three R’s of computer vision: Recognition, reconstruction and reorganization",
        "year": null
    },
    "A unified architecture for instance and semantic segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "[1] He, K., Zhang, X., Ren, S., & Sun, J. Deep residual learning for image recognition. CVPR 2016.[2] Xie, S., Girshick, R., Dollár, P., Tu, Z., & He, K. Aggregated residual transformations for deep neural networks. CVPR 2017.[3] Lin, TY, Dollár, P., Girshick, R., He, K., Hariharan, B., & Belongie, S. Feature pyramid networks for object detection. CVPR 2017.",
            "저자": "Alexander Kirillov, Kaiming He, Ross Girshick, Piotr Dollár",
            "전체 인용횟수": "44회 인용20182019202020212022202332681312",
            "학술 문서": "A unified architecture for instance and semantic segmentationA Kirillov, K He, R Girshick, P Dollár - CVPR, 201744회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A unified architecture for instance and semantic segmentation",
        "year": null
    },
    "Method and system for using machine-learning for object instance segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020/7/14",
            "발명자": "Kaiming He, Georgia Gkioxari, Piotr Dollar, Ross Girshick",
            "설명": "In one embodiment, a method includes a computing system accessing a training image. The system may generate a feature map for the training image using a first neural network. The system may identify a region of interest in the feature map and generate a regional feature map for the region of interest based on sampling locations defined by a sampling region. The sampling region and the region of interest may correspond to the same region in the feature map. The system may generate an instance segmentation mask associated with the region of interest by processing the regional feature map using a second neural network. The second neural network may be trained using the instance segmentation mask. Once trained, the second neural network is configured to generate instance segmentation masks for object instances depicted in images.",
            "전체 인용횟수": "43회 인용2020202120222023414178",
            "출원번호": "15922734",
            "특허 번호": "10713794",
            "특허청": "US",
            "학술 문서": "Method and system for using machine-learning for object instance segmentationK He, G Gkioxari, P Dollar, R Girshick - US Patent 10,713,794, 202043회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Method and system for using machine-learning for object instance segmentation",
        "year": null
    },
    "Evaluating large-vocabulary object detectors: The devil is in the details": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021/2/1",
            "설명": "By design, average precision (AP) for object detection aims to treat all classes independently: AP is computed independently per category and averaged. On one hand, this is desirable as it treats all classes equally. On the other hand, it ignores cross-category confidence calibration, a key property in real-world use cases. Unfortunately, under important conditions (i.e., large vocabulary, high instance counts) the default implementation of AP is neither category independent, nor does it directly reward properly calibrated detectors. In fact, we show that on LVIS the default implementation produces a gameable metric, where a simple, un-intuitive re-ranking policy can improve AP by a large margin. To address these limitations, we introduce two complementary metrics. First, we present a simple fix to the default AP implementation, ensuring that it is independent across categories as originally intended. We benchmark recent LVIS detection advances and find that many reported gains do not translate to improvements under our new evaluation, suggesting recent improvements may arise from difficult to interpret changes to cross-category rankings. Given the importance of reliably benchmarking cross-category rankings, we consider a pooled version of AP (AP-Pool) that rewards properly calibrated detectors by directly comparing cross-category rankings. Finally, we revisit classical approaches for calibration and find that explicitly calibrating detectors improves state-of-the-art on AP-Pool by 1.7 points",
            "저널": "arXiv preprint arXiv:2102.01066",
            "저자": "Achal Dave, Piotr Dollár, Deva Ramanan, Alexander Kirillov, Ross Girshick",
            "전체 인용횟수": "36회 인용20212022202310188",
            "학술 문서": "Evaluating large-vocabulary object detectors: The devil is in the detailsA Dave, P Dollár, D Ramanan, A Kirillov, R Girshick - arXiv preprint arXiv:2102.01066, 202136회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Evaluating large-vocabulary object detectors: The devil is in the details",
        "year": null
    },
    "Training deformable part models with decorrelated features": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013",
            "설명": "In this paper, we show how to train a deformable part model (DPM) fast--typically in less than 20 minutes, or four times faster than the current fastest method--while maintaining high average precision on the PASCAL VOC datasets. At the core of our approach is\" latent LDA,\" a novel generalization of linear discriminant analysis for learning latent variable models. Unlike latent SVM, latent LDA uses efficient closed-form updates and does not require an expensive search for hard negative examples. Our approach also acts as a springboard for a detailed experimental study of DPM training. We isolate and quantify the impact of key training factors for the first time (eg, How important are discriminative SVM filters? How important is joint parameter estimation? How many negative images are needed for training?). Our findings yield useful insights for researchers working with Markov random fields and partbased models, and have practical implications for speeding up tasks such as model selection.",
            "저자": "Ross Girshick, Jitendra Malik",
            "전체 인용횟수": "35회 인용20142015201620172018201920202021202261171232",
            "컨퍼런스": "Proceedings of the IEEE International Conference on Computer Vision",
            "페이지": "3016-3023",
            "학술 문서": "Training deformable part models with decorrelated featuresR Girshick, J Malik - Proceedings of the IEEE International Conference on …, 201335회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Training deformable part models with decorrelated features",
        "year": null
    },
    "PyTorchVideo: A deep learning library for video understanding": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021/10/17",
            "도서": "Proceedings of the 29th ACM International Conference on Multimedia",
            "설명": "We introduce PyTorchVideo, an open-source deep-learning library that provides a rich set of modular, efficient, and reproducible components for a variety of video understanding tasks, including classification, detection, self-supervised learning, and low-level processing. The library covers a full stack of video understanding tools including multimodal data loading, transformations, and models that reproduce state-of-the-art performance. PyTorchVideo further supports hardware acceleration that enables real-time inference on mobile devices. The library is based on PyTorch and can be used by any training framework; for example, PyTorchLightning, PySlowFast, or Classy Vision. PyTorchVideo is available at https://pytorchvideo.org/.",
            "저자": "Haoqi Fan, Tullie Murrell, Heng Wang, Kalyan Vasudev Alwala, Yanghao Li, Yilei Li, Bo Xiong, Nikhila Ravi, Meng Li, Haichuan Yang, Jitendra Malik, Ross Girshick, Matt Feiszli, Aaron Adcock, Wan-Yen Lo, Christoph Feichtenhofer",
            "전체 인용횟수": "33회 인용20212022202322110",
            "페이지": "3783-3786",
            "학술 문서": "PyTorchVideo: A deep learning library for video understandingH Fan, T Murrell, H Wang, KV Alwala, Y Li, Y Li… - Proceedings of the 29th ACM international conference …, 202133회 인용 관련 학술자료 전체 5개의 버전 PyTorchVideoH Fan, T Murrell, H Wang, KV Alwala, Y Li, Y Li… - Proceedings of the 29th ACM International Conference …, 2021"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "PyTorchVideo: A deep learning library for video understanding",
        "year": null
    },
    "Predicting joint positions": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/10/29",
            "발명자": "Jamie Daniel Joseph Shotton, Pushmeet Kohli, Ross Brook Girshick, Andrew Fitzgibbon, Antonio Criminisi",
            "설명": "Predicting joint positions is described, for example, to find joint positions of humans or animals (or parts thereof) in an image to control a computer game or for other applications. In an embodiment image elements of a depth image make joint position votes so that for example, an image element depicting part of a torso may vote for a position of a neck joint, a left knee joint and a right knee joint. A random decision forest may be trained to enable image elements to vote for the positions of one or more joints and the training process may use training images of bodies with specified joint positions. In an example a joint position vote is expressed as a vector representing a distance and a direction of a joint position from an image element making the vote. The random decision forest may be trained using a mixture of objectives.",
            "전체 인용횟수": "33회 인용2013201420152016201720182019202020212022161356533",
            "출원번호": "13050858",
            "특허 번호": "8571263",
            "특허청": "US",
            "학술 문서": "Predicting joint positionsJDJ Shotton, P Kohli, RB Girshick, A Fitzgibbon… - US Patent 8,571,263, 201333회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Predicting joint positions",
        "year": null
    },
    "Discriminatively activated sparselets": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/2/13",
            "게시자": "PMLR",
            "설명": "Shared representations are highly appealing due to their potential for gains in computational and statistical efficiency. Compressing a shared representation leads to greater computational savings, but at the same time can severely decrease performance on a target task. Recently, sparselets (Song et al., 2012) were introduced as a new shared intermediate representation for multiclass object detection with deformable part models (Felzenszwalb et al., 2010a), showing significant speedup factors, but with a large decrease in task performance. In this paper we describe a new training framework that learns which sparselets to activate in order to optimize a discriminative objective, leading to larger speedup factors with no decrease in task performance. We first reformulate sparselets in a general structured output prediction framework, then analyze when sparselets lead to computational efficiency gains, and lastly show experimental results on object detection and image classification tasks. Our experimental results demonstrate that discriminative activation substantially outperforms the previous reconstructive approach which, together with our structured output prediction formulation, make sparselets broadly applicable and significantly more effective.",
            "저자": "Ross Girshick, Hyun Oh Song, Trevor Darrell",
            "전체 인용횟수": "33회 인용20132014201520162017201820192020212102131",
            "컨퍼런스": "International Conference on Machine Learning",
            "페이지": "196-204",
            "학술 문서": "Discriminatively activated sparseletsR Girshick, HO Song, T Darrell - International Conference on Machine Learning, 201333회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Discriminatively activated sparselets",
        "year": null
    },
    "Machine-learning models based on non-local neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2023/1/24",
            "발명자": "Kaiming He, Ross Girshick, Xiaolong Wang",
            "설명": "In one embodiment, a method includes training a baseline machine-learning model based on a neural network comprising a plurality of stages, wherein each stage comprises a plurality of neural blocks, accessing a plurality of training samples comprising a plurality of content objects, respectively, determining one or more non-local operations, wherein each non-local operation is based on one or more pairwise functions and one or more unary functions, generating one or more non-local blocks based on the plurality of training samples and the one or more non-local operations, determining a stage from the plurality of stages of the neural network, and training a non-local machine-learning model by inserting each of the one or more non-local blocks in between at least two of the plurality of neural blocks in the determined stage of the neural network.",
            "전체 인용횟수": "29회 인용202020212022202387113",
            "출원번호": "16192649",
            "특허 번호": "11562243",
            "특허청": "US",
            "학술 문서": "Machine-learning models based on non-local neural networksK He, R Girshick, X Wang - US Patent 11,562,243, 202329회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Machine-learning models based on non-local neural networks",
        "year": null
    },
    "Systems and methods for optimizing pose estimation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020/8/4",
            "발명자": "Peizhao Zhang, Peter Vajda, Kevin Matzen, Ross Girshick",
            "설명": "In one embodiment, a system may access first, second, and third probability models that are respectively associated with predetermined first and second body parts and a predetermined segment connecting the first and second body parts. Each model includes probability values associated with regions in an image, with each value representing the probability of the associated region containing the associated body part or segment. The system may select a first and second region based on the first probability model and a third region based on the second probability model. Based on the third probability model, the system may compute a first probability score for regions connecting the first and third regions and a second probability score for regions connecting the second and third regions. Based on the first and second probability scores, the system may select the first region to indicate where the predetermined first …",
            "전체 인용횟수": "26회 인용202020212022202321266",
            "출원번호": "16236974",
            "특허 번호": "10733431",
            "특허청": "US",
            "학술 문서": "Systems and methods for optimizing pose estimationP Zhang, P Vajda, K Matzen, R Girshick - US Patent 10,733,431, 202026회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Systems and methods for optimizing pose estimation",
        "year": null
    },
    "Revisiting weakly supervised pre-training of visual perception models": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2022",
            "설명": "Model pre-training is a cornerstone of modern visual recognition systems. Although fully supervised pre-training on datasets like ImageNet is still the de-facto standard, recent studies suggest that large-scale weakly supervised pre-training can outperform fully supervised approaches. This paper revisits weakly-supervised pre-training of models using hashtag supervision with modern versions of residual networks and the largest-ever dataset of images and corresponding hashtags. We study the performance of the resulting models in various transfer-learning settings including zero-shot transfer. We also compare our models with those obtained via large-scale self-supervised learning. We find our weakly-supervised models to be very competitive across all settings, and find they substantially outperform their self-supervised counterparts. We also include an investigation into whether our models learned potentially troubling associations or stereotypes. Overall, our results provide a compelling argument for the use of weakly supervised learning in the development of visual recognition systems. Our models, Supervised Weakly through hashtAGs (SWAG), are available publicly.",
            "저자": "Mannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de Freitas Reis, Bugra Gedik, Raj Prateek Kosaraju, Dhruv Mahajan, Ross Girshick, Piotr Dollár, Laurens Van Der Maaten",
            "전체 인용횟수": "25회 인용20222023419",
            "컨퍼런스": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "페이지": "804-814",
            "학술 문서": "Revisiting weakly supervised pre-training of visual perception modelsM Singh, L Gustafson, A Adcock, V de Freitas Reis… - Proceedings of the IEEE/CVF Conference on Computer …, 202225회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Revisiting weakly supervised pre-training of visual perception models",
        "year": null
    },
    "Generalized sparselet models for real-time multiclass object recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/9/4",
            "게시자": "IEEE",
            "권": "37",
            "설명": "The problem of real-time multiclass object recognition is of great practical importance in object recognition. In this paper, we describe a framework that simultaneously utilizes shared representation, reconstruction sparsity, and parallelism to enable real-time multiclass object detection with deformable part models at 5Hz on a laptop computer with almost no decrease in task performance. Our framework is trained in the standard structured output prediction formulation and is generically applicable for speeding up object recognition systems where the computational bottleneck is in multiclass, multi-convolutional inference. We experimentally demonstrate the efficiency and task performance of our method on PASCAL VOC, subset of ImageNet, Caltech101 and Caltech256 dataset.",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Hyun Oh Song, Ross Girshick, Stefan Zickler, Christopher Geyer, Pedro Felzenszwalb, Trevor Darrell",
            "전체 인용횟수": "25회 인용201520162017201820192020202120222435317",
            "페이지": "1001-1012",
            "학술 문서": "Generalized sparselet models for real-time multiclass object recognitionHO Song, R Girshick, S Zickler, C Geyer… - IEEE transactions on pattern analysis and machine …, 201425회 인용 관련 학술자료 전체 11개의 버전 ",
            "호": "5"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Generalized sparselet models for real-time multiclass object recognition",
        "year": null
    },
    "Visibility constraints on features of 3D objects": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/6/20",
            "게시자": "IEEE",
            "설명": "To recognize three-dimensional objects it is important to model how their appearances can change due to changes in viewpoint. A key aspect of this involves understanding which object features can be simultaneously visible under different viewpoints. We address this problem in an image-based framework, in which we use a limited number of images of an object taken from unknown viewpoints to determine which subsets of features might be simultaneously visible in other views. This leads to the problem of determining whether a set of images, each containing a set of features, is consistent with a single 3D object. We assume that each feature is visible from a disk of viewpoints on the viewing sphere. In this case we show the problem is NP-hard in general, but can be solved efficiently when all views come from a circle on the viewing sphere. We also give iterative algorithms that can handle noisy data and …",
            "저자": "Ronen Basri, Pedro F Felzenszwalb, Ross B Girshick, David W Jacobs, Caroline J Klivans",
            "전체 인용횟수": "22회 인용20092010201120122013201420152016201720182019202020212022211124312112",
            "컨퍼런스": "2009 IEEE Conference on Computer Vision and Pattern Recognition",
            "페이지": "1231-1238",
            "학술 문서": "Visibility constraints on features of 3D objectsR Basri, PF Felzenszwalb, RB Girshick, DW Jacobs… - 2009 IEEE Conference on Computer Vision and …, 200922회 인용 관련 학술자료 전체 21개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Visibility constraints on features of 3D objects",
        "year": null
    },
    "Discriminatively trained mixtures of deformable part models": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008",
            "설명": "Discriminatively Trained Mixtures of Deformable Part Models Page 1 Discriminatively \nTrained Mixtures of Deformable Part Models Pedro Felzenszwalb and Ross Girshick \nUniversity of Chicago David McAllester Toyota Technological Institute at Chicago Deva \nRamanan UC Irvine http://www.cs.uchicago.edu/~pff/latent Page 2 Model Overview • \nMixture of deformable part models (pictorial structures) • Each component has global \ntemplate + deformable parts • Fully trained from bounding boxes alone Page 3 2 component \nbicycle model root filters coarse resolution part filters finer resolution deformation models \nPage 4 Object Hypothesis Image pyramid HOG feature pyramid Multiscale model captures \nfeatures at two resolutions Score of object hypothesis is sum of filter scores minus \ndeformation costs Score of filter is dot product of filter with HOG features underneath it Page \n5 Connection with linear classifier concatenation …",
            "저널": "PASCAL VOC Challenge",
            "저자": "Pedro Felzenszwalb, Ross Girshick, David McAllester, Deva Ramanan",
            "전체 인용횟수": "16회 인용2009201020112012201320142015201620172018201920202021202220233118111",
            "학술 문서": "Discriminatively trained mixtures of deformable part modelsP Felzenszwalb, R Girshick, D McAllester, D Ramanan - PASCAL VOC Challenge, 200816회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Discriminatively trained mixtures of deformable part models",
        "year": null
    },
    "Simulating Chinese brush painting: the parametric hairy brush": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004/8/8",
            "도서": "ACM SIGGRAPH 2004 Posters",
            "설명": "In Chinese brush painting each stroke exists on its own as a piece of art. The distinct characteristics of a brush stroke come from the combination of a painter’s skill with the unique physical properties of the paper, ink, and brush. As an artist paints the brush bristles deposit ink and water on the paper, the ink and water diffuse through the paper, and the bristles deform due to external forces.Digitizing the painting process requires a solution to three distinct problems: modeling ink and water diffusion in paper, modeling soft brush dynamics, and creating a stroke input method. Recent research demonstrates that solutions to the latter two problems are more fundamental for capturing the style of Chinese brush painting [Chu and Tai 2002]. Until recently, however, nearly all research has focused exclusively on the first problem. The Parametric Hairy Brush (PHB) is a new brush model designed to realistically simulate the …",
            "저자": "Ross B Girshick",
            "전체 인용횟수": "15회 인용200620072008200920102011201220132014201520162017201820192020202122122111111",
            "페이지": "22",
            "학술 문서": "Simulating Chinese brush painting: the parametric hairy brushRB Girshick - ACM SIGGRAPH 2004 Posters, 200415회 인용 관련 학술자료 전체 5개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Simulating Chinese brush painting: the parametric hairy brush",
        "year": null
    },
    "The effectiveness of MAE pre-pretraining for billion-scale pretraining": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2023/3/23",
            "저널": "arXiv preprint arXiv:2303.13496",
            "저자": "Mannat Singh, Quentin Duval, Kalyan Vasudev Alwala, Haoqi Fan, Vaibhav Aggarwal, Aaron Adcock, Armand Joulin, Piotr Dollár, Christoph Feichtenhofer, Ross Girshick, Rohit Girdhar, Ishan Misra",
            "전체 인용횟수": "9회 인용20239",
            "학술 문서": "The effectiveness of MAE pre-pretraining for billion-scale pretrainingM Singh, Q Duval, KV Alwala, H Fan, V Aggarwal… - arXiv preprint arXiv:2303.13496, 20239회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The effectiveness of MAE pre-pretraining for billion-scale pretraining",
        "year": null
    },
    "Large scale weakly and semi-supervised learning for low-resource video ASR": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020/5/16",
            "설명": "Many semi- and weakly-supervised approaches have been investigated for overcoming the labeling cost of building high quality speech recognition systems. On the challenging task of transcribing social media videos in low-resource conditions, we conduct a large scale systematic comparison between two self-labeling methods on one hand, and weakly-supervised pretraining using contextual metadata on the other. We investigate distillation methods at the frame level and the sequence level for hybrid, encoder-only CTC-based, and encoder-decoder speech recognition systems on Dutch and Romanian languages using 27,000 and 58,000 hours of unlabeled audio respectively. Although all approaches improved upon their respective baseline WERs by more than 8%, sequence-level distillation for encoder-decoder models provided the largest relative WER reduction of 20% compared to the strongest data-augmented supervised baseline.",
            "저널": "arXiv preprint arXiv:2005.07850",
            "저자": "Kritika Singh, Vimal Manohar, Alex Xiao, Sergey Edunov, Ross Girshick, Vitaliy Liptchinsky, Christian Fuegen, Yatharth Saraf, Geoffrey Zweig, Abdelrahman Mohamed",
            "전체 인용횟수": "9회 인용202020212022153",
            "학술 문서": "Large scale weakly and semi-supervised learning for low-resource video asrK Singh, V Manohar, A Xiao, S Edunov, R Girshick… - arXiv preprint arXiv:2005.07850, 20209회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Large scale weakly and semi-supervised learning for low-resource video ASR",
        "year": null
    },
    "Training asr models by generation of contextual information": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020/5/4",
            "게시자": "IEEE",
            "설명": "Supervised ASR models have reached unprecedented levels of accuracy, thanks in part to ever-increasing amounts of labelled training data. However, in many applications and locales, only moderate amounts of data are available, which has led to a surge in semi- and weakly-supervised learning research. In this paper, we conduct a large-scale study evaluating the effectiveness of weakly-supervised learning for speech recognition by using loosely related contextual information as a surrogate for ground-truth labels. For weakly supervised training, we use 50k hours of public English social media videos along with their respective titles and post text to train an encoder-decoder transformer model. Our best encoder-decoder models achieve an average of 20.8% WER reduction over a 1000 hours supervised baseline, and an average of 13.4% WER reduction when using only the weakly supervised encoder for CTC …",
            "저자": "Kritika Singh, Dmytro Okhonko, Jun Liu, Yongqiang Wang, Frank Zhang, Ross Girshick, Sergey Edunov, Fuchun Peng, Yatharth Saraf, Geoffrey Zweig, Abdelrahman Mohamed",
            "전체 인용횟수": "6회 인용2020202120222023231",
            "컨퍼런스": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "페이지": "7864-7868",
            "학술 문서": "Training asr models by generation of contextual informationK Singh, D Okhonko, J Liu, Y Wang, F Zhang… - ICASSP 2020-2020 IEEE International Conference on …, 20206회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Training asr models by generation of contextual information",
        "year": null
    },
    "Discriminative Latent Variable Models for Object Detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010",
            "설명": "(To be presented by Deva Ramanan). In this talk, I will discuss recent work by colleagues and myself on discriminative latent-variable models for object detection. Object recognition is one of the fundamental challenges of computer vision. We specifically consider the task of localizing and detecting instances of a generic object category, such as people or cars, in cluttered real-word images. Recent benchmark competitions such as the PASCAL Visual Object Challenge suggest our method is the state-of-the-art system for such tasks. This success, combined with publicallyavailable code that runs orders of magnitude faster than comparable approaches, has turned our system into a standard baseline for contemporary research on object recognition (Felzenszwalb et al., 2008; 2009).This talk will focus on the machine learning aspects of our approach. Our system is trained with a latent variable extension of support vector machines that we call a latent SVM. The formulation is equivalent to the MI-SVM framework for multiple instance learning. Latent variables provide a formalism for modeling structured variation in object appearance due to deformation, viewpoint, and other factors. The resulting learning problem is no longer convex, but admits a coordinate descent algorithm that exploits a ‘semiconvex’property. Notable aspects of our system involve (a) weakly-supervised learning, in which hidden latent structure is automatically inferred;(b) out-ofcore learning algorithms for learning from large-scale datasets that do not fit in memory; and (c) efficient algorithms for searching over latent variables.",
            "저자": "Pedro Felzenszwalb, Ross Girshick, David McAllester, Deva Ramanan, UC Irvine",
            "전체 인용횟수": "4회 인용20142015201631",
            "컨퍼런스": "ICML",
            "학술 문서": "Discriminative latent variable models for object detectionPF Felzenszwalb, RB Girshick, DA McAllester… - Proceedings of the 27th International Conference on …, 20104회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Discriminative Latent Variable Models for Object Detection",
        "year": null
    },
    "Object detection with heuristic coarse-to-fine search": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/5/29",
            "기관": "Master's Thesis. Chicago: University of Chicago",
            "설명": "We consider the task of localizing and labeling instances of a generic object class within real-world images. Our focus is on a generalized class of pictorial structure models that are defined in terms of visual grammars. In particular, we address the challenging problem of performing detection efficiently even as model complexity grows within this class. Our proposed solution is a blend of heuristic best-first search and a coarse-to-fine detection process. This paper demonstrates that our algorithm can be successfully applied to two special cases of visual grammars: multiscale star models and mixtures of multiscale star models. We show that for problems where the desired output is the local optima of a thresholded function, best-first search gives additional pruning power to coarse-to-fine processes. Unfortunately, admissible heuristics that also provide good best-first search behavior can be difficult or impossible to find in practice. To resolve this deficiency, we provide theoretical results demonstrating that inadmissible heuristics can be used to increase detection speed while only slightly increasing the likelihood of suffering mistakes. The theoretical results are bolstered by strong experimental evidence obtained by applying inadmissible heuristic coarse-to-fine detection to our object recognition system during both training and testing. We increase testing speed by a factor of 2-3 for some classes while maintaining comparable average precision scores on the challenging PASCAL 2007 dataset. Ultimately we expect to see even more significant speed gains when we explore more complex grammar models in future work.",
            "저자": "Ross Girshick",
            "전체 인용횟수": "3회 인용20172018201920202021202212",
            "학술 문서": "Object detection with heuristic coarse-to-fine searchR Girshick - 20093회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Object detection with heuristic coarse-to-fine search",
        "year": null
    },
    "Very deep convolutional networks for large-scale image recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/9/4",
            "설명": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",
            "저널": "arXiv preprint arXiv:1409.1556",
            "저자": "Karen Simonyan, Andrew Zisserman",
            "전체 인용횟수": "114016회 인용20152016201720182019202020212022202369726186048108641494217120204842121917520",
            "학술 문서": "Very deep convolutional networks for large-scale image recognitionK Simonyan, A Zisserman - arXiv preprint arXiv:1409.1556, 2014113275회 인용 관련 학술자료 전체 43개의 버전 Very deep convolutional networks for large-scale image recognition. arXiv 2014K Simonyan, A Zisserman - arXiv preprint arXiv:1409.1556, 20144531회 인용 관련 학술자료 Very deep convolutional networks for large-scale image recognition. arXiv preprint (2014)K Simonyan, A Zisserman - arXiv preprint arXiv:1409.1556, 2014206회 인용 관련 학술자료 Very deep convolutional networks for large-scale image recognition. arXiv: 14091556*K Simonyan, A Zisserman - 2014187회 인용 관련 학술자료 Very deep convolutional neural networks for large-scale image recognition*K Simonyan, A Zisserman - Proceedings of the International Conference on …, 2013106회 인용 관련 학술자료 Very deep convnets for large-scale image recognition*K Simonyan, A Zisserman - Computing Research Repository, 20149회 인용 관련 학술자료 전체 5개의 버전 Very Deep Convolutional Networks for Large-Scale Image Recognition. 1409.1556 [cs]*K Simonyan, A Zisserman - 20143회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Very deep convolutional networks for large-scale image recognition",
        "year": null
    },
    "Multiple view geometry in computer vision": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2003",
            "설명": "A basic problem in computer vision is to understand the structure of a real world scene given several images of it. Techniques for solving this problem are taken from projective geometry and photogrammetry. Here, the authors cover the geometric principles and their algebraic representation in terms of camera projection matrices, the fundamental matrix and the trifocal tensor. The theory and methods of computation of these entities are discussed with real examples, as is their use in the reconstruction of scenes from multiple images. The new edition features an extended introduction covering the key ideas in the book (which itself has been updated with additional examples and appendices) and significant new results which have appeared since the first edition. Comprehensive background material is provided, so readers familiar with linear algebra and basic numerical methods can understand the projective geometry and estimation algorithms presented, and implement the algorithms directly from the book.",
            "저자": "R Hartley, A Zisserman",
            "전체 인용횟수": "33996회 인용20012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023184368578725949120313811562169118711865196220462084200619251816175517181656159114161200",
            "출처": "Vision, 2nd ed., New York: Cambridge",
            "학술 문서": "Multiple view geometry in computer visionR Hartley, A Zisserman - 200333790회 인용 관련 학술자료 전체 21개의 버전 Multiple view geometry in computer vision*AM Andrew - Kybernetes, 2001225회 인용 관련 학술자료 전체 2개의 버전 Multiple view geometry in computerR Hartley, A Zisserman - Vision, 2nd ed., New York: Cambridge, 20036회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Multiple view geometry in computer vision",
        "year": null
    },
    "The pascal visual object classes (voc) challenge": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/6/1",
            "게시자": "Springer Netherlands",
            "권": "88",
            "설명": " The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.",
            "저널": "International journal of computer vision",
            "저자": "Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, Andrew Zisserman",
            "전체 인용횟수": "20413회 인용20092010201120122013201420152016201720182019202020212022202360912493584986278449331137149519652335283630162695",
            "페이지": "303-338",
            "학술 문서": "The pascal visual object classes (voc) challengeM Everingham, L Van Gool, CKI Williams, J Winn… - International journal of computer vision, 201020410회 인용 관련 학술자료 전체 28개의 버전 The pascal voc challengeM Everingham, L Van Gool, CKI Williams, J Winn… - Int. J. Comput. Vis5회 인용 관련 학술자료 Thepascal visual object classes (voc) challenge*M Everingham, L Van Gool, CKI Williams, J Winn… - International Journal of ComputerVision, 88 (2): 303 …4회 인용 관련 학술자료 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The pascal visual object classes (voc) challenge",
        "year": null
    },
    "Two-stream convolutional networks for action recognition in videos": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "권": "27",
            "설명": "We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multi-task learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.",
            "저널": "Advances in neural information processing systems",
            "저자": "Karen Simonyan, Andrew Zisserman",
            "전체 인용횟수": "8546회 인용20152016201720182019202020212022202314338271310511270130713721170995",
            "학술 문서": "Two-stream convolutional networks for action recognition in videosK Simonyan, A Zisserman - Advances in neural information processing systems, 20148546회 인용 관련 학술자료 전체 18개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Two-stream convolutional networks for action recognition in videos",
        "year": null
    },
    "Video Google: A text retrieval approach to object matching in videos": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2003/10/13",
            "게시자": "IEEE",
            "설명": "We describe an approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video. The object is represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion. The temporal continuity of the video within a shot is used to track the regions in order to reject unstable regions and reduce the effects of noise in the descriptors. The analogy with text retrieval is in the implementation where matches on descriptors are pre-computed (using vector quantization), and inverted file systems and document rankings are used. The result is that retrieved is immediate, returning a ranked list of key frames/shots in the manner of Google. The method is illustrated for matching in two full length feature films.",
            "저자": "Josef Sivic, Andrew Zisserman",
            "전체 인용횟수": "8394회 인용200420052006200720082009201020112012201320142015201620172018201920202021202220233978117165238325494518643737765834676621503398364320259170",
            "컨퍼런스": "Computer Vision, 2003. ICCV 2003. IEEE International Conference on",
            "페이지": "1470",
            "학술 문서": "Video Google: A text retrieval approach to object matching in videosSivic, Zisserman - Proceedings ninth IEEE international conference on …, 20038393회 인용 관련 학술자료 전체 39개의 버전 Video Google*J Sivic, A Zisserman, F Schaffalitzky1회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Video Google: A text retrieval approach to object matching in videos",
        "year": null
    },
    "Spatial transformer networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "권": "28",
            "설명": "Convolutional Neural Networks define an exceptionallypowerful class of model, but are still limited by the lack of abilityto be spatially invariant to the input data in a computationally and parameterefficient manner. In this work we introduce a new learnable module, theSpatial Transformer, which explicitly allows the spatial manipulation ofdata within the network. This differentiable module can be insertedinto existing convolutional architectures, giving neural networks the ability toactively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the useof spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-artperformance on several benchmarks, and for a numberof classes of transformations.",
            "저널": "Advances in neural information processing systems",
            "저자": "Max Jaderberg, Karen Simonyan, Andrew Zisserman",
            "전체 인용횟수": "8075회 인용2015201620172018201920202021202220232517537773511331281148714961282",
            "학술 문서": "Spatial transformer networksM Jaderberg, K Simonyan, A Zisserman - Advances in neural information processing systems, 20158075회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Spatial transformer networks",
        "year": null
    },
    "Quo vadis, action recognition? a new model and the kinetics dataset": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.2% on HMDB-51 and 97.9% on UCF-101.",
            "저자": "Joao Carreira, Andrew Zisserman",
            "전체 인용횟수": "7786회 인용2017201820192020202120222023283547831174168718861830",
            "컨퍼런스": "proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "페이지": "6299-6308",
            "학술 문서": "Quo vadis, action recognition? a new model and the kinetics datasetJ Carreira, A Zisserman - proceedings of the IEEE Conference on Computer …, 20177630회 인용 관련 학술자료 전체 14개의 버전 Quo vadis, action recognition*J Carreira, A Zisserman - A new model and the kinetics dataset. CoRR, abs …, 2017288회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
        "year": null
    },
    "Deep inside convolutional networks: Visualising image classification models and saliency maps": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/12/20",
            "설명": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].",
            "저널": "arXiv preprint arXiv:1312.6034",
            "저자": "Karen Simonyan, Andrea Vedaldi, Andrew Zisserman",
            "전체 인용횟수": "7404회 인용201420152016201720182019202020212022202321842163506238561057137814411305",
            "학술 문서": "Deep inside convolutional networks: Visualising image classification models and saliency mapsK Simonyan, A Vedaldi, A Zisserman - arXiv preprint arXiv:1312.6034, 20137286회 인용 관련 학술자료 전체 11개의 버전 Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv 2013K Simonyan, A Vedaldi, A Zisserman - arXiv preprint arXiv:1312.6034, 2019309회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
        "year": null
    },
    "The pascal visual object classes challenge: A retrospective": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/1",
            "게시자": "Springer US",
            "권": "111",
            "설명": " The Pascal Visual Object Classes (VOC) challenge consists of two components: (i) a publicly available dataset of images together with ground truth annotation and standardised evaluation software; and (ii) an annual competition and workshop. There are five challenges: classification, detection, segmentation, action classification, and person layout. In this paper we provide a review of the challenge from 2008–2012. The paper is intended for two audiences: algorithm designers, researchers who want to see what the state of the art is, as measured by performance on the VOC datasets, along with the limitations and weak points of the current generation of algorithms; and, challenge designers, who want to see what we as organisers have learnt from the process and our recommendations for the organisation of future challenges. To analyse the performance of submitted algorithms on the VOC datasets we …",
            "저널": "International journal of computer vision",
            "저자": "Mark Everingham, SM Ali Eslami, Luc Van Gool, Christopher KI Williams, John Winn, Andrew Zisserman",
            "전체 인용횟수": "6294회 인용20142015201620172018201920202021202220232811725836459278786810511018992",
            "페이지": "98-136",
            "학술 문서": "The pascal visual object classes challenge: A retrospectiveM Everingham, SMA Eslami, L Van Gool, CKI Williams… - International journal of computer vision, 20156294회 인용 관련 학술자료 전체 37개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The pascal visual object classes challenge: A retrospective",
        "year": null
    },
    "Deep face recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "게시자": "British Machine Vision Association",
            "설명": "The goal of this paper is face recognition – from either a single photograph or from a set of faces tracked in a video. Recent progress in this area has been due to two factors: (i) end to end learning for the task using a convolutional neural network (CNN), and (ii) the availability of very large scale training datasets. We make two contributions: first, we show how a very large scale dataset (2.6M images, over 2.6K people) can be assembled by a combination of automation and human in the loop, and discuss the trade off between data purity and time; second, we traverse through the complexities of deep network training and face recognition to present methods and procedures to achieve comparable state of the art results on the standard LFW and YTF face benchmarks.",
            "저널": "BMVC 2015-Proceedings of the British Machine Vision Conference 2015",
            "저자": "Omkar Parkhi, Andrea Vedaldi, Andrew Zisserman",
            "전체 인용횟수": "6011회 인용201520162017201820192020202120222023202495939301063963883678515",
            "학술 문서": "Deep face recognitionO Parkhi, A Vedaldi, A Zisserman - BMVC 2015-Proceedings of the British Machine Vision …, 20156011회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep face recognition",
        "year": null
    },
    "A comparison of affine region detectors": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/11",
            "게시자": "Kluwer Academic Publishers",
            "권": "65",
            "설명": " The paper gives a snapshot of the state of the art in affine covariant region detectors, and compares their performance on a set of test images under varying imaging conditions. Six types of detectors are included: detectors based on affine normalization around Harris  (Mikolajczyk and  Schmid, 2002; Schaffalitzky and  Zisserman, 2002) and Hessian points  (Mikolajczyk and  Schmid, 2002), a detector of ‘maximally stable extremal regions', proposed by Matas et al. (2002); an edge-based region detector  (Tuytelaars and Van Gool, 1999) and a detector based on intensity extrema (Tuytelaars and Van Gool, 2000), and a detector of ‘salient regions', proposed by Kadir, Zisserman and Brady (2004). The performance is measured against changes in viewpoint, scale, illumination, defocus and image compression. The objective of this paper is also to establish a reference test set of images and performance …",
            "저널": "International journal of computer vision",
            "저자": "Krystian Mikolajczyk, Tinne Tuytelaars, Cordelia Schmid, Andrew Zisserman, Jiri Matas, Frederik Schaffalitzky, Timor Kadir, L Van Gool",
            "전체 인용횟수": "4410회 인용20042005200620072008200920102011201220132014201520162017201820192020202120222023133012215825031236736832236031533730424621819714111010372",
            "페이지": "43-72",
            "학술 문서": "A comparison of affine region detectorsK Mikolajczyk, T Tuytelaars, C Schmid, A Zisserman… - International journal of computer vision, 20054410회 인용 관련 학술자료 전체 41개의 버전 A Comparison of Affine Region Detectors. Int*K Mikolajczyk, T Tuytelaars, C Schmid, A Zisserman… - Journal of Computer Vision (accepted)2회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A comparison of affine region detectors",
        "year": null
    },
    "Return of the devil in the details: Delving deep into convolutional nets": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/5/14",
            "설명": "The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source code and models to reproduce the experiments in the paper is made publicly available.",
            "저널": "arXiv preprint arXiv:1405.3531",
            "저자": "Ken Chatfield, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman",
            "전체 인용횟수": "4162회 인용201420152016201720182019202020212022202329265483664678595446348327211",
            "학술 문서": "Return of the devil in the details: Delving deep into convolutional netsK Chatfield, K Simonyan, A Vedaldi, A Zisserman - arXiv preprint arXiv:1405.3531, 20144162회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Return of the devil in the details: Delving deep into convolutional nets",
        "year": null
    },
    "Object retrieval with large vocabularies and fast spatial matching": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2007/6/17",
            "게시자": "IEEE",
            "설명": "In this paper, we present a large-scale object retrieval system. The user supplies a query object by selecting a region of a query image, and the system returns a ranked list of images that contain the same object, retrieved from a large corpus. We demonstrate the scalability and performance of our system on a dataset of over 1 million images crawled from the photo-sharing site, Flickr [3], using Oxford landmarks as queries. Building an image-feature vocabulary is a major time and performance bottleneck, due to the size of our dataset. To address this problem we compare different scalable methods for building a vocabulary and introduce a novel quantization method based on randomized trees which we show outperforms the current state-of-the-art on an extensive ground-truth. Our experiments show that the quantization has a major effect on retrieval quality. To further improve query performance, we add an efficient …",
            "저자": "James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, Andrew Zisserman",
            "전체 인용횟수": "3701회 인용200620072008200920102011201220132014201520162017201820192020202120222023111556100176206253306294353351289279231205209161141",
            "컨퍼런스": "2007 IEEE conference on computer vision and pattern recognition",
            "페이지": "1-8",
            "학술 문서": "Object retrieval with large vocabularies and fast spatial matchingJ Philbin, O Chum, M Isard, J Sivic, A Zisserman - 2007 IEEE conference on computer vision and pattern …, 20073701회 인용 관련 학술자료 전체 19개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Object retrieval with large vocabularies and fast spatial matching",
        "year": null
    },
    "The kinetics human action video dataset": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/5/19",
            "설명": "We describe the DeepMind Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers.",
            "저널": "arXiv preprint arXiv:1705.06950",
            "저자": "Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, Andrew Zisserman",
            "전체 인용횟수": "3632회 인용201720182019202020212022202324196366513794889828",
            "학술 문서": "The kinetics human action video datasetW Kay, J Carreira, K Simonyan, B Zhang, C Hillier… - arXiv preprint arXiv:1705.06950, 20173632회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The kinetics human action video dataset",
        "year": null
    },
    "The PASCAL visual object classes challenge 2008 (VOC2008) results": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008",
            "설명": "The PASCAL Visual Object Classes Challenge 2008 (VOC2008) Results | CiNii Research \nCiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ] 論文・データをさがす 大学図書館の本を\nさがす 日本の博士論文をさがす English 検索 タイトル 人物/団体名 所属機関 ISSN DOI 期間 ~ \n本文リンク 本文リンクあり データソース JaLC IRDB Crossref DataCite NDL NDL-Digital IDR \nJDCat NINJAL CiNii Articles CiNii Books CiNii Dissertations RUDA DBpedia Nikkei BP \nKAKEN Integbio すべて 研究データ 論文 本 博士論文 プロジェクト [4/18更新]CiNii Articlesの\nCiNii Researchへの統合について The PASCAL Visual Object Classes Challenge 2008 (VOC2008) \nResults 被引用文献1件 EVERINGHAM M. 収録刊行物 http://www.pascal-network.org/challenges/VOC/voc2008/year=workshop/index.html \nhttp://www.pascal-network.org/challenges/VOC/voc2008/year=workshop/index.html 2008 …",
            "저널": "http://www. pascal-network. org/challenges/VOC/voc2008/year= workshop/index. html",
            "저자": "Mark Everingham",
            "전체 인용횟수": "3438회 인용20052006200720082009201020112012201320142015201620172018201920202021202220239122451111141144136158173215211253307300301284325238",
            "학술 문서": "The PASCAL visual object classes challenge 2008 (VOC2008) resultsM Everingham - http://www. pascal-network. org/challenges/VOC …, 20083211회 인용 관련 학술자료 전체 2개의 버전 The pascal visual object classes challenge 2006 (voc2006) resultsM Everingham, A Zisserman, CKI Williams, L Van Gool - 2007186회 인용 관련 학술자료 Pascal visual object classes challenge results*M Everingham, L Van Gool, C Williams, J Winn… - Available from www. pascal-network. org, 200595회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The PASCAL visual object classes challenge 2008 (VOC2008) results",
        "year": null
    },
    "Visual reconstruction": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1987/11/12",
            "게시자": "MIT press",
            "설명": "This book deals with vision as a computational problem. It presents visual reconstruction from the mechanical viewpoint, which is more natural for representation of a priori knowledge about visible surfaces or about distributions of visual quantities such as intensity, reflectance, optic flow, and curve orientation. An important class of reconstruction processes is presented. Two new concepts are introduced, analyzed, and illustrated: the weak continuity constraint in vision and the graduated nonconvexity algorithm for fitting piecewise continuous functions to visual data. The chapters of the book are as follows:(1) Modeling Piecewise Continuity (2) Applications of Piecewise Continuous Reconstructions (3) Introduction to Weak Continuity Constraints (4) Properties of the Weak String and Membrane (5) Properties of the Weak Rod and Plates (6) The Discrete Problem (7) The Graduated Non Convexity Algorithm and (8 …",
            "저자": "Andrew Blake, Andrew Zisserman",
            "전체 인용횟수": "3251회 인용1988198919901991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220232160104119108124151129119127142120103988310779851049598758179918171796154695341554160",
            "학술 문서": "Visual reconstructionA Blake, A Zisserman - 19873251회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Visual reconstruction",
        "year": null
    },
    "Automated flower classification over a large number of classes": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008/12/16",
            "게시자": "IEEE",
            "설명": "We investigate to what extent combinations of features can improve classification performance on a large dataset of similar classes. To this end we introduce a 103 class flower dataset. We compute four different features for the flowers, each describing different aspects, namely the local shape/texture, the shape of the boundary, the overall spatial distribution of petals, and the colour. We combine the features using a multiple kernel framework with a SVM classifier. The weights for each class are learnt using the method of Varma and Ray, which has achieved state of the art performance on other large dataset, such as Caltech 101/256. Our dataset has a similar challenge in the number of classes, but with the added difficulty of large between class similarity and small within class similarity. Results show that learning the optimum kernel combination of multiple features vastly improves the performance, from 55.1% for …",
            "저자": "Maria-Elena Nilsback, Andrew Zisserman",
            "전체 인용횟수": "3145회 인용20092010201120122013201420152016201720182019202020212022202312283743567395133137215260304455552708",
            "컨퍼런스": "2008 Sixth Indian conference on computer vision, graphics & image processing",
            "페이지": "722-729",
            "학술 문서": "Automated flower classification over a large number of classesME Nilsback, A Zisserman - 2008 Sixth Indian conference on computer vision …, 20083145회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Automated flower classification over a large number of classes",
        "year": null
    },
    "Convolutional two-stream network fusion for video action recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "Recent applications of Convolutional Neural Networks (ConvNets) for human action recognition in videos have proposed different solutions for incorporating the appearance and motion information. We study a number of ways of fusing ConvNet towers both spatially and temporally in order to best take advantage of this spatio-temporal information. We make the following findings:(i) that rather than fusing at the softmax layer, a spatial and temporal network can be fused at a convolution layer without loss of performance, but with a substantial saving in parameters;(ii) that it is better to fuse such networks spatially at the last convolutional layer than earlier, and that additionally fusing at the class prediction layer can boost accuracy; finally (iii) that pooling of abstract convolutional features over spatiotemporal neighbourhoods further boosts performance. Based on these studies we propose a new ConvNet architecture for spatiotemporal fusion of video snippets, and evaluate its performance on standard benchmarks where this architecture achieves state-of-the-art results.",
            "저자": "Christoph Feichtenhofer, Axel Pinz, Andrew Zisserman",
            "전체 인용횟수": "3139회 인용2016201720182019202020212022202317226386498554578465374",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "1933-1941",
            "학술 문서": "Convolutional two-stream network fusion for video action recognitionC Feichtenhofer, A Pinz, A Zisserman - Proceedings of the IEEE conference on computer …, 20163139회 인용 관련 학술자료 전체 19개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Convolutional two-stream network fusion for video action recognition",
        "year": null
    },
    "Object class recognition by unsupervised scale-invariant learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2003/6/18",
            "게시자": "IEEE",
            "권": "2",
            "설명": "We present a method to learn and recognize object class models from unlabeled and unsegmented cluttered scenes in a scale invariant manner. Objects are modeled as flexible constellations of parts. A probabilistic representation is used for all aspects of the object: shape, appearance, occlusion and relative scale. An entropy-based feature detector is used to select regions and their scale within the image. In learning the parameters of the scale-invariant object model are estimated. This is done using expectation-maximization in a maximum-likelihood setting. In recognition, this model is used in a Bayesian manner to classify images. The flexible nature of the model is demonstrated by excellent results over a range of datasets including geometrically constrained classes (e.g. faces, cars) and flexible objects (such as animals).",
            "저자": "Robert Fergus, Pietro Perona, Andrew Zisserman",
            "전체 인용횟수": "3043회 인용2003200420052006200720082009201020112012201320142015201620172018201920202021202220231611120224624726524225124421420716515010288645137352424",
            "컨퍼런스": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings.",
            "페이지": "II-II",
            "학술 문서": "Object class recognition by unsupervised scale-invariant learningR Fergus, P Perona, A Zisserman - 2003 IEEE Computer Society Conference on Computer …, 20033043회 인용 관련 학술자료 전체 41개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Object class recognition by unsupervised scale-invariant learning",
        "year": null
    },
    "MLESAC: A new robust estimator with application to estimating image geometry": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2000/4/1",
            "게시자": "Academic Press",
            "권": "78",
            "설명": "A new method is presented for robustly estimating multiple view relations from point correspondences. The method comprises two parts. The first is a new robust estimator MLESAC which is a generalization of the RANSAC estimator. It adopts the same sampling strategy as RANSAC to generate putative solutions, but chooses the solution that maximizes the likelihood rather than just the number of inliers. The second part of the algorithm is a general purpose method for automatically parameterizing these relations, using the output of MLESAC. A difficulty with multiview image relations is that there are often nonlinear constraints between the parameters, making optimization a difficult task. The parameterization method overcomes the difficulty of nonlinear constraints and conducts a constrained optimization. The method is general and its use is illustrated for the estimation of fundamental matrices, image–image …",
            "저널": "Computer vision and image understanding",
            "저자": "Philip HS Torr, Andrew Zisserman",
            "전체 인용횟수": "2887회 인용20012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023122641394655627411210988104121124162176225237250217233167171",
            "페이지": "138-156",
            "학술 문서": "MLESAC: A new robust estimator with application to estimating image geometryPHS Torr, A Zisserman - Computer vision and image understanding, 20002887회 인용 관련 학술자료 전체 13개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "MLESAC: A new robust estimator with application to estimating image geometry",
        "year": null
    },
    "Deep learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/5/28",
            "게시자": "Nature Publishing Group UK",
            "권": "521",
            "설명": "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.",
            "저자": "Yann LeCun, Yoshua Bengio, Geoffrey Hinton",
            "전체 인용횟수": "72040회 인용201520162017201820192020202120222023213165741527073981311535127941276911288",
            "출처": "nature",
            "페이지": "436-444",
            "학술 문서": "Deep learningY LeCun, Y Bengio, G Hinton - nature, 201571811회 인용 관련 학술자료 전체 89개의 버전 Yoshua bengio, and geoffrey hinton*Y LeCun - Deep learning. nature, 2015130회 인용 관련 학술자료 Deep learning*G Hinton, Y LeCun, Y Bengio - Nature, 2015112회 인용 관련 학술자료 et Hinton, G.(2015)*Y LeCun, Y Bengio - Deep learning. nature31회 인용 관련 학술자료 Deep learning. nature*Y LeCun, Y Bengio, G Hinton - Nature, 201821회 인용 관련 학술자료 Imagenet classification with deep convolutional neural networks*Y LeCun, J Denker, D Henderson, R Howard… - Advances in neural information processing systems, 199012회 인용 관련 학술자료 Y., Bengio, Y., Hinton*G LeCun - Nature, 20158회 인용 관련 학술자료 ",
            "호": "7553"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep learning",
        "year": null
    },
    "Gradient-based learning applied to document recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1998/11",
            "게시자": "Ieee",
            "권": "86",
            "설명": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows …",
            "저자": "Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner",
            "전체 인용횟수": "60099회 인용200620072008200920102011201220132014201520162017201820192020202120222023173193204226304360368549814162126203934594675848553918790447592",
            "출처": "Proceedings of the IEEE",
            "페이지": "2278-2324",
            "학술 문서": "Gradient-based learning applied to document recognitionY LeCun, L Bottou, Y Bengio, P Haffner - Proceedings of the IEEE, 199860099회 인용 관련 학술자료 전체 41개의 버전 ",
            "호": "11"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Gradient-based learning applied to document recognition",
        "year": null
    },
    "Backpropagation applied to handwritten zip code recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1989/12",
            "게시자": "MIT Press",
            "권": "1",
            "설명": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.",
            "저널": "Neural computation",
            "저자": "Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, Lawrence D Jackel",
            "전체 인용횟수": "15620회 인용198919901991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220233946739311315110414613816813015012282595454657566758084106131180382593882134016761848215722791659",
            "페이지": "541-551",
            "학술 문서": "Backpropagation applied to handwritten zip code recognitionY LeCun, B Boser, JS Denker, D Henderson… - Neural computation, 198915616회 인용 관련 학술자료 전체 15개의 버전 Neural Comput.*Y LeCun, BE Boser, JS Denker, D Henderson… - 19908회 인용 관련 학술자료 Backpropagation Applied to Handwritten Zip Code Recognition, Neural Computation*Y LeCun, B Boser, JS Denker, D Henderson… - Neural Computation, 19906회 인용 관련 학술자료 Denker, 1*Y LeCun, B Boser - S., Henderson, D., Howard, RE, Hubbard, W., and …, 19893회 인용 관련 학술자료 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Backpropagation applied to handwritten zip code recognition",
        "year": null
    },
    "Convolutional networks for images, speech, and time series": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1995/4",
            "권": "3361",
            "설명": "The ability of multilayer back-propagation networks to learn complex, high-dimensional, nonlinear mappings from large collections of examples makes them obvious candidates for image recognition or speech recognition tasks (see PATTERN RECOGNITION AND NEURAL NETWORKS). In the traditional model of pattern recognition, a hand-designed feature extractor gathers relevant information from the input and eliminates irrelevant variabilities. A trainable classi er then categorizes the resulting feature vectors (or strings of symbols) into classes. In this scheme, standard, fully-connected multilayer networks can be used as classi ers. A potentially more interesting scheme is to eliminate the feature extractor, feeding the network with\\raw\" inputs (eg normalized images), and to rely on backpropagation to turn the rst few layers into an appropriate feature extractor. While this can be done with an ordinary fully connected feed-forward network with some success for tasks such as character recognition, there are problems.Firstly, typical images, or spectral representations of spoken words, are large, often with several hundred variables. A fully-connected rst layer with, say a few 100 hidden units, would already contain several 10,000 weights. Over tting problems may occur if training data is scarce. In addition, the memory requirement for that many weights may rule out certain hardware implementations. But, the main de ciency of unstructured nets for image or speech aplications is that they have no built-in invariance with respect to translations, or",
            "저널": "The handbook of brain theory and neural networks",
            "저자": "Yann LeCun, Yoshua Bengio",
            "전체 인용횟수": "7517회 인용2007200820092010201120122013201420152016201720182019202020212022202323202133333560111210328507753919104612241134878",
            "페이지": "1995",
            "학술 문서": "Convolutional networks for images, speech, and time seriesY LeCun, Y Bengio - The handbook of brain theory and neural networks, 19957277회 인용 관련 학술자료 전체 21개의 버전 The handbook of brain theory and neural networks*Y LeCun, Y Bengio - 1998456회 인용 관련 학술자료 ",
            "호": "10"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Convolutional networks for images, speech, and time series",
        "year": null
    },
    "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/4/14",
            "게시자": "arXiv preprint arXiv:1312.6229",
            "설명": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.",
            "저자": "Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, Yann LeCun",
            "전체 인용횟수": "7323회 인용19881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202319353634384471568380998210688501418122415243329325046171463667765868856748633504341",
            "컨퍼런스": "International Conference on Learning Representations (ICLR 2014)",
            "학술 문서": "Overfeat: Integrated recognition, localization and detection using convolutional networksP Sermanet, D Eigen, X Zhang, M Mathieu, R Fergus… - arXiv preprint arXiv:1312.6229, 20137323회 인용 관련 학술자료 전체 30개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",
        "year": null
    },
    "The MNIST database of handwritten digits": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1998",
            "저자": "Yann LeCun, Corinna Cortes",
            "전체 인용횟수": "6954회 인용200420052006200720082009201020112012201320142015201620172018201920202021202220231923242741474257507211416427542473085194210411037904",
            "학술 문서": "The MNIST database of handwritten digitsY LeCun - http://yann. lecun. com/exdb/mnist/, 19986944회 인용 관련 학술자료 전체 2개의 버전 The MNIST Database of handwritten images*Y LeCun, C Cortes, CJC Burgess - 201210회 인용 관련 학술자료 ",
            "호": "http://yann.lecun.com/exdb/mnist/"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The MNIST database of handwritten digits",
        "year": null
    },
    "Efficient backprop": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2002/3/28",
            "게시자": "Springer Berlin Heidelberg",
            "도서": "Neural networks: Tricks of the trade",
            "설명": " The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most “classical” second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.",
            "저자": "Yann LeCun, Léon Bottou, Genevieve B Orr, Klaus-Robert Müller",
            "전체 인용횟수": "6856회 인용19881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202319343734364271548077988111592614537526458709496102138160174231395510651669642668563428",
            "페이지": "9-50",
            "학술 문서": "Efficient backpropY LeCun, L Bottou, GB Orr, KR Müller - Neural networks: Tricks of the trade, 20026856회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Efficient backprop",
        "year": null
    },
    "Character-level convolutional networks for text classification": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "권": "28",
            "설명": "This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.",
            "저널": "Advances in neural information processing systems",
            "저자": "Xiang Zhang, Junbo Zhao, Yann LeCun",
            "전체 인용횟수": "6021회 인용201620172018201920202021202220231423246358629999691097949",
            "학술 문서": "Character-level convolutional networks for text classificationX Zhang, J Zhao, Y LeCun - Advances in neural information processing systems, 20156021회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Character-level convolutional networks for text classification",
        "year": null
    },
    "Handwritten digit recognition with a back-propagation network": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1990",
            "게시자": "Morgan Kaufmann Publishers",
            "설명": "We present an application of back-propagation networks to hand (cid: 173) written digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the US Postal Service.",
            "저자": "Y LeCun, B Boser, JS Denker, D Henderson, RE Howard, W Hubbard, LD Jackel",
            "전체 인용횟수": "5962회 인용199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202316274040485224492822272032282534313330362440355580135275419661664777753752599",
            "컨퍼런스": "Advances in neural information processing systems 2, NIPS 1989",
            "페이지": "396-404",
            "학술 문서": "Handwritten digit recognition with a back-propagation networkY LeCun, B Boser, J Denker, D Henderson, R Howard… - Advances in neural information processing systems, 19895958회 인용 관련 학술자료 전체 12개의 버전 Handwritten digit recognition with*BP Network - 19895회 인용 관련 학술자료 전체 5개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Handwritten digit recognition with a back-propagation network",
        "year": null
    },
    "Optimal Brain Damage": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1990/2",
            "게시자": "Morgan-Kaufmann Publishers",
            "권": "2",
            "설명": "We have used information-theoretic ideas to derive a class of prac (cid: 173) tical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, sev (cid: 173) eral improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative informa (cid: 173) tion to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.",
            "저자": "Yann LeCun, John S Denker, Sara A Solla",
            "전체 인용횟수": "5541회 인용19901991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220232245609110210511910711986869898747864941177684636567715978125175308408570585652527",
            "컨퍼런스": "Advances in neural information processing systems 2, NIPS 1989",
            "페이지": "598-605",
            "학술 문서": "Optimal brain damageY LeCun, J Denker, S Solla - Advances in neural information processing systems, 19895472회 인용 관련 학술자료 전체 16개의 버전 Solla. 1990. Optimal brain damage*Y LeCun, JS Denker, A Sara - Advances in neural information processing systems100회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Optimal Brain Damage",
        "year": null
    },
    "Spectral Networks and Locally Connected Networks on Graphs": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/4/14",
            "게시자": "arXiv preprint arXiv:1312.6203",
            "설명": "Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.",
            "저자": "Joan Bruna, Wojciech Zaremba, Arthur Szlam, Yann LeCun",
            "전체 인용횟수": "5400회 인용2015201620172018201920202021202220231441106287592891117312461010",
            "컨퍼런스": "International Conference on Learning Representations (ICLR 2014)",
            "학술 문서": "Spectral networks and locally connected networks on graphsJ Bruna, W Zaremba, A Szlam, Y LeCun - arXiv preprint arXiv:1312.6203, 20135400회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Spectral Networks and Locally Connected Networks on Graphs",
        "year": null
    },
    "Dimensionality reduction by learning an invariant mapping": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006",
            "게시자": "IEEE",
            "권": "2",
            "설명": "Dimensionality reduction involves mapping a set of high dimensional input points onto a low dimensional manifold so that 'similar\" points in input space are mapped to nearby points on the manifold. We present a method - called Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) - for learning a globally coherent nonlinear function that maps the data evenly to the output manifold. The learning relies solely on neighborhood relationships and does not require any distancemeasure in the input space. The method can learn mappings that are invariant to certain transformations of the inputs, as is demonstrated with a number of experiments. Comparisons are made to other techniques, in particular LLE.",
            "저자": "Raia Hadsell, Sumit Chopra, Yann LeCun",
            "전체 인용횟수": "5375회 인용20092010201120122013201420152016201720182019202020212022202317201722383063133193324490724102711751036",
            "컨퍼런스": "Computer vision and pattern recognition 2006. CVPR 2006. IEEE computer society conference on",
            "페이지": "1735-1742",
            "학술 문서": "Dimensionality reduction by learning an invariant mappingR Hadsell, S Chopra, Y LeCun - 2006 IEEE computer society conference on computer …, 20065375회 인용 관련 학술자료 전체 18개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Dimensionality reduction by learning an invariant mapping",
        "year": null
    },
    "MNIST handwritten digit database": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/1",
            "권": "2",
            "저자": "Yann LeCun, Corinna Cortes, Chris Burges",
            "전체 인용횟수": "5017회 인용20132014201520162017201820192020202120222023151836641413705778781179955732",
            "페이지": "18",
            "학술 문서": "MNIST handwritten digit databaseY LeCun, C Cortes, C Burges - 20105017회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "MNIST handwritten digit database",
        "year": null
    },
    "Learning a similarity metric discriminatively, with application to face verification": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/6/20",
            "게시자": "IEEE",
            "권": "1",
            "설명": "We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the L/sub 1/ norm in the target space approximates the \"semantic\" distance in the input space. The method is applied to a face verification task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons. The mapping from raw to the target space is a convolutional network whose architecture is designed for robustness to geometric distortions. The system is tested on the Purdue/AR face database which has a very high degree of …",
            "저자": "Sumit Chopra, Raia Hadsell, Yann LeCun",
            "전체 인용횟수": "4875회 인용20062007200820092010201120122013201420152016201720182019202020212022202321262730362727325299207362510576656712751661",
            "컨퍼런스": "2005 IEEE computer society conference on computer vision and pattern recognition (CVPR'05)",
            "페이지": "539-546",
            "학술 문서": "Learning a similarity metric discriminatively, with application to face verificationS Chopra, R Hadsell, Y LeCun - 2005 IEEE computer society conference on computer …, 20054875회 인용 관련 학술자료 전체 24개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning a similarity metric discriminatively, with application to face verification",
        "year": null
    },
    "Signature verification using a\" siamese\" time delay neural network": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1993",
            "권": "6",
            "설명": "This paper describes an algorithm for verification of signatures written on a pen-input tablet. The algorithm is based on a novel, artificial neural network, called a\" Siamese\" neural network. This network consists of two identical sub-networks joined at their out (cid: 173) puts. During training the two sub-networks extract features from two signatures, while the joining neuron measures the distance be (cid: 173) tween the two feature vectors. Verification consists of comparing an extracted feature vector~ ith a stored feature vector for the signer. Signatures closer to this stored representation than a chosen thresh (cid: 173) old are accepted, all other signatures are rejected as forgeries.",
            "저널": "Advances in neural information processing systems",
            "저자": "Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Säckinger, Roopak Shah",
            "전체 인용횟수": "4500회 인용20112012201320142015201620172018201920202021202220231511101454146259382588700762781657",
            "학술 문서": "Signature verification using a\" siamese\" time delay neural networkJ Bromley, I Guyon, Y LeCun, E Säckinger, R Shah - Advances in neural information processing systems, 19934500회 인용 관련 학술자료 전체 22개의 버전 Signature Veriﬁcation using a “Siamese” Time Delay Neural Network*J Bromley, I Guyon, Y LeCun, E Siickinger, R Shah - 1994관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Signature verification using a\" siamese\" time delay neural network",
        "year": null
    },
    "Geometric deep learning: going beyond euclidean data": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/7/11",
            "게시자": "IEEE",
            "권": "34",
            "설명": "Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains, such as graphs and manifolds. The purpose of this article is to overview different examples of geometric deep-learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.",
            "저널": "IEEE Signal Processing Magazine",
            "저자": "Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, Pierre Vandergheynst",
            "전체 인용횟수": "3436회 인용201720182019202020212022202343240436585673783638",
            "페이지": "18-42",
            "학술 문서": "Geometric deep learning: going beyond euclidean dataMM Bronstein, J Bruna, Y LeCun, A Szlam… - IEEE Signal Processing Magazine, 20173436회 인용 관련 학술자료 전체 14개의 버전 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Geometric deep learning: going beyond euclidean data",
        "year": null
    },
    "Learning Hierarchical Features for Scene Labeling": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/8",
            "게시자": "IEEE",
            "권": "8",
            "설명": "Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona …",
            "저널": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "저자": "Clément Farabet, Camille Couprie, Laurent Najman, Yann LeCun",
            "전체 인용횟수": "3385회 인용2013201420152016201720182019202020212022202334120273420487474400387289288151",
            "페이지": "1915-1929",
            "학술 문서": "Learning hierarchical features for scene labelingC Farabet, C Couprie, L Najman, Y LeCun - IEEE transactions on pattern analysis and machine …, 20123385회 인용 관련 학술자료 전체 35개의 버전 ",
            "호": "35"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning Hierarchical Features for Scene Labeling",
        "year": null
    },
    "Regularization of neural networks using dropconnect": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/5/26",
            "게시자": "PMLR",
            "설명": "We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recoginition benchmarks can be obtained by aggregating multiple DropConnect-trained models.",
            "저자": "Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, Rob Fergus",
            "전체 인용횟수": "3104회 인용201320142015201620172018201920202021202220231681156271335414451413362321246",
            "컨퍼런스": "International conference on machine learning",
            "페이지": "1058-1066",
            "학술 문서": "Regularization of neural networks using dropconnectL Wan, M Zeiler, S Zhang, Y Le Cun, R Fergus - International conference on machine learning, 20133104회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Regularization of neural networks using dropconnect",
        "year": null
    },
    "A Closer Look at Spatiotemporal Convolutions for Action Recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/11/30",
            "게시자": "arXiv preprint arXiv:1711.11248",
            "설명": "In this paper we discuss several forms of spatiotemporal convolutions for video analysis and study their effects on action recognition. Our motivation stems from the observation that 2D CNNs applied to individual frames of the video have remained solid performers in action recognition. In this work we empirically demonstrate the accuracy advantages of 3D CNNs over 2D CNNs within the framework of residual learning. Furthermore, we show that factorizing the 3D convolutional filters into separate spatial and temporal components yields significantly gains in accuracy. Our empirical study leads to the design of a new spatiotemporal convolutional block``R (2+ 1) D''which produces CNNs that achieve results comparable or superior to the state-of-the-art on Sports-1M, Kinetics, UCF101, and HMDB51.",
            "저자": "Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, Manohar Paluri",
            "전체 인용횟수": "2882회 인용20182019202020212022202339217426697782700",
            "컨퍼런스": "computer vision and pattern recognition conference (CVPR 2018)",
            "학술 문서": "A closer look at spatiotemporal convolutions for action recognitionD Tran, H Wang, L Torresani, J Ray, Y LeCun, M Paluri - Proceedings of the IEEE conference on Computer …, 20182882회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A Closer Look at Spatiotemporal Convolutions for Action Recognition",
        "year": null
    },
    "What is the best multi-stage architecture for object recognition?": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/9/29",
            "게시자": "IEEE",
            "설명": "In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition …",
            "저자": "Kevin Jarrett, Koray Kavukcuoglu, Marc'Aurelio Ranzato, Yann LeCun",
            "전체 인용횟수": "2870회 인용20102011201220132014201520162017201820192020202120222023347374120165196274299334328304282200131",
            "컨퍼런스": "2009 IEEE 12th international conference on computer vision",
            "페이지": "2146-2153",
            "학술 문서": "What is the best multi-stage architecture for object recognition?K Jarrett, K Kavukcuoglu, MA Ranzato, Y LeCun - 2009 IEEE 12th international conference on computer …, 20092870회 인용 관련 학술자료 전체 26개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "What is the best multi-stage architecture for object recognition?",
        "year": null
    },
    "Deep residual learning for image recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
            "저자": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
            "전체 인용횟수": "189280회 인용2016201720182019202020212022202311814991117422035228043377524409339614",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "770-778",
            "학술 문서": "Deep residual learning for image recognitionK He, X Zhang, S Ren, J Sun - Proceedings of the IEEE conference on computer …, 2016189280회 인용 관련 학술자료 전체 76개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep residual learning for image recognition",
        "year": null
    },
    "Faster r-cnn: Towards real-time object detection with region proposal networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/6/4",
            "설명": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP) using 300 proposals per image. Code is available at https://github. com/ShaoqingRen/faster_rcnn.",
            "저널": "Conference on Neural Information Processing Systems (NIPS 2015)",
            "저자": "Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun",
            "전체 인용횟수": "67084회 인용2016201720182019202020212022202369123324971809410149128971420011938",
            "학술 문서": "Faster r-cnn: Towards real-time object detection with region proposal networksS Ren, K He, R Girshick, J Sun - Advances in neural information processing systems, 201567038회 인용 관련 학술자료 전체 42개의 버전 Faster r-cnn: Towards real-time object detection with region proposal networks. arXiv 2015*S Ren, K He, R Girshick, J Sun - arXiv preprint arXiv:1506.01497, 2015381회 인용 관련 학술자료 Faster R 鄄 CNN: towards real 鄄 time object detection with region proposal networks*S REN, K HE, R GIRSHICK - Proc of the 28th International Conference on Neural …, 20157회 인용 관련 학술자료 Faster R-CNN: 利用区域提案网络实现实时目标检测*S Ren, K He, R Girshick, J Sun - arXiv preprint arXiv:1506.01497, 2015관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
        "year": null
    },
    "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%) on this dataset.",
            "저자": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
            "전체 인용횟수": "22483회 인용2015201620172018201920202021202220232176471284213729213475378236452966",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "1026-1034",
            "학술 문서": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classificationK He, X Zhang, S Ren, J Sun - Proceedings of the IEEE international conference on …, 201522483회 인용 관련 학술자료 전체 21개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
        "year": null
    },
    "Spatial pyramid pooling in deep convolutional networks for visual recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/1/9",
            "게시자": "IEEE",
            "권": "37",
            "설명": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224   224) input image. This requirement is “artificial” and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, “spatial pyramid pooling”, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
            "전체 인용횟수": "13825회 인용201420152016201720182019202020212022202357214462772103814431634210725662200",
            "페이지": "1904-1916",
            "학술 문서": "Spatial pyramid pooling in deep convolutional networks for visual recognitionK He, X Zhang, S Ren, J Sun - IEEE transactions on pattern analysis and machine …, 201513825회 인용 관련 학술자료 전체 26개의 버전 ",
            "호": "9"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Spatial pyramid pooling in deep convolutional networks for visual recognition",
        "year": null
    },
    "Identity mappings in deep residual networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "게시자": "Springer International Publishing",
            "설명": " Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62 % error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at:                      https://github.com/KaimingHe/resnet-1k-layers                                        .",
            "저자": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
            "전체 인용횟수": "10833회 인용2016201720182019202020212022202312049698314151772207821321722",
            "컨퍼런스": "Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14",
            "페이지": "630-645",
            "학술 문서": "Identity mappings in deep residual networksK He, X Zhang, S Ren, J Sun - Computer Vision–ECCV 2016: 14th European …, 201610833회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Identity mappings in deep residual networks",
        "year": null
    },
    "Face Alignment at 3000 FPS via Regressing Local Binary Features": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "설명": "This paper presents a highly efficient, very accurate regression approach for face alignment. Our approach has two novel components: a set of local binary features, and a locality principle for learning those features. The locality principle guides us to learn a set of highly discriminative local binary features for each facial landmark independently. The obtained local binary features are used to jointly learn a linear regression for the final output. Our approach achieves the state-of-the-art results when tested on the current most challenging benchmarks. Furthermore, because extracting and regressing local binary features is computationally very cheap, our system is much faster than previous methods. It achieves over 3,000 fps on a desktop or 300 fps on a mobile phone for locating a few dozens of landmarks.",
            "저널": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2014)",
            "저자": "Shaoqing Ren, Xudong Cao, Yichen Wei, Jian Sun",
            "전체 인용횟수": "1186회 인용20132014201520162017201820192020202120222023313981601991711701341096235",
            "학술 문서": "Face alignment at 3000 fps via regressing local binary featuresS Ren, X Cao, Y Wei, J Sun - Proceedings of the IEEE conference on computer …, 20141113회 인용 관련 학술자료 전체 20개의 버전 Face alignment via regressing local binary features*S Ren, X Cao, Y Wei, J Sun - IEEE Transactions on Image Processing, 2016110회 인용 관련 학술자료 전체 5개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Face Alignment at 3000 FPS via Regressing Local Binary Features",
        "year": null
    },
    "Joint cascade face detection and alignment": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "게시자": "Springer International Publishing",
            "설명": " We present a new state-of-the-art approach for face detection. The key idea is to combine face alignment with detection, observing that aligned face shapes provide better features for face classification. To make this combination more effective, our approach learns the two tasks jointly in the same cascade framework, by exploiting recent advances in face alignment. Such joint learning greatly enhances the capability of cascade detection and still retains its realtime performance. Extensive experiments show that our approach achieves the best accuracy on challenging datasets, where all existing solutions are either inaccurate or too slow.",
            "저자": "Dong Chen, Shaoqing Ren, Yichen Wei, Xudong Cao, Jian Sun",
            "전체 인용횟수": "521회 인용2013201420152016201720182019202020212022202322334987748757542929",
            "컨퍼런스": "Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13",
            "페이지": "109-122",
            "학술 문서": "Joint cascade face detection and alignmentD Chen, S Ren, Y Wei, X Cao, J Sun - Computer Vision–ECCV 2014: 13th European …, 2014521회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Joint cascade face detection and alignment",
        "year": null
    },
    "Instance-sensitive fully convolutional networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "게시자": "Springer International Publishing",
            "설명": " Fully convolutional networks (FCNs) have been proven very successful for semantic segmentation, but the FCN outputs are unaware of object instances. In this paper, we develop FCNs that are capable of proposing instance-level segment candidates. In contrast to the previous FCN that generates one score map, our FCN is designed to compute a small set of instance-sensitive score maps, each of which is the outcome of a pixel-wise classifier of a relative position to instances. On top of these instance-sensitive score maps, a simple assembling module is able to output instance candidate at each position. In contrast to the recent DeepMask method for segmenting instances, our method does not have any high-dimensional layer related to the mask resolution, but instead exploits image local coherence for estimating instances. We present competitive results of instance segment proposal on both PASCAL …",
            "저자": "Jifeng Dai, Kaiming He, Yi Li, Shaoqing Ren, Jian Sun",
            "전체 인용횟수": "499회 인용20152016201720182019202020212022202329397110375756749",
            "컨퍼런스": "Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI 14",
            "페이지": "534-549",
            "학술 문서": "Instance-sensitive fully convolutional networksJ Dai, K He, Y Li, S Ren, J Sun - Computer Vision–ECCV 2016: 14th European …, 2016499회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Instance-sensitive fully convolutional networks",
        "year": null
    },
    "Faster R-CNN: towards real-time object detection with region proposal networks. İn: International Conference on Neural Information Processing Systems": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "저자": "SQ Ren, KM He, R Girshick, J Sun",
            "전체 인용횟수": "207회 인용202120222023713186",
            "페이지": "91-99",
            "학술 문서": "Faster R-CNN: towards real-time object detection with region proposal networks. İn: International Conference on Neural Information Processing SystemsSQ Ren, KM He, R Girshick, J Sun - 2018207회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Faster R-CNN: towards real-time object detection with region proposal networks. İn: International Conference on Neural Information Processing Systems",
        "year": null
    },
    "Deep residual learning for image recognition. arXiv e-prints": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/12",
            "권": "10",
            "저널": "arXiv preprint arXiv:1512.03385",
            "저자": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
            "전체 인용횟수": "157회 인용2016201720182019202020212022202331093831252019",
            "학술 문서": "Deep residual learning for image recognition. arXiv e-printsK He, X Zhang, S Ren, J Sun - arXiv preprint arXiv:1512.03385, 2015157회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep residual learning for image recognition. arXiv e-prints",
        "year": null
    },
    "Global refinement of random forest": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "Random forest is well known as one of the best learning methods. In spite of its great success, it also has certain drawbacks: the heuristic learning rule does not effectively minimize the global training loss; the model size is usually too large for many real applications. To address the issues, we propose two techniques, global refinement and global pruning, to improve a pre-trained random forest. The proposed global refinement jointly relearns the leaf nodes of all trees under a global objective function so that the complementary information between multiple trees is well exploited. In this way, the fitting power of the forest is significantly enhanced. The global pruning is developed to reduce the model size as well as the over-fitting risk. The refined model has better performance and smaller storage cost, as verified in extensive experiments.",
            "저자": "Shaoqing Ren, Xudong Cao, Yichen Wei, Jian Sun",
            "전체 인용횟수": "143회 인용20152016201720182019202020212022202391423171914132013",
            "컨퍼런스": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2015)",
            "페이지": "723-730",
            "학술 문서": "Global refinement of random forestS Ren, X Cao, Y Wei, J Sun - Proceedings of the IEEE conference on computer …, 2015143회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Global refinement of random forest",
        "year": null
    },
    "Deep Learning and Image Recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2023/7/21",
            "게시자": "IEEE",
            "설명": "Over the next decade, we can expect artificial intelligence (AI) to have a profound impact on our society. As AI technologies continue to advance and become more integrated into various industries, we will witness significant changes that are shaping the human being's future for the better. This paper reviews the major deep learning algorithms that have accomplished latest achievements on various fields such as manufacturing robots, smart assistants, automated financial investing, social media monitoring, marketing chat-bots and self-driving car. These algorithms are trained on large datasets of labeled images, where each image is associated with a set of labels or categories. This paper provides an overview of popular techniques for image recognition. It discusses the convolutional neural network (CNN), which is made up of multiple convolutional layers followed by pooling layers, dropout layers, batch …",
            "저자": "Chaoyang Li, Xiaohan Li, Manni Chen, Xinyao Sun",
            "전체 인용횟수": "110회 인용201720182019202020212022202359716332513",
            "출처": "2023 IEEE 6th International Conference on Electronic Information and Communication Technology (ICEICT)",
            "페이지": "557-562",
            "학술 문서": "Deep Learning and Image RecognitionC Li, X Li, M Chen, X Sun - 2023 IEEE 6th International Conference on Electronic …, 2023106회 인용 관련 학술자료 Deep Learning Image Recognition*DQT Le, SN Tiwari, B Merialdo - Stuwww. eurecom. fr, 20154회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep Learning and Image Recognition",
        "year": null
    },
    "Computer Vision–ECCV 2016": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/10",
            "저널": "European conference on computer vision",
            "저자": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, B Leibe, J Matas, N Sebe, M Welling",
            "전체 인용횟수": "101회 인용2015201620172018201920202021202220233131316162523",
            "페이지": "645",
            "학술 문서": "Computer Vision–ECCV 2016K He, X Zhang, S Ren, J Sun, B Leibe, J Matas, N Sebe… - European conference on computer vision, 2016101회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Computer Vision–ECCV 2016",
        "year": null
    },
    "Deep residual learning for image recognition In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 770–778": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "저널": "IEEE. https://doi. org/10.1109/cvpr",
            "저자": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
            "전체 인용횟수": "98회 인용20162017201820192020202120222023151115133122",
            "학술 문서": "Deep residual learning for image recognition In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 770–778K He, X Zhang, S Ren, J Sun - IEEE. https://doi. org/10.1109/cvpr, 201698회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep residual learning for image recognition In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 770–778",
        "year": null
    },
    "Deep residual learning for image recognition. CoRR (2015)": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "저널": "arXiv preprint arXiv:1512.03385",
            "저자": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
            "전체 인용횟수": "80회 인용20172018201920202021202220233787162017",
            "학술 문서": "Deep residual learning for image recognition. CoRR (2015)K He, X Zhang, S Ren, J Sun - arXiv preprint arXiv:1512.03385, 201680회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep residual learning for image recognition. CoRR (2015)",
        "year": null
    },
    "Deep residual learning for image recognition. 2016 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "게시자": "IEEE",
            "저자": "K He, X Zhang, Sh Ren, J Sun",
            "전체 인용횟수": "64회 인용201720182019202020212022202313212141715",
            "페이지": "770-778",
            "학술 문서": "Deep residual learning for image recognition. 2016 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NVK He, X Zhang, S Ren, J Sun - 201664회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep residual learning for image recognition. 2016 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV",
        "year": null
    },
    "Deep Residual Learning for Image Recognition. 2015. doi: 10.48550": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "저널": "arXiv preprint ARXIV.1512.03385",
            "저자": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
            "전체 인용횟수": "63회 인용20182019202020212022202313230",
            "학술 문서": "Deep Residual Learning for Image Recognition. 2015. doi: 10.48550K He, X Zhang, S Ren, J Sun - arXiv preprint ARXIV.1512.0338563회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep Residual Learning for Image Recognition. 2015. doi: 10.48550",
        "year": null
    },
    "Shufflenet: An extremely efficient convolutional neural network for mobile devices": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (eg, 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, eg lower top-1 error (absolute 7.8%) than recent MobileNet~ cite {howard2017mobilenets} on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves $ sim $13 $ imes $ actual speedup over AlexNet while maintaining comparable accuracy.",
            "저자": "Xiangyu Zhang*, Xinyu Zhou*, Mengxiao Lin, Jian Sun",
            "전체 인용횟수": "7035회 인용2017201820192020202120222023212756741095139017951740",
            "컨퍼런스": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "페이지": "6848-6856",
            "학술 문서": "Shufflenet: An extremely efficient convolutional neural network for mobile devicesX Zhang, X Zhou, M Lin, J Sun - Proceedings of the IEEE conference on computer …, 20187035회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Shufflenet: An extremely efficient convolutional neural network for mobile devices",
        "year": null
    },
    "Shufflenet v2: Practical guidelines for efficient cnn architecture design": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "Current network architecture design is mostly guided by the indirect metric of computation complexity, ie, FLOPs. However, the direct metric, such as speed, also depends on the other factors such as memory access cost and platform characterics. Taking these factors into account, this work proposes practical guidelines for efficient network de-sign. Accordingly, a new architecture called ShuffleNet V2 is presented. Comprehensive experiments verify that it is the state-of-the-art in both speed and accuracy.",
            "저자": "Ningning Ma*, Xiangyu Zhang*, Hai-Tao Zheng, Jian Sun",
            "전체 인용횟수": "4706회 인용2018201920202021202220232030860491613401490",
            "컨퍼런스": "Proceedings of the European conference on computer vision (ECCV)",
            "페이지": "116-131",
            "학술 문서": "Shufflenet v2: Practical guidelines for efficient cnn architecture designN Ma, X Zhang, HT Zheng, J Sun - Proceedings of the European conference on computer …, 20184706회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Shufflenet v2: Practical guidelines for efficient cnn architecture design",
        "year": null
    },
    "Channel pruning for accelerating very deep neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "In this paper, we introduce a new channel pruning method to accelerate very deep convolutional neural networks. Given a trained CNN model, we propose an iterative two-step algorithm to effectively prune each layer, by a LASSO regression based channel selection and least square reconstruction. We further generalize this algorithm to multi-layer and multi-branch cases. Our method reduces the accumulated error and enhance the compatibility with various architectures. Our pruned VGG-16 achieves the state-of-the-art results by 5x speed-up along with only 0.3% increase of error. More importantly, our method is able to accelerate modern networks like ResNet, Xception and suffers only 1.4%, 1.0% accuracy loss under 2x speed-up respectively, which is significant.",
            "저자": "Yihui He, Xiangyu Zhang, Jian Sun",
            "전체 인용횟수": "2660회 인용201820192020202120222023159307515556629482",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "1389-1397",
            "학술 문서": "Channel pruning for accelerating very deep neural networksY He, X Zhang, J Sun - Proceedings of the IEEE international conference on …, 20172660회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Channel pruning for accelerating very deep neural networks",
        "year": null
    },
    "Large kernel matters--improve semantic segmentation by global convolutional network": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "Convolution Neural Network (CNN) has boosted the per-formanceofalotofcomputervisiontasks, likeimageclassi-fication [31], segmentation [25], and detection [28]. Based on the observations from [31, 32, 14], recent model design-ers prefer to employ stacking of small kernels, like 3 x 3 over large-size filters. However, in the field of semantic seg-mentation, where we need to perform dense per-pixel pre-diction, we find that large kernel plays an important role to relieve the contradictories when optimizing the classi-fication and localization tasks simultaneously. Following the design principle of large-size kernel, We propose the Global Convolutional Network to address both the classi-fication and localization issue in the semantic segmentation task. To further refine the object category boundaries, we presentBoundaryRefinementblockbasedonresidualstruc-ture. Qualitatively, our model achieves state-of-art perfor-mance on two public benchmarks and outperforms previous results on a large margin, 82.2%(vs 80.2%) on PASCAL VOC 2012 dataset and 76.9%(vs 71.8%) on Cityscapes dataset.",
            "저자": "Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, Jian Sun",
            "전체 인용횟수": "1626회 인용201720182019202020212022202319135267298315300283",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "4353-4361",
            "학술 문서": "Large kernel matters--improve semantic segmentation by global convolutional networkC Peng, X Zhang, G Yu, G Luo, J Sun - Proceedings of the IEEE conference on computer …, 20171626회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Large kernel matters--improve semantic segmentation by global convolutional network",
        "year": null
    },
    "Repvgg: Making vgg-style convnets great again": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021",
            "설명": "We present a simple but powerful architecture of convolutional neural network, which has a VGG-like inference-time body composed of nothing but a stack of 3x3 convolution and ReLU, while the training-time model has a multi-branch topology. Such decoupling of the training-time and inference-time architecture is realized by a structural re-parameterization technique so that the model is named RepVGG. On ImageNet, RepVGG reaches over 80% top-1 accuracy, which is the first time for a plain model, to the best of our knowledge. On NVIDIA 1080Ti GPU, RepVGG models run 83% faster than ResNet-50 or 101% faster than ResNet-101 with higher accuracy and show favorable accuracy-speed trade-off compared to the state-of-the-art models like EfficientNet and RegNet. The code and trained models are available at https://github. com/megvii-model/RepVGG.",
            "저자": "Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, Jian Sun",
            "전체 인용횟수": "1030회 인용20212022202378347595",
            "컨퍼런스": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "페이지": "13733-13742",
            "학술 문서": "Repvgg: Making vgg-style convnets great againX Ding, X Zhang, N Ma, J Han, G Ding, J Sun - Proceedings of the IEEE/CVF conference on computer …, 20211027회 인용 관련 학술자료 전체 13개의 버전 Repvgg: Making vgg-style convnets great again. arXiv 2021*X Ding, X Zhang, N Ma, J Han, G Ding, J Sun - arXiv preprint arXiv:2101.0369713회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Repvgg: Making vgg-style convnets great again",
        "year": null
    },
    "Accelerating very deep convolutional networks for classification and detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/11/20",
            "게시자": "IEEE",
            "권": "38",
            "설명": "This paper aims to accelerate the test-time computation of convolutional neural networks (CNNs), especially very deep CNNs    that have substantially impacted the computer vision community. Unlike previous methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We develop an effective solution to the resulting nonlinear optimization problem without the need of stochastic gradient descent (SGD). More importantly, while previous methods mainly focus on optimizing one or two layers, our nonlinear method enables an asymmetric reconstruction that reduces the rapidly accumulated error when multiple (e.g.,   10) layers are approximated. For the widely used very deep VGG-16 model    , our method achieves a whole-model speedup of 4   with merely a 0.3 percent increase of top-5 error in ImageNet classification. Our 4    accelerated VGG …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Xiangyu Zhang, Jianhua Zou, Kaiming He, Jian Sun",
            "전체 인용횟수": "942회 인용2015201620172018201920202021202220237214288132140188193117",
            "페이지": "1943-1955",
            "학술 문서": "Accelerating very deep convolutional networks for classification and detectionX Zhang, J Zou, K He, J Sun - IEEE transactions on pattern analysis and machine …, 2015942회 인용 관련 학술자료 전체 10개의 버전 ",
            "호": "10"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Accelerating very deep convolutional networks for classification and detection",
        "year": null
    },
    "Single path one-shot neural architecture search with uniform sampling": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020/8/23",
            "게시자": "Springer, Cham",
            "설명": " We revisit the one-shot Neural Architecture Search (NAS) paradigm and analyze its advantages over existing NAS approaches. Existing one-shot method, however, is hard to train and not yet effective on large scale datasets like ImageNet. This work propose a Single Path One-Shot model to address the challenge in the training. Our central idea is to construct a simplified supernet, where all architectures are single paths so that weight co-adaption problem is alleviated. Training is performed by uniform path sampling. All architectures (and their weights) are trained fully and equally. Comprehensive experiments verify that our approach is flexible and effective. It is easy to train and fast to search. It effortlessly supports complex search spaces (e.g., building blocks, channel, mixed-precision quantization) and different search constraints (e.g., FLOPs, latency). It is thus convenient to use for various needs …",
            "저자": "Zichao Guo*, Xiangyu Zhang*, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, Jian Sun",
            "전체 인용횟수": "798회 인용2019202020212022202336111219220208",
            "컨퍼런스": "European Conference on Computer Vision",
            "페이지": "544-560",
            "학술 문서": "Single path one-shot neural architecture search with uniform samplingZ Guo, X Zhang, H Mu, W Heng, Z Liu, Y Wei, J Sun - Computer Vision–ECCV 2020: 16th European …, 2020771회 인용 관련 학술자료 전체 10개의 버전 Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun*Z Guo, X Zhang - Single path one-shot neural architecture search with …, 201949회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Single path one-shot neural architecture search with uniform sampling",
        "year": null
    },
    "Bounding box regression with uncertainty for accurate object detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "Large-scale object detection datasets (eg, MS-COCO) try to define the ground truth bounding boxes as clear as possible. However, we observe that ambiguities are still introduced when labeling the bounding boxes. In this paper, we propose a novel bounding box regression loss for learning bounding box transformation and localization variance together. Our loss greatly improves the localization accuracies of various architectures with nearly no additional computation. The learned localization variance allows us to merge neighboring bounding boxes during non-maximum suppression (NMS), which further improves the localization performance. On MS-COCO, we boost the Average Precision (AP) of VGG-16 Faster R-CNN from 23.6% to 29.1%. More importantly, for ResNet-50-FPN Mask R-CNN, our method improves the AP and AP90 by 1.8% and 6.2% respectively, which significantly outperforms previous state-of-the-art bounding box refinement methods. Our code and models are available at github. com/yihui-he/KL-Loss",
            "저자": "Yihui He, Chenchen Zhu, Jianren Wang, Marios Savvides, Xiangyu Zhang",
            "전체 인용횟수": "578회 인용201920202021202220232796161160130",
            "컨퍼런스": "Proceedings of the ieee/cvf conference on computer vision and pattern recognition",
            "페이지": "2888-2897",
            "학술 문서": "Bounding box regression with uncertainty for accurate object detectionY He, C Zhu, J Wang, M Savvides, X Zhang - Proceedings of the ieee/cvf conference on computer …, 2019506회 인용 관련 학술자료 전체 11개의 버전 Softer-nms: Rethinking bounding box regression for accurate object detection*Y He, X Zhang, M Savvides, K Kitani - arXiv preprint arXiv:1809.08545, 201886회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Bounding box regression with uncertainty for accurate object detection",
        "year": null
    },
    "Crowdhuman: A benchmark for detecting human in a crowd": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/4/30",
            "설명": "Human detection has witnessed impressive progress in recent years. However, the occlusion issue of detecting human in highly crowded environments is far from solved. To make matters worse, crowd scenarios are still under-represented in current human detection benchmarks. In this paper, we introduce a new dataset, called CrowdHuman, to better evaluate detectors in crowd scenarios. The CrowdHuman dataset is large, rich-annotated and contains high diversity. There are a total of  human instances from the train and validation subsets, and  persons per image, with various kinds of occlusions in the dataset. Each human instance is annotated with a head bounding-box, human visible-region bounding-box and human full-body bounding-box. Baseline performance of state-of-the-art detection frameworks on CrowdHuman is presented. The cross-dataset generalization results of CrowdHuman dataset demonstrate state-of-the-art performance on previous dataset including Caltech-USA, CityPersons, and Brainwash without bells and whistles. We hope our dataset will serve as a solid baseline and help promote future research in human detection tasks.",
            "저널": "arXiv preprint arXiv:1805.00123",
            "저자": "Shuai Shao, Zijian Zhao, Boxun Li, Tete Xiao, Gang Yu, Xiangyu Zhang, Jian Sun",
            "전체 인용횟수": "567회 인용20182019202020212022202331960107186192",
            "학술 문서": "Crowdhuman: A benchmark for detecting human in a crowdS Shao, Z Zhao, B Li, T Xiao, G Yu, X Zhang, J Sun - arXiv preprint arXiv:1805.00123, 2018567회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Crowdhuman: A benchmark for detecting human in a crowd",
        "year": null
    },
    "Exfuse: Enhancing feature fusion for semantic segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "Modern semantic segmentation frameworks usually combine low-level and high-level features from pre-trained backbone convolutional models to boost performance. In this paper, we first point out that a simple fusion of low-level and high-level features could be less effective because of the gap in semantic levels and spatial resolution. We find that introducing semantic information into low-level features and high-resolution details into high-level features are more effective for the later fusion. Based on this observation, we propose a new framework, named ExFuse, to bridge the gap between low-level and high-level features thus significantly improve the segmentation quality by 4.0% in total. Furthermore, we evaluate our approach on the challenging PASCAL VOC 2012 segmentation benchmark and achieve 87.9% mean IoU, which outperforms the previous state-of-the-art results.",
            "저자": "Zhenli Zhang, Xiangyu Zhang, Chao Peng, Xiangyang Xue, Jian Sun",
            "전체 인용횟수": "529회 인용20182019202020212022202357597108130111",
            "컨퍼런스": "Proceedings of the European conference on computer vision (ECCV)",
            "페이지": "269-284",
            "학술 문서": "Exfuse: Enhancing feature fusion for semantic segmentationZ Zhang, X Zhang, C Peng, X Xue, J Sun - Proceedings of the European conference on computer …, 2018529회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Exfuse: Enhancing feature fusion for semantic segmentation",
        "year": null
    },
    "Detnet: A backbone network for object detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/4/17",
            "설명": "Recent CNN based object detectors, no matter one-stage methods like YOLO, SSD, and RetinaNe or two-stage detectors like Faster R-CNN, R-FCN and FPN are usually trying to directly finetune from ImageNet pre-trained models designed for image classification. There has been little work discussing on the backbone feature extractor specifically designed for the object detection. More importantly, there are several differences between the tasks of image classification and object detection. 1. Recent object detectors like FPN and RetinaNet usually involve extra stages against the task of image classification to handle the objects with various scales. 2. Object detection not only needs to recognize the category of the object instances but also spatially locate the position. Large downsampling factor brings large valid receptive field, which is good for image classification but compromises the object location ability. Due to the gap between the image classification and object detection, we propose DetNet in this paper, which is a novel backbone network specifically designed for object detection. Moreover, DetNet includes the extra stages against traditional backbone network for image classification, while maintains high spatial resolution in deeper layers. Without any bells and whistles, state-of-the-art results have been obtained for both object detection and instance segmentation on the MSCOCO benchmark based on our DetNet~(4.8G FLOPs) backbone. The code will be released for the reproduction.",
            "저널": "Proceedings of the European Conference on Computer Vision (ECCV), 334-350",
            "저자": "Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yangdong Deng, Jian Sun",
            "전체 인용횟수": "519회 인용20182019202020212022202387411013110586",
            "학술 문서": "Detnet: A backbone network for object detectionZ Li, C Peng, G Yu, X Zhang, Y Deng, J Sun - arXiv preprint arXiv:1804.06215, 2018308회 인용 관련 학술자료 전체 2개의 버전 Detnet: Design backbone for object detection*Z Li, C Peng, G Yu, X Zhang, Y Deng, J Sun - Proceedings of the European conference on computer …, 2018213회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Detnet: A backbone network for object detection",
        "year": null
    },
    "Metapruning: Meta learning for automatic neural network channel pruning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "In this paper, we propose a novel meta learning approach for automatic channel pruning of very deep neural networks. We first train a PruningNet, a kind of meta network, which is able to generate weight parameters for any pruned structure given the target network. We use a simple stochastic structure sampling method for training the PruningNet. Then, we apply an evolutionary procedure to search for good-performing pruned networks. The search is highly efficient because the weights are directly generated by the trained PruningNet and we do not need any finetuning at search time. With a single PruningNet trained for the target network, we can search for various Pruned Networks under different constraints with little human participation. Compared to the state-of-the-art pruning methods, we have demonstrated superior performances on MobileNet V1/V2 and ResNet. Codes are available on https://github. com/liuzechun/MetaPruning.",
            "저자": "Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Kwang-Ting Cheng, Jian Sun",
            "전체 인용횟수": "515회 인용201920202021202220231470139157133",
            "컨퍼런스": "Proceedings of the IEEE/CVF international conference on computer vision",
            "페이지": "3296-3305",
            "학술 문서": "Metapruning: Meta learning for automatic neural network channel pruningZ Liu, H Mu, X Zhang, Z Guo, X Yang, KT Cheng, J Sun - Proceedings of the IEEE/CVF international conference …, 2019380회 인용 관련 학술자료 전체 9개의 버전 Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Kwang-Ting Cheng, and Jian Sun. Metapruning: Meta learning for automatic neural network channel pruning*Z Liu - Proceedings of the IEEE/CVF international conference …, 2019202회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Metapruning: Meta learning for automatic neural network channel pruning",
        "year": null
    },
    "You only look one-level feature": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021",
            "설명": "This paper revisits feature pyramids networks (FPN) for one-stage detectors and points out that the success of FPN is due to its divide-and-conquer solution to the optimization problem in object detection rather than multi-scale feature fusion. From the perspective of optimization, we introduce an alternative way to address the problem instead of adopting the complex feature pyramids--utilizing only one-level feature for detection. Based on the simple and efficient solution, we present You Only Look One-level Feature (YOLOF). In our method, two key components, Dilated Encoder and Uniform Matching, are proposed and bring considerable improvements. Extensive experiments on the COCO benchmark prove the effectiveness of the proposed model. Our YOLOF achieves comparable results with its feature pyramids counterpart RetinaNet while being 2.5 times faster. Without transformer layers, YOLOF can match the performance of DETR in a single-level feature manner with 7 times less training epochs.",
            "저자": "Qiang Chen, Yingming Wang, Tong Yang, Xiangyu Zhang, Jian Cheng, Jian Sun",
            "전체 인용횟수": "481회 인용20212022202326208241",
            "컨퍼런스": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "페이지": "13039-13048",
            "학술 문서": "You only look one-level featureQ Chen, Y Wang, T Yang, X Zhang, J Cheng, J Sun - Proceedings of the IEEE/CVF conference on computer …, 2021481회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "You only look one-level feature",
        "year": null
    },
    "Objects365: A large-scale, high-quality dataset for object detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "In this paper, we introduce a new large-scale object detection dataset, Objects365, which has 365 object categories over 600K training images. More than 10 million, high-quality bounding boxes are manually labeled through a three-step, carefully designed annotation pipeline. It is the largest object detection dataset (with full annotation) so far and establishes a more challenging benchmark for the community. Objects365 can serve as a better feature learning dataset for localization-sensitive tasks like object detection and semantic segmentation. The Objects365 pre-trained models significantly outperform ImageNet pre-trained models with 5.6 points gain (42 vs 36.4) based on the standard setting of 90K iterations on COCO benchmark. Even compared with much long training time like 540K iterations, our Objects365 pretrained model with 90K iterations still have 2.7 points gain (42 vs 39.3). Meanwhile, the finetuning time can be greatly reduced (up to 10 times) when reaching the same accuracy. Better generalization ability of Object365 has also been verified on CityPersons, VOC segmentation, and ADE tasks. The dataset as well as the pretrained-models have been released at www. objects365. org.",
            "저자": "Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, Jian Sun",
            "전체 인용횟수": "410회 인용2019202020212022202322558126196",
            "컨퍼런스": "Proceedings of the IEEE/CVF international conference on computer vision",
            "페이지": "8430-8439",
            "학술 문서": "Objects365: A large-scale, high-quality dataset for object detectionS Shao, Z Li, T Zhang, C Peng, G Yu, X Zhang, J Li… - Proceedings of the IEEE/CVF international conference …, 2019410회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Objects365: A large-scale, high-quality dataset for object detection",
        "year": null
    },
    "Scaling up your kernels to 31x31: Revisiting large kernel design in cnns": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2022",
            "설명": "We revisit large kernel design in modern convolutional neural networks (CNNs). Inspired by recent advances in vision transformers (ViTs), in this paper, we demonstrate that using a few large convolutional kernels instead of a stack of small kernels could be a more powerful paradigm. We suggested five guidelines, eg, applying re-parameterized large depth-wise convolutions, to design efficient high-performance large-kernel CNNs. Following the guidelines, we propose RepLKNet, a pure CNN architecture whose kernel size is as large as 31x31, in contrast to commonly used 3x3. RepLKNet greatly closes the performance gap between CNNs and ViTs, eg, achieving comparable or superior results than Swin Transformer on ImageNet and a few typical downstream tasks, with lower latency. RepLKNet also shows nice scalability to big data and large models, obtaining 87.8% top-1 accuracy on ImageNet and 56.0% mIoU on ADE20K, which is very competitive among the state-of-the-arts with similar model sizes. Our study further reveals that, in contrast to small-kernel CNNs, large-kernel CNNs have much larger effective receptive fields and higher shape bias rather than texture bias. Code & models at https://github. com/megvii-research/RepLKNet.",
            "저자": "Xiaohan Ding, Xiangyu Zhang**, Jungong Han, Guiguang Ding",
            "전체 인용횟수": "407회 인용2022202391310",
            "컨퍼런스": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "페이지": "11963-11975",
            "학술 문서": "Scaling up your kernels to 31x31: Revisiting large kernel design in cnnsX Ding, X Zhang, J Han, G Ding - Proceedings of the IEEE/CVF conference on computer …, 2022407회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Scaling up your kernels to 31x31: Revisiting large kernel design in cnns",
        "year": null
    },
    "The effects of titania nanotubes with embedded silver oxide nanoparticles on bacteria and osteoblasts": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/4/1",
            "게시자": "Elsevier",
            "권": "35",
            "설명": "A versatile strategy to endow biomaterials with long-term antibacterial ability without compromising the cytocompatibility is highly desirable to combat biomaterial related infection. TiO2 nanotube (NT) arrays can significantly enhance the functions of many cell types including osteoblasts thus having promising applications in orthopedics, orthodontics, as well as other biomedical fields. In this study, TiO2 NT arrays with Ag2O nanoparticle embedded in the nanotube wall (NT-Ag2O arrays) are prepared on titanium (Ti) by TiAg magnetron sputtering and anodization. Well-defined NT arrays containing Ag concentrations in a wide range from 0 to 15 at % are formed. Ag incorporation has little influence on the NT diameter, but significantly decreases the tube length. Crystallized Ag2O nanoparticles with diameters ranging from 5 nm to 20 nm are embedded in the amorphous TiO2 nanotube wall and this unique structure …",
            "저널": "Biomaterials",
            "저자": "Ang Gao, Ruiqiang Hang, Xiaobo Huang, Lingzhou Zhao, Xiangyu Zhang, Lin Wang, Bin Tang, Shengli Ma, Paul K Chu",
            "전체 인용횟수": "355회 인용201420152016201720182019202020212022202310293557503139512524",
            "페이지": "4223-4235",
            "학술 문서": "The effects of titania nanotubes with embedded silver oxide nanoparticles on bacteria and osteoblastsA Gao, R Hang, X Huang, L Zhao, X Zhang, L Wang… - Biomaterials, 2014355회 인용 관련 학술자료 전체 11개의 버전 ",
            "호": "13"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The effects of titania nanotubes with embedded silver oxide nanoparticles on bacteria and osteoblasts",
        "year": null
    },
    "A multifaceted coating on titanium dictates osteoimmunomodulation and osteo/angio-genesis towards ameliorative osseointegration": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/4/1",
            "게시자": "Elsevier",
            "권": "162",
            "설명": "A multifaceted coating for hard tissue implants, with favorable osteogenesis, angiogenesis, and osteoimmunomodulation abilities, would be of great value since it could improve osseointegration and alleviate prosthesis loosening. However, to date there are few coatings that fully satisfy these criteria. Herein we describe a microporous TiO2 coating decorated with hydroxyapatite (HA) nanoparticles that is generated by micro-arc oxidation of pure titanium (Ti) and followed annealing. By altering the annealing temperature, it is possible to simultaneously tune the coating's physical (morphology and wettability) and chemical (composites and crystallinity) properties. A coating produced with micro-arc oxidization (MAO) with an annealing temperature of 650 °C (MAO-650) exhibits numerous favorable physicochemical properties, such as hybrid micro-nano morphology, superhydrophilicity, and highly crystalline HA …",
            "저널": "Biomaterials",
            "저자": "Long Bai, Zhibin Du, Jingjing Du, Wei Yao, Jiaming Zhang, Zeming Weng, Si Liu, Ya Zhao, Yanlian Liu, Xiangyu Zhang, Xiaobo Huang, Xiaohong Yao, Ross Crawford, Ruiqiang Hang, Di Huang, Bin Tang, Yin Xiao",
            "전체 인용횟수": "212회 인용201820192020202120222023112128575635",
            "페이지": "154-169",
            "학술 문서": "A multifaceted coating on titanium dictates osteoimmunomodulation and osteo/angio-genesis towards ameliorative osseointegrationL Bai, Z Du, J Du, W Yao, J Zhang, Z Weng, S Liu… - Biomaterials, 2018212회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A multifaceted coating on titanium dictates osteoimmunomodulation and osteo/angio-genesis towards ameliorative osseointegration",
        "year": null
    },
    "Microstructure and antibacterial properties of Cu-doped TiO2 coating on titanium by micro-arc oxidation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/2/15",
            "게시자": "North-Holland",
            "권": "292",
            "설명": "Infection associated with titanium implants remains the most common serious complication after surgery. In this work, Cu-doped antibacterial TiO2 coating was synthesized by micro-arc oxidation of titanium in an electrolyte bearing Cu nanoparticles. Surface morphology and structure of the coating were characterized with scanning electron microscopy (SEM), X-ray diffraction (XRD), and X-ray photoelectron spectroscopy (XPS). The results indicated that Cu nanoparticles were not only distributed on the surface and inside the pores but also embedded in the coating. Cu mainly exists in the Cu2+ state in the TiO2 coating. The Cu-doped coating exhibited excellent antibacterial activities against Escherichia coli (E. coli) and Staphylococcus aureus (S. aureus).",
            "저널": "Applied Surface Science",
            "저자": "Xiaohong Yao, Xiangyu Zhang, Haibo Wu, Linhai Tian, Yong Ma, Bin Tang",
            "전체 인용횟수": "148회 인용2014201520162017201820192020202120222023351716131726211910",
            "페이지": "944-947",
            "학술 문서": "Microstructure and antibacterial properties of Cu-doped TiO2 coating on titanium by micro-arc oxidationX Yao, X Zhang, H Wu, L Tian, Y Ma, B Tang - Applied Surface Science, 2014148회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Microstructure and antibacterial properties of Cu-doped TiO2 coating on titanium by micro-arc oxidation",
        "year": null
    },
    "Microenvironment of alginate-based microcapsules for cell culture and tissue engineering": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/7/1",
            "게시자": "Elsevier",
            "권": "114",
            "설명": "As a type of 3D model, the technology of microencapsulation holds significant promise for tissue engineering and cell therapy due to its unique performance. The microenvironmental factors within microcapsules play an important role in influencing the behaviors of encapsulated cells. The aim of this review article is to give an overview on the construction of the microenvironmental factors, which include 3D space, physicochemical properties of alginate matrix, cell spheroids, nutritional status, and so on. Furthermore, we clarified the effect of microenvironmental factors on the behaviors of encapsulated cells and the methods about improving the microenvironment of microcapsules. This review will help to understand the interaction of the microenvironment and the encapsulated cells and lay a solid foundation for microcapsule-based cell therapy and tissue engineering.",
            "저자": "Xiaobo Huang, Xiangyu Zhang, Xiaoguang Wang, Chan Wang, Bin Tang",
            "전체 인용횟수": "124회 인용201220132014201520162017201820192020202120222023111171715138914955",
            "출처": "Journal of bioscience and bioengineering",
            "페이지": "1-8",
            "학술 문서": "Microenvironment of alginate-based microcapsules for cell culture and tissue engineeringX Huang, X Zhang, X Wang, C Wang, B Tang - Journal of bioscience and bioengineering, 2012124회 인용 관련 학술자료 전체 11개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Microenvironment of alginate-based microcapsules for cell culture and tissue engineering",
        "year": null
    },
    "A bifunctional hydrogel incorporated with CuS@MoS2 microspheres for disinfection and improved wound healing": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020/2",
            "권": "382",
            "설명": "Recent advances in antibacterial technology has made it possible to obviate the needs for antibiotics to combat bacterial infection during would healing. However, few current wound dressings can simultaneously kill bacteria efficiently and promote would healing by facilitating revascularization. Herein, a hybrid hydrogel embedded with CuS@MoS2 microspheres is synthesized. This hydrogel exhibits outstanding antibacterial activities in a short time and enhances wound healing at the same time. Within 15 min, 99.3% of Escherichia coli (E. coli) and 99.5% of Staphylococcus aureus (S. aureus) are killed due to the synergistic effects rendered by the photodynamic and photothermal antibacterial treatments under co-irradiation of 660 nm visible light (VL) and 808 near infrared (NIR) light. The synergistic effects rendered by hyperthermia and reactive oxygen species (ROS) generated by the hydrogel during light …",
            "저널": "Chemical Engineering Journal",
            "저자": "Xiangyu Zhang Xingyu Zhang, Guannan Zhang, Hongyu Zhang, Xiaoping Liu, Jing Shi, Huixian Shi, Xiaohong Yao, Paul K. Chu",
            "전체 인용횟수": "122회 인용202020212022202311294932",
            "페이지": "122849",
            "학술 문서": "A bifunctional hydrogel incorporated with CuS@ MoS2 microspheres for disinfection and improved wound healingX Zhang, G Zhang, H Zhang, X Liu, J Shi, H Shi, X Yao… - Chemical Engineering Journal, 2020122회 인용 관련 학술자료 전체 4개의 버전 ",
            "호": "15"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A bifunctional hydrogel incorporated with CuS@MoS2 microspheres for disinfection and improved wound healing",
        "year": null
    },
    "Biocompatibility, corrosion resistance and antibacterial activity of TiO2/CuO coating on titanium": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "In this work, TiO2/CuO coating was prepared on titanium (Ti) by combination of magnetron sputtering and annealing treatment. The microstructure, biocompatibility, corrosion resistance and antibacterial property of TiO2/CuO coating were investigated in comparison with pure Ti and TiO2 coating. The results show that TiO2/CuO coating is mainly composed of TiO2 and CuO. In vitro cytocompatibility evaluation suggests that no obvious toxicity appears on the TiO2/CuO coating, and the coating stimulates the osteoblast spreading and proliferation. Compared with Ti and TiO2 coating, TiO2/CuO coating exhibits improved corrosion resistance and antibacterial ability against S.aureus. This study is the first attempt to apply the combination of magnetron sputtering and annealing treatment to introduce the Cu into TiO2 coating for surface modification of Ti-based implant materials, which may provide a research foundation …",
            "저널": "Ceramics Internationa",
            "저자": "Xiangyu Zhang Xiaojing He, Guannan Zhang, Xin Wang, Ruiqiang Hang, Xiaobo Huang, Lin Qin, Bin Tang",
            "전체 인용횟수": "114회 인용20172018201920202021202220231141924211619",
            "학술 문서": "Biocompatibility, corrosion resistance and antibacterial activity of TiO2/CuO coating on titaniumX He, G Zhang, X Wang, R Hang, X Huang, L Qin… - Ceramics International, 2017114회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Biocompatibility, corrosion resistance and antibacterial activity of TiO2/CuO coating on titanium",
        "year": null
    },
    "Antimicrobial property, cytocompatibility and corrosion resistance of Zn-doped ZrO2/TiO2 coatings on Ti6Al4V implants": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/6/1",
            "게시자": "Elsevier",
            "권": "75",
            "설명": "Zn-doped ZrO2/TiO2 porous coatings (Zn-ZrO2/TiO2) were prepared on the surface of titanium alloy (Ti6Al4V) by a hybrid approach of magnetron sputtering and micro-arc oxidation (MAO). The microstructures, phase constituents and elemental states of the coating were investigated by scanning electron microscope (SEM) equipped with energy dispersive spectroscopy (EDS), X-ray diffraction (XRD), and X-ray photoelectron spectroscopy (XPS). The results demonstrate that the Zn-ZrO2/TiO2 coatings are porous and its thickness is approximately 13 μm. The major phases in the oxidation coating are tetragonal ZrO2 (t-ZrO2), cubic ZrO2 (c-ZrO2) and rutile TiO2. XPS result reveals that Zn exists as ZnO in the Zn-ZrO2/TiO2 coatings. The biological experiments indicate that Zn-ZrO2/TiO2 coatings exhibit not only excellent antibacterial property against Gram-positive Staphylococcus aureus (S. aureus), but also favorable …",
            "저널": "Materials Science and Engineering: C",
            "저자": "Ruoyun Wang, Xiaojing He, Yuee Gao, Xiangyu Zhang, Xiaohong Yao, Bin Tang",
            "전체 인용횟수": "110회 인용20172018201920202021202220237151319221618",
            "페이지": "7-15",
            "학술 문서": "Antimicrobial property, cytocompatibility and corrosion resistance of Zn-doped ZrO2/TiO2 coatings on Ti6Al4V implantsR Wang, X He, Y Gao, X Zhang, X Yao, B Tang - Materials Science and Engineering: C, 2017110회 인용 관련 학술자료 전체 5개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Antimicrobial property, cytocompatibility and corrosion resistance of Zn-doped ZrO2/TiO2 coatings on Ti6Al4V implants",
        "year": null
    },
    "Synergistic antibacterial activity of physical-chemical multi-mechanism by TiO2 nanorod arrays for safe biofilm eradication on implant": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021/1/1",
            "게시자": "Elsevier",
            "권": "6",
            "설명": "Treatment of implant-associated infection is becoming more challenging, especially when bacterial biofilms form on the surface of the implants. Developing multi-mechanism antibacterial methods to combat bacterial biofilm infections by the synergistic effects are superior to those based on single modality due to avoiding the adverse effects arising from the latter. In this work, TiO2 nanorod arrays in combination with irradiation with 808 near-infrared (NIR) light are proven to eradicate single specie biofilms by combining photothermal therapy, photodynamic therapy, and physical killing of bacteria. The TiO2 nanorod arrays possess efficient photothermal conversion ability and produce a small amount of reactive oxygen species (ROS). Physiologically, the combined actions of hyperthermia, ROS, and puncturing by nanorods give rise to excellent antibacterial properties on titanium requiring irradiation for only 15 min as …",
            "저널": "Bioactive materials",
            "저자": "Xiangyu Zhang, Guannan Zhang, Maozhou Chai, Xiaohong Yao, Weiyi Chen, Paul K Chu",
            "전체 인용횟수": "107회 인용202120222023293840",
            "페이지": "12-25",
            "학술 문서": "Synergistic antibacterial activity of physical-chemical multi-mechanism by TiO2 nanorod arrays for safe biofilm eradication on implantX Zhang, G Zhang, M Chai, X Yao, W Chen, PK Chu - Bioactive materials, 2021107회 인용 관련 학술자료 전체 10개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Synergistic antibacterial activity of physical-chemical multi-mechanism by TiO2 nanorod arrays for safe biofilm eradication on implant",
        "year": null
    },
    "DLP printing photocurable chitosan to build bio-constructs for tissue engineering": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020/5/1",
            "게시자": "Elsevier",
            "권": "235",
            "설명": "In this study, we provide a photocurable chitosan bioink (CHI-MA), which can be used for the digital light processing (DLP) technology. The CHI-MA precursors were facilely synthesized by grafting chitosan molecular chains with methacryloyl groups. We investigated the effect of parameters, including the concentration and substitution degree (DS) of CHI-MA, on the rheology and the photocuring of bioinks and the mechanical property of photo-crosslinked gels. Using the CHI-MA with a high DS (33.6 %), the curing time to print a 150 μm thick hydrogel layer can be controlled within a reasonable short time period. Additionally, the cytotoxicity test shows that both the photocuring process and the photo-crosslinked hydrogels exhibit an excellent biocompatibility. Through the DLP printing, the CHI-MA bioink can be processed into complex 3D hydrogel structures with high-resolution, high-fidelity and good biocompatibility …",
            "저널": "Carbohydrate polymers",
            "저자": "Yi Shen, Haifeng Tang, Xiaobo Huang, Ruiqiang Hang, Xiangyu Zhang, Yueyue Wang, Xiaohong Yao",
            "전체 인용횟수": "105회 인용20202021202220236233638",
            "페이지": "115970",
            "학술 문서": "DLP printing photocurable chitosan to build bio-constructs for tissue engineeringY Shen, H Tang, X Huang, R Hang, X Zhang, Y Wang… - Carbohydrate polymers, 2020105회 인용 관련 학술자료 전체 5개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "DLP printing photocurable chitosan to build bio-constructs for tissue engineering",
        "year": null
    },
    "Differential effect of hydroxyapatite nano-particle versus nano-rod decorated titanium micro-surface on osseointegration": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/8/1",
            "게시자": "Elsevier",
            "권": "76",
            "설명": "Coating materials applied for intraosseous implants must be optimized to stimulate osseointegration. Osseointegration is a temporal and spatial physiological process that not only requires interactions between osteogenesis and angiogenesis but also necessitates a favorable immune microenvironment. It is now well-documented that hierarchical nano-micro surface structures promote the long-term stability of implants, the interactions between nano-micro structure and the immune response are largely unknown. Here, we report the effects of microporous titanium (Ti) surfaces coated with nano-hydroxyapatite (HA) produced by micro-arc oxidation and steam-hydrothermal treatment (SHT) on multiple cell behavior and osseointegration. By altering the processing time of SHT it was possible to shift HA structures from nano-particles to nano-rods on the microporous Ti surfaces. Ti surfaces coated with HA nano-particles …",
            "저널": "Acta biomaterialia",
            "저자": "Long Bai, Yanlian Liu, Zhibin Du, Zeming Weng, Wei Yao, Xiangyu Zhang, Xiaobo Huang, Xiaohong Yao, Ross Crawford, Ruiqiang Hang, Di Huang, Bin Tang, Yin Xiao",
            "전체 인용횟수": "100회 인용2018201920202021202220232916282122",
            "페이지": "344-358",
            "학술 문서": "Differential effect of hydroxyapatite nano-particle versus nano-rod decorated titanium micro-surface on osseointegrationL Bai, Y Liu, Z Du, Z Weng, W Yao, X Zhang, X Huang… - Acta biomaterialia, 2018100회 인용 관련 학술자료 전체 5개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Differential effect of hydroxyapatite nano-particle versus nano-rod decorated titanium micro-surface on osseointegration",
        "year": null
    },
    "Effects of copper nanoparticles in porous TiO2 coatings on bacterial resistance and cytocompatibility of osteoblasts and endothelial cells": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "Copper (Cu) has garnered increasing interest due to its excellent antimicrobial activity and important roles in human metabolism. Although the biological effects of Cu have been studied, the effects of Cu nanoparticles (NPs) on cell behavior are not well understood. In this study, porous TiO2 coatings doped with different amounts of Cu NPs (designated as 0 Cu, 0.3 Cu, and 3.0 Cu) are deposited on titanium by micro-arc oxidation (MAO). The Cu NPs coated samples exhibit excellent antibacterial activity against Staphylococcus aureus (S. aureus). In vitro cytocompatibility evaluation discloses that 0 Cu and 0.3 Cu have no toxicity to osteoblasts but 3.0 Cu shows cytotoxicity. 0.3 Cu promotes proliferation and adhesion of osteoblasts and enhances extracellular matrix mineralization (ECM), but has little effects on the alkaline phosphatase activity (ALP) and collagen secretion. Surprisingly, the Cu NPs coated samples …",
            "저널": "Materials Science and Engineering C,",
            "저자": "Paul K. Chu Xiangyu Zhang, Jianfang Li, Xin Wang, Yueyue Wang, Ruiqiang Hang, Xiaobo Huang, Bin Tang",
            "전체 인용횟수": "95회 인용2018201920202021202220235119222224",
            "페이지": "110–120",
            "학술 문서": "Effects of copper nanoparticles in porous TiO2 coatings on bacterial resistance and cytocompatibility of osteoblasts and endothelial cellsX Zhang, J Li, X Wang, Y Wang, R Hang, X Huang… - Materials Science and Engineering: C, 201895회 인용 관련 학술자료 전체 7개의 버전 ",
            "호": "82"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Effects of copper nanoparticles in porous TiO2 coatings on bacterial resistance and cytocompatibility of osteoblasts and endothelial cells",
        "year": null
    },
    "Preparation, antibacterial effects and corrosion resistant of porous Cu–TiO2 coatings": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/7/30",
            "게시자": "North-Holland",
            "권": "308",
            "설명": "Antibacterial TiO2 coatings with different concentrations of Cu (Cu–TiO2) were prepared by micro-arc oxidation (MAO) on pre-sputtered CuTi films. The effect of Cu concentrations in CuTi films on the MAO process was investigated. The Cu–TiO2 coatings were analyzed by scanning electron microscopy (SEM), X-ray photoelectron spectroscopy (XPS) and X-ray diffraction (XRD). The corrosion resistance of Cu–TiO2 coatings was evaluated via potentiodynamic polarization method. The antibacterial properties were assessed by two methods: spread plate method and fluorescence staining. The experimental results demonstrate that the coatings are porous and consist of anatase phase, rutile phase and unoxidized titanium. The CuTi films are almost completely oxidized and the thickness of all MAO coatings is about 5–10 μm. Cu mainly exists as CuO in the TiO2 coatings. The Cu–TiO2 coatings exhibit excellent …",
            "저널": "Applied Surface Science",
            "저자": "Haibo Wu, Xiangyu Zhang, Zhenhua Geng, Yan Yin, Ruiqiang Hang, Xiaobo Huang, Xiaohong Yao, Bin Tang",
            "전체 인용횟수": "95회 인용2014201520162017201820192020202120222023221214712178714",
            "페이지": "43-49",
            "학술 문서": "Preparation, antibacterial effects and corrosion resistant of porous Cu–TiO2 coatingsH Wu, X Zhang, Z Geng, Y Yin, R Hang, X Huang… - Applied Surface Science, 201495회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Preparation, antibacterial effects and corrosion resistant of porous Cu–TiO2 coatings",
        "year": null
    },
    "In vitro assessments on bacterial adhesion and corrosion performance of TiN coating on Ti6Al4V titanium alloy synthesized by multi-arc ion plating": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/7/1",
            "게시자": "North-Holland",
            "권": "258",
            "설명": "TiN coating was synthesized on Ti6Al4V titanium alloy surface by multi-arc ion plating (MIP) technique. Surface morphology, cross sectional microstructure, elemental distributions and phase compositions of the obtained coating were analyzed by means of scanning electron microscope (SEM), optical microscope (OM), glow discharge optical emission spectroscope (GDOES) and X-ray diffraction (XRD). Bacterial adhesion and corrosion performance of Ti6Al4V and the TiN coating were assessed via in vitro bacterial adhesion tests and corrosion experiments, respectively. The results indicated that continuous and compact coating which was built up by pure TiN with a typical columnar crystal structure has reached a thickness of 1.5μm. This TiN coating could significantly reduce the bacterial adhesion and enhance the corrosion resistance of Ti6Al4V substrate.",
            "저널": "Applied surface science",
            "저자": "Naiming Lin, Xiaobo Huang, Xiangyu Zhang, Ailan Fan, Lin Qin, Bin Tang",
            "전체 인용횟수": "90회 인용20122013201420152016201720182019202020212022202323611510610119116",
            "페이지": "7047-7051",
            "학술 문서": "In vitro assessments on bacterial adhesion and corrosion performance of TiN coating on Ti6Al4V titanium alloy synthesized by multi-arc ion platingN Lin, X Huang, X Zhang, A Fan, L Qin, B Tang - Applied surface science, 201290회 인용 관련 학술자료 전체 8개의 버전 ",
            "호": "18"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "In vitro assessments on bacterial adhesion and corrosion performance of TiN coating on Ti6Al4V titanium alloy synthesized by multi-arc ion plating",
        "year": null
    },
    "Antibacterial activity and cytocompatibility of Cu–Ti–O nanotubes": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/6",
            "권": "102",
            "설명": " TiO2 nanotubes (NTs) have favorable biological properties, but the poor antibacterial activity limits their application especially in orthopedics fields. In this article, Cu–Ti–O nanotubes with different Cu contents are fabricated on sputtered TiCu films. Scanning electron microscopy reveals the NTs can be formed on sputtered TiCu films when the Cu content is less than 14.6 at %. X‐ray photoelectron spectroscopy results indicate the NTs are consist of CuO mixed with TiO2 and the Cu content in NTs decreases dramatically compared with that in TiCu films. Biological experiments show that although these NTs have poor release antibacterial activity, their contact antibacterial activity has proven to be excellent, indicating the NT surface can effectively inhibit biomaterial‐associated infections. The cytocompatibility of the NTs is closely related to the Cu content and when its content is relatively low (1.01 at %), there is no …",
            "저널": "Journal of Biomedical Materials Research Part A",
            "저자": "Ruiqiang Hang, Ang Gao, Xiaobo Huang, Xiaoguang Wang, Xiangyu Zhang, Lin Qin, Bin Tang",
            "전체 인용횟수": "87회 인용20142015201620172018201920202021202220234592011410977",
            "페이지": "1850-1858",
            "학술 문서": "Antibacterial activity and cytocompatibility of Cu–Ti–O nanotubesR Hang, A Gao, X Huang, X Wang, X Zhang, L Qin… - Journal of Biomedical Materials Research Part A, 201487회 인용 관련 학술자료 전체 6개의 버전 ",
            "호": "6"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Antibacterial activity and cytocompatibility of Cu–Ti–O nanotubes",
        "year": null
    },
    "Near-infrared light II-assisted rapid biofilm elimination platform for bone implants at mild temperature": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021/2/1",
            "게시자": "Elsevier",
            "권": "269",
            "설명": "Light-triggered therapy is a prospective method to combat implant-associated infection but near-infrared I (NIR-I) light has insufficient penetrating ability in tissues and local hyperthermia induced by the photothermal treatment may destroy surrounding healthy tissues. Herein, a near-infrared II (NIR-II) phototherapy system composed of upconversion elements doped titanium dioxide nanorods (TiO2 NRs)/curcumin (Cur)/hyaluronic acid (HA)/bone morphogenetic protein-2 (BMP-2) is designed for biomedical titanium and demonstrated to overcome the above hurdles simultaneously. Incorporation of F, Yb, and Ho not only improves the photocatalytic ability, but also renders the implants with the upconversion capability, so that the NRs can generate enough reactive oxygen species (ROS) when irradiated by the NIR-II laser. Furthermore, the combined actions of quorum sensing inhibitors, ROS, and physical puncture by …",
            "저널": "Biomaterials",
            "저자": "Guannan Zhang, Yongqiang Yang, Jing Shi, Xiaohong Yao, Weiyi Chen, Xiaochun Wei, Xiangyu Zhang, Paul K Chu",
            "전체 인용횟수": "80회 인용20212022202363935",
            "페이지": "120634",
            "학술 문서": "Near-infrared light II-assisted rapid biofilm elimination platform for bone implants at mild temperatureG Zhang, Y Yang, J Shi, X Yao, W Chen, X Wei… - Biomaterials, 202180회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Near-infrared light II-assisted rapid biofilm elimination platform for bone implants at mild temperature",
        "year": null
    },
    "Corrosion behavior of Zn-incorporated antibacterial TiO2 porous coating on titanium": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/11/15",
            "게시자": "Elsevier",
            "권": "42",
            "설명": "Incorporation of antibacterial agents (e.g. Ag and Cu) at the surface of biomedical materials has evolved as a potentially effective method for preventing the bacterial infections. However, the antibacterial efficacy of medical device implants must necessarily be balanced by good corrosion resistance and the corrosion behavior of the antibacterial coatings has seldom been reported. In this work, Zn-incorporated antibacterial TiO2 coating was produced on pure titanium (Ti) by micro-arc oxidization (MAO) and the electrochemical behavior was assessed. The results obtained from the antibacterial studies suggest that the Zn-incorporated TiO2 coating provides bactericidal activity against both Gram-negative Escherichia coli (E. coli) and Gram-positive Staphylococcus aureus (S. aureus) over 90%. The corrosion behavior of Zn-incorporated TiO2 coating were investigated using a combination of complementary …",
            "저널": "Ceramics International",
            "저자": "Xiangyu Zhang, Huizhen Wang, Jiangfang Li, Xiaojing He, Ruiqiang Hang, Xiaobo Huang, Linhai Tian, Bin Tang",
            "전체 인용횟수": "80회 인용2016201720182019202020212022202319169919512",
            "페이지": "17095-17100",
            "학술 문서": "Corrosion behavior of Zn-incorporated antibacterial TiO2 porous coating on titaniumX Zhang, H Wang, J Li, X He, R Hang, X Huang, L Tian… - Ceramics International, 201680회 인용 관련 학술자료 전체 3개의 버전 ",
            "호": "15"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Corrosion behavior of Zn-incorporated antibacterial TiO2 porous coating on titanium",
        "year": null
    },
    "Facile synthesis of mesoporous Mn 3 O 4 nanorods as a promising anode material for high performance lithium-ion batteries": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "게시자": "Royal Society of Chemistry",
            "권": "2",
            "설명": "In this work, porous Mn3O4 nanorods have been fabricated through the decomposition of MnOOH nanorods under an inert gas. The sample shows a high BET surface area of 27.6 m2 g−1 and a narrow pore size distribution of 3.9 nm. Because of the excellent porous geometry and one-dimensional structure, the porous Mn3O4 nanorods display outstanding electrochemical performance, such as high specific capacity (901.5 mA h g−1 at a current density of 500 mA g−1), long cycling stability (coulombic efficiency of 99.3% after 150 cycles) and high rate capability (387.5 mA h g−1 at 2000 mA g−1). Very interestingly, the porous Mn3O4 nanorods are converted to Mn3O4 following electrochemical reaction, which does not occur with nonporous Mn3O4 nanorods. The possible reason may be ascribed to the improved kinetics of the porous structure.",
            "저널": "Journal of Materials Chemistry A",
            "저자": "Zhongchao Bai, Xiangyu Zhang, Yuwen Zhang, Chunli Guo, Bin Tang",
            "전체 인용횟수": "77회 인용2014201520162017201820192020202120222023113131410611351",
            "페이지": "16755-16760",
            "학술 문서": "Facile synthesis of mesoporous Mn 3 O 4 nanorods as a promising anode material for high performance lithium-ion batteriesZ Bai, X Zhang, Y Zhang, C Guo, B Tang - Journal of Materials Chemistry A, 201477회 인용 관련 학술자료 전체 2개의 버전 ",
            "호": "39"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Facile synthesis of mesoporous Mn 3 O 4 nanorods as a promising anode material for high performance lithium-ion batteries",
        "year": null
    },
    "Review of antibacterial activity of titanium-based implants’ surfaces fabricated by micro-arc oxidation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/3/22",
            "게시자": "MDPI",
            "권": "7",
            "설명": "Ti and its alloys are the most commonly-used materials for biomedical applications. However, bacterial infection after implant placement is still one of the significant rising complications. Therefore, the application of the antimicrobial agents into implant surfaces to prevent implant-associated infection has attracted much attention. Scientific papers have shown that inorganic antibacterial metal elements (e.g., Ag, Cu, Zn) can be introduced into implant surfaces with the addition of metal nanoparticles or metallic compounds into an electrolyte via micro-arc oxidation (MAO) technology. In this review, the effects of the composition and concentration of electrolyte and process parameters (e.g., voltage, current density, oxidation time) on the morphological characteristics (e.g., surface morphology, bonding strength), antibacterial ability and biocompatibility of MAO antimicrobial coatings are discussed in detail. Anti-infection and osseointegration can be simultaneously accomplished with the selection of the proper antibacterial elements and operating parameters. Besides, MAO assisted by magnetron sputtering (MS) to endow Ti-based implant materials with superior antibacterial ability and biocompatibility is also discussed. Finally, the development trend of MAO technology in the future is forecasted.",
            "저자": "Xiaojing He, Xiangyu Zhang, Xin Wang, Lin Qin",
            "전체 인용횟수": "74회 인용201720182019202020212022202331181714714",
            "출처": "Coatings",
            "페이지": "45",
            "학술 문서": "Review of antibacterial activity of titanium-based implants’ surfaces fabricated by micro-arc oxidationX He, X Zhang, X Wang, L Qin - Coatings, 201774회 인용 관련 학술자료 전체 10개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Review of antibacterial activity of titanium-based implants’ surfaces fabricated by micro-arc oxidation",
        "year": null
    },
    "Bactericidal behavior of Cu-containing stainless steel surfaces": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/10/1",
            "게시자": "North-Holland",
            "권": "258",
            "설명": "Stainless steels are one of the most common materials used in health care environments. However, the lack of antibacterial advantage has limited their use in practical application. In this paper, antibacterial stainless steel surfaces with different Cu contents have been prepared by plasma surface alloying technology (PSAT). The steel surface with Cu content 90wt.% (Cu-SS) exhibits strong bactericidal activity against Escherichia coli (E. coli) and Staphylococcus aureus (S. aureus) within 3h. Although the Cu-containing surface with Cu content 2.5wt.% (CuNi-SS) can also kill all tested bacteria, this process needs 12h. SEM observation of the bacterial morphology and an agarose gel electrophoresis were performed to study the antibacterial mechanism of Cu-containing stainless steel surfaces against E. coli. The results indicated that Cu ions are released when the Cu-containing surfaces are in contact with bacterial …",
            "저널": "Applied Surface Science",
            "저자": "Xiangyu Zhang, Xiaobo Huang, Yong Ma, Naiming Lin, Ailan Fan, Bin Tang",
            "전체 인용횟수": "73회 인용201320142015201620172018201920202021202220236771114626752",
            "페이지": "10058-10063",
            "학술 문서": "Bactericidal behavior of Cu-containing stainless steel surfacesX Zhang, X Huang, Y Ma, N Lin, A Fan, B Tang - Applied Surface Science, 201273회 인용 관련 학술자료 전체 7개의 버전 ",
            "호": "24"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Bactericidal behavior of Cu-containing stainless steel surfaces",
        "year": null
    },
    "Fabrication of Ni-Ti-O nanotube arrays by anodization of NiTi alloy and their potential applications": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/12/17",
            "게시자": "Nature Publishing Group UK",
            "권": "4",
            "설명": "Nickel-titanium-oxide (Ni-Ti-O) nanotube arrays (NTAs) prepared on nearly equiatomic NiTi alloy shall have broad application potential such as for energy storage and biomedicine, but their precise structure control is a great challenge because of the high content of alloying element of Ni, a non-valve metal that cannot form a compact electronic insulating passive layer when anodized. In the present work, we systemically investigated the influence of various anodization parameters on the formation and structure of Ni-Ti-O NTAs and their potential applications. Our results show that well controlled NTAs can be fabricated during relatively wide ranges of the anodization voltage (5–90 V), electrolyte temperature (10–50°C) and electrolyte NH4F content (0.025–0.8 wt%) but within a narrow window of the electrolyte H2O content (0.0–1.0 vol%). Through modulating these parameters, the Ni-Ti-O NTAs with different …",
            "저널": "Scientific reports",
            "저자": "Ruiqiang Hang, Yanlian Liu, Lingzhou Zhao, Ang Gao, Long Bai, Xiaobo Huang, Xiangyu Zhang, Bin Tang, Paul K Chu",
            "전체 인용횟수": "69회 인용20152016201720182019202020212022202319710591477",
            "페이지": "7547",
            "학술 문서": "Fabrication of Ni-Ti-O nanotube arrays by anodization of NiTi alloy and their potential applicationsR Hang, Y Liu, L Zhao, A Gao, L Bai, X Huang, X Zhang… - Scientific reports, 201469회 인용 관련 학술자료 전체 13개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Fabrication of Ni-Ti-O nanotube arrays by anodization of NiTi alloy and their potential applications",
        "year": null
    },
    "Exploiting macrophage autophagy-lysosomal biogenesis as a therapy for atherosclerosis": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/6/7",
            "게시자": "Nature Publishing Group UK",
            "권": "8",
            "설명": "Macrophages specialize in removing lipids and debris present in the atherosclerotic plaque. However, plaque progression renders macrophages unable to degrade exogenous atherogenic material and endogenous cargo including dysfunctional proteins and organelles. Here we show that a decline in the autophagy–lysosome system contributes to this as evidenced by a derangement in key autophagy markers in both mouse and human atherosclerotic plaques. By augmenting macrophage TFEB, the master transcriptional regulator of autophagy–lysosomal biogenesis, we can reverse the autophagy dysfunction of plaques, enhance aggrephagy of p62-enriched protein aggregates and blunt macrophage apoptosis and pro-inflammatory IL-1β levels, leading to reduced atherosclerosis. In order to harness this degradative response therapeutically, we also describe a natural sugar called trehalose as an inducer of …",
            "저널": "Nature communications",
            "저자": "Ismail Sergin, Trent D Evans, Xiangyu Zhang, Somashubhra Bhattacharya, Carl J Stokes, Eric Song, Sahl Ali, Babak Dehestani, Karyn B Holloway, Paul S Micevych, Ali Javaheri, Jan R Crowley, Andrea Ballabio, Joel D Schilling, Slava Epelman, Conrad C Weihl, Abhinav Diwan, Daping Fan, Mohamed A Zayed, Babak Razani",
            "전체 인용횟수": "268회 인용20172018201920202021202220235244347625234",
            "페이지": "15750",
            "학술 문서": "Exploiting macrophage autophagy-lysosomal biogenesis as a therapy for atherosclerosisI Sergin, TD Evans, X Zhang, S Bhattacharya… - Nature communications, 2017268회 인용 관련 학술자료 전체 15개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Exploiting macrophage autophagy-lysosomal biogenesis as a therapy for atherosclerosis",
        "year": null
    },
    "TFEB and trehalose drive the macrophage autophagy-lysosome system to protect against atherosclerosis": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/4/3",
            "게시자": "Taylor & Francis",
            "권": "14",
            "설명": "In the atherosclerotic plaque, macrophages are the key catabolic workhorse responsible for clearing lipid and dead cell debris. To survive the highly proinflammatory and lipotoxic plaque environment, macrophages must adopt strategies for maintaining tight homeostasis and self-renewal. Macroautophagy/autophagy is a pro-survival cellular pathway wherein damaged or excess cellular cargoes are encapsulated by a double-membrane compartment and delivered to the lysosome for hydrolysis. Previously, macrophage-specific autophagy deficiency has been shown to be atherogenic through several complementary mechanisms including hyperactivation of the inflammasome, defective efferocytosis, accumulation of cytotoxic protein aggregates, and impaired lipid degradation. Conversely, in a recent study we hypothesized that enhancing the macrophage autophagy-lysosomal system through genetic or …",
            "저널": "Autophagy",
            "저자": "Trent D Evans, Se-Jin Jeong, Xiangyu Zhang, Ismail Sergin, Babak Razani",
            "전체 인용횟수": "134회 인용20182019202020212022202321527333522",
            "페이지": "724-726",
            "학술 문서": "TFEB and trehalose drive the macrophage autophagy-lysosome system to protect against atherosclerosisTD Evans, SJ Jeong, X Zhang, I Sergin, B Razani - Autophagy, 2018134회 인용 관련 학술자료 전체 8개의 버전 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "TFEB and trehalose drive the macrophage autophagy-lysosome system to protect against atherosclerosis",
        "year": null
    },
    "Acetyl-CoA derived from hepatic peroxisomal β-oxidation inhibits autophagy and promotes steatosis via mTORC1 activation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020/7/2",
            "게시자": "Elsevier",
            "권": "79",
            "설명": "Autophagy is activated by prolonged fasting but cannot overcome the ensuing hepatic lipid overload, resulting in fatty liver. Here, we describe a peroxisome-lysosome metabolic link that restricts autophagic degradation of lipids. Acyl-CoA oxidase 1 (Acox1), the enzyme that catalyzes the first step in peroxisomal β-oxidation, is enriched in liver and further increases with fasting or high-fat diet (HFD). Liver-specific Acox1 knockout (Acox1-LKO) protected mice against hepatic steatosis caused by starvation or HFD due to induction of autophagic degradation of lipid droplets. Hepatic Acox1 deficiency markedly lowered total cytosolic acetyl-CoA levels, which led to decreased Raptor acetylation and reduced lysosomal localization of mTOR, resulting in impaired activation of mTORC1, a central regulator of autophagy. Dichloroacetic acid treatment elevated acetyl-CoA levels, restored mTORC1 activation, inhibited autophagy …",
            "저널": "Molecular cell",
            "저자": "Anyuan He, Xiaowen Chen, Min Tan, Yali Chen, Dongliang Lu, Xiangyu Zhang, John M Dean, Babak Razani, Irfan J Lodhi",
            "전체 인용횟수": "109회 인용20202021202220236163155",
            "페이지": "30-42. e4",
            "학술 문서": "Acetyl-CoA derived from hepatic peroxisomal β-oxidation inhibits autophagy and promotes steatosis via mTORC1 activationA He, X Chen, M Tan, Y Chen, D Lu, X Zhang, JM Dean… - Molecular cell, 2020109회 인용 관련 학술자료 전체 9개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Acetyl-CoA derived from hepatic peroxisomal β-oxidation inhibits autophagy and promotes steatosis via mTORC1 activation",
        "year": null
    },
    "Protective Effect of Procyanidin B2 against CCl4-Induced Acute Liver Injury in Mice": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/7/3",
            "게시자": "MDPI",
            "권": "20",
            "설명": "Procyanidin B2 has demonstrated several health benefits and medical properties. However, its protective effects against CCl4-induced hepatotoxicity have not been clarified. The present study aimed to investigate the hepatoprotective effects of procyanidin B2 in CCl4-treated mice. Our data showed that procyanidin B2 significantly decreased the CCl4-induced elevation of serum alanine aminotransferase activities, as well as improved hepatic histopathological abnormalities. Procyanidin B2 also significantly decreased the content of MDA but enhanced the activities of antioxidant enzymes SOD, CAT and GSH-Px. Further research demonstrated that procyanidin B2 decreased the expression of TNF-α, IL-1β, cyclooxygenase-2 (COX-2) and inducible nitric oxide synthase (iNOS), as well as inhibited the translocation of nuclear factor-kappa B (NF-κB) p65 from the cytosol to the nuclear fraction in mouse liver. Moreover, CCl4-induced apoptosis in mouse liver was measured by (terminal-deoxynucleotidyl transferase mediated nick end labeling) TUNEL assay and the cleaved caspase-3. Meanwhile, the expression of apoptosis-related proteins Bax and Bcl-xL was analyzed by Western blot. Results showed that procyanidin B2 significantly inhibited CCl4-induced hepatocyte apoptosis, markedly suppressed the upregulation of Bax expression and restored the downregulation of Bcl-xL expression. Overall, the findings indicated that procyanidin B2 exhibited a protective effect on CCl4-induced hepatic injury by elevating the antioxidative defense potential and consequently suppressing the inflammatory response and apoptosis of liver tissues.",
            "저널": "Molecules",
            "저자": "Bing-Ya Yang, Xiang-Yu Zhang, Sheng-Wen Guan, Zi-Chun Hua",
            "전체 인용횟수": "109회 인용2016201720182019202020212022202315921131413158",
            "페이지": "12250-12265",
            "학술 문서": "Protective effect of procyanidin B2 against CCl4-induced acute liver injury in miceBY Yang, XY Zhang, SW Guan, ZC Hua - Molecules, 2015109회 인용 관련 학술자료 전체 11개의 버전 ",
            "호": "7"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Protective Effect of Procyanidin B2 against CCl4-Induced Acute Liver Injury in Mice",
        "year": null
    },
    "Peroxisome-derived lipids regulate adipose thermogenesis by mediating cold-induced mitochondrial fission": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/2/1",
            "게시자": "American Society for Clinical Investigation",
            "권": "129",
            "설명": "Peroxisomes perform essential functions in lipid metabolism, including fatty acid oxidation and plasmalogen synthesis. Here, we describe a role for peroxisomal lipid metabolism in mitochondrial dynamics in brown and beige adipocytes. Adipose tissue peroxisomal biogenesis was induced in response to cold exposure through activation of the thermogenic coregulator PRDM16. Adipose-specific knockout of the peroxisomal biogenesis factor Pex16 (Pex16-AKO) in mice impaired cold tolerance, decreased energy expenditure, and increased diet-induced obesity. Pex16 deficiency blocked cold-induced mitochondrial fission, decreased mitochondrial copy number, and caused mitochondrial dysfunction. Adipose-specific knockout of the peroxisomal β-oxidation enzyme acyl-CoA oxidase 1 (Acox1-AKO) was not sufficient to affect adiposity, thermogenesis, or mitochondrial copy number, but knockdown of the …",
            "저널": "The Journal of clinical investigation",
            "저자": "Hongsuk Park, Anyuan He, Min Tan, Jordan M Johnson, John M Dean, Terri A Pietka, Yali Chen, Xiangyu Zhang, Fong-Fu Hsu, Babak Razani, Katsuhiko Funai, Irfan J Lodhi",
            "전체 인용횟수": "106회 인용20192020202120222023422252727",
            "페이지": "694-711",
            "학술 문서": "Peroxisome-derived lipids regulate adipose thermogenesis by mediating cold-induced mitochondrial fissionH Park, A He, M Tan, JM Johnson, JM Dean, TA Pietka… - The Journal of clinical investigation, 2019106회 인용 관련 학술자료 전체 13개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Peroxisome-derived lipids regulate adipose thermogenesis by mediating cold-induced mitochondrial fission",
        "year": null
    },
    "Inclusion bodies enriched for p62 and polyubiquitinated proteins in macrophages protect against atherosclerosis": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/1/5",
            "게시자": "American Association for the Advancement of Science",
            "권": "9",
            "설명": "Autophagy is a catabolic cellular mechanism that degrades dysfunctional proteins and organelles. Atherosclerotic plaque formation is enhanced in mice with macrophages deficient for the critical autophagy protein ATG5. We showed that exposure of macrophages to lipids that promote atherosclerosis increased the abundance of the autophagy chaperone p62 and that p62 colocalized with polyubiquitinated proteins in cytoplasmic inclusions, which are characterized by insoluble protein aggregates. ATG5-null macrophages developed further p62 accumulation at the sites of large cytoplasmic ubiquitin-positive inclusion bodies. Aortas from atherosclerotic mice and plaques from human endarterectomy samples showed increased abundance of p62 and polyubiquitinated proteins that colocalized with plaque macrophages, suggesting that p62-enriched protein aggregates were characteristic of atherosclerosis. The …",
            "저널": "Science signaling",
            "저자": "Ismail Sergin, Somashubhra Bhattacharya, Roy Emanuel, Emel Esen, Carl J Stokes, Trent D Evans, Batool Arif, John A Curci, Babak Razani",
            "전체 인용횟수": "93회 인용20172018201920202021202220231511916141610",
            "페이지": "ra2-ra2",
            "학술 문서": "Inclusion bodies enriched for p62 and polyubiquitinated proteins in macrophages protect against atherosclerosisI Sergin, S Bhattacharya, R Emanuel, E Esen… - Science signaling, 201693회 인용 관련 학술자료 전체 8개의 버전 ",
            "호": "409"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Inclusion bodies enriched for p62 and polyubiquitinated proteins in macrophages protect against atherosclerosis",
        "year": null
    },
    "High-protein diets increase cardiovascular risk by activating macrophage mTOR to suppress mitophagy": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020/1",
            "게시자": "Nature Publishing Group UK",
            "권": "2",
            "설명": "High-protein diets are commonly utilized for weight loss, yet they have been reported to raise cardiovascular risk. The mechanisms underlying this risk are unknown. Here, we show that dietary protein drives atherosclerosis and lesion complexity. Protein ingestion acutely elevates amino acid levels in blood and atherosclerotic plaques, stimulating macrophage mammalian target of rapamycin (mTOR) signalling. This is causal in plaque progression, because the effects of dietary protein are abrogated in macrophage-specific Raptor-null mice. Mechanistically, we find amino acids exacerbate macrophage apoptosis induced by atherogenic lipids, a process that involves mammalian target of rapamycin complex 1 (mTORC1)-dependent inhibition of mitochondrial autophagy (mitophagy), accumulation of dysfunctional mitochondria and mitochondrial apoptosis. Using macrophage-specific mTORC1- and autophagy …",
            "저널": "Nature metabolism",
            "저자": "Xiangyu Zhang, Ismail Sergin, Trent D Evans, Se-Jin Jeong, Astrid Rodriguez-Velez, Divya Kapoor, Sunny Chen, Eric Song, Karyn B Holloway, Jan R Crowley, Slava Epelman, Conrad C Weihl, Abhinav Diwan, Daping Fan, Bettina Mittendorfer, Nathan O Stitziel, Joel D Schilling, Irfan J Lodhi, Babak Razani",
            "전체 인용횟수": "85회 인용20202021202220238202233",
            "페이지": "110-125",
            "학술 문서": "High-protein diets increase cardiovascular risk by activating macrophage mTOR to suppress mitophagyX Zhang, I Sergin, TD Evans, SJ Jeong… - Nature metabolism, 202085회 인용 관련 학술자료 전체 9개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "High-protein diets increase cardiovascular risk by activating macrophage mTOR to suppress mitophagy",
        "year": null
    },
    "p62/SQSTM1 and Selective Autophagy in Cardiometabolic Diseases": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/8/20",
            "게시자": "Mary Ann Liebert, Inc., publishers",
            "권": "31",
            "설명": " Significance: p62/SQSTM1 is a multifunctional scaffolding protein involved in the regulation of various signaling pathways as well as autophagy. In particular, p62/SQSTM1 serves as an essential adaptor to identify and deliver specific organelles and protein aggregates to autophagosomes for degradation, a process known as selective autophagy. Critical Issues: With the emergence of autophagy as a critical process in cellular metabolism and the development of cardiometabolic diseases, it is increasingly important to understand p62's role in the integration of signaling and autophagic pathways. Recent Advances: This review first discusses the features that make p62/SQSTM1 an ideal chaperone in integrating signaling pathways with autophagy and details the current understanding of its diverse roles in selective autophagy processes. Distinct and overlapping roles of other chaperones with similar functions are …",
            "저자": "Se-Jin Jeong, Xiangyu Zhang, Astrid Rodriguez-Velez, Trent D Evans, Babak Razani",
            "전체 인용횟수": "76회 인용2019202020212022202326152824",
            "출처": "Antioxidants & Redox Signaling",
            "페이지": "458-471",
            "학술 문서": "p62/SQSTM1 and selective autophagy in cardiometabolic diseasesSJ Jeong, X Zhang, A Rodriguez-Velez, TD Evans… - Antioxidants & Redox Signaling, 201976회 인용 관련 학술자료 전체 8개의 버전 ",
            "호": "6"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "p62/SQSTM1 and Selective Autophagy in Cardiometabolic Diseases",
        "year": null
    },
    "Classical and alternative roles for autophagy in lipid metabolism": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/6",
            "게시자": "NIH Public Access",
            "권": "29",
            "설명": "As the field of autophagy has evolved and expanded to include functional roles in various aspects of cellular degradation, so has its role in intracellular lipid metabolism. Understanding the mechanisms underlying these classical and alternative roles of autophagy will not only enhance our knowledge in lipid biology but also provide new avenues of translation to human lipid disorders.",
            "저자": "Xiangyu Zhang, Trent D Evans, Se-Jin Jeong, Babak Razani",
            "전체 인용횟수": "73회 인용20182019202020212022202311119131413",
            "출처": "Current opinion in lipidology",
            "페이지": "203",
            "학술 문서": "Classical and alternative roles for autophagy in lipid metabolismX Zhang, TD Evans, SJ Jeong, B Razani - Current opinion in lipidology, 201873회 인용 관련 학술자료 전체 9개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Classical and alternative roles for autophagy in lipid metabolism",
        "year": null
    },
    "Endoplasmic reticulum stress regulates rat mandibular cartilage thinning under compressive mechanical stress": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/6/21",
            "게시자": "Elsevier",
            "권": "288",
            "설명": "Compressive mechanical stress-induced cartilage thinning has been characterized as a key step in the progression of temporomandibular joint diseases, such as osteoarthritis. However, the regulatory mechanisms underlying this loss have not been thoroughly studied. Here, we used an established animal model for loading compressive mechanical stress to induce cartilage thinning in vivo. The mechanically stressed mandibular chondrocytes were then isolated to screen potential candidates using a proteomics approach. A total of 28 proteins were identified that were directly or indirectly associated with endoplasmic reticulum stress, including protein disulfide-isomerase, calreticulin, translationally controlled tumor protein, and peptidyl-prolyl cis/trans-isomerase protein. The altered expression of these candidates was validated at both the mRNA and protein levels. The induction of endoplasmic reticulum stress by …",
            "저널": "Journal of Biological Chemistry",
            "저자": "Huang Li, Xiang-Yu Zhang, Tuo-Jiang Wu, Wei Cheng, Xin Liu, Ting-Ting Jiang, Juan Wen, Jie Li, Qiao-Ling Ma, Zi-Chun Hua",
            "전체 인용횟수": "73회 인용201320142015201620172018201920202021202220232768975411104",
            "페이지": "18172-18183",
            "학술 문서": "Endoplasmic reticulum stress regulates rat mandibular cartilage thinning under compressive mechanical stressH Li, XY Zhang, TJ Wu, W Cheng, X Liu, TT Jiang… - Journal of Biological Chemistry, 201373회 인용 관련 학술자료 전체 9개의 버전 ",
            "호": "25"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Endoplasmic reticulum stress regulates rat mandibular cartilage thinning under compressive mechanical stress",
        "year": null
    },
    "Target acquired: selective autophagy in cardiometabolic disease": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/2/28",
            "게시자": "American Association for the Advancement of Science",
            "권": "10",
            "설명": "The accumulation of damaged or excess proteins and organelles is a defining feature of metabolic disease in nearly every tissue. Thus, a central challenge in maintaining metabolic homeostasis is the identification, sequestration, and degradation of these cellular components, including protein aggregates, mitochondria, peroxisomes, inflammasomes, and lipid droplets. A primary route through which this challenge is met is selective autophagy, the targeting of specific cellular cargo for autophagic compartmentalization and lysosomal degradation. In addition to its roles in degradation, selective autophagy is emerging as an integral component of inflammatory and metabolic signaling cascades. In this Review, we focus on emerging evidence and key questions about the role of selective autophagy in the cell biology and pathophysiology of metabolic diseases such as obesity, diabetes, atherosclerosis, and …",
            "저자": "Trent D Evans, Ismail Sergin, Xiangyu Zhang, Babak Razani",
            "전체 인용횟수": "61회 인용2017201820192020202120222023871112698",
            "출처": "Science signaling",
            "페이지": "eaag2298",
            "학술 문서": "Target acquired: selective autophagy in cardiometabolic diseaseTD Evans, I Sergin, X Zhang, B Razani - Science signaling, 201761회 인용 관련 학술자료 전체 8개의 버전 ",
            "호": "468"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Target acquired: selective autophagy in cardiometabolic disease",
        "year": null
    },
    "Modulation of Salmonella tumor-colonization and intratumoral anti-angiogenesis by triptolide and its mechanism": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "게시자": "Ivyspring International Publisher",
            "권": "7",
            "설명": "The weakened tumour colonization of attenuated Salmonella has severely hampered its clinical development. In this study, we investigated whether an anti-inflammation and antiangiogenesis compound triptolide could improve the efficacy of",
            "저널": "Theranostics",
            "저자": "Jianxiang Chen, Yiting Qiao, Bo Tang, Guo Chen, Xiufeng Liu, Bingya Yang, Jing Wei, Xiangyu Zhang, Xiawei Cheng, Pan Du, Wenhui Jiang, Qingang Hu, Zi-Chun Hua",
            "전체 인용횟수": "53회 인용20182019202020212022202339910139",
            "페이지": "2250",
            "학술 문서": "Modulation of Salmonella tumor-colonization and intratumoral anti-angiogenesis by triptolide and its mechanismJ Chen, Y Qiao, B Tang, G Chen, X Liu, B Yang, J Wei… - Theranostics, 201753회 인용 관련 학술자료 전체 5개의 버전 ",
            "호": "8"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Modulation of Salmonella tumor-colonization and intratumoral anti-angiogenesis by triptolide and its mechanism",
        "year": null
    },
    "Trehalose causes low-grade lysosomal stress to activate TFEB and the autophagy-lysosome biogenesis response": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021/11/2",
            "게시자": "Taylor & Francis",
            "권": "17",
            "설명": "The autophagy-lysosome system is an important cellular degradation pathway that recycles dysfunctional organelles and cytotoxic protein aggregates. A decline in this system is pathogenic in many human diseases including neurodegenerative disorders, fatty liver disease, and atherosclerosis. Thus there is intense interest in discovering therapeutics aimed at stimulating the autophagy-lysosome system. Trehalose is a natural disaccharide composed of two glucose molecules linked by a ɑ-1,1-glycosidic bond with the unique ability to induce cellular macroautophagy/autophagy and with reported efficacy on mitigating several diseases where autophagy is dysfunctional. Interestingly, the mechanism by which trehalose induces autophagy is unknown. One suggested mechanism is its ability to activate TFEB (transcription factor EB), the master transcriptional regulator of autophagy-lysosomal biogenesis. Here we …",
            "저널": "Autophagy",
            "저자": "Se-Jin Jeong, Jeremiah Stitham, Trent D Evans, Xiangyu Zhang, Astrid Rodriguez-Velez, Yu-Sheng Yeh, Joan Tao, Koki Takabatake, Slava Epelman, Irfan J Lodhi, Joel D Schilling, Brian J DeBosch, Abhinav Diwan, Babak Razani",
            "전체 인용횟수": "51회 인용20212022202352224",
            "페이지": "3740-3752",
            "학술 문서": "Trehalose causes low-grade lysosomal stress to activate TFEB and the autophagy-lysosome biogenesis responseSJ Jeong, J Stitham, TD Evans, X Zhang… - Autophagy, 202151회 인용 관련 학술자료 전체 6개의 버전 ",
            "호": "11"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Trehalose causes low-grade lysosomal stress to activate TFEB and the autophagy-lysosome biogenesis response",
        "year": null
    },
    "TFEB drives PGC-1α expression in adipocytes to protect against diet-induced metabolic dysfunction": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/11/5",
            "게시자": "American Association for the Advancement of Science",
            "권": "12",
            "설명": "TFEB is a basic helix-loop-helix transcription factor that confers protection against metabolic diseases such as atherosclerosis by targeting a network of genes involved in autophagy-lysosomal biogenesis and lipid catabolism. In this study, we sought to characterize the role of TFEB in adipocyte and adipose tissue physiology and evaluate the therapeutic potential of adipocyte-specific TFEB overexpression in obesity. We demonstrated that mice with adipocyte-specific TFEB overexpression (Adipo-TFEB) were protected from diet-induced obesity, insulin resistance, and metabolic sequelae. Adipo-TFEB mice were lean primarily through increased metabolic rate, suggesting a role for adipose tissue browning and enhanced nonshivering thermogenesis in fat. Transcriptional characterization revealed that TFEB targeted genes involved in adipose tissue browning rather than those involved in autophagy. One such gene …",
            "저널": "Science signaling",
            "저자": "Trent D Evans, Xiangyu Zhang, Se-Jin Jeong, Anyuan He, Eric Song, Somashubhra Bhattacharya, Karyn B Holloway, Irfan J Lodhi, Babak Razani",
            "전체 인용횟수": "49회 인용2020202120222023817915",
            "페이지": "eaau2281",
            "학술 문서": "TFEB drives PGC-1α expression in adipocytes to protect against diet-induced metabolic dysfunctionTD Evans, X Zhang, SJ Jeong, A He, E Song… - Science signaling, 201949회 인용 관련 학술자료 전체 8개의 버전 ",
            "호": "606"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "TFEB drives PGC-1α expression in adipocytes to protect against diet-induced metabolic dysfunction",
        "year": null
    },
    "Assessment of copper nanoclusters for accurate in vivo tumor imaging and potential for translation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/5/10",
            "게시자": "American Chemical Society",
            "권": "11",
            "설명": "Nanoparticles have been widely used for preclinical cancer imaging. However, their successful clinical translation is largely hampered by potential toxicity, unsatisfactory detection of malignancy at early stages, inaccurate diagnosis of tumor biomarkers, and histology for imaging-guided treatment. Herein, a targeted copper nanocluster (CuNC) is reported with high potential to address these challenges for future translation. Its ultrasmall structure enables efficient renal/bowel clearance, minimized off-target effects in nontargeted organs, and low nonspecific tumor retention. The pH-dependent in vivo dissolution of CuNCs affords minimal toxicity and potentially selective drug delivery to tumors. The intrinsic radiolabeling through the direct addition of 64Cu to CuNC (64Cu-CuNCs-FC131) synthesis offers high specific activity for sensitive and accurate detection of CXCR4 via FC131-directed targeting in novel triple …",
            "저널": "ACS applied materials & interfaces",
            "저자": "Gyu Seong Heo, Yongfeng Zhao, Deborah Sultan, Xiaohui Zhang, Lisa Detering, Hannah P Luehmann, Xiangyu Zhang, Richen Li, Ankur Choksi, Savannah Sharp, Sidney Levingston, Tina Primeau, David E Reichert, Guorong Sun, Babak Razani, Shunqiang Li, Katherine N Weilbaecher, Farrokh Dehdashti, Karen L Wooley, Yongjian Liu",
            "전체 인용횟수": "43회 인용201920202021202220233131574",
            "페이지": "19669-19678",
            "학술 문서": "Assessment of copper nanoclusters for accurate in vivo tumor imaging and potential for translationGS Heo, Y Zhao, D Sultan, X Zhang, L Detering… - ACS applied materials & interfaces, 201943회 인용 관련 학술자료 전체 8개의 버전 ",
            "호": "22"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Assessment of copper nanoclusters for accurate in vivo tumor imaging and potential for translation",
        "year": null
    },
    "Suppression of HSP70 expression sensitizes NSCLC cell lines to TRAIL-induced apoptosis by upregulating DR4 and DR5 and downregulating c-FLIP-L expressions": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/2",
            "게시자": "Springer-Verlag",
            "권": "91",
            "설명": " Many cancer cell types are resistant to tumor necrosis factor-related apoptosis-inducing ligand (TRAIL)-induced apoptosis. Here, we examined whether HSP70 suppression by small interfering RNA (siRNA) sensitized non-small cell lung cancer (NSCLC) cells to TRAIL-induced apoptosis and the underlying mechanisms. We demonstrated that HSP70 suppression by siRNA sensitized NSCLC cells to TRAIL-induced apoptosis by upregulating the expressions of death receptor 4 (DR4) and death receptor 5 (DR5) through activating NF-κB, JNK, and, subsequently, p53, consequently significantly amplifying TRAIL-mediated caspase-8 processing and activity, cytosolic translocation of cytochrome c, and cell death. Consistently, the pro-apoptotic proteins Bad and Bax were upregulated, while the anti-apoptotic protein Bcl-2 was downregulated. The luciferase activity of the DR4 promoter was blocked by a NF-κB …",
            "저널": "Journal of molecular medicine",
            "저자": "Hongqin Zhuang, Weiwei Jiang, Xiangyu Zhang, Fan Qiu, Ziyi Gan, Wei Cheng, Jing Zhang, Shengwen Guan, Bo Tang, Qilai Huang, Xinhua Wu, Xiaofeng Huang, Wenhui Jiang, Qingang Hu, Min Lu, Zi-Chun Hua",
            "전체 인용횟수": "42회 인용201320142015201620172018201920202021202220232778237311",
            "페이지": "219-235",
            "학술 문서": "Suppression of HSP70 expression sensitizes NSCLC cell lines to TRAIL-induced apoptosis by upregulating DR4 and DR5 and downregulating c-FLIP-L expressionsH Zhuang, W Jiang, X Zhang, F Qiu, Z Gan, W Cheng… - Journal of molecular medicine, 201342회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Suppression of HSP70 expression sensitizes NSCLC cell lines to TRAIL-induced apoptosis by upregulating DR4 and DR5 and downregulating c-FLIP-L expressions",
        "year": null
    },
    "Role of H-1 and H-2 subunits of soybean seed ferritin in oxidative deposition of iron in protein": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/10/15",
            "게시자": "Elsevier",
            "권": "285",
            "설명": "Naturally occurring phytoferritin is a heteropolymer consisting of two different H-type subunits, H-1 and H-2. Prior to this study, however, the function of the two subunits in oxidative deposition of iron in ferritin was unknown. The data show that, upon aerobic addition of 48–200 Fe2+/shell to apoferritin, iron oxidation occurs only at the diiron ferroxidase center of recombinant H1 (rH-1). In addition to the diiron ferroxidase mechanism, such oxidation is catalyzed by the extension peptide (a specific domain found in phytoferritin) of rH-2, because the H-1 subunit is able to remove Fe3+ from the center to the inner cavity better than the H-2 subunit. These findings support the idea that the H-1 and H-2 subunits play different roles in iron mineralization in protein. Interestingly, at medium iron loading (200 irons/shell), wild-type (WT) soybean seed ferritin (SSF) exhibits a stronger activity in catalyzing iron oxidation (1.10 ± 0.13 …",
            "저널": "Journal of Biological Chemistry",
            "저자": "Jianjun Deng, Xiayun Liao, Haixia Yang, Xiangyu Zhang, Zichun Hua, Taro Masuda, Fumiyuki Goto, Toshihiro Yoshihara, Guanghua Zhao",
            "전체 인용횟수": "38회 인용20112012201320142015201620172018201920202021202220231436431222334",
            "페이지": "32075-32086",
            "학술 문서": "Role of H-1 and H-2 subunits of soybean seed ferritin in oxidative deposition of iron in proteinJ Deng, X Liao, H Yang, X Zhang, Z Hua, T Masuda… - Journal of Biological Chemistry, 201038회 인용 관련 학술자료 전체 10개의 버전 ",
            "호": "42"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Role of H-1 and H-2 subunits of soybean seed ferritin in oxidative deposition of iron in protein",
        "year": null
    },
    "Proteomic screening of anaerobically regulated promoters from Salmonella and its antitumor applications": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/6/1",
            "게시자": "Elsevier",
            "권": "10",
            "설명": "Solid tumors often contain hypoxic and necrotic areas that can be targeted by attenuated Salmonella typhimurium VNP20009 (VNP). We sought to develop a hypoxia- inducible promoter system based on the tumor-specific delivered strain VNP to confine expression of therapeutic gene specifically or selectively within the tumor microenvironment. A hypoxia-inducible promoter - adhE promoter was screened from the hypoxia-regulated endogenous proteins of Salmonella through two-dimensional gel electrophoresis and matrix-assisted laser desorption ionization-time-of-flight/time-of-flight MS-based proteomics approaches. The efficiency and specificity of the selected adhE promoter were validated first in both bacteria and animal tumor models. The adhE promoter could specifically drive GFP gene expression under hypoxia, but not under normoxia. Furthermore, luciferase reporter expression controlled by the system …",
            "저널": "Molecular & Cellular Proteomics",
            "저자": "Jianxiang Chen, Dongping Wei, Hongqin Zhuang, Yiting Qiao, Bo Tang, Xiangyu Zhang, Jing Wei, Shentong Fang, Guo Chen, Pan Du, Xiaofeng Huang, Wenhui Jiang, Qingang Hu, Zi-Chun Hua",
            "전체 인용횟수": "30회 인용20122013201420152016201720182019202020212022135310121112",
            "학술 문서": "Proteomic screening of anaerobically regulated promoters from Salmonella and its antitumor applicationsJ Chen, D Wei, H Zhuang, Y Qiao, B Tang, X Zhang… - Molecular & Cellular Proteomics, 201130회 인용 관련 학술자료 전체 8개의 버전 ",
            "호": "6"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Proteomic screening of anaerobically regulated promoters from Salmonella and its antitumor applications",
        "year": null
    },
    "Systemic administration of attenuated Salmonella typhimurium in combination with interleukin-21 for cancer therapy": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/5/1",
            "게시자": "Spandidos Publications",
            "권": "1",
            "설명": "Attenuated Salmonella typhimurium (S. typhimurium) strain VNP20009 has been employed as a powerful anticancer agent due to its selective accumulation in tumors for targeted therapy. S. typhimurium has been demonstrated to constitute a delivery tool carrying antiangiogenic or proapoptotic genes that treat cancer. The hydrodynamic tail vein (HTV) injection of naked plasmid DNA has been developed as an effective gene delivery strategy, which has been successfully used in vivo. The aim of this study was to develop a combination therapy of S. typhimurium VNP20009 and HTV injection of interleukin-21 (IL-21) expression plasmid to evaluate the antitumor potential on an experimental melanoma model. Consistent with previous results, single VNP20009 treatment was demonstrated to possess effective activities to suppress tumor growth and prolong animal survival. Moreover, HTV injection of IL-21 plasmid …",
            "저널": "Molecular and clinical oncology",
            "저자": "Yuxuan Wang, Jianxiang Chen, BO Tang, Xiangyu Zhang, Zi-Chun Hua",
            "전체 인용횟수": "23회 인용2014201520162017201820192020202120222023232124333",
            "페이지": "461-465",
            "학술 문서": "Systemic administration of attenuated Salmonella typhimurium in combination with interleukin-21 for cancer therapyY Wang, J Chen, BO Tang, X Zhang, ZC Hua - Molecular and clinical oncology, 201323회 인용 관련 학술자료 전체 10개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Systemic administration of attenuated Salmonella typhimurium in combination with interleukin-21 for cancer therapy",
        "year": null
    },
    "FADD is a key regulator of lipid metabolism": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/8",
            "권": "8",
            "설명": " FADD, a classical apoptotic signaling adaptor, was recently reported to have non‐apoptotic functions. Here, we report the discovery that FADD regulates lipid metabolism. PPAR‐α is a dietary lipid sensor, whose activation results in hypolipidemic effects. We show that FADD interacts with RIP140, which is a corepressor for PPAR‐α, and FADD phosphorylation‐mimic mutation (FADD‐D) or FADD deficiency abolishes RIP140‐mediated transcriptional repression, leading to the activation of PPAR‐α. FADD‐D‐mutant mice exhibit significantly decreased adipose tissue mass and triglyceride accumulation. Also, they exhibit increased energy expenditure with enhanced fatty acid oxidation in adipocytes due to the activation of PPAR‐α. Similar metabolic phenotypes, such as reduced fat formation, insulin resistance, and resistance to HFD‐induced obesity, are shown in adipose‐specific FADD knockout mice. Additionally …",
            "저널": "EMBO molecular medicine",
            "저자": "Hongqin Zhuang, Xueshi Wang, Daolong Zha, Ziyi Gan, Fangfang Cai, Pan Du, Yunwen Yang, Bingya Yang, Xiangyu Zhang, Chun Yao, Yuqiang Zhou, Chizhou Jiang, Shengwen Guan, Xuerui Zhang, Jing Zhang, Wenhui Jiang, Qingang Hu, Zi‐Chun Hua",
            "전체 인용횟수": "17회 인용20172018201920202021202220231114235",
            "페이지": "895-918",
            "학술 문서": "FADD is a key regulator of lipid metabolismH Zhuang, X Wang, D Zha, Z Gan, F Cai, P Du, Y Yang… - EMBO molecular medicine, 201617회 인용 관련 학술자료 전체 7개의 버전 ",
            "호": "8"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "FADD is a key regulator of lipid metabolism",
        "year": null
    },
    "Data clustering: a review": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1999/9/1",
            "게시자": "Acm",
            "권": "31",
            "설명": "Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify  cross-cutting themes and recent advances. We also describe some important …",
            "저자": "Anil K Jain, M Narasimha Murty, Patrick J Flynn",
            "전체 인용횟수": "19909회 인용2001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202311919435644969278889810041088110110541137119611601180115411221031987862831698538",
            "출처": "ACM computing surveys (CSUR)",
            "페이지": "264-323",
            "학술 문서": "Data clustering: a reviewAK Jain, MN Murty, PJ Flynn - ACM computing surveys (CSUR), 199919808회 인용 관련 학술자료 전체 77개의 버전 Flynn: Data Clustering: A Review*M Jain, MN Murty - ACM Computing Surveys, 1999157회 인용 관련 학술자료 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Data clustering: a review",
        "year": null
    },
    "Algorithms for Clustering Data": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1988",
            "설명": "A cluster is a group of similar objects; objects from different clusters are not alike. Clustering is an important tool in exploratory data analysis and is used in several disciplines, such as artificial intelligence, pattern recognition, geology, biology, psychology, and information retrieval. A clustering algorithm generates clusters from the definitions of objects, and cluster analysis is the formal analysis of these algorithms. This excellent book emphasizes informal algorithms for clustering data and interpreting results. The authors, whose names should be familiar to researchers working in cluster analysis, masterfully introduce mathematical and statistical theory only when necessary. The book consists of five chapters. Chapter 1 introduces the general concepts and the literature. Chapter 2 presents the authors' view of data and introduces the representation of objects (pattern matrix), the idea of a proximity matrix, different ways …",
            "저자": "Anil K Jain, Richard C Dubes",
            "전체 인용횟수": "16834회 인용19911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202371100111140146201188284304303397490535586688700760746763744725749784791769735701634634568455438348",
            "학술 문서": "Algorithms for clustering dataAK Jain, RC Dubes - 198816834회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Algorithms for Clustering Data",
        "year": null
    },
    "Data clustering: 50 years beyond K-means": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/6/1",
            "게시자": "North-Holland",
            "권": "31",
            "설명": "Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms into a system of ranked taxa: domain, kingdom, phylum, class, etc. Cluster analysis is the formal study of methods and algorithms for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). The aim of clustering is to find structure in data and is therefore exploratory in nature. Clustering has a long and rich history in a variety of scientific fields. One of the most popular and simple clustering algorithms, K-means, was first published in …",
            "저널": "Pattern recognition letters",
            "저자": "Anil K Jain",
            "전체 인용횟수": "10916회 인용201020112012201320142015201620172018201920202021202220236618728349268976790810241198127911731118967645",
            "페이지": "651-666",
            "학술 문서": "Data clustering: 50 years beyond K-meansAK Jain - Pattern recognition letters, 201010914회 인용 관련 학술자료 전체 42개의 버전 Data Clustering: 50 Years Beyond K-Means. ECML/PKDD (1)*AK Jain - Lecture Notes in Computer Science, Springer20082회 인용 관련 학술자료 ",
            "호": "8"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Data clustering: 50 years beyond K-means",
        "year": null
    },
    "Statistical pattern recognition: a review": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2000/1/1",
            "권": "1",
            "설명": "The primary goal of pattern recognition is supervised or unsupervised classification. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical approach has been most intensively studied and used in practice. More recently, neural network techniques and methods imported from statistical learning theory have been receiving increasing attention. The design of a recognition system requires careful attention to the following issues: definition of pattern classes, sensing environment, pattern representation, feature extraction and selection, cluster analysis, classifier design and learning, selection of training and test samples, and performance evaluation. In spite of almost 50 years of research and development in this field, the general problem of recognizing complex patterns with arbitrary orientation, location, and scale remains unsolved. New and emerging applications …",
            "저널": "IEEE Transaction on Pattern Analysis and Machine Intelligence, PAMI-22",
            "저자": "Anil K Jain, Robert PW Duin, Jianchang Mao",
            "전체 인용횟수": "9260회 인용20002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202347129213240395394437447487493524537574501544519456421404369304284242180",
            "페이지": "4-37",
            "학술 문서": "Statistical pattern recognition: A reviewAK Jain, RPW Duin, J Mao - IEEE Transactions on pattern analysis and machine …, 20009112회 인용 관련 학술자료 전체 12개의 버전 J. Mao Statistical Pattern Recognition: A Review*AK Jain, RPW Duin - IEEE Transactions on pattern analysis and machine …, 2000221회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Statistical pattern recognition: a review",
        "year": null
    },
    "Introduction to biometric recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011",
            "게시자": "Springer US",
            "도서": "Introduction to Biometrics",
            "저자": "Anil K Jain, Arun A Ross, Karthik Nandakumar",
            "전체 인용횟수": "6953회 인용2004200520062007200820092010201120122013201420152016201720182019202020212022202346132143198218301366427426468467514539442503452359359304191",
            "페이지": "1-49",
            "학술 문서": "An introduction to biometric recognition*AK Jain, A Ross, S Prabhakar - IEEE Transactions on circuits and systems for video …, 20046953회 인용 관련 학술자료 전체 26개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Introduction to biometric recognition",
        "year": null
    },
    "Handbook Of Fingerprint Recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009",
            "권": "1",
            "설명": "Biometric recognition, or simply biometrics, refers to the use of distinctive anatomical and/or behavioral characteristics or identifiers (eg, fingerprints, face, iris, voice, and hand geometry) for automatically recognizing a person. Questions such as “Is this person authorized to enter the facility?”,“Is this individual entitled to access the privileged information?”, and “Did this person previously apply for a passport?” are routinely asked in a variety of organizations in both public and private sectors. Traditional person recognition systems that are based on ID documents and password/PIN no longer suffice to verify a person’s identity. Because biometric identifiers cannot be easily misplaced, forged, or shared, they are considered more reliable for person recognition than traditional token-(eg, keys or ID cards) or knowledge-(eg, password or PIN) based methods. Biometric recognition provides better security, higher efficiency …",
            "저자": "Davide Maltoni, Dario Maio, Anil K Jain, Salil Probhakar",
            "전체 인용횟수": "6804회 인용20042005200620072008200920102011201220132014201520162017201820192020202120222023111180227304319328414435481425487497415399420361244265222134",
            "학술 문서": "Handbook of fingerprint recognitionD Maltoni, D Maio, AK Jain, S Prabhakar - 20096781회 인용 관련 학술자료 전체 16개의 버전 Handbook of fingerprint recognition*S Prabhakar, D Maltoni, D Maio, AK Jain - New York, 200322회 인용 관련 학술자료 K., S. Prabhakar (2003). Handbook of Fingerprint RecognitionD Maltoni, D Maio, A Jain6회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Handbook Of Fingerprint Recognition",
        "year": null
    },
    "Handbook of fingerprint recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/4/21",
            "게시자": "springer",
            "권": "2",
            "설명": "Biometric recognition, or simply biometrics, refers to the use of distinctive anatomical and/or behavioral characteristics or identifiers (eg, fingerprints, face, iris, voice, and hand geometry) for automatically recognizing a person. Questions such as “Is this person authorized to enter the facility?”,“Is this individual entitled to access the privileged information?”, and “Did this person previously apply for a passport?” are routinely asked in a variety of organizations in both public and private sectors. Traditional person recognition systems that are based on ID documents and password/PIN no longer suffice to verify a person’s identity. Because biometric identifiers cannot be easily misplaced, forged, or shared, they are considered more reliable for person recognition than traditional token-(eg, keys or ID cards) or knowledge-(eg, password or PIN) based methods. Biometric recognition provides better security, higher efficiency …",
            "저자": "Davide Maltoni, Dario Maio, Anil K Jain, Salil Prabhakar",
            "전체 인용횟수": "6781회 인용20042005200620072008200920102011201220132014201520162017201820192020202120222023111180226304314327412435480425485496413399418359244264220134",
            "학술 문서": "Handbook of fingerprint recognitionD Maltoni, D Maio, AK Jain, S Prabhakar - 20096781회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Handbook of fingerprint recognition",
        "year": null
    },
    "Artificial neural networks: A tutorial": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1996/3",
            "게시자": "IEEE",
            "권": "29",
            "설명": "Artificial neural nets (ANNs) are massively parallel systems with large numbers of interconnected simple processors. The article discusses the motivations behind the development of ANNs and describes the basic biological neuron and the artificial computational model. It outlines network architectures and learning processes, and presents some of the most commonly used ANN models. It concludes with character recognition, a successful ANN application.",
            "저자": "Anil K Jain, Jianchang Mao, K Moidin Mohiuddin",
            "전체 인용횟수": "4761회 인용199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023363431434158455375638184110114120153182224172227263294409405435462483",
            "출처": "Computer",
            "페이지": "31-44",
            "학술 문서": "Artificial neural networks: A tutorialAK Jain, J Mao, KM Mohiuddin - Computer, 19964761회 인용 관련 학술자료 전체 16개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Artificial neural networks: A tutorial",
        "year": null
    },
    "Handbook of multibiometrics": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006/8/11",
            "게시자": "Springer Science & Business Media",
            "권": "6",
            "설명": "Reliable human authentication schemes are of paramount importance in our highly networked society. Advances in biometrics help address the myriad of problems associated with traditional human recognition methods. The performance and benefits of a biometric system can be significantly enhanced by consolidating the evidence presented by multiple biometric sources. Multibiometric systems are expected to meet the stringent performance requirements imposed by large-scale authentication systems. Handbook of Multibiometrics, a professional book, introduces multibiometric systems, and demonstrates the noteworthy advantages of these systems over their unimodal counterparts. In addition, this book describes in detail the various scenarios possible when fusing biometric evidence from multiple information sources. This comprehensive volume on multibiometric systems, concisely and clearly outlines the different fusion methodologies that have been proposed by researchers to integrate multiple biometric traits.",
            "저자": "Arun A Ross, Karthik Nandakumar, Anil Jain",
            "전체 인용횟수": "4310회 인용2006200720082009201020112012201320142015201620172018201920202021202220231650128205282281328349358354313299270266201212192131",
            "학술 문서": "Handbook of biometrics*AK Jain, P Flynn, AA Ross - 20072580회 인용 관련 학술자료 전체 11개의 버전 Handbook of multibiometricsAA Ross, K Nandakumar, AK Jain - 20061981회 인용 관련 학술자료 전체 8개의 버전 Handbook of biometrics*AA Ross, P Flynn - 200829회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Handbook of multibiometrics",
        "year": null
    },
    "Biometrics: personal identification in networked society": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1999/1/31",
            "게시자": "Springer Science & Business Media",
            "권": "479",
            "설명": "Biometrics: Personal Identification in Networked Society is a comprehensive and accessible source of state-of-the-art information on all existing and emerging biometrics: the science of automatically identifying individuals based on their physiological or behavior characteristics. In particular, the book covers:* General principles and ideas of designing biometric-based systems and their underlying tradeoffs* Identification of important issues in the evaluation of biometrics-based systems* Integration of biometric cues, and the integration of biometrics with other existing technologies* Assessment of the capabilities and limitations of different biometrics* The comprehensive examination of biometric methods in commercial use and in research development* Exploration of some of the numerous privacy and security implications of biometrics. Also included are chapters on face and eye identification, speaker recognition, networking, and other timely technology-related issues. All chapters are written by leading internationally recognized experts from academia and industry. Biometrics: Personal Identification in Networked Society is an invaluable work for scientists, engineers, application developers, systems integrators, and others working in biometrics.",
            "저자": "Anil Jain, Ruud Bolle, Sharath Pankanti",
            "전체 인용횟수": "3769회 인용199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202320367810014219420616618717819617320222523823719116815416314685897645",
            "학술 문서": "Biometrics: personal identification in networked societyA Jain, R Bolle, S Pankanti - 19993769회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Biometrics: personal identification in networked society",
        "year": null
    },
    "Unsupervised texture segmentation using Gabor filters": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1991/1/1",
            "게시자": "Pergamon",
            "권": "24",
            "설명": "This paper presents a texture segmentation algorithm inspired by the multi-channel filtering theory for visual information processing in the early stages of human visual system. The channels are characterized by a bank of Gabor filters that nearly uniformly covers the spatial-frequency domain, and a systematic filter selection scheme is proposed, which is based on reconstruction of the input image from the filtered images. Texture features are obtained by subjecting each (selected) filtered image to a nonlinear transformation and computing a measure of “energy” in a window around each pixel. A square-error clustering algorithm is then used to integrate the feature images and produce a segmentation. A simple procedure to incorporate spatial information in the clustering process is proposed. A relative index is used to estimate the “true” number of texture categories.",
            "저널": "Pattern recognition",
            "저자": "Anil K Jain, Farshid Farrokhnia",
            "전체 인용횟수": "3524회 인용19921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023212448447368848294100134121124151160159148160150141153158152144117124130110114846351",
            "페이지": "1167-1186",
            "학술 문서": "Unsupervised texture segmentation using Gabor filtersAK Jain, F Farrokhnia - Pattern recognition, 19913524회 인용 관련 학술자료 전체 13개의 버전 ",
            "호": "12"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Unsupervised texture segmentation using Gabor filters",
        "year": null
    },
    "Fingerprint image enhancement: algorithm and performance evaluation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1998/8",
            "게시자": "IEEE",
            "권": "20",
            "설명": "In order to ensure that the performance of an automatic fingerprint identification/verification system will be robust with respect to the quality of input fingerprint images, it is essential to incorporate a fingerprint enhancement algorithm in the minutiae extraction module. We present a fast fingerprint enhancement algorithm, which can adaptively improve the clarity of ridge and valley structures of input fingerprint images based on the estimated local ridge orientation and frequency. We have evaluated the performance of the image enhancement algorithm using the goodness index of the extracted minutiae and the accuracy of an online fingerprint verification system. Experimental results show that incorporating the enhancement algorithm improves both the goodness index and the verification accuracy.",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Lin Hong, Yifei Wan, Anil Jain",
            "전체 인용횟수": "3523회 인용1999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023102034667011615218620419823320622025917923418715815014813485868255",
            "페이지": "777-789",
            "학술 문서": "Fingerprint image enhancement: algorithm and performance evaluationL Hong, Y Wan, A Jain - IEEE transactions on pattern analysis and machine …, 19983523회 인용 관련 학술자료 전체 11개의 버전 ",
            "호": "8"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Fingerprint image enhancement: algorithm and performance evaluation",
        "year": null
    },
    "Face detection in color images": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2002/5",
            "게시자": "IEEE",
            "권": "24",
            "설명": "Human face detection plays an important role in applications such as video surveillance, human computer interface, face recognition, and face image database management. We propose a face detection algorithm for color images in the presence of varying lighting conditions as well as complex backgrounds. Based on a novel lighting compensation technique and a nonlinear color transformation, our method detects skin regions over the entire image and then generates face candidates based on the spatial arrangement of these skin patches. The algorithm constructs eye, mouth, and boundary maps for verifying each face candidate. Experimental results demonstrate successful face detection over a wide range of facial variations in color, position, scale, orientation, 3D pose, and expression in images from several photo collections (both indoors and outdoors).",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Rein-Lien Hsu, Mohamed Abdel-Mottaleb, Anil K Jain",
            "전체 인용횟수": "3371회 인용2001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202312391091591871962132382402552382312011971671321031106581655029",
            "페이지": "696-706",
            "학술 문서": "Face detection in color imagesRL Hsu, M Abdel-Mottaleb, AK Jain - IEEE transactions on pattern analysis and machine …, 20023371회 인용 관련 학술자료 전체 17개의 버전 ",
            "호": "5"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Face detection in color images",
        "year": null
    },
    "Handbook of face recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011",
            "게시자": "springer",
            "설명": "Face recognition is one of the most important abilities that we use in our daily lives. There are several reasons for the growing interest in automated face recognition, including rising concerns for public security, the need for identity verification for physical and logical access, and the need for face analysis and modeling techniques in multimedia data management and digital entertainment. Research in automatic face recognition started in the 1960s. Recent years have seen significant progress in this area and a number of face recognition and modeling systems have been developed and deployed. However, accurate and robust face recognition still offers a number of challenges to computer vision and pattern recognition researchers, especially under unconstrained environments.This book is written with two primary motivations. The first is to compile major approaches, algorithms, and technologies available for …",
            "저자": "Anil K Jain, Stan Z Li",
            "전체 인용횟수": "3181회 인용200420052006200720082009201020112012201320142015201620172018201920202021202220238356311814016917417122622721323221918422517614616213999",
            "학술 문서": "Handbook of face recognitionAK Jain, SZ Li - 20113128회 인용 관련 학술자료 전체 16개의 버전 Psychological and neural perspectives on human face recognition*SZ Li, AK Jain, AJ O'Toole - Handbook of face recognition, 200557회 인용 관련 학술자료 전체 10개의 버전 Handbook of face recognition. 2011*AK Jain, SZ Li4회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Handbook of face recognition",
        "year": null
    },
    "Feature selection: Evaluation, application, and small sample performance": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1997/2",
            "게시자": "IEEE",
            "권": "19",
            "설명": "A large number of algorithms have been proposed for feature subset selection. Our experimental results show that the sequential forward floating selection algorithm, proposed by Pudil et al. (1994), dominates the other algorithms tested. We study the problem of choosing an optimal feature set for land use classification based on SAR satellite images using four different texture models. Pooling features derived from different texture models, followed by a feature selection results in a substantial improvement in the classification accuracy. We also illustrate the dangers of using feature selection in small sample size situations.",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Anil Jain, Douglas Zongker",
            "전체 인용횟수": "2999회 인용19981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023243358558687122135144148182145136135137170142153130137131125107959053",
            "페이지": "153-158",
            "학술 문서": "Feature selection: Evaluation, application, and small sample performanceA Jain, D Zongker - IEEE transactions on pattern analysis and machine …, 19972999회 인용 관련 학술자료 전체 17개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Feature selection: Evaluation, application, and small sample performance",
        "year": null
    },
    "Unsupervised learning of finite mixture models": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2002/3",
            "게시자": "Ieee",
            "권": "24",
            "설명": "This paper proposes an unsupervised algorithm for learning a finite mixture model from multivariate data. The adjective \"unsupervised\" is justified by two properties of the algorithm: 1) it is capable of selecting the number of components and 2) unlike the standard expectation-maximization (EM) algorithm, it does not require careful initialization. The proposed method also avoids another drawback of EM for mixture fitting: the possibility of convergence toward a singular estimate at the boundary of the parameter space. The novelty of our approach is that we do not use a model selection criterion to choose one among a set of preestimated candidate models; instead, we seamlessly integrate estimation and model selection in a single algorithm. Our technique can be applied to any type of parametric mixture model for which it is possible to write an EM algorithm; in this paper, we illustrate it with experiments involving …",
            "저널": "IEEE Transactions on pattern analysis and machine intelligence",
            "저자": "Mario A. T.  Figueiredo, Anil K.  Jain",
            "전체 인용횟수": "2907회 인용2002200320042005200620072008200920102011201220132014201520162017201820192020202120222023185578126129150149166159178184164194174163124147139110958579",
            "페이지": "381-396",
            "학술 문서": "Unsupervised learning of finite mixture modelsMAT Figueiredo, AK Jain - IEEE Transactions on pattern analysis and machine …, 20022907회 인용 관련 학술자료 전체 9개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Unsupervised learning of finite mixture models",
        "year": null
    },
    "Score normalization in multimodal biometric systems": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/12/1",
            "게시자": "Pergamon",
            "권": "38",
            "설명": "Multimodal biometric systems consolidate the evidence presented by multiple biometric sources and typically provide better recognition performance compared to systems based on a single biometric modality. Although information fusion in a multimodal system can be performed at various levels, integration at the matching score level is the most common approach due to the ease in accessing and combining the scores generated by different matchers. Since the matching scores output by the various modalities are heterogeneous, score normalization is needed to transform these scores into a common domain, prior to combining them. In this paper, we have studied the performance of different normalization techniques and fusion rules in the context of a multimodal biometric system based on the face, fingerprint and hand-geometry traits of a user. Experiments conducted on a database of 100 users indicate that the …",
            "저널": "Pattern recognition",
            "저자": "Anil Jain, Karthik Nandakumar, Arun Ross",
            "전체 인용횟수": "2631회 인용200520062007200820092010201120122013201420152016201720182019202020212022202329506811995131141157174175184147147169176165179177112",
            "페이지": "2270-2285",
            "학술 문서": "Score normalization in multimodal biometric systemsA Jain, K Nandakumar, A Ross - Pattern recognition, 20052631회 인용 관련 학술자료 전체 17개의 버전 ",
            "호": "12"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Score normalization in multimodal biometric systems",
        "year": null
    },
    "Fully convolutional networks for semantic segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build\" fully convolutional\" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.",
            "저자": "Jonathan Long, Evan Shelhamer, Trevor Darrell",
            "전체 인용횟수": "44713회 인용20152016201720182019202020212022202323410182450430061246768751379686646",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "3431-3440",
            "학술 문서": "Fully convolutional networks for semantic segmentationJ Long, E Shelhamer, T Darrell - Proceedings of the IEEE conference on computer …, 201544713회 인용 관련 학술자료 전체 52개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Fully convolutional networks for semantic segmentation",
        "year": null
    },
    "Long-term recurrent convolutional networks for visual recognition and description": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "Models comprised of deep convolutional network layers have dominated recent image interpretation tasks; we investigate whether models which are also compositional, or\" deep\", temporally are effective on tasks involving visual sequences or label sequences. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image to sentence generation problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are\" doubly deep\" in that they can be compositional in spatial and temporal\" layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable length inputs (ie video frames) to variable length outputs (ie natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to state-of-the-art visual convnet models and can jointly trained, updating temporal dynamics and convolutional perceptual representations simultaneously. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized.",
            "저자": "Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell",
            "전체 인용횟수": "7288회 인용2015201620172018201920202021202220231314187801053112310351057874661",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "2625-2634",
            "학술 문서": "Long-term recurrent convolutional networks for visual recognition and descriptionJ Donahue, L Anne Hendricks, S Guadarrama… - Proceedings of the IEEE conference on computer …, 20157288회 인용 관련 학술자료 전체 31개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Long-term recurrent convolutional networks for visual recognition and description",
        "year": null
    },
    "Pfinder: Real-time tracking of the human body": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1997/7",
            "게시자": "IEEE",
            "권": "19",
            "설명": "Pfinder is a real-time system for tracking people and interpreting their behavior. It runs at 10 Hz on a standard SGI Indy computer, and has performed reliably on thousands of people in many different physical locations. The system uses a multiclass statistical model of color and shape to obtain a 2D representation of head and hands in a wide range of viewing conditions. Pfinder has been successfully used in a wide range of applications including wireless interfaces, video databases, and low-bandwidth coding.",
            "저널": "IEEE Transactions on pattern analysis and machine intelligence",
            "저자": "Christopher Richard  Wren, Ali Azarbayejani, Trevor Darrell, Alex Paul Pentland",
            "전체 인용횟수": "7038회 인용19961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220232659111136187188238268313396419402420423425356392357359324283244165155116877337",
            "페이지": "780-785",
            "학술 문서": "Pfinder: Real-time tracking of the human bodyCR Wren, A Azarbayejani, T Darrell, AP Pentland - IEEE Transactions on pattern analysis and machine …, 19976735회 인용 관련 학술자료 전체 19개의 버전 Pfinder: real-time tracking of the human bodyC Wren, A Azarbayejani, T Darrell, A Pentland - Proceedings of the second international conference on …, 1996343회 인용 관련 학술자료 전체 16개의 버전 Pfinder: Real-time tracking of tge human body*C Wren, A Azarbayejani, T Darrell, A Pentland - FG'96, 19959회 인용 관련 학술자료 ",
            "호": "7"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pfinder: Real-time tracking of the human body",
        "year": null
    },
    "Decaf: A deep convolutional activation feature for generic visual recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/1/27",
            "게시자": "PMLR",
            "설명": "We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.",
            "저자": "Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, Trevor Darrell",
            "전체 인용횟수": "5779회 인용2014201520162017201820192020202120222023165421611721787784735592487354",
            "컨퍼런스": "International conference on machine learning",
            "페이지": "647-655",
            "학술 문서": "Decaf: A deep convolutional activation feature for generic visual recognitionJ Donahue, Y Jia, O Vinyals, J Hoffman, N Zhang… - International conference on machine learning, 20145779회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Decaf: A deep convolutional activation feature for generic visual recognition",
        "year": null
    },
    "Context encoders: Feature learning by inpainting": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders--a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part (s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.",
            "저자": "Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, Alexei A Efros",
            "전체 인용횟수": "5759회 인용201620172018201920202021202220232721052772989011111218975",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "2536-2544",
            "학술 문서": "Context encoders: Feature learning by inpaintingD Pathak, P Krahenbuhl, J Donahue, T Darrell… - Proceedings of the IEEE conference on computer …, 20165759회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Context encoders: Feature learning by inpainting",
        "year": null
    },
    "Adversarial discriminative domain adaptation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "Adversarial learning methods are a promising approach to training robust deep networks, and can generate complex samples across diverse domains. They can also improve recognition despite the presence of domain shift or dataset bias: recent adversarial approaches to unsupervised domain adaptation reduce the difference between the training and test domain distributions and thus improve generalization performance. However, while generative adversarial networks (GANs) show compelling visualizations, they are not optimal on discriminative tasks and can be limited to smaller shifts. On the other hand, discriminative approaches can handle larger domain shifts, but impose tied weights on the model and do not exploit a GAN-based loss. In this work, we first outline a novel generalized framework for adversarial adaptation, which subsumes recent state-of-the-art approaches as special cases, and use this generalized view to better relate prior approaches. We then propose a previously unexplored instance of our general framework which combines discriminative modeling, untied weight sharing, and a GAN loss, which we call Adversarial Discriminative Domain Adaptation (ADDA). We show that ADDA is more effective yet considerably simpler than competing domain-adversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adaptation results on standard domain adaptation tasks as well as a difficult cross-modality object classification task.",
            "저자": "Eric Tzeng, Judy Hoffman, Kate Saenko, Trevor Darrell",
            "전체 인용횟수": "4796회 인용20172018201920202021202220234925961987210371003928",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "7167-7176",
            "학술 문서": "Adversarial discriminative domain adaptationE Tzeng, J Hoffman, K Saenko, T Darrell - Proceedings of the IEEE conference on computer …, 20174796회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Adversarial discriminative domain adaptation",
        "year": null
    },
    "End-to-end training of deep visuomotor policies": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/1/1",
            "게시자": "JMLR. org",
            "권": "17",
            "설명": "Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-toend provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot’s motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.",
            "저널": "The Journal of Machine Learning Research",
            "저자": "Sergey Levine, Chelsea Finn, Trevor Darrell, Pieter Abbeel",
            "전체 인용횟수": "3622회 인용20152016201720182019202020212022202334145324468576571548507416",
            "페이지": "1334-1373",
            "학술 문서": "End-to-end training of deep visuomotor policiesS Levine, C Finn, T Darrell, P Abbeel - The Journal of Machine Learning Research, 20163622회 인용 관련 학술자료 전체 19개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "End-to-end training of deep visuomotor policies",
        "year": null
    },
    "Adapting visual category models to new domains": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010",
            "게시자": "Springer Berlin Heidelberg",
            "설명": " Domain adaptation is an important emerging topic in computer vision. In this paper, we present one of the first studies of domain shift in the context of object recognition. We introduce a method that adapts object models acquired in a particular visual domain to new imaging conditions by learning a transformation that minimizes the effect of domain-induced changes in the feature distribution. The transformation is learned in a supervised manner and can be applied to categories for which there are no labeled examples in the new domain. While we focus our evaluation on object recognition tasks, the transform-based adaptation technique we develop is general and could be applied to non-image data. Another contribution is a new multi-domain object database, freely available for download. We experimentally demonstrate the ability of our method to improve recognition on categories with few or no target …",
            "저자": "Kate Saenko, Brian Kulis, Mario Fritz, Trevor Darrell",
            "전체 인용횟수": "3030회 인용2010201120122013201420152016201720182019202020212022202310263377111131183158216287393455451461",
            "컨퍼런스": "Computer Vision–ECCV 2010: 11th European Conference on Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part IV 11",
            "페이지": "213-226",
            "학술 문서": "Adapting visual category models to new domainsK Saenko, B Kulis, M Fritz, T Darrell - Computer Vision–ECCV 2010: 11th European …, 20103030회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Adapting visual category models to new domains",
        "year": null
    },
    "Cycada: Cycle-consistent adversarial domain adaptation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/7",
            "설명": "Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models have shown tremendous progress towards adapting to new environments by focusing either on discovering domain invariant representations or by mapping between unpaired image domains. While feature space methods are difficult to interpret and sometimes fail to capture pixel-level and low-level domain shifts, image space methods sometimes fail to incorporate high level semantic knowledge relevant for the end task. We propose a model which adapts between domains using both generative image space alignment and latent representation space alignment. Our approach, Cycle-Consistent Adversarial Domain Adaptation (CyCADA), guides transfer between domains according to a specific discriminatively trained task and avoids divergence by enforcing consistency of the relevant semantics before and after adaptation. We evaluate our method on a variety of visual recognition and prediction settings, including digit classification and semantic segmentation of road scenes, advancing state-of-the-art performance for unsupervised adaptation from synthetic to real world driving domains.",
            "저자": "Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A Efros, Trevor Darrell",
            "전체 인용횟수": "2974회 인용20182019202020212022202390358545703664598",
            "컨퍼런스": "Proceedings of the 35th International Conference on Machine Learning (ICML)",
            "학술 문서": "Cycada: Cycle-consistent adversarial domain adaptationJ Hoffman, E Tzeng, T Park, JY Zhu, P Isola, K Saenko… - International conference on machine learning, 20182974회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Cycada: Cycle-consistent adversarial domain adaptation",
        "year": null
    },
    "Region-based convolutional networks for accurate object detection and segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/5/25",
            "게시자": "IEEE",
            "권": "38",
            "설명": "Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition. The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50 percent relative to the previous best result on VOC 2012-achieving a mAP of 62.4 percent. Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly. Since we combine region proposals with CNNs, we call the resulting model …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik",
            "전체 인용횟수": "2905회 인용2015201620172018201920202021202220231499248367396414496453371",
            "페이지": "142-158",
            "학술 문서": "Region-based convolutional networks for accurate object detection and segmentationR Girshick, J Donahue, T Darrell, J Malik - IEEE transactions on pattern analysis and machine …, 20152905회 인용 관련 학술자료 전체 9개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Region-based convolutional networks for accurate object detection and segmentation",
        "year": null
    },
    "Deep domain confusion: Maximizing for domain invariance": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/12/10",
            "설명": "Recent reports suggest that a generic supervised deep CNN model trained on a large-scale dataset reduces, but does not remove, dataset bias on a standard benchmark. Fine-tuning deep models in a new domain can require a significant amount of data, which for many applications is simply not available. We propose a new CNN architecture which introduces an adaptation layer and an additional domain confusion loss, to learn a representation that is both semantically meaningful and domain invariant. We additionally show that a domain confusion metric can be used for model selection to determine the dimension of an adaptation layer and the best position for the layer in the CNN architecture. Our proposed adaptation method offers empirical performance which exceeds previously published results on a standard benchmark visual domain adaptation task.",
            "저널": "arXiv preprint arXiv:1412.3474",
            "저자": "Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, Trevor Darrell",
            "전체 인용횟수": "2738회 인용201520162017201820192020202120222023113897161295453567578517",
            "학술 문서": "Deep domain confusion: Maximizing for domain invarianceE Tzeng, J Hoffman, N Zhang, K Saenko, T Darrell - arXiv preprint arXiv:1412.3474, 20142738회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep domain confusion: Maximizing for domain invariance",
        "year": null
    },
    "A convnet for the 2020s": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2022",
            "설명": "The\" Roaring 20s\" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (eg, Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually\" modernize\" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.",
            "저자": "Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie",
            "전체 인용횟수": "2638회 인용20212022202376801922",
            "컨퍼런스": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "페이지": "11976-11986",
            "학술 문서": "A convnet for the 2020sZ Liu, H Mao, CY Wu, C Feichtenhofer, T Darrell, S Xie - Proceedings of the IEEE/CVF conference on computer …, 20222638회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A convnet for the 2020s",
        "year": null
    },
    "Curiosity-driven exploration by self-supervised prediction": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/8",
            "권": "70",
            "설명": "In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent’s ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (eg new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.",
            "저자": "Deepak Pathak, Pulkit Agrawal, Alexei A Efros, Trevor Darrell",
            "전체 인용횟수": "2384회 인용201720182019202020212022202328166314417488502455",
            "컨퍼런스": "Proceedings of the 34th International Conference on Machine Learning (ICML)",
            "페이지": "2778-2787",
            "학술 문서": "Curiosity-driven exploration by self-supervised predictionD Pathak, P Agrawal, AA Efros, T Darrell - International conference on machine learning, 20172376회 인용 관련 학술자료 전체 14개의 버전 Curiosity-driven exploration by self-supervised prediction (2017)*D Pathak, P Agrawal, AA Efros, T Darrell - arXiv preprint arXiv:1705.05363, 201713회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Curiosity-driven exploration by self-supervised prediction",
        "year": null
    },
    "Adversarial feature learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/5/31",
            "설명": "The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.",
            "저널": "arXiv preprint arXiv:1605.09782",
            "저자": "Jeff Donahue, Philipp Krähenbühl, Trevor Darrell",
            "전체 인용횟수": "2192회 인용2016201720182019202020212022202324122257295382403392305",
            "학술 문서": "Adversarial feature learningJ Donahue, P Krähenbühl, T Darrell - arXiv preprint arXiv:1605.09782, 20162192회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Adversarial feature learning",
        "year": null
    },
    "The pyramid match kernel: Discriminative classification with sets of image features": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/10/17",
            "게시자": "IEEE",
            "권": "2",
            "설명": "Discriminative learning is challenging when examples are sets of features, and the sets vary in cardinality and lack any sort of meaningful ordering. Kernel-based classification methods can learn complex decision boundaries, but a kernel over unordered set inputs must somehow solve for correspondences epsivnerally a computationally expensive task that becomes impractical for large set sizes. We present a new fast kernel function which maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in this space. This \"pyramid match\" computation is linear in the number of features, and it implicitly finds correspondences based on the finest resolution histogram cell where a matched pair first appears. Since the kernel does not penalize the presence of extra features, it is robust to clutter. We show the kernel function is positive-definite, making it valid for use in learning …",
            "저자": "Kristen Grauman, Trevor Darrell",
            "전체 인용횟수": "2139회 인용2005200620072008200920102011201220132014201520162017201820192020202120222023105776132149194176170156178149160104836881666239",
            "컨퍼런스": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1",
            "페이지": "1458-1465",
            "학술 문서": "The pyramid match kernel: Discriminative classification with sets of image featuresK Grauman, T Darrell - Tenth IEEE International Conference on Computer …, 20052139회 인용 관련 학술자료 전체 31개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The pyramid match kernel: Discriminative classification with sets of image features",
        "year": null
    },
    "Sequence to sequence-video to text": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "Real-world videos often have complex dynamics; methods for generating open-domain video descriptions should be senstive to temporal structure and allow both input (sequence of frames) and output (sequence of words) of variable length. To approach this problem we propose a novel end-to-end sequence-to-sequence model to generate captions for videos. For this we exploit recurrent neural networks, specifically LSTMs, which have demonstrated state-of-the-art performance in image caption generation. Our LSTM model is trained on video-sentence pairs and learns to associate a sequence of video frames to a sequence of words in order to generate a description of the event in the video clip. Our model naturally is able to learn the temporal structure of the sequence of frames as well as the sequence model of the generated sentences, ie a language model. We evaluate several variants of our model that exploit different visual features on a standard set of YouTube videos and two movie description datasets (M-VAD and MPII-MD).",
            "저자": "Subhashini Venugopalan, Marcus Rohrbach, Jeffrey Donahue, Raymond Mooney, Trevor Darrell, Kate Saenko",
            "전체 인용횟수": "1642회 인용2015201620172018201920202021202220231684175225259254240204146",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "4534-4542",
            "학술 문서": "Sequence to sequence-video to textS Venugopalan, M Rohrbach, J Donahue, R Mooney… - Proceedings of the IEEE international conference on …, 20151642회 인용 관련 학술자료 전체 21개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Sequence to sequence-video to text",
        "year": null
    },
    "Multimodal compact bilinear pooling for visual question answering and visual grounding": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/6/6",
            "설명": "Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge.",
            "저널": "arXiv preprint arXiv:1606.01847",
            "저자": "Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, Marcus Rohrbach",
            "전체 인용횟수": "1634회 인용2016201720182019202020212022202325129217241270256270199",
            "학술 문서": "Multimodal compact bilinear pooling for visual question answering and visual groundingA Fukui, DH Park, D Yang, A Rohrbach, T Darrell… - arXiv preprint arXiv:1606.01847, 20161634회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Multimodal compact bilinear pooling for visual question answering and visual grounding",
        "year": null
    },
    "Toward multimodal image-to-image translation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "권": "30",
            "설명": "Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a distribution of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.",
            "저널": "Advances in neural information processing systems",
            "저자": "Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, Eli Shechtman",
            "전체 인용횟수": "1601회 인용20182019202020212022202390258317369298258",
            "학술 문서": "Toward multimodal image-to-image translationJY Zhu, R Zhang, D Pathak, T Darrell, AA Efros… - Advances in neural information processing systems, 20171601회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Toward multimodal image-to-image translation",
        "year": null
    },
    "Imagenet: A large-scale hierarchical image database": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/6/20",
            "게시자": "Ieee",
            "설명": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets …",
            "저자": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei",
            "전체 인용횟수": "59533회 인용201120122013201420152016201720182019202020212022202319926535351286013642309391058788115109281226611906",
            "컨퍼런스": "2009 IEEE conference on computer vision and pattern recognition",
            "페이지": "248-255",
            "학술 문서": "Imagenet: A large-scale hierarchical image databaseJ Deng, W Dong, R Socher, LJ Li, K Li, L Fei-Fei - 2009 IEEE conference on computer vision and pattern …, 200959533회 인용 관련 학술자료 전체 33개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Imagenet: A large-scale hierarchical image database",
        "year": null
    },
    "Imagenet large scale visual recognition challenge": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/12",
            "게시자": "Springer US",
            "권": "115",
            "설명": " The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.",
            "저널": "International journal of computer vision",
            "저자": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, Li Fei-Fei",
            "전체 인용횟수": "41089회 인용20152016201720182019202020212022202352114492737436154816107695870095906",
            "페이지": "211-252",
            "학술 문서": "Imagenet large scale visual recognition challengeO Russakovsky, J Deng, H Su, J Krause, S Satheesh… - International journal of computer vision, 201541089회 인용 관련 학술자료 전체 21개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Imagenet large scale visual recognition challenge",
        "year": null
    },
    "Perceptual losses for real-time style transfer and super-resolution": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "게시자": "Springer International Publishing",
            "설명": " We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment …",
            "저자": "Justin Johnson, Alexandre Alahi, Li Fei-Fei",
            "전체 인용횟수": "10296회 인용201620172018201920202021202220234429783113701748207820141819",
            "컨퍼런스": "Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14",
            "페이지": "694-711",
            "학술 문서": "Perceptual losses for real-time style transfer and super-resolutionJ Johnson, A Alahi, L Fei-Fei - Computer Vision–ECCV 2016: 14th European …, 201610296회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Perceptual losses for real-time style transfer and super-resolution",
        "year": null
    },
    "Large-scale video classification with convolutional neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "설명": "Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3% to 63.9%), but only a surprisingly modest improvement compared to single-frame models (59.3% to 60.9%). We further study the generalization performance of our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3% up from 43.9%).",
            "저자": "Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, Li Fei-Fei",
            "전체 인용횟수": "8031회 인용2014201520162017201820192020202120222023382505739351125121611671021902649",
            "컨퍼런스": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition",
            "페이지": "1725-1732",
            "학술 문서": "Large-scale video classification with convolutional neural networksA Karpathy, G Toderici, S Shetty, T Leung… - Proceedings of the IEEE conference on Computer …, 20148031회 인용 관련 학술자료 전체 44개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Large-scale video classification with convolutional neural networks",
        "year": null
    },
    "Deep visual-semantic alignments for generating image descriptions": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.",
            "저자": "Andrej Karpathy, Li Fei-Fei",
            "전체 인용횟수": "6372회 인용201520162017201820192020202120222023195474666884889806809823683",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "3128-3137",
            "학술 문서": "Deep visual-semantic alignments for generating image descriptionsA Karpathy, L Fei-Fei - Proceedings of the IEEE conference on computer …, 20156372회 인용 관련 학술자료 전체 40개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep visual-semantic alignments for generating image descriptions",
        "year": null
    },
    "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004/6/27",
            "게시자": "IEEE",
            "설명": "Current computational approaches to learning visual object categories require thousands of training images, are slow, cannot learn in an incremental manner and cannot incorporate prior information into the learning process. In addition, no algorithm presented in the literature has been tested on more than a handful of object categories. We present an method for learning object categories from just a few training images. It is quick and it uses prior information in a principled way. We test it on a dataset composed of images of objects belonging to 101 widely varied categories. Our proposed method is based on making use of prior information, assembled from (unrelated) object categories which were previously learnt. A generative probabilistic model is used, which represents the shape and appearance of a constellation of features belonging to the object. The parameters of the model are learnt incrementally in a …",
            "저자": "Li Fei-Fei, Rob Fergus, Pietro Perona",
            "전체 인용횟수": "5101회 인용2004200520062007200820092010201120122013201420152016201720182019202020212022202314437096135165234243282336348362365310303307301314360440",
            "컨퍼런스": "2004 conference on computer vision and pattern recognition workshop",
            "페이지": "178-178",
            "학술 문서": "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categoriesL Fei-Fei, R Fergus, P Perona - 2004 conference on computer vision and pattern …, 20045101회 인용 관련 학술자료 전체 26개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories",
        "year": null
    },
    "A bayesian hierarchical model for learning natural scene categories": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/6/20",
            "게시자": "IEEE",
            "권": "2",
            "설명": "We propose a novel approach to learn and recognize natural scene categories. Unlike previous work, it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a \"theme\". In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes.",
            "저자": "Li Fei-Fei, Pietro Perona",
            "전체 인용횟수": "4997회 인용20052006200720082009201020112012201320142015201620172018201920202021202220232260109195278383378397439440449408294270229190151117108",
            "컨퍼런스": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)",
            "페이지": "524-531",
            "학술 문서": "A bayesian hierarchical model for learning natural scene categoriesL Fei-Fei, P Perona - 2005 IEEE Computer Society Conference on Computer …, 20054995회 인용 관련 학술자료 전체 35개의 버전 A bayesian hierarchical model for learning natural scene categories; 2005*L Fei-Fei, P Perona - Proc. of IEEE Computer Vision and Pattern Recognition4회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A bayesian hierarchical model for learning natural scene categories",
        "year": null
    },
    "Visual genome: Connecting language and vision using crowdsourced dense image annotations": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/5",
            "게시자": "Springer US",
            "권": "123",
            "설명": " Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked “What vehicle is the person riding?”, computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) to answer correctly that “the person is riding a horse-drawn carriage.” In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense …",
            "저널": "International journal of computer vision",
            "저자": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael S Bernstein, Li Fei-Fei",
            "전체 인용횟수": "4675회 인용201620172018201920202021202220235716227848066686610571076",
            "페이지": "32-73",
            "학술 문서": "Visual genome: Connecting language and vision using crowdsourced dense image annotationsR Krishna, Y Zhu, O Groth, J Johnson, K Hata, J Kravitz… - International journal of computer vision, 20174675회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
        "year": null
    },
    "3d object representations for fine-grained categorization": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013",
            "설명": "While 3D object representations are being revived in the context of multi-view object class detection and scene understanding, they have not yet attained wide-spread use in fine-grained categorization. State-of-the-art approaches achieve remarkable performance when training data is plentiful, but they are typically tied to flat, 2D representations that model objects as a collection of unconnected views, limiting their ability to generalize across viewpoints. In this paper, we therefore lift two state-of-the-art 2D object representations to 3D, on the level of both local feature appearance and location. In extensive experiments on existing and newly proposed datasets, we show our 3D object representations outperform their state-of-the-art 2D counterparts for fine-grained categorization and demonstrate their efficacy for estimating 3D geometry from images via ultrawide baseline matching and 3D reconstruction.",
            "저자": "Jonathan Krause, Michael Stark, Jia Deng, Li Fei-Fei",
            "전체 인용횟수": "3156회 인용201420152016201720182019202020212022202392055123191291391557689797",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision workshops",
            "페이지": "554-561",
            "학술 문서": "3d object representations for fine-grained categorizationJ Krause, M Stark, J Deng, L Fei-Fei - Proceedings of the IEEE international conference on …, 20133156회 인용 관련 학술자료 전체 20개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "3d object representations for fine-grained categorization",
        "year": null
    },
    "Social lstm: Human trajectory prediction in crowded spaces": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "Humans navigate complex crowded environments based on social conventions: they respect personal space, yielding right-of-way and avoid collisions. In our work, we propose a data-driven approach to learn these human-human interactions for predicting their future trajectories. This is in contrast to traditional approaches which use hand-crafted functions such as Social forces. We present a new Long Short-Term Memory (LSTM) model which jointly reasons across multiple individuals in a scene. Different from the conventional LSTM, we share the information between multiple LSTMs through a new pooling layer. This layer pools the hidden representation from LSTMs corresponding to neighboring trajectories to capture interactions within this neighborhood. We demonstrate the performance of our method on several public datasets. Our model outperforms previous forecasting methods by more than 42%. We also analyze the trajectories predicted by our model to demonstrate social behaviours such as collision avoidance and group movement, learned by our model.",
            "저자": "Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li Fei-Fei, Silvio Savarese",
            "전체 인용횟수": "2977회 인용201620172018201920202021202220231591195337510609652542",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "961-971",
            "학술 문서": "Social lstm: Human trajectory prediction in crowded spacesA Alahi, K Goel, V Ramanathan, A Robicquet, L Fei-Fei… - Proceedings of the IEEE conference on computer …, 20162977회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Social lstm: Human trajectory prediction in crowded spaces",
        "year": null
    },
    "One-shot learning of object categories": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006/2/21",
            "게시자": "IEEE",
            "권": "28",
            "설명": "Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models. Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Li Fei-Fei, Robert Fergus, Pietro Perona",
            "전체 인용횟수": "2837회 인용200620072008200920102011201220132014201520162017201820192020202120222023133560629796100959187103150208273344367351265",
            "페이지": "594-611",
            "학술 문서": "One-shot learning of object categoriesL Fei-Fei, R Fergus, P Perona - IEEE transactions on pattern analysis and machine …, 20062799회 인용 관련 학술자료 전체 13개의 버전 One-shot learning of object categories. Pattern Analysis and Machine IntelligenceL Fei-Fei, R Fergus, P Perona - IEEE Transactions on, 200667회 인용 관련 학술자료 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "One-shot learning of object categories",
        "year": null
    },
    "Unsupervised learning of human action categories using spatial-temporal words": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008/9/1",
            "게시자": "Springer Netherlands",
            "권": "79",
            "설명": " We present a novel unsupervised learning method for human action categories. A video sequence is represented as a collection of spatial-temporal words by extracting space-time interest points. The algorithm automatically learns the probability distributions of the spatial-temporal words and the intermediate topics corresponding to human action categories. This is achieved by using latent topic models such as the probabilistic Latent Semantic Analysis (pLSA) model and Latent Dirichlet Allocation (LDA). Our approach can handle noisy feature points arisen from dynamic background and moving cameras due to the application of the probabilistic models. Given a novel video sequence, the algorithm can categorize and localize the human action(s) contained in the video. We test our algorithm on three challenging datasets: the KTH human motion dataset, the Weizmann human action dataset, and a recent …",
            "저널": "International Journal of Computer Vision",
            "저자": "Juan Carlos Niebles, Hongcheng Wang, Li Fei-Fei",
            "전체 인용횟수": "2246회 인용2007200820092010201120122013201420152016201720182019202020212022202330961452232242072402151761851361056959523017",
            "페이지": "299-318",
            "학술 문서": "Unsupervised learning of human action categories using spatial-temporal wordsJC Niebles, H Wang, L Fei-Fei - International journal of computer vision, 20082246회 인용 관련 학술자료 전체 36개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Unsupervised learning of human action categories using spatial-temporal words",
        "year": null
    },
    "Progressive neural architecture search": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "We propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms. Our approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space. Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of Zoph et al.(2018) in terms of number of models evaluated, and 8 times faster in terms of total compute. The structures we discover in this way achieve state of the art classification accuracies on CIFAR-10 and ImageNet.",
            "저자": "Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, Kevin Murphy",
            "전체 인용횟수": "2130회 인용20182019202020212022202375319440508440334",
            "컨퍼런스": "Proceedings of the European conference on computer vision (ECCV)",
            "페이지": "19-34",
            "학술 문서": "Progressive neural architecture searchC Liu, B Zoph, M Neumann, J Shlens, W Hua, LJ Li… - Proceedings of the European conference on computer …, 20182130회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Progressive neural architecture search",
        "year": null
    },
    "On the opportunities and risks of foundation models": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021/8/16",
            "설명": "AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.",
            "저널": "arXiv preprint arXiv:2108.07258",
            "저자": "Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W Thomas, Florian Tramèr, Rose E Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, Percy Liang",
            "전체 인용횟수": "1858회 인용202120222023695491230",
            "학술 문서": "On the opportunities and risks of foundation modelsR Bommasani, DA Hudson, E Adeli, R Altman, S Arora… - arXiv preprint arXiv:2108.07258, 20211858회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "On the opportunities and risks of foundation models",
        "year": null
    },
    "Social gan: Socially acceptable trajectories with generative adversarial networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "Understanding human motion behavior is critical for autonomous moving platforms (like self-driving cars and social robots) if they are to navigate human-centric environments. This is challenging because human motion is inherently multimodal: given a history of human motion paths, there are many socially plausible ways that people could move in the future. We tackle this problem by combining tools from sequence prediction and generative adversarial networks: a recurrent sequence-to-sequence model observes motion histories and predicts future behavior, using a novel pooling mechanism to aggregate information across people. We predict socially plausible futures by training adversarially against a recurrent discriminator, and encourage diverse predictions with a novel variety loss. Through experiments on several datasets we demonstrate that our approach outperforms prior work in terms of accuracy, variety, collision avoidance, and computational complexity.",
            "저자": "Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, Alexandre Alahi",
            "전체 인용횟수": "1777회 인용20182019202020212022202319146307421459420",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "2255-2264",
            "학술 문서": "Social gan: Socially acceptable trajectories with generative adversarial networksA Gupta, J Johnson, L Fei-Fei, S Savarese, A Alahi - Proceedings of the IEEE conference on computer …, 20181777회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Social gan: Socially acceptable trajectories with generative adversarial networks",
        "year": null
    },
    "Target-driven visual navigation in indoor scenes using deep reinforcement learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "Two less addressed issues of deep reinforcement learning are (1) lack of generalization capability to new goals, and (2) data inefficiency, i.e., the model requires several (and often costly) episodes of trial and error to converge, which makes it impractical to be applied to real-world scenarios. In this paper, we address these two issues and apply our model to target-driven visual navigation. To address the first issue, we propose an actor-critic model whose policy is a function of the goal as well as the current state, which allows better generalization. To address the second issue, we propose the AI2-THOR framework, which provides an environment with high-quality 3D scenes and a physics engine. Our framework enables agents to take actions and interact with objects. Hence, we can collect a huge number of training samples efficiently. We show that our proposed method (1) converges faster than the state-of-the-art …",
            "저널": "ICRA",
            "저자": "Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, Ali Farhadi",
            "전체 인용횟수": "1657회 인용201720182019202020212022202376185280293306275226",
            "학술 문서": "Target-driven visual navigation in indoor scenes using deep reinforcement learningY Zhu, R Mottaghi, E Kolve, JJ Lim, A Gupta, L Fei-Fei… - 2017 IEEE international conference on robotics and …, 20171657회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning",
        "year": null
    },
    "Novel dataset for fine-grained image categorization: Stanford dogs": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/6/25",
            "게시자": "Citeseer",
            "권": "2",
            "설명": "We introduce a 120 class Stanford Dogs dataset, a challenging and large-scale dataset aimed at fine-grained image categorization. Stanford Dogs includes over 22,000 annotated images of dogs belonging to 120 species. Each image is annotated with a bounding box and object class label. Fig. 1 shows examples of images from Stanford Dogs. This dataset is extremely challenging due to a variety of reasons. First, being a fine-grained categorization problem, there is little inter-class variation. For example the basset hound and bloodhound share very similar facial characteristics but differ significantly in their color, while the Japanese spaniel and papillion share very similar color but greatly differ in their facial characteristics. Second, there is very large intra-class variation. The images show that dogs within a class could have different ages (eg beagle), poses (eg blenheim spaniel), occlusion/self-occlusion and even color (eg Shih-tzu). Furthermore, compared to other animal datasets that tend to exist in natural scenes, a large proportion of the images contain humans and are taken in manmade environments leading to greater background variation. The aforementioned reasons make this an extremely challenging dataset.",
            "저널": "Proc. CVPR workshop on fine-grained visual categorization (FGVC)",
            "저자": "Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, Fei-Fei Li",
            "전체 인용횟수": "1444회 인용20122013201420152016201720182019202020212022202351725325578106150189256254260",
            "학술 문서": "Novel dataset for fine-grained image categorization: Stanford dogsA Khosla, N Jayadevaprakash, B Yao, FF Li - Proc. CVPR workshop on fine-grained visual …, 20111428회 인용 관련 학술자료 전체 4개의 버전 Novel dataset for fgvc: Stanford dogs*A Khosla, N Jayadevaprakash, B Yao, FF Li - San Diego: CVPR Workshop on FGVC, 201116회 인용 관련 학술자료 Novel dataset for fine-grained image categorization: Stanford dogsJN KhoslaA, BP Yao - 2022-01-04]. https://people. csail. mit. edu/khosla …, 20112회 인용 관련 학술자료 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Novel dataset for fine-grained image categorization: Stanford dogs",
        "year": null
    },
    "Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/7/3",
            "게시자": "PMLR",
            "설명": "Recent deep networks are capable of memorizing the entire data even when the labels are completely random. To overcome the overfitting on corrupted labels, we propose a novel technique of learning another neural network, called MentorNet, to supervise the training of the base deep networks, namely, StudentNet. During training, MentorNet provides a curriculum (sample weighting scheme) for StudentNet to focus on the sample the label of which is probably correct. Unlike the existing curriculum that is usually predefined by human experts, MentorNet learns a data-driven curriculum dynamically with StudentNet. Experimental results demonstrate that our approach can significantly improve the generalization performance of deep networks trained on corrupted training data. Notably, to the best of our knowledge, we achieve the best-published result on WebVision, a large benchmark containing 2.2 million images of real-world noisy labels.",
            "저자": "Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, Li Fei-Fei",
            "전체 인용횟수": "1425회 인용20182019202020212022202325135244335334345",
            "컨퍼런스": "International conference on machine learning",
            "페이지": "2304-2313",
            "학술 문서": "Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labelsL Jiang, Z Zhou, T Leung, LJ Li, L Fei-Fei - International conference on machine learning, 20181425회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels",
        "year": null
    },
    "Visualizing and understanding recurrent networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/6/5",
            "설명": "Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.",
            "저널": "arXiv preprint arXiv:1506.02078",
            "저자": "Andrej Karpathy, Justin Johnson, Li Fei-Fei",
            "전체 인용횟수": "1351회 인용2015201620172018201920202021202220231011215022921421716514390",
            "학술 문서": "Visualizing and understanding recurrent networksA Karpathy, J Johnson, L Fei-Fei - arXiv preprint arXiv:1506.02078, 20151351회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Visualizing and understanding recurrent networks",
        "year": null
    },
    "Normalized cuts and image segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2000/8",
            "게시자": "Ieee",
            "권": "22",
            "설명": "We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We applied this approach to segmenting static images, as well as motion sequences, and found the results to be very encouraging.",
            "저널": "IEEE Transactions on pattern analysis and machine intelligence",
            "저자": "Jianbo Shi, Jitendra Malik",
            "전체 인용횟수": "19836회 인용20012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023598617926439048659075691510241104129812791293132713701236119611351006985909737",
            "페이지": "888-905",
            "학술 문서": "Normalized cuts and image segmentationJ Shi, J Malik - IEEE Transactions on pattern analysis and machine …, 200019836회 인용 관련 학술자료 전체 34개의 버전 ",
            "호": "8"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Normalized cuts and image segmentation",
        "year": null
    },
    "Scale-space and edge detection using anisotropic diffusion": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1990/7",
            "게시자": "IEEE",
            "권": "12",
            "설명": "A new definition of scale-space is suggested, and a class of algorithms used to realize a diffusion process is introduced. The diffusion coefficient is chosen to vary spatially in such a way as to encourage intraregion smoothing rather than interregion smoothing. It is shown that the 'no new maxima should be generated at coarse scales' property of conventional scale space is preserved. As the region boundaries in the approach remain sharp, a high-quality edge detector which successfully exploits global information is obtained. Experimental results are shown on a number of images. Parallel hardware implementations are made feasible because the algorithm involves elementary, local operations replicated over the image.< >",
            "저널": "IEEE Transactions on pattern analysis and machine intelligence",
            "저자": "Pietro Perona, Jitendra Malik",
            "전체 인용횟수": "17183회 인용1992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202357781121061451801881842072552963514085687237497668428939059801026951880893777706682566547513406",
            "페이지": "629-639",
            "학술 문서": "Scale-space and edge detection using anisotropic diffusionP Perona, J Malik - IEEE Transactions on pattern analysis and machine …, 199017183회 인용 관련 학술자료 전체 29개의 버전 ",
            "호": "7"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Scale-space and edge detection using anisotropic diffusion",
        "year": null
    },
    "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2001/7/7",
            "게시자": "IEEE",
            "권": "2",
            "설명": "This paper presents a database containing 'ground truth' segmentations produced by humans for images of a wide variety of natural scenes. We define an error measure which quantifies the consistency between segmentations of differing granularities and find that different human segmentations of the same image are highly consistent. Use of this dataset is demonstrated in two applications: (1) evaluating the performance of segmentation algorithms and (2) measuring probability distributions associated with Gestalt grouping factors as well as statistics of image region properties.",
            "저자": "David Martin, Charless Fowlkes, Doron Tal, Jitendra Malik",
            "전체 인용횟수": "8710회 인용200220032004200520062007200820092010201120122013201420152016201720182019202020212022202326394885126157195259276326368434416408480484578685758791860808",
            "컨퍼런스": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001",
            "페이지": "416-423",
            "학술 문서": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statisticsD Martin, C Fowlkes, D Tal, J Malik - Proceedings Eighth IEEE International Conference on …, 20018710회 인용 관련 학술자료 전체 22개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics",
        "year": null
    },
    "Shape matching and object recognition using shape contexts": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2002/4",
            "게시자": "IEEE",
            "권": "24",
            "설명": "We present a novel approach to measuring similarity between shapes and exploit it for object recognition. In our framework, the measurement of similarity is preceded by: (1) solving for correspondences between points on the two shapes; (2) using the correspondences to estimate an aligning transform. In order to solve the correspondence problem, we attach a descriptor, the shape context, to each point. The shape context at a reference point captures the distribution of the remaining points relative to it, thus offering a globally discriminative characterization. Corresponding points on two similar shapes will have similar shape contexts, enabling us to solve for correspondences as an optimal assignment problem. Given the point correspondences, we estimate the transformation that best aligns the two shapes; regularized thin-plate splines provide a flexible class of transformation maps for this purpose. The …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Serge Belongie, Jitendra Malik, Jan Puzicha",
            "전체 인용횟수": "8685회 인용20022003200420052006200720082009201020112012201320142015201620172018201920202021202220232682128238292364461472584602608729669593611508406322263236197155",
            "페이지": "509-522",
            "학술 문서": "Shape matching and object recognition using shape contextsS Belongie, J Malik, J Puzicha - IEEE transactions on pattern analysis and machine …, 20028685회 인용 관련 학술자료 전체 36개의 버전 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Shape matching and object recognition using shape contexts",
        "year": null
    },
    "Contour detection and hierarchical image segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/8/26",
            "게시자": "IEEE",
            "권": "33",
            "설명": "This paper investigates two fundamental problems in computer vision: contour detection and image segmentation. We present state-of-the-art algorithms for both of these tasks. Our contour detector combines multiple local cues into a globalization framework based on spectral clustering. Our segmentation algorithm consists of generic machinery for transforming the output of any contour detector into a hierarchical region tree. In this manner, we reduce the problem of image segmentation to that of contour detection. Extensive experimental evaluation demonstrates that both our contour detection and segmentation methods significantly outperform competing algorithms. The automatically generated hierarchical segmentations can be interactively refined by user-specified annotations. Computation at multiple image resolutions provides a means of coupling our system to recognition applications.",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Pablo Arbelaez, Michael Maire, Charless Fowlkes, Jitendra Malik",
            "전체 인용횟수": "5919회 인용201120122013201420152016201720182019202020212022202357169285393500561533558599587569509472",
            "페이지": "898-916",
            "학술 문서": "Contour detection and hierarchical image segmentationP Arbelaez, M Maire, C Fowlkes, J Malik - IEEE transactions on pattern analysis and machine …, 20105919회 인용 관련 학술자료 전체 18개의 버전 ",
            "호": "5"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Contour detection and hierarchical image segmentation",
        "year": null
    },
    "Recovering high dynamic range radiance maps from photographs": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2023/8/1",
            "도서": "Seminal Graphics Papers: Pushing the Boundaries, Volume 2",
            "설명": "We present a method of recovering high dynamic range radiance maps from photographs taken with conventional imaging equipment. In our method, multiple photographs of the scene are taken with different amounts of exposure. Our algorithm uses these differently exposed photographs to recover the response function of the imaging process, up to factor of scale, using the assumption of reciprocity. With the known response function, the algorithm can fuse the multiple photographs into a single, high dynamic range radiance map whose pixel values are proportional to the true radiance values in the scene. We demonstrate our method on images acquired with both photochemical and digital imaging processes. We discuss how this work is applicable in many areas of computer graphics involving digitized photographs, including image-based modeling, image compositing, and image processing. Lastly, we …",
            "저자": "Paul E Debevec, Jitendra Malik",
            "전체 인용횟수": "4427회 인용199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220231220386170106131162176228183218229234237229250244216214206178186205173149",
            "페이지": "643-652",
            "학술 문서": "Recovering high dynamic range radiance maps from photographsPE Debevec, J Malik - Seminal Graphics Papers: Pushing the Boundaries …, 20234427회 인용 관련 학술자료 전체 50개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Recovering high dynamic range radiance maps from photographs",
        "year": null
    },
    "Modeling and rendering architecture from photographs: A hybrid geometry-and image-based approach": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2023/8/1",
            "도서": "Seminal Graphics Papers: Pushing the Boundaries, Volume 2",
            "설명": "We present a new approach for modeling and rendering existing architectural scenes from a sparse set of still photographs. Our modeling approach, which combines both geometry-based and imagebased techniques, has two components. The first component is a photogrammetric modeling method which facilitates the recovery of the basic geometry of the photographed scene. Our photogrammetric modeling approach is effective, convenient, and robust because it exploits the constraints that are characteristic of architectural scenes. The second component is a model-based stereo algorithm, which recovers how the real scene deviates from the basic model. By making use of the model, our stereo technique robustly recovers accurate depth from widely-spaced image pairs. Consequently, our approach canmodel large architectural environmentswith far fewer photographs than current image-based modeling …",
            "저자": "Paul E Debevec, Camillo J Taylor, Jitendra Malik",
            "전체 인용횟수": "3256회 인용1997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220234999123146175194182198203172169131130105124118116941037379555050738081",
            "페이지": "465-474",
            "학술 문서": "Modeling and rendering architecture from photographs: A hybrid geometry-and image-based approachPE Debevec, CJ Taylor, J Malik - Seminal Graphics Papers: Pushing the Boundaries …, 20233256회 인용 관련 학술자료 전체 52개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Modeling and rendering architecture from photographs: A hybrid geometry-and image-based approach",
        "year": null
    },
    "Learning to detect natural image boundaries using local brightness, color, and texture cues": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004/3/15",
            "게시자": "IEEE",
            "권": "26",
            "설명": "The goal of this work is to accurately detect and localize boundaries in natural scenes using local image measurements. We formulate features that respond to characteristic changes in brightness, color, and texture associated with natural boundaries. In order to combine the information from these features in an optimal way, we train a classifier using human labeled images as ground truth. The output of this classifier provides the posterior probability of a boundary at each image location and orientation. We present precision-recall curves showing that the resulting detector significantly outperforms existing approaches. Our two main results are 1) that cue combination can be performed adequately with a simple linear model and 2) that a proper, explicit treatment of texture is required to detect boundaries in natural images.",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "David R Martin, Charless C Fowlkes, Jitendra Malik",
            "전체 인용횟수": "3097회 인용20042005200620072008200920102011201220132014201520162017201820192020202120222023235083102135168179173219204223207212208136165160149132121",
            "페이지": "530-549",
            "학술 문서": "Learning to detect natural image boundaries using local brightness, color, and texture cuesDR Martin, CC Fowlkes, J Malik - IEEE transactions on pattern analysis and machine …, 20043097회 인용 관련 학술자료 전체 18개의 버전 ",
            "호": "5"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning to detect natural image boundaries using local brightness, color, and texture cues",
        "year": null
    },
    "Slowfast networks for video recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "We present SlowFast networks for video recognition. Our model involves (i) a Slow pathway, operating at low frame rate, to capture spatial semantics, and (ii) a Fast pathway, operating at high frame rate, to capture motion at fine temporal resolution. The Fast pathway can be made very lightweight by reducing its channel capacity, yet can learn useful temporal information for video recognition. Our models achieve strong performance for both action classification and detection in video, and large improvements are pin-pointed as contributions by our SlowFast concept. We report state-of-the-art accuracy on major video recognition benchmarks, Kinetics, Charades and AVA. Code has been made available at: https://github. com/facebookresearch/SlowFast.",
            "저자": "Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, Kaiming He",
            "전체 인용횟수": "2755회 인용2019202020212022202366295592848940",
            "컨퍼런스": "Proceedings of the IEEE/CVF international conference on computer vision",
            "페이지": "6202-6211",
            "학술 문서": "Slowfast networks for video recognitionC Feichtenhofer, H Fan, J Malik, K He - Proceedings of the IEEE/CVF international conference …, 20192755회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Slowfast networks for video recognition",
        "year": null
    },
    "Learning a classification model for segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2003/10/13",
            "게시자": "IEEE",
            "설명": "We propose a two-class classification model for grouping. Human segmented natural images are used as positive examples. Negative examples of grouping are constructed by randomly matching human segmentations and images. In a preprocessing stage an image is over-segmented into super-pixels. We define a variety of features derived from the classical Gestalt cues, including contour, texture, brightness and good continuation. Information-theoretic analysis is applied to evaluate the power of these grouping cues. We train a linear classifier to combine these features. To demonstrate the power of the classification model, a simple algorithm is used to randomly search for good segmentations. Results are shown on a wide range of images.",
            "저널": "Proceedings ninth IEEE international conference on computer vision",
            "저자": "Ren, Malik",
            "전체 인용횟수": "2297회 인용200420052006200720082009201020112012201320142015201620172018201920202021202220231521234247668912411113814917420416814817917215414794",
            "페이지": "10-17 vol. 1",
            "학술 문서": "Learning a classification model for segmentationRen, Malik - Proceedings ninth IEEE international conference on …, 20032297회 인용 관련 학술자료 전체 31개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning a classification model for segmentation",
        "year": null
    },
    "Representing and recognizing the visual appearance of materials using three-dimensional textons": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2001/6",
            "게시자": "Kluwer Academic Publishers",
            "권": "43",
            "설명": " We study the recognition of surfaces made from different materials such as concrete, rug, marble, or leather on the basis of their textural appearance. Such natural textures arise from spatial variation of two surface attributes: (1) reflectance and (2) surface normal. In this paper, we provide a unified model to address both these aspects of natural texture. The main idea is to construct a vocabulary of prototype tiny surface patches with associated local geometric and photometric properties. We call these 3D textons. Examples might be ridges, grooves, spots or stripes or combinations thereof. Associated with each texton is an appearance vector, which characterizes the local irradiance distribution, represented as a set of linear Gaussian derivative filter outputs, under different lighting and viewing conditions. Given a large collection of images of different materials, a clustering approach is used to acquire a …",
            "저널": "International journal of computer vision",
            "저자": "Thomas Leung, Jitendra Malik",
            "전체 인용횟수": "2122회 인용2001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202361324334772849010913814412215317016917213211510570415329",
            "페이지": "29-44",
            "학술 문서": "Representing and recognizing the visual appearance of materials using three-dimensional textonsT Leung, J Malik - International journal of computer vision, 20012122회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Representing and recognizing the visual appearance of materials using three-dimensional textons",
        "year": null
    },
    "Blobworld: Image segmentation using expectation-maximization and its application to image querying": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2002/8",
            "게시자": "IEEE",
            "권": "24",
            "설명": "Retrieving images from large and varied collections using image content as a key is a challenging and important problem. We present a new image representation that provides a transformation from the raw pixel data to a small set of image regions that are coherent in color and texture. This \"Blobworld\" representation is created by clustering pixels in a joint color-texture-position feature space. The segmentation algorithm is fully automatic and has been run on a collection of 10,000 natural images. We describe a system that uses the Blobworld representation to retrieve images from this collection. An important aspect of the system is that the user is allowed to view the internal representation of the submitted image and the query results. Similar systems do not offer the user this view into the workings of the system; consequently, query results from these systems can be inexplicable, despite the availability of knobs for …",
            "저널": "IEEE Transactions on pattern analysis and machine intelligence",
            "저자": "Chad Carson, Serge Belongie, Hayit Greenspan, Jitendra Malik",
            "전체 인용횟수": "2124회 인용20012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023151286108157158172159168140122121137118776671603437341411",
            "페이지": "1026-1038",
            "학술 문서": "Blobworld: Image segmentation using expectation-maximization and its application to image queryingC Carson, S Belongie, H Greenspan, J Malik - IEEE Transactions on pattern analysis and machine …, 20022054회 인용 관련 학술자료 전체 13개의 버전 Blobworld: Color-and texture-based image segmentation using EM and its application to image querying and classification*C Carson, S Belongie, H Greenspan, J Malik - IEEE Transactions on Pattern Analysis and Machine …, 200278회 인용 관련 학술자료 ",
            "호": "8"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Blobworld: Image segmentation using expectation-maximization and its application to image querying",
        "year": null
    },
    "Multiscale vision transformers": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021",
            "설명": "We present Multiscale Vision Transformers (MViT) for video and image recognition, by connecting the seminal idea of multiscale feature hierarchies with transformer models. Multiscale Transformers have several channel-resolution scale stages. Starting from the input resolution and a small channel dimension, the stages hierarchically expand the channel capacity while reducing the spatial resolution. This creates a multiscale pyramid of features with early layers operating at high spatial resolution to model simple low-level visual information, and deeper layers at spatially coarse, but complex, high-dimensional features. We evaluate this fundamental architectural prior for modeling the dense nature of visual signals for a variety of video recognition tasks where it outperforms concurrent vision transformers that rely on large scale external pre-training and are 5-10 more costly in computation and parameters. We further remove the temporal dimension and apply our model for image classification where it outperforms prior work on vision transformers. Code is available at: https://github. com/facebookresearch/SlowFast.",
            "저자": "Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, Christoph Feichtenhofer",
            "전체 인용횟수": "1894회 인용2021202220231077401033",
            "컨퍼런스": "Proceedings of the IEEE/CVF international conference on computer vision",
            "페이지": "6824-6835",
            "학술 문서": "Multiscale vision transformersH Fan, B Xiong, K Mangalam, Y Li, Z Yan, J Malik… - Proceedings of the IEEE/CVF international conference …, 20211811회 인용 관련 학술자료 전체 11개의 버전 Multiscale Vision Transformers.H Fan, B Xiong, K Mangalam, Y Li, Z Yan, J Malik… - ICCV, 2021103회 인용 관련 학술자료 Christoph Feichtenhofer. Multiscale vision transformersH Fan, B Xiong, K Mangalam, Y Li, Z Yan, J Malik - Proceedings of the IEEE/CVF International Conference …, 20212회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Multiscale vision transformers",
        "year": null
    },
    "Recognizing action at a distance": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2003/10/13",
            "게시자": "IEEE",
            "설명": "Our goal is to recognize human action at a distance, at resolutions where a whole person may be, say, 30 pixels tall. We introduce a novel motion descriptor based on optical flow measurements in a spatiotemporal volume for each stabilized human figure, and an associated similarity measure to be used in a nearest-neighbor framework. Making use of noisy optical flow measurements is the key challenge, which is addressed by treating optical flow not as precise pixel displacements, but rather as a spatial pattern of noisy measurements which are carefully smoothed and aggregated to form our spatiotemporal motion descriptor. To classify the action being performed by a human figure in a query sequence, we retrieve nearest neighbor(s) from a database of stored, annotated video sequences. We can also use these retrieved exemplars to transfer 2D/3D skeletons onto the figures in the query sequence, as well as …",
            "저널": "Proceedings Ninth IEEE International Conference on Computer Vision",
            "저자": "Efros, Berg, Mori, Malik",
            "전체 인용횟수": "1833회 인용20042005200620072008200920102011201220132014201520162017201820192020202120222023445563851481501591451421611509710271575433403217",
            "페이지": "726-733 vol. 2",
            "학술 문서": "Recognizing action at a distanceEfros, Berg, Mori, Malik - Proceedings Ninth IEEE International Conference on …, 20031833회 인용 관련 학술자료 전체 43개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Recognizing action at a distance",
        "year": null
    },
    "Spectral grouping using the nystrom method": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004/2",
            "게시자": "IEEE",
            "권": "26",
            "설명": "Spectral graph theoretic methods have recently shown great promise for the problem of image segmentation. However, due to the computational demands of these approaches, applications to large problems such as spatiotemporal data and high resolution imagery have been slow to appear. The contribution of this paper is a method that substantially reduces the computational requirements of grouping algorithms based on spectral partitioning making it feasible to apply them to very large grouping problems. Our approach is based on a technique for the numerical solution of eigenfunction problems known as the Nystrom method. This method allows one to extrapolate the complete grouping solution using only a small number of samples. In doing so, we leverage the fact that there are far fewer coherent groups in a scene than pixels.",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Charless Fowlkes, Serge Belongie, Fan Chung, Jitendra Malik",
            "전체 인용횟수": "1779회 인용20042005200620072008200920102011201220132014201520162017201820192020202120222023153739537470791111411141331441461061208690774865",
            "페이지": "214-225",
            "학술 문서": "Spectral grouping using the nystrom methodC Fowlkes, S Belongie, F Chung, J Malik - IEEE transactions on pattern analysis and machine …, 20041768회 인용 관련 학술자료 전체 39개의 버전 Spectral grouping using the Nystrom method*F Charless - IEEE Trans. PAMI, 200418회 인용 관련 학술자료 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Spectral grouping using the nystrom method",
        "year": null
    },
    "Large displacement optical flow: descriptor matching in variational motion estimation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/8/19",
            "게시자": "IEEE",
            "권": "33",
            "설명": "Optical flow estimation is classically marked by the requirement of dense sampling in time. While coarse-to-fine warping schemes have somehow relaxed this constraint, there is an inherent dependency between the scale of structures and the velocity that can be estimated. This particularly renders the estimation of detailed human motion problematic, as small body parts can move very fast. In this paper, we present a way to approach this problem by integrating rich descriptors into the variational optical flow setting. This way we can estimate a dense optical flow field with almost the same high accuracy as known from variational optical flow, while reaching out to new domains of motion analysis where the requirement of dense sampling in time is no longer satisfied.",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Thomas Brox, Jitendra Malik",
            "전체 인용횟수": "1736회 인용20102011201220132014201520162017201820192020202120222023945751131461622072041871721281017268",
            "페이지": "500-513",
            "학술 문서": "Large displacement optical flow: descriptor matching in variational motion estimationT Brox, J Malik - IEEE transactions on pattern analysis and machine …, 20101736회 인용 관련 학술자료 전체 17개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Large displacement optical flow: descriptor matching in variational motion estimation",
        "year": null
    },
    "BSurf: Speeded up robust features,’’Computer Vision ECCV 2006": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006",
            "저널": "Lecture Notes in Computer Science",
            "저자": "Herbert Bay, Tinne Tuytelaars, Luc Van Gool",
            "전체 인용횟수": "36468회 인용2007200820092010201120122013201420152016201720182019202020212022202316335671213261809226126652878311930673015296727622533251822351650",
            "페이지": "404-417",
            "학술 문서": "Surf: Speeded up robust features*H Bay, T Tuytelaars, L Van Gool - Computer Vision–ECCV 2006: 9th European …, 200623066회 인용 관련 학술자료 전체 39개의 버전 Speeded-up robust features (SURF)*H Bay, A Ess, T Tuytelaars, L Van Gool - Computer vision and image understanding, 200814444회 인용 관련 학술자료 전체 26개의 버전 European conference on computer vision*H Bay, T Tuytelaars, L Van Gool - 2006123회 인용 관련 학술자료 Luc Van Gool*H Bay, T Tuytelaars - SURF: speeded up robust features, 200618회 인용 관련 학술자료 SURF: Speed up robust features (SURF)*H Bay, T Tuytelaars, L Van Gool - 20062회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "BSurf: Speeded up robust features,’’Computer Vision ECCV 2006",
        "year": null
    },
    "Speeded-up robust features (SURF)": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008/6",
            "권": "110",
            "설명": "This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURF’s application to two challenging, yet converse goals …",
            "저널": "Computer Vision and Image Understanding",
            "저자": "Herbert Baya, Andreas Essa, Tinne Tuytelaarsb, Luc Van Goola",
            "전체 인용횟수": "36399회 인용2007200820092010201120122013201420152016201720182019202020212022202316335571213241808225926632873312130673016296427572527249722231637",
            "페이지": "346-359",
            "학술 문서": "Surf: Speeded up robust features*H Bay, T Tuytelaars, L Van Gool - Computer Vision–ECCV 2006: 9th European …, 200623066회 인용 관련 학술자료 전체 39개의 버전 Speeded-up robust features (SURF)H Bay, A Ess, T Tuytelaars, L Van Gool - Computer vision and image understanding, 200814444회 인용 관련 학술자료 전체 26개의 버전 Speeded-Up Robust Features (SURF) Comput. Vis. Image Underst*H Bay, A Ess, T Tuytelaars, L Van Gool - 20085회 인용 관련 학술자료 van Gool [2008],‘Speeded up robust features (SURF)’*H Bay, A Ess, T Tuytelaars - Computer Vision and Image Understanding3회 인용 관련 학술자료 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Speeded-up robust features (SURF)",
        "year": null
    },
    "The Pascal Visual Object Classes Challenge: A Retrospective": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/1",
            "게시자": "Springer US",
            "권": "111",
            "설명": " The Pascal Visual Object Classes (VOC) challenge consists of two components: (i) a publicly available dataset of images together with ground truth annotation and standardised evaluation software; and (ii) an annual competition and workshop. There are five challenges: classification, detection, segmentation, action classification, and person layout. In this paper we provide a review of the challenge from 2008–2012. The paper is intended for two audiences: algorithm designers, researchers who want to see what the state of the art is, as measured by performance on the VOC datasets, along with the limitations and weak points of the current generation of algorithms; and, challenge designers, who want to see what we as organisers have learnt from the process and our recommendations for the organisation of future challenges. To analyse the performance of submitted algorithms on the VOC datasets we …",
            "저널": "International journal of computer vision",
            "저자": "Mark Everingham, SM Ali Eslami, Luc Van Gool, Christopher KI Williams, John Winn, Andrew Zisserman",
            "전체 인용횟수": "6294회 인용20142015201620172018201920202021202220232811725836459278786810511018992",
            "페이지": "98-136",
            "학술 문서": "The pascal visual object classes challenge: A retrospectiveM Everingham, SMA Eslami, L Van Gool, CKI Williams… - International journal of computer vision, 20156294회 인용 관련 학술자료 전체 37개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The Pascal Visual Object Classes Challenge: A Retrospective",
        "year": null
    },
    "Temporal segment networks: Towards good practices for deep action recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "게시자": "Springer, Cham",
            "설명": "Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 …",
            "저자": "Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, Luc Van Gool",
            "전체 인용횟수": "4034회 인용2016201720182019202020212022202317197414576637758718667",
            "컨퍼런스": "European conference on computer vision",
            "페이지": "20-36",
            "학술 문서": "Temporal segment networks: Towards good practices for deep action recognitionL Wang, Y Xiong, Z Wang, Y Qiao, D Lin, X Tang… - European conference on computer vision, 20164034회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Temporal segment networks: Towards good practices for deep action recognition",
        "year": null
    },
    "Efficient non-maximum suppression": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006/8/20",
            "게시자": "IEEE",
            "권": "3",
            "설명": "In this work we scrutinize a low level computer vision task - non-maximum suppression (NMS) - which is a crucial preprocessing step in many computer vision applications. Especially in real time scenarios, efficient algorithms for such preprocessing algorithms, which operate on the full image resolution, are important. In the case of NMS, it seems that merely the straightforward implementation or slight improvements are known. We show that these are far from being optimal, and derive several algorithms ranging from easy-to-implement to highly-efficient",
            "저자": "Alexander Neubeck, Luc Van Gool",
            "전체 인용횟수": "2010회 인용200920102011201220132014201520162017201820192020202120222023141023242344585070100188263340418363",
            "컨퍼런스": "18th international conference on pattern recognition (ICPR'06)",
            "페이지": "850-855",
            "학술 문서": "Efficient non-maximum suppressionA Neubeck, L Van Gool - 18th international conference on pattern recognition …, 20062010회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Efficient non-maximum suppression",
        "year": null
    },
    "An adaptive color-based particle filter": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2003/1/10",
            "게시자": "Elsevier",
            "권": "21",
            "설명": "Robust real-time tracking of non-rigid objects is a challenging task. Particle filtering has proven very successful for non-linear and non-Gaussian estimation problems. The article presents the integration of color distributions into particle filtering, which has typically been used in combination with edge-based image features. Color distributions are applied, as they are robust to partial occlusion, are rotation and scale invariant and computationally efficient. As the color of an object can vary over time dependent on the illumination, the visual angle and the camera parameters, the target model is adapted during temporally stable image observations. An initialization based on an appearance condition is introduced since tracked objects may disappear and reappear. Comparisons with the mean shift tracker and a combination between the mean shift tracker and Kalman filtering show the advantages and limitations of the …",
            "저널": "Image and vision computing",
            "저자": "Katja Nummiaro, Esther Koller-Meier, Luc Van Gool",
            "전체 인용횟수": "1790회 인용20032004200520062007200820092010201120122013201420152016201720182019202020212022202311316065939413915316514813412310512870695641374016",
            "페이지": "99-110",
            "학술 문서": "An adaptive color-based particle filterK Nummiaro, E Koller-Meier, L Van Gool - Image and vision computing, 20031790회 인용 관련 학술자료 전체 14개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "An adaptive color-based particle filter",
        "year": null
    },
    "A benchmark dataset and evaluation methodology for video object segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "Over the years, datasets and benchmarks have proven their fundamental importance in computer vision research, enabling targeted progress and objective comparisons in many fields. At the same time, legacy datasets may impend the evolution of a field due to saturated algorithm performance and the lack of contemporary, high quality data. In this work we present a new benchmark dataset and evaluation methodology for the area of video object segmentation. The dataset, named DAVIS (Densely Annotated VIdeo Segmentation), consists of fifty high quality, Full HD video sequences, spanning multiple occurrences of common video object segmentation challenges such as occlusions, motion-blur and appearance changes. Each video is accompanied by densely annotated, pixel-accurate and per-frame ground truth segmentation. In addition, we provide a comprehensive analysis of several state-of-the-art segmentation approaches using three complementary metrics that measure the spatial extent of the segmentation, the accuracy of the silhouette contours and the temporal coherence. The results uncover strengths and weaknesses of current approaches, opening up promising directions for future works.",
            "저자": "Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, Alexander Sorkine-Hornung",
            "전체 인용횟수": "1769회 인용201620172018201920202021202220231094152210262313348366",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "724-732",
            "학술 문서": "A benchmark dataset and evaluation methodology for video object segmentationF Perazzi, J Pont-Tuset, B McWilliams, L Van Gool… - Proceedings of the IEEE conference on computer …, 20161769회 인용 관련 학술자료 전체 19개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A benchmark dataset and evaluation methodology for video object segmentation",
        "year": null
    },
    "A+: Adjusted anchored neighborhood regression for fast super-resolution": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "게시자": "Springer International Publishing",
            "설명": " We address the problem of image upscaling in the form of single image super-resolution based on a dictionary of low- and high-resolution exemplars. Two recently proposed methods, Anchored Neighborhood Regression (ANR) and Simple Functions (SF), provide state-of-the-art quality performance. Moreover, ANR is among the fastest known super-resolution methods. ANR learns sparse dictionaries and regressors anchored to the dictionary atoms. SF relies on clusters and corresponding learned functions. We propose A+, an improved variant of ANR, which combines the best qualities of ANR and SF. A+ builds on the features and anchored regressors from ANR but instead of learning the regressors on the dictionary it uses the full training material, similar to SF. We validate our method on standard images and compare with state-of-the-art methods. We obtain improved quality (i.e. 0.2–0.7 dB PSNR …",
            "저자": "Radu Timofte, Vincent De Smet, Luc Van Gool",
            "전체 인용횟수": "1664회 인용20152016201720182019202020212022202338108196312265232209168114",
            "컨퍼런스": "Computer Vision--ACCV 2014: 12th Asian Conference on Computer Vision, Singapore, Singapore, November 1-5, 2014, Revised Selected Papers, Part IV 12",
            "페이지": "111-126",
            "학술 문서": "A+: Adjusted anchored neighborhood regression for fast super-resolutionR Timofte, V De Smet, L Van Gool - Computer Vision--ACCV 2014: 12th Asian Conference …, 20151664회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A+: Adjusted anchored neighborhood regression for fast super-resolution",
        "year": null
    },
    "Food-101–mining discriminative components with random forests": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "게시자": "Springer International Publishing",
            "설명": " In this paper we address the problem of automatically recognizing pictured dishes. To this end, we introduce a novel method to mine discriminative parts using Random Forests (rf), which allows us to mine for parts simultaneously for all classes and to share knowledge among them. To improve efficiency of mining and classification, we only consider patches that are aligned with image superpixels, which we call components. To measure the performance of our rf component mining for food recognition, we introduce a novel and challenging dataset of 101 food categories, with 101’000 images. With an average accuracy of 50.76%, our model outperforms alternative classification methods except for cnn, including svm classification on Improved Fisher Vectors and existing discriminative part-mining algorithms by 11.88% and 8.13%, respectively. On the challenging mit-Indoor dataset, our method compares …",
            "저자": "Lukas Bossard, Matthieu Guillaumin, Luc Van Gool",
            "전체 인용횟수": "1644회 인용201520162017201820192020202120222023285476102155171228355455",
            "컨퍼런스": "Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13",
            "페이지": "446-461",
            "학술 문서": "Food-101–mining discriminative components with random forestsL Bossard, M Guillaumin, L Van Gool - Computer Vision–ECCV 2014: 13th European …, 20141644회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Food-101–mining discriminative components with random forests",
        "year": null
    },
    "Self-calibration and metric reconstruction inspite of varying and unknown intrinsic camera parameters": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1999/8/1",
            "게시자": "Springer Netherlands",
            "권": "32",
            "설명": " In this paper the theoretical and practical feasibility of self-calibration in the presence of varying intrinsic camera parameters is under investigation. The paper's main contribution is to propose a self-calibration method which efficiently deals with all kinds of constraints on the intrinsic camera parameters. Within this framework a practical method is proposed which can retrieve metric reconstruction from image sequences obtained with uncalibrated zooming/focusing cameras. The feasibility of the approach is illustrated on real and synthetic examples. Besides this a theoretical proof is given which shows that the absence of skew in the image plane is sufficient to allow for self-calibration. A counting argument is developed which—depending on the set of constraints—gives the minimum sequence length for self-calibration and a method to detect critical motion sequences is proposed.",
            "저널": "International Journal of Computer Vision",
            "저자": "Marc Pollefeys, Reinhard Koch, Luc Van Gool",
            "전체 인용횟수": "1642회 인용199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220233974937479108109118986888726373576754454331303525222215",
            "페이지": "7-25",
            "학술 문서": "Self-calibration and metric reconstruction inspite of varying and unknown intrinsic camera parametersM Pollefeys, R Koch, LV Gool - International journal of computer vision, 19991642회 인용 관련 학술자료 전체 31개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Self-calibration and metric reconstruction inspite of varying and unknown intrinsic camera parameters",
        "year": null
    },
    "You'll never walk alone: Modeling social behavior for multi-target tracking": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/9/29",
            "게시자": "IEEE",
            "설명": "Object tracking typically relies on a dynamic model to predict the object's location from its past trajectory. In crowded scenarios a strong dynamic model is particularly important, because more accurate predictions allow for smaller search regions, which greatly simplifies data association. Traditional dynamic models predict the location for each target solely based on its own history, without taking into account the remaining scene objects. Collisions are resolved only when they happen. Such an approach ignores important aspects of human behavior: people are driven by their future destination, take into account their environment, anticipate collisions, and adjust their trajectories at an early stage in order to avoid them. In this work, we introduce a model of dynamic social behavior, inspired by models developed for crowd simulation. The model is trained with videos recorded from birds-eye view at busy locations, and …",
            "저자": "Stefano Pellegrini, Andreas Ess, Konrad Schindler, Luc Van Gool",
            "전체 인용횟수": "1635회 인용201020112012201320142015201620172018201920202021202220233028587196971089079140178207227201",
            "컨퍼런스": "2009 IEEE 12th international conference on computer vision",
            "페이지": "261-268",
            "학술 문서": "You'll never walk alone: Modeling social behavior for multi-target trackingS Pellegrini, A Ess, K Schindler, L Van Gool - 2009 IEEE 12th international conference on computer …, 20091635회 인용 관련 학술자료 전체 23개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "You'll never walk alone: Modeling social behavior for multi-target tracking",
        "year": null
    },
    "Procedural modeling of buildings": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006/7/1",
            "게시자": "ACM",
            "권": "25",
            "설명": "CGA shape, a novel shape grammar for the procedural modeling of CG architecture, produces building shells with high visual quality and geometric detail. It produces extensive architectural models for computer games and movies, at low cost. Context sensitive shape rules allow the user to specify interactions between the entities of the hierarchical shape descriptions. Selected examples demonstrate solutions to previously unsolved modeling problems, especially to consistent mass modeling with volumetric shapes of arbitrary orientation. CGA shape is shown to efficiently generate massive urban models with unprecedented level of detail, with the virtual rebuilding of the archaeological site of Pompeii as a case in point.",
            "저널": "ACM Transactions on Graphics (TOG)",
            "저자": "Pascal Müller, Peter Wonka, Simon Haegler, Andreas Ulmer, Luc Van Gool",
            "전체 인용횟수": "1580회 인용200620072008200920102011201220132014201520162017201820192020202120222023144276110122123129137117909496817256637348",
            "페이지": "614-623",
            "학술 문서": "Procedural modeling of buildingsP Müller, P Wonka, S Haegler, A Ulmer, L Van Gool - ACM SIGGRAPH 2006 Papers, 20061575회 인용 관련 학술자료 전체 31개의 버전 Procedural Modeling of Buildings*PV Gool, P Müller, P Wonka, S Haegler, A Ulmer… - ACMT on G.(TOG)(Ed.), ACM SIGGRAPH, 20065회 인용 관련 학술자료 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Procedural modeling of buildings",
        "year": null
    },
    "Ntire 2017 challenge on single image super-resolution: Methods and results": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "This paper reviews the first challenge on single image super-resolution (restoration of rich details in an low resolution image) with focus on proposed solutions and results. A new DIVerse 2K resolution image dataset (DIV2K) was employed. The challenge had 6 competitions divided into 2 tracks with 3 magnification factors each. Track 1 employed the standard bicubic downscaling setup, while Track 2 had unknown downscaling operators (blur kernel and decimation) but learnable through low and high res train images. Each competition had 100 registered participants and 20 teams competed in the final testing phase. They gauge the state-of-the-art in single image super-resolution.",
            "저자": "Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-Hsuan Yang, Lei Zhang",
            "전체 인용횟수": "1485회 인용201720182019202020212022202311111182278302299292",
            "컨퍼런스": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops",
            "페이지": "114-125",
            "학술 문서": "Ntire 2017 challenge on single image super-resolution: Methods and resultsR Timofte, E Agustsson, L Van Gool, MH Yang… - Proceedings of the IEEE conference on computer …, 20171485회 인용 관련 학술자료 전체 19개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Ntire 2017 challenge on single image super-resolution: Methods and results",
        "year": null
    },
    "Anchored neighborhood regression for fast example-based super-resolution": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013",
            "설명": "Recently there have been significant advances in image upscaling or image super-resolution based on a dictionary of low and high resolution exemplars. The running time of the methods is often ignored despite the fact that it is a critical factor for real applications. This paper proposes fast super-resolution methods while making no compromise on quality. First, we support the use of sparse learned dictionaries in combination with neighbor embedding methods. In this case, the nearest neighbors are computed using the correlation with the dictionary atoms rather than the Euclidean distance. Moreover, we show that most of the current approaches reach top performance for the right parameters. Second, we show that using global collaborative coding has considerable speed advantages, reducing the super-resolution mapping to a precomputed projective matrix. Third, we propose the anchored neighborhood regression. That is to anchor the neighborhood embedding of a low resolution patch to the nearest atom in the dictionary and to precompute the corresponding embedding matrix. These proposals are contrasted with current state-ofthe-art methods on standard images. We obtain similar or improved quality and one or two orders of magnitude speed improvements.",
            "저자": "Radu Timofte, Vincent De Smet, Luc Van Gool",
            "전체 인용횟수": "1474회 인용20142015201620172018201920202021202220231869150179256212178151143101",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "1920-1927",
            "학술 문서": "Anchored neighborhood regression for fast example-based super-resolutionR Timofte, V De Smet, L Van Gool - Proceedings of the IEEE international conference on …, 20131474회 인용 관련 학술자료 전체 20개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Anchored neighborhood regression for fast example-based super-resolution",
        "year": null
    },
    "Swinir: Image restoration using swin transformer": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021",
            "설명": "Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from low-quality images (eg, downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by up to 0.14 0.45 dB, while the total number of parameters can be reduced by up to 67%.",
            "저자": "Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, Radu Timofte",
            "전체 인용횟수": "1454회 인용20212022202323439979",
            "컨퍼런스": "Proceedings of the IEEE/CVF international conference on computer vision",
            "페이지": "1833-1844",
            "학술 문서": "Swinir: Image restoration using swin transformerJ Liang, J Cao, G Sun, K Zhang, L Van Gool, R Timofte - Proceedings of the IEEE/CVF international conference …, 20211454회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Swinir: Image restoration using swin transformer",
        "year": null
    },
    "Visual modeling with a hand-held camera": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004/9/1",
            "게시자": "Springer Netherlands",
            "권": "59",
            "설명": " In this paper a complete system to build visual models from camera images is presented. The system can deal with uncalibrated image sequences acquired with a hand-held camera. Based on tracked or matched features the relations between multiple views are computed. From this both the structure of the scene and the motion of the camera are retrieved. The ambiguity on the reconstruction is restricted from projective to metric through self-calibration. A flexible multi-view stereo matching scheme is used to obtain a dense estimation of the surface geometry. From the computed data different types of visual models are constructed. Besides the traditional geometry- and image-based approaches, a combined approach with view-dependent geometry and texture is presented. As an application fusion of real and virtual scenes is also shown.",
            "저널": "International Journal of Computer Vision",
            "저자": "Marc Pollefeys, Luc Van Gool, Maarten Vergauwen, Frank Verbiest, Kurt Cornelis, Jan Tops, Reinhard Koch",
            "전체 인용횟수": "1336회 인용20042005200620072008200920102011201220132014201520162017201820192020202120222023123877101103126119134878684846260372622211711",
            "페이지": "207-232",
            "학술 문서": "Visual modeling with a hand-held cameraM Pollefeys, L Van Gool, M Vergauwen, F Verbiest… - International Journal of Computer Vision, 20041336회 인용 관련 학술자료 전체 33개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Visual modeling with a hand-held camera",
        "year": null
    },
    "An efficient dense and scale-invariant spatio-temporal interest point detector": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008",
            "게시자": "Springer Berlin/Heidelberg",
            "설명": " Over the years, several spatio-temporal interest point detectors have been proposed. While some detectors can only extract a sparse set of scale-invariant features, others allow for the detection of a larger amount of features at user-defined scales. This paper presents for the first time spatio-temporal interest points that are at the same time scale-invariant (both spatially and temporally) and densely cover the video content. Moreover, as opposed to earlier work, the features can be computed efficiently. Applying scale-space theory, we show that this can be achieved by using the determinant of the Hessian as the saliency measure. Computations are speeded-up further through the use of approximative box-filter operations on an integral video structure. A quantitative evaluation and experimental results on action recognition show the strengths of the proposed detector in terms of repeatability, accuracy and …",
            "저널": "Computer Vision–ECCV 2008",
            "저자": "Geert Willems, Tinne Tuytelaars, Luc Van Gool",
            "전체 인용횟수": "1251회 인용20082009201020112012201320142015201620172018201920202021202220236114667108139124128144118908565513616",
            "페이지": "650-663",
            "학술 문서": "An efficient dense and scale-invariant spatio-temporal interest point detectorG Willems, T Tuytelaars, L Van Gool - Computer Vision–ECCV 2008: 10th European …, 20081251회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "An efficient dense and scale-invariant spatio-temporal interest point detector",
        "year": null
    },
    "Mask r-cnn": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/10/22",
            "게시자": "IEEE",
            "설명": "We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, eg, allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.",
            "저자": "Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick",
            "전체 인용횟수": "30941회 인용2017201820192020202120222023159133734505005661174676728",
            "컨퍼런스": "Computer Vision (ICCV), 2017 IEEE International Conference on",
            "페이지": "2980-2988",
            "학술 문서": "Mask r-cnnK He, G Gkioxari, P Dollár, R Girshick - Proceedings of the IEEE international conference on …, 201730662회 인용 관련 학술자료 전체 23개의 버전 Mask R-CNN*GG HE KM, P Dollár - 2017 IEEE International Conference on Computer …, 2017176회 인용 관련 학술자료 Mask r-cnn. arXiv 2017*K He, G Gkioxari, P Dollár, R Girshick - arXiv preprint arXiv:1703.06870, 2020135회 인용 관련 학술자료 CNN [J]*K He, G Gkioxari, P Dollár, R Mask - IEEE Transactions on Pattern Analysis and Machine …, 2020103회 인용 관련 학술자료 Mask r-cnn. arXiv*K He, G Gkioxari, P Dollár, R Girshick - arXiv preprint arXiv:1703.06870, 201772회 인용 관련 학술자료 Mask r-cnn*G Gkioxari, P Dollár, R Girshick - IEEE International conference on Computer Vision …, 201754회 인용 관련 학술자료 Mask r-cnn*R Mask, K He, G Gkioxari, P Dollár, R Girshick - Proceedings of the IEEE International Conference on …, 20176회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Mask r-cnn",
        "year": null
    },
    "Pedestrian detection: An evaluation of the state of the art": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/4",
            "게시자": "IEEE",
            "권": "34",
            "설명": "Pedestrian detection is a key problem in computer vision, with several applications that have the potential to positively impact quality of life. In recent years, the number of approaches to detecting pedestrians in monocular images has grown steadily. However, multiple data sets and widely varying evaluation protocols are used, making direct comparisons difficult. To address these shortcomings, we perform an extensive evaluation of the state of the art in a unified framework. We make three primary contributions: 1) We put together a large, well-annotated, and realistic monocular pedestrian detection data set and study the statistics of the size, position, and occlusion patterns of pedestrians in urban scenes, 2) we propose a refined per-frame evaluation methodology that allows us to carry out probing and informative comparisons, including measuring performance in relation to scale and occlusion, and 3) we evaluate …",
            "저널": "Pattern Analysis and Machine Intelligence (PAMI), IEEE Transactions on",
            "저자": "Piotr Dollar, Christian Wojek, Bernt Schiele, Pietro Perona",
            "전체 인용횟수": "3925회 인용20122013201420152016201720182019202020212022202388220303374408434366403360373294228",
            "페이지": "743-761",
            "학술 문서": "Pedestrian detection: An evaluation of the state of the artP Dollar, C Wojek, B Schiele, P Perona - IEEE transactions on pattern analysis and machine …, 20113925회 인용 관련 학술자료 전체 28개의 버전 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pedestrian detection: An evaluation of the state of the art",
        "year": null
    },
    "Behavior recognition via sparse spatio-temporal features": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/10/15",
            "게시자": "IEEE",
            "설명": "A common trend in object recognition is to detect and leverage the use of sparse, informative feature points. The use of such features makes the problem more manageable while providing increased robustness to noise and pose variation. In this work we develop an extension of these ideas to the spatio-temporal case. For this purpose, we show that the direct 3D counterparts to commonly used 2D interest point detectors are inadequate, and we propose an alternative. Anchoring off of these interest points, we devise a recognition algorithm based on spatio-temporally windowed data. We present recognition results on a variety of datasets including both human and rodent behavior.",
            "저자": "Piotr Dollár, Vincent Rabaud, Garrison Cottrell, Serge Belongie",
            "전체 인용횟수": "3471회 인용20052006200720082009201020112012201320142015201620172018201920202021202220239133691175218261295359361302304252185175151978551",
            "컨퍼런스": "2005 IEEE international workshop on visual surveillance and performance evaluation of tracking and surveillance",
            "페이지": "65-72",
            "학술 문서": "Behavior recognition via sparse spatio-temporal featuresP Dollár, V Rabaud, G Cottrell, S Belongie - 2005 IEEE international workshop on visual …, 20053467회 인용 관련 학술자료 전체 18개의 버전 S. Belongie, GC, 2005. Behavior recognition via sparse spatio-temporal features*P Dollar, V Rabaud3회 인용 관련 학술자료 Abstract: Behavior Recognition via Sparse Spatio-Temporal Features*P Dollar - Department of Computer Science and Engineering …, 20052회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Behavior recognition via sparse spatio-temporal features",
        "year": null
    },
    "Edge boxes: Locating object proposals from edges": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "게시자": "Springer International Publishing",
            "설명": " The use of object proposals is an effective recent approach for increasing the computational efficiency of object detection. We propose a novel method for generating object bounding box proposals using edges. Edges provide a sparse yet informative representation of an image. Our main observation is that the number of contours that are wholly contained in a bounding box is indicative of the likelihood of the box containing an object. We propose a simple box objectness score that measures the number of edges that exist in the box minus those that are members of contours that overlap the box’s boundary. Using efficient data structures, millions of candidate boxes can be evaluated in a fraction of a second, returning a ranked set of a few thousand top-scoring proposals. Using standard metrics, we show results that are significantly more accurate than the current state-of-the-art while being faster to compute …",
            "저자": "C Lawrence Zitnick, Piotr Dollár",
            "전체 인용횟수": "3295회 인용201420152016201720182019202020212022202313170352512496523371313262188",
            "컨퍼런스": "Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13",
            "페이지": "391-405",
            "학술 문서": "Edge boxes: Locating object proposals from edgesCL Zitnick, P Dollár - Computer Vision–ECCV 2014: 13th European …, 20143295회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Edge boxes: Locating object proposals from edges",
        "year": null
    },
    "Fast feature pyramids for object detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/1/16",
            "게시자": "IEEE",
            "권": "36",
            "설명": "Multi-resolution image features may be approximated via extrapolation from nearby scales, rather than being computed explicitly. This fundamental insight allows us to design object detection algorithms that are as accurate, and considerably faster, than the state-of-the-art. The computational bottleneck of many modern detectors is the computation of features at every scale of a finely-sampled image pyramid. Our key insight is that one may compute finely sampled feature pyramids at a fraction of the cost, without sacrificing performance: for a broad family of features we find that features computed at octave-spaced scale intervals are sufficient to approximate features on a finely-sampled pyramid. Extrapolation is inexpensive as compared to direct feature computation. As a result, our approximation yields considerable speedups with negligible loss in detection accuracy. We modify three diverse visual recognition …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Piotr Dollár, Ron Appel, Serge Belongie, Pietro Perona",
            "전체 인용횟수": "2345회 인용201420152016201720182019202020212022202358196257383323353254200161107",
            "페이지": "1532-1545",
            "학술 문서": "Fast feature pyramids for object detectionP Dollár, R Appel, S Belongie, P Perona - IEEE transactions on pattern analysis and machine …, 20142345회 인용 관련 학술자료 전체 21개의 버전 ",
            "호": "8"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Fast feature pyramids for object detection",
        "year": null
    },
    "Microsoft coco captions: Data collection and evaluation server": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/4/1",
            "설명": "In this paper we describe the Microsoft COCO Caption dataset and evaluation server. When completed, the dataset will contain over one and a half million captions describing over 330,000 images. For the training and validation images, five independent human generated captions will be provided. To ensure consistency in evaluation of automatic caption generation algorithms, an evaluation server is used. The evaluation server receives candidate captions and scores them using several popular metrics, including BLEU, METEOR, ROUGE and CIDEr. Instructions for using the evaluation server are provided.",
            "저널": "arXiv preprint arXiv:1504.00325",
            "저자": "Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, C Lawrence Zitnick",
            "전체 인용횟수": "2043회 인용2015201620172018201920202021202220231966125164226219333425442",
            "학술 문서": "Microsoft coco captions: Data collection and evaluation serverX Chen, H Fang, TY Lin, R Vedantam, S Gupta… - arXiv preprint arXiv:1504.00325, 20152043회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Microsoft coco captions: Data collection and evaluation server",
        "year": null
    },
    "Pedestrian detection: A benchmark": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/6/20",
            "게시자": "IEEE",
            "설명": "Pedestrian detection is a key problem in computer vision, with several applications including robotics, surveillance and automotive safety. Much of the progress of the past few years has been driven by the availability of challenging public datasets. To continue the rapid rate of innovation, we introduce the Caltech Pedestrian Dataset, which is two orders of magnitude larger than existing datasets. The dataset contains richly annotated video, recorded from a moving vehicle, with challenging images of low resolution and frequently occluded people. We propose improved evaluation metrics, demonstrating that commonly used per-window measures are flawed and can fail to predict performance on full images. We also benchmark several promising detection systems, providing an overview of state-of-the-art performance and a direct, unbiased comparison of existing methods. Finally, by analyzing common failure cases …",
            "저자": "Piotr Dollár, Christian Wojek, Bernt Schiele, Pietro Perona",
            "전체 인용횟수": "1742회 인용2009201020112012201320142015201620172018201920202021202220231771108879293108108139133138144174174125",
            "컨퍼런스": "2009 IEEE conference on computer vision and pattern recognition",
            "페이지": "304-311",
            "학술 문서": "Pedestrian detection: A benchmarkP Dollár, C Wojek, B Schiele, P Perona - 2009 IEEE conference on computer vision and pattern …, 20091742회 인용 관련 학술자료 전체 19개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pedestrian detection: A benchmark",
        "year": null
    },
    "Integral channel features": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009",
            "게시자": "BMVC Press",
            "설명": "We study the performance of ‘integral channel features’ for image classification tasks, focusing in particular on pedestrian detection.  The general idea behind integral channel features is that multiple registered image channels are computed using linear and non-linear transformations of the input image, and then features such as local sums, histograms,  and  Haar  features and  their  various  generalizations are  efficiently  computed using integral images.  Such features have been used in recent literature for a variety of tasks – indeed,  variations appear to have been invented independently multiple times. Although integral channel features have proven effective, little effort has been devoted to analyzing or optimizing the features themselves. In this work we present a unified view of the relevant work in this area and perform a detailed experimental evaluation.   We demonstrate that when designed properly, integral channel features not only outperform other features including histogram of oriented gradient (HOG), they also (1) naturally integrate heterogeneous sources of information, (2) have few parameters and are insensitive to exact parameter settings, (3) allow for more accurate spatial localization during detection, and (4) result in fast detectors when coupled with cascade classifiers.",
            "저자": "Piotr Dollár, Zhuowen Tu, Pietro Perona, Serge Belongie",
            "전체 인용횟수": "1654회 인용20102011201220132014201520162017201820192020202120222023111843891702142091921891641181006639",
            "페이지": "91.1-91.11",
            "학술 문서": "Integral channel featuresP Dollár, Z Tu, P Perona, S Belongie - 20091654회 인용 관련 학술자료 전체 21개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Integral channel features",
        "year": null
    },
    "From captions to visual concepts and back": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "This paper presents a novel approach for automatically generating image descriptions: visual detectors, language models, and multimodal similarity models learnt directly from a dataset of image captions. We use multiple instance learning to train visual detectors for words that commonly occur in captions, including many different parts of speech such as nouns, verbs, and adjectives. The word detector outputs serve as conditional inputs to a maximum-entropy language model. The language model learns from a set of over 400,000 image descriptions to capture the statistics of word usage. We capture global semantics by re-ranking caption candidates using sentence-level features and a deep multimodal similarity model. Our system is state-of-the-art on the official Microsoft COCO benchmark, producing a BLEU-4 score of 29.1%. When human judges compare the system captions to ones written by other people on our held-out test set, the system captions have equal or better quality 34% of the time.",
            "저자": "Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh K Srivastava, Li Deng, Piotr Dollár, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John C Platt, C Lawrence Zitnick, Geoffrey Zweig",
            "전체 인용횟수": "1555회 인용2014201520162017201820192020202120222023775139214216239203176130111",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "1473-1482",
            "학술 문서": "From captions to visual concepts and backH Fang, S Gupta, F Iandola, RK Srivastava, L Deng… - Proceedings of the IEEE conference on computer …, 20151555회 인용 관련 학술자료 전체 25개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "From captions to visual concepts and back",
        "year": null
    },
    "Structured forests for fast edge detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013",
            "설명": "Edge detection is a critical component of many vision systems, including object detectors and image segmentation algorithms. Patches of edges exhibit well-known forms of local structure, such as straight lines or T-junctions. In this paper we take advantage of the structure present in local image patches to learn both an accurate and computationally efficient edge detector. We formulate the problem of predicting local edge masks in a structured learning framework applied to random decision forests. Our novel approach to learning decision trees robustly maps the structured labels to a discrete space on which standard information gain measures may be evaluated. The result is an approach that obtains realtime performance that is orders of magnitude faster than many competing state-of-the-art approaches, while also achieving state-of-the-art edge detection results on the BSDS500 Segmentation dataset and NYU Depth dataset. Finally, we show the potential of our approach as a general purpose edge detector by showing our learned edge models generalize well across datasets.",
            "저자": "Piotr Dollár, C Lawrence Zitnick",
            "전체 인용횟수": "1163회 인용20132014201520162017201820192020202120222023362168194174131118110735539",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "1841-1848",
            "학술 문서": "Structured forests for fast edge detectionP Dollár, CL Zitnick - Proceedings of the IEEE international conference on …, 20131163회 인용 관련 학술자료 전체 28개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Structured forests for fast edge detection",
        "year": null
    },
    "Human-level control through deep reinforcement learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/2",
            "게시자": "Nature Publishing Group",
            "권": "518",
            "설명": "The theory of reinforcement learning provides a normative account 1, deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems 4, 5, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms 3. While …",
            "저널": "nature",
            "저자": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, Demis Hassabis",
            "전체 인용횟수": "26771회 인용2015201620172018201920202021202220231786911327259136834381477748484069",
            "페이지": "529-533",
            "학술 문서": "Human-level control through deep reinforcement learningV Mnih, K Kavukcuoglu, D Silver, AA Rusu, J Veness… - nature, 201526771회 인용 관련 학술자료 전체 59개의 버전 ",
            "호": "7540"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Human-level control through deep reinforcement learning",
        "year": null
    },
    "Mastering the game of Go with deep neural networks and tree search": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/1",
            "게시자": "Nature Publishing Group",
            "권": "529",
            "설명": "The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and …",
            "저널": "nature",
            "저자": "David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, Demis Hassabis",
            "전체 인용횟수": "17214회 인용201620172018201920202021202220237531482213223832565272327442224",
            "페이지": "484-489",
            "학술 문서": "Mastering the game of Go with deep neural networks and tree searchD Silver, A Huang, CJ Maddison, A Guez, L Sifre… - nature, 201617214회 인용 관련 학술자료 전체 97개의 버전 ",
            "호": "7587"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Mastering the game of Go with deep neural networks and tree search",
        "year": null
    },
    "Highly accurate protein structure prediction with AlphaFold": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021/8",
            "게시자": "Nature Publishing Group",
            "권": "596",
            "설명": "Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort 1, 2, 3, 4, the structures of around 100,000 unique proteins have been determined 5, but this represents a small fraction of the billions of known protein sequences 6, 7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years 9. Despite recent progress 10, 11, 12, 13, 14, existing methods fall far short of atomic accuracy …",
            "저널": "Nature",
            "저자": "John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon AA Kohl, Andrew J Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W Senior, Koray Kavukcuoglu, Pushmeet Kohli, Demis Hassabis",
            "전체 인용횟수": "16567회 인용20212022202388064369096",
            "페이지": "583-589",
            "학술 문서": "Highly accurate protein structure prediction with AlphaFoldJ Jumper, R Evans, A Pritzel, T Green, M Figurnov… - Nature, 202116567회 인용 관련 학술자료 전체 20개의 버전 ",
            "호": "7873"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Highly accurate protein structure prediction with AlphaFold",
        "year": null
    },
    "Playing atari with deep reinforcement learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/12/19",
            "설명": "We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.",
            "저널": "arXiv preprint arXiv:1312.5602",
            "저자": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller",
            "전체 인용횟수": "13388회 인용201420152016201720182019202020212022202354138289558113416102008241627822242",
            "학술 문서": "Playing atari with deep reinforcement learningV Mnih, K Kavukcuoglu, D Silver, A Graves… - arXiv preprint arXiv:1312.5602, 201313388회 인용 관련 학술자료 전체 42개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Playing atari with deep reinforcement learning",
        "year": null
    },
    "Asynchronous methods for deep reinforcement learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/6/11",
            "게시자": "PMLR",
            "설명": "We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.",
            "저자": "Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu",
            "전체 인용횟수": "9894회 인용201620172018201920202021202220239938094813761616177719181703",
            "컨퍼런스": "International conference on machine learning",
            "페이지": "1928-1937",
            "학술 문서": "Asynchronous methods for deep reinforcement learningV Mnih, AP Badia, M Mirza, A Graves, T Lillicrap… - International conference on machine learning, 20169894회 인용 관련 학술자료 전체 25개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Asynchronous methods for deep reinforcement learning",
        "year": null
    },
    "Natural language processing (almost) from scratch": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011",
            "권": "12",
            "설명": "We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.",
            "저널": "Journal of machine learning research",
            "저자": "Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, Pavel Kuksa",
            "전체 인용횟수": "9716회 인용2012201320142015201620172018201920202021202220233812928659189510961410135913081045873590",
            "페이지": "2493− 2537",
            "학술 문서": "Natural language processing (almost) from scratchR Collobert, J Weston, L Bottou, M Karlen… - Journal of machine learning research, 20119716회 인용 관련 학술자료 전체 43개의 버전 ",
            "호": "ARTICLE"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Natural language processing (almost) from scratch",
        "year": null
    },
    "Matching networks for one shot learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "권": "29",
            "설명": "Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 82.2% to 87.8% and from 88% accuracy to 95% accuracy on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.",
            "저널": "Advances in neural information processing systems",
            "저자": "Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra",
            "전체 인용횟수": "6971회 인용20162017201820192020202120222023181283316221117146916881556",
            "학술 문서": "Matching networks for one shot learningO Vinyals, C Blundell, T Lillicrap, D Wierstra - Advances in neural information processing systems, 20166971회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Matching networks for one shot learning",
        "year": null
    },
    "Wavenet: A generative model for raw audio": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/9/12",
            "설명": "This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.",
            "저널": "arXiv preprint arXiv:1609.03499",
            "저자": "Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu",
            "전체 인용횟수": "6056회 인용2016201720182019202020212022202324256554900100012281130903",
            "학술 문서": "Wavenet: A generative model for raw audioA Oord, S Dieleman, H Zen, K Simonyan, O Vinyals… - arXiv preprint arXiv:1609.03499, 20166044회 인용 관련 학술자료 전체 13개의 버전 A generative model for raw audioA Oord, S Dieleman, H Zen, K Simonyan, O Vinyals… - arXiv preprint arXiv:1609.03499, 201612회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Wavenet: A generative model for raw audio",
        "year": null
    },
    "Bootstrap your own latent-a new approach to self-supervised learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020",
            "권": "33",
            "설명": "We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods intrinsically rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches 74.3% top-1 classification accuracy on ImageNet using the standard linear evaluation protocol with a standard ResNet-50 architecture and 79.6% with a larger ResNet. We also show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks.",
            "저널": "Advances in neural information processing systems",
            "저자": "Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Remi Munos, Michal Valko",
            "전체 인용횟수": "4821회 인용20202021202220238276118392107",
            "페이지": "21271-21284",
            "학술 문서": "Bootstrap your own latent-a new approach to self-supervised learningJB Grill, F Strub, F Altché, C Tallec, P Richemond… - Advances in neural information processing systems, 20204821회 인용 관련 학술자료 전체 19개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Bootstrap your own latent-a new approach to self-supervised learning",
        "year": null
    },
    "Recurrent models of visual attention": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "권": "27",
            "설명": "Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.",
            "저널": "Advances in neural information processing systems",
            "저자": "Volodymyr Mnih, Nicolas Heess, Alex Graves",
            "전체 인용횟수": "4256회 인용20152016201720182019202020212022202354189289419579665667697623",
            "학술 문서": "Recurrent models of visual attentionV Mnih, N Heess, A Graves - Advances in neural information processing systems, 20144256회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Recurrent models of visual attention",
        "year": null
    },
    "Grandmaster level in StarCraft II using multi-agent reinforcement learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/11/14",
            "게시자": "Nature Publishing Group UK",
            "권": "575",
            "설명": "Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions–, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement …",
            "저널": "Nature",
            "저자": "Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P Agapiou, Max Jaderberg, Alexander S Vezhnevets, Rémi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James Molloy, Tom L Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, David Silver",
            "전체 인용횟수": "3604회 인용20192020202120222023395368891134973",
            "페이지": "350-354",
            "학술 문서": "Grandmaster level in StarCraft II using multi-agent reinforcement learningO Vinyals, I Babuschkin, WM Czarnecki, M Mathieu… - Nature, 20193604회 인용 관련 학술자료 전체 13개의 버전 ",
            "호": "7782"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning",
        "year": null
    },
    "Weight uncertainty in neural network": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/6/1",
            "게시자": "PMLR",
            "설명": "We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.",
            "저자": "Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra",
            "전체 인용횟수": "3456회 인용201520162017201820192020202120222023206299226327502748745701",
            "컨퍼런스": "International conference on machine learning",
            "페이지": "1613-1622",
            "학술 문서": "Weight uncertainty in neural networkC Blundell, J Cornebise, K Kavukcuoglu, D Wierstra - International conference on machine learning, 20153456회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Weight uncertainty in neural network",
        "year": null
    },
    "Convolutional networks and applications in vision": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/5/30",
            "게시자": "IEEE",
            "설명": "Intelligent tasks, such as visual perception, auditory perception, and language understanding require the construction of good internal representations of the world (or \"features\")? which must be invariant to irrelevant variations of the input while, preserving relevant information. A major question for Machine Learning is how to learn such good features automatically. Convolutional Networks (ConvNets) are a biologically-inspired trainable architecture that can learn invariant features. Each stage in a ConvNets is composed of a filter bank, some nonlinearities, and feature pooling layers. With multiple stages, a ConvNet can learn multi-level hierarchies of features. While ConvNets have been successfully deployed in many commercial applications from OCR to video surveillance, they require large amounts of labeled training samples. We describe new unsupervised learning algorithms, and new non-linear stages that …",
            "저자": "Yann LeCun, Koray Kavukcuoglu, Clément Farabet",
            "전체 인용횟수": "2795회 인용201120122013201420152016201720182019202020212022202318324579122206301326348360344336236",
            "컨퍼런스": "Proceedings of 2010 IEEE international symposium on circuits and systems",
            "페이지": "253-256",
            "학술 문서": "Convolutional networks and applications in visionY LeCun, K Kavukcuoglu, C Farabet - Proceedings of 2010 IEEE international symposium on …, 20102795회 인용 관련 학술자료 전체 28개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Convolutional networks and applications in vision",
        "year": null
    },
    "Neural discrete representation learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "권": "30",
            "설명": "Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of``posterior collapse''-—where the latents are ignored when they are paired with a powerful autoregressive decoder-—typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.",
            "저널": "Advances in neural information processing systems",
            "저자": "Aaron Van Den Oord, Oriol Vinyals",
            "전체 인용횟수": "2763회 인용201820192020202120222023641473184157501049",
            "학술 문서": "Neural discrete representation learningA Van Den Oord, O Vinyals - Advances in neural information processing systems, 20172763회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Neural discrete representation learning",
        "year": null
    },
    "Improved protein structure prediction using potentials from deep learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020/1/30",
            "게시자": "Nature Publishing Group UK",
            "권": "577",
            "설명": "Protein structure prediction can be used to determine the three-dimensional shape of a protein from its amino acid sequence. This problem is of fundamental importance as the structure of a protein largely determines its function; however, protein structures can be difficult to determine experimentally. Considerable progress has recently been made by leveraging genetic information. It is possible to infer which amino acid residues are in contact by analysing covariation in homologous sequences, which aids in the prediction of protein structures. Here we show that we can train a neural network to make accurate predictions of the distances between pairs of residues, which convey more information about the structure than contact predictions. Using this information, we construct a potential of mean force that can accurately describe the shape of a protein. We find that the resulting potential can be optimized by a simple …",
            "저널": "Nature",
            "저자": "Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Žídek, Alexander WR Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David T Jones, David Silver, Koray Kavukcuoglu, Demis Hassabis",
            "전체 인용횟수": "2692회 인용2020202120222023271931842607",
            "페이지": "706-710",
            "학술 문서": "Improved protein structure prediction using potentials from deep learningAW Senior, R Evans, J Jumper, J Kirkpatrick, L Sifre… - Nature, 20202692회 인용 관련 학술자료 전체 13개의 버전 ",
            "호": "7792"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Improved protein structure prediction using potentials from deep learning",
        "year": null
    },
    "Pixel recurrent neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/6/11",
            "게시자": "PMLR",
            "설명": "Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.",
            "저자": "Aäron Van Den Oord, Nal Kalchbrenner, Koray Kavukcuoglu",
            "전체 인용횟수": "2480회 인용2016201720182019202020212022202362189304373408400406328",
            "컨퍼런스": "International conference on machine learning",
            "페이지": "1747-1756",
            "학술 문서": "Pixel recurrent neural networksA Van Den Oord, N Kalchbrenner, K Kavukcuoglu - International conference on machine learning, 20162480회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pixel recurrent neural networks",
        "year": null
    },
    "Conditional image generation with pixelcnn decoders": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "권": "29",
            "설명": "This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.",
            "저널": "Advances in neural information processing systems",
            "저자": "Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves",
            "전체 인용횟수": "2403회 인용2016201720182019202020212022202321152261353409426433322",
            "학술 문서": "Conditional image generation with pixelcnn decodersA Van den Oord, N Kalchbrenner, L Espeholt… - Advances in neural information processing systems, 20162403회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Conditional image generation with pixelcnn decoders",
        "year": null
    },
    "Progressive neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/6/15",
            "설명": "Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.",
            "저널": "arXiv preprint arXiv:1606.04671",
            "저자": "Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, Raia Hadsell",
            "전체 인용횟수": "2399회 인용2016201720182019202020212022202316102180250337433528541",
            "학술 문서": "Progressive neural networksAA Rusu, NC Rabinowitz, G Desjardins, H Soyer… - arXiv preprint arXiv:1606.04671, 20162399회 인용 관련 학술자료 전체 5개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Progressive neural networks",
        "year": null
    },
    "The ATLAS simulation infrastructure": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/12",
            "게시자": "Springer-Verlag",
            "권": "70",
            "설명": " The simulation software for the ATLAS Experiment at the Large Hadron Collider is being used for large-scale production of events on the LHC Computing Grid. This simulation requires many components, from the generators that simulate particle collisions, through packages simulating the response of the various detectors and triggers. All of these components come together under the ATLAS simulation infrastructure. In this paper, that infrastructure is discussed, including that supporting the detector description, interfacing the event generation, and combining the GEANT4 simulation of the response of the individual detectors. Also described are the tools allowing the software validation, performance testing, and the validation of the simulated output against known physics processes.",
            "저널": "The European Physical Journal C",
            "저자": "Georges Aad, B Abbott, J Abdallah, AA Abdelalim, Abdelmalek Abdesselam, B Abi, M Abolins, H Abramowicz, H Abreu, BS Acharya, DL Adams, TN Addy, J Adelman, C Adorisio, P Adragna, T Adye, S Aefsky, JA Aguilar-Saavedra, M Aharrouche, SP Ahlen, F Ahles, A Ahmad, H Ahmed, M Ahsan, G Aielli, T Akdogan, TPA Åkesson, G Akimoto, AV Akimov, A Aktas, MS Alam, MA Alam, S Albrand, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, M Aliev, G Alimonti, J Alison, M Aliyev, PP Allport, SE Allwood-Spiers, J Almond, A Aloisio, R Alon, Alejandro Alonso, MG Alviggi, K Amako, C Amelung, A Amorim, G Amoros, N Amram, C Anastopoulos, T Andeen, CF Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, A Angerami, F Anghinolfi, N Anjos, A Annovi, A Antonaki, M Antonelli, S Antonelli, J Antos, B Antunovic, F Anulli, S Aoun, G Arabidze, I Aracena, Y Arai, ATH Arce, JP Archambault, S Arfaoui, J-F Arguin, T Argyropoulos, M Arik, AJ Armbruster, O Arnaez, C Arnault, A Artamonov, D Arutinov, M Asai, S Asai, R Asfandiyarov, S Ask, B Åsman, D Asner, L Asquith, K Assamagan, A Astbury, A Astvatsatourov, G Atoian, B Auerbach, K Augsten, M Aurousseau, N Austin, G Avolio, R Avramidou, D Axen, C Ay, G Azuelos, Y Azuma, MA Baak, AM Bach, H Bachacou, K Bachas, M Backes, E Badescu, P Bagnaia, Y Bai, T Bain, JT Baines, OK Baker, MD Baker, S Baker, F Baltasar Dos Santos Pedrosa, E Banas, P Banerjee, S Banerjee, D Banfi, A Bangert, V Bansal, SP Baranov, S Baranov, A Barashkou, T Barber, EL Barberio, D Barberis, M Barbero, DY Bardin, T Barillari, M Barisonzi, T Barklow, N Barlow, BM Barnett, RM Barnett, A Baroncelli, AJ Barr, F Barreiro, J Barreiro Guimaraes da Costa, P Barrillon, R Bartoldus, D Bartsch",
            "전체 인용횟수": "8481회 인용20102011201220132014201520162017201820192020202120222023233517295806459231005929952684482427326406",
            "페이지": "823-874",
            "학술 문서": "The ATLAS simulation infrastructureATLAS Collaboration atlas. secretariat@ cern. ch… - The European Physical Journal C, 20108481회 인용 관련 학술자료 전체 79개의 버전 The ATLAS simulation infrastructure*ATLAS Collaboration - 2010관련 학술자료 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The ATLAS simulation infrastructure",
        "year": null
    },
    "Luminosity determination in pp collisions at  TeV using the ATLAS detector at the LHC": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/4",
            "게시자": "Springer-Verlag",
            "권": "71",
            "설명": " Measurements of luminosity obtained using the ATLAS detector during early running of the Large Hadron Collider (LHC) at  TeV are presented. The luminosity is independently determined using several detectors and multiple algorithms, each having different acceptances, systematic uncertainties and sensitivity to background. The ratios of the luminosities obtained from these methods are monitored as a function of time and of μ, the average number of inelastic interactions per bunch crossing. Residual time- and μ-dependence between the methods is less than 2% for 0<μ<2.5. Absolute luminosity calibrations, performed using beam separation scans, have a common systematic uncertainty of ±11%, dominated by the measurement of the LHC beam currents. After calibration, the luminosities obtained from the different methods differ by at most ±2%. The visible cross sections measured using the beam …",
            "저널": "The European Physical Journal C",
            "저자": "Georges Aad, Brad Abbott, Jalal Abdallah, AA Abdelalim, Abdelmalek Abdesselam, B Abi, M Abolins, H Abramowicz, H Abreu, E Acerbi, BS Acharya, M Ackers, DL Adams, TN Addy, J Adelman, M Aderholz, S Adomeit, P Adragna, T Adye, S Aefsky, JA Aguilar-Saavedra, M Aharrouche, SP Ahlen, F Ahles, A Ahmad, H Ahmed, M Ahsan, G Aielli, T Akdogan, TPA Åkesson, G Akimoto, AV Akimov, MS Alam, MA Alam, S Albrand, M Aleksa, IN Aleksandrov, M Aleppo, F Alessandria, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, M Aliev, G Alimonti, J Alison, M Aliyev, PP Allport, SE Allwood-Spiers, J Almond, A Aloisio, R Alon, A Alonso, J Alonso, MG Alviggi, K Amako, P Amaral, C Amelung, VV Ammosov, A Amorim, G Amoros, N Amram, C Anastopoulos, T Andeen, CF Anders, KJ Anderson, A Andreazza, V Andrei, M-L Andrieux, XS Anduaga, A Angerami, F Anghinolfi, N Anjos, A Annovi, A Antonaki, M Antonelli, S Antonelli, J Antos, F Anulli, S Aoun, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, ATH Arce, JP Archambault, S Arfaoui, J-F Arguin, E Arik, M Arik, AJ Armbruster, KE Arms, SR Armstrong, O Arnaez, C Arnault, A Artamonov, G Artoni, D Arutinov, S Asai, R Asfandiyarov, S Ask, B Åsman, L Asquith, K Assamagan, A Astbury, A Astvatsatourov, G Atoian, B Aubert, B Auerbach, E Auge, K Augsten, M Aurousseau, N Austin, R Avramidou, D Axen, C Ay, G Azuelos, Y Azuma, MA Baak, G Baccaglioni, C Bacci, AM Bach, H Bachacou, K Bachas, G Bachy, M Backes, E Badescu, P Bagnaia, S Bahinipati, Y Bai, DC Bailey, T Bain, JT Baines, OK Baker, S Baker, F Baltasar Dos Santos Pedrosa, E Banas, P Banerjee, Sw Banerjee, D Banfi, A Bangert, V Bansal, HS Bansil, L Barak, SP Baranov, A Barashkou, A Barbaro Galtieri, T Barber",
            "전체 인용횟수": "5012회 인용201120122013201420152016201720182019202020212022202319365937410794195613713540451389300369",
            "페이지": "1-37",
            "학술 문서": "Luminosity determination in pp collisions at s s= 8 TeV using the ATLAS detector at the LHC*M Aaboud, G Aad, B Abbott, J Abdallah, O Abdinov… - The European Physical Journal C, 20163001회 인용 관련 학술자료 전체 84개의 버전 Luminosity determination in pp collisions at s=7 TeV using the ATLAS detector at the LHCATLAS Collaboration atlas. secretariat@ cern. ch… - The European Physical Journal C, 20112367회 인용 관련 학술자료 전체 81개의 버전 Luminosity determination in pp collisions at root s= 8 TeV using the ATLAS detector at the LHC*M Aaboud, G Aad, B Abbott, J Abdallah… - European Physical Journal C, 20162회 인용 G. Aad et al.,“Luminosity Determination in pp Collisions at√(s)= 7 TeV Using the ATLAS Detector at the LHC”*ATLAS Collaboration - European Journal of Physics (submitted)2회 인용 관련 학술자료 Luminosity determination in pp collisions at√ s= 7 TeV using the ATLAS detector at the LHCMS Alam, J Ernst, V Rojo, H Ahmed, S Bahinipati… - The European Physical Journal C-Particles and Fields, 20111회 인용 관련 학술자료 전체 2개의 버전 Luminosity determination in pp collisions at $$\\sqrt {s}= 13$$ TeV using the ATLAS detector at the LHC*G Aad, B Abbott, K Abeling, SH Abidi, A Aboulhorma… - The European Physical Journal C, 2023전체 13개의 버전 Luminosity determination in pp collisions at√ s= 7 TeV using the ATLAS detector at the LHC*XS Anduaga, MT Dova, FG Monticelli, MF Tripiana… - The European Physical Journal C, 2011관련 학술자료 전체 4개의 버전 Luminosity Determination in pp Collisions at sqrt (s)= 7 TeV Using the ATLAS Detector at the LHCV Mitsou, A Ferrer Soria, JA Valls Ferrer… - European Physical Journal C, 2011, vol. 71, num. 4 …, 2011관련 학술자료 전체 5개의 버전 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Luminosity determination in pp collisions at  TeV using the ATLAS detector at the LHC",
        "year": null
    },
    "Performance of the ATLAS trigger system in 2015": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/5/1",
            "게시자": "Springer Berlin Heidelberg",
            "권": "77",
            "설명": "During 2015 the ATLAS experiment recorded of proton–proton collision data at a centre-of-mass energy of. The ATLAS trigger system is a crucial component of the experiment, responsible for selecting events of interest at a recording rate of approximately 1 kHz from up to 40 MHz of collisions. This paper presents a short overview of the changes to the trigger and data acquisition systems during the first long shutdown of the LHC and shows the performance of the trigger system and its components based on the 2015 proton–proton collision data.",
            "저널": "The European Physical Journal C",
            "저자": "Morad Aaboud, Georges Aad, Brad Abbott, Jalal Abdallah, B Abeloos, R Aben, OS AbouZeid, NL Abraham, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, S Adachi, L Adamczyk, DL Adams, J Adelman, S Adomeit, T Adye, AA Affolder, T Agatonovic-Jovin, JA Aguilar-Saavedra, SP Ahlen, F Ahmadov, G Aielli, H Akerstedt, TPA Åkesson, AV Akimov, GL Alberghi, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, T Alexopoulos, M Alhroob, B Ali, M Aliev, G Alimonti, J Alison, SP Alkire, BMM Allbrooke, BW Allen, PP Allport, A Aloisio, A Alonso, F Alonso, C Alpigiani, AA Alshehri, M Alstaty, B Alvarez Gonzalez, D Álvarez Piqueras, MG Alviggi, BT Amadio, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, JK Anders, KJ Anderson, A Andreazza, V Andrei, S Angelidakis, I Angelozzi, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, C Antel, M Antonelli, A Antonov, DJ Antrim, F Anulli, M Aoki, L Aperio Bella, G Arabidze, Y Arai, JP Araque, ATH Arce, FA Arduh, J-F Arguin, S Argyropoulos, M Arik, AJ Armbruster, LJ Armitage, O Arnaez, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Artz, S Asai, N Asbah, A Ashkenazi, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, K Augsten, G Avolio, B Axen, MK Ayoub, G Azuelos, MA Baak, AE Baas, MJ Baca, H Bachacou, K Bachas, M Backes, M Backhaus, P Bagiacchi, P Bagnaia, Y Bai, JT Baines, M Bajic, OK Baker, EM Baldin, P Balek, T Balestri, F Balli, WK Balunas, E Banas, Sw Banerjee, AAE Bannoura, L Barak, EL Barberio, D Barberis, M Barbero, T Barillari, M-S Barisits, T Barklow, N Barlow, SL Barnes, BM Barnett, RM Barnett",
            "전체 인용횟수": "4401회 인용20112012201320142015201620172018201920202021202220231296107137235361517837674488348265317",
            "페이지": "317",
            "학술 문서": "Performance of the ATLAS trigger system in 2015M Aaboud, G Aad, B Abbott, J Abdallah, B Abeloos… - The European Physical Journal C, 20174392회 인용 관련 학술자료 전체 120개의 버전 Performance of the ATLAS Trigger System in 2015M Aaboud, G Aad, B Abbott, J Abdallah, B Abcloos… - … PHYSICAL JOURNAL. C, PARTICLES AND FIELDS, 201711회 인용 관련 학술자료 Performance of the ATLAS trigger system in 2015*A Collaboration - Eur. Phys. J. C, 2017관련 학술자료 전체 3개의 버전 ",
            "호": "5"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Performance of the ATLAS trigger system in 2015",
        "year": null
    },
    "Luminosity determination in pp collisions at                                           s = 8 TeV using the ATLAS detector at the LHC": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/12",
            "게시자": "Springer Berlin Heidelberg",
            "권": "76",
            "설명": " The luminosity determination for the ATLAS detector at the LHC during pp collisions at  8 TeV in 2012 is presented. The evaluation of the luminosity scale is performed using several luminometers, and comparisons between these luminosity detectors are made to assess the accuracy, consistency and long-term stability of the results. A luminosity uncertainty of  is obtained for the  of pp collision data delivered to ATLAS at  8 TeV in 2012.",
            "저널": "The European Physical Journal C",
            "저자": "Morad Aaboud, G Aad, B Abbott, J Abdallah, B Abeloos, R Aben, OS AbouZeid, NL Abraham, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, J Adelman, S Adomeit, T Adye, AA Affolder, T Agatonovic-Jovin, JA Aguilar-Saavedra, SP Ahlen, F Ahmadov, G Aielli, H Akerstedt, TPA Åkesson, AV Akimov, GL Alberghi, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, T Alexopoulos, M Alhroob, M Aliev, G Alimonti, J Alison, SP Alkire, BMM Allbrooke, BW Allen, PP Allport, A Aloisio, A Alonso, F Alonso, C Alpigiani, M Alstaty, B Alvarez Gonzalez, D Álvarez Piqueras, MG Alviggi, BT Amadio, K Amako, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, JK Anders, KJ Anderson, A Andreazza, V Andrei, S Angelidakis, I Angelozzi, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, M Antonelli, A Antonov, F Anulli, M Aoki, L Aperio Bella, G Arabidze, Y Arai, JP Araque, ATH Arce, FA Arduh, J-F Arguin, S Argyropoulos, M Arik, AJ Armbruster, LJ Armitage, O Arnaez, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Artz, S Asai, N Asbah, A Ashkenazi, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, K Augsten, G Avolio, B Axen, MK Ayoub, G Azuelos, MA Baak, AE Baas, MJ Baca, H Bachacou, K Bachas, M Backes, M Backhaus, P Bagiacchi, P Bagnaia, Y Bai, JT Baines, OK Baker, EM Baldin, P Balek, T Balestri, F Balli, WK Balunas, E Banas, Sw Banerjee, AAE Bannoura, L Barak, EL Barberio, D Barberis, M Barbero, T Barillari, T Barklow, N Barlow, SL Barnes, BM Barnett, RM Barnett, Z Barnovska, A Baroncelli, G Barone, AJ Barr, L Barranco Navarro",
            "전체 인용횟수": "3003회 인용20162017201820192020202120222023102597693504286259213265",
            "페이지": "1-45",
            "학술 문서": "Luminosity determination in pp collisions at s s= 8 TeV using the ATLAS detector at the LHCM Aaboud, G Aad, B Abbott, J Abdallah, O Abdinov… - The European Physical Journal C, 20163001회 인용 관련 학술자료 전체 84개의 버전 Luminosity determination in pp collisions at root s= 8 TeV using the ATLAS detector at the LHCM Aaboud, G Aad, B Abbott, J Abdallah… - European Physical Journal C, 20162회 인용 ",
            "호": "12"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Luminosity determination in pp collisions at                                           s = 8 TeV using the ATLAS detector at the LHC",
        "year": null
    },
    "Topological cell clustering in the ATLAS calorimeters and its performance in LHC Run 1": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/7/24",
            "게시자": "SpringerOpen",
            "설명": "The reconstruction of the signal from hadrons and jets emerging from the proton–proton collisions at the Large Hadron Collider (LHC) and entering the ATLAS calorimeters is based on a three-dimensional topological clustering of individual calorimeter cell signals. The cluster formation follows cell signal-significance patterns generated by electromagnetic and hadronic showers. In this, the clustering algorithm implicitly performs a topological noise suppression by removing cells with insignificant signals which are not in close proximity to cells with significant signals. The resulting topological cell clusters have shape and location information, which is exploited to apply a local energy calibration and corrections depending on the nature of the cluster. Topological cell clustering is established as a wellperforming calorimeter signal definition for jet and missing transverse momentum reconstruction in ATLAS.",
            "저자": "Atlas Collaboration",
            "전체 인용횟수": "2595회 인용2016201720182019202020212022202398260637474407309188211",
            "학술 문서": "Topological cell clustering in the ATLAS calorimeters and its performance in LHC Run 1Atlas Collaboration - 20172424회 인용 관련 학술자료 전체 62개의 버전 Topological cell clustering in the ATLAS calorimeters and its performance in LHC Run 1G Aad, B Abbott, J Abdallah, R Aben, M Abolins… - The European Physical Journal C, 2017201회 인용 관련 학술자료 전체 61개의 버전 Topological cell clustering in the ATLAS calorimeters and its performance in LHC RunA Collaboration - Eur. Phys. J. C, 20172회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Topological cell clustering in the ATLAS calorimeters and its performance in LHC Run 1",
        "year": null
    },
    "Combined Measurement of the Higgs Boson Mass in  Collisions at  and 8 TeV with the ATLAS and CMS Experiments": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/5/14",
            "게시자": "American Physical Society",
            "권": "114",
            "설명": "A measurement of the Higgs boson mass is presented based on the combined data samples of the ATLAS and CMS experiments at the CERN LHC in the  and  decay channels. The results are obtained from a simultaneous fit to the reconstructed invariant mass peaks in the two channels and for the two experiments. The measured masses from the individual channels and the two experiments are found to be consistent among themselves. The combined measured mass of the Higgs boson is .",
            "저널": "Physical review letters",
            "저자": "Georges Aad, Brad Abbott, Jalal Abdallah, R Aben, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, J Adelman, S Adomeit, T Adye, AA Affolder, T Agatonovic-Jovin, JA Aguilar-Saavedra, SP Ahlen, F Ahmadov, G Aielli, H Akerstedt, TPA Åkesson, G Akimoto, AV Akimov, GL Alberghi, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, T Alexopoulos, M Alhroob, G Alimonti, L Alio, J Alison, SP Alkire, BMM Allbrooke, PP Allport, A Aloisio, A Alonso, F Alonso, C Alpigiani, A Altheimer, B Alvarez Gonzalez, D Álvarez Piqueras, MG Alviggi, BT Amadio, K Amako, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, JK Anders, KJ Anderson, A Andreazza, V Andrei, S Angelidakis, I Angelozzi, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, G Arabidze, Y Arai, JP Araque, ATH Arce, FA Arduh, JF Arguin, S Argyropoulos, M Arik, AJ Armbruster, O Arnaez, V Arnal, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, A Ashkenazi, B Åsman, Lily Asquith, Ketevi Assamagan, Robert Astalos, Markus Atkinson, Naim Bora Atlay, Benjamin Auerbach, Kamil Augsten, Mathieu Aurousseau, Giuseppe Avolio, Bradley Axen, Mohamad Kassem Ayoub, Georges Azuelos, MA Baak, AE Baas, Cesare Bacci, Henri Bachacou, Konstantinos Bachas, Moritz Backes, Malte Backhaus, Elisabeta Badescu, Paolo Bagiacchi, Paolo Bagnaia, Yu Bai, Travis Bain, JT Baines, Oliver Keith Baker, Petr Balek, Thomas Balestri, Fabrice Balli, Elzbieta Banas, Sw Banerjee, Arwa AE Bannoura, Hardeep Singh Bansil, Liron Barak, SP Baranov, Elisabetta Luigia Barberio, Dario Barberis, Marlon Barbero, Teresa Barillari, Marcello Barisonzi, Timothy Barklow, Nick Barlow, Sarah Louise Barnes, BM Barnett, RM Barnett, Zuzana Barnovska",
            "전체 인용횟수": "2572회 인용20152016201720182019202020212022202323047247246730321316712985",
            "페이지": "191803",
            "학술 문서": "Combined Measurement of the Higgs Boson Mass in $ pp $ Collisions at $\\sqrt {s}= 7$ and 8 TeV with the ATLAS and CMS ExperimentsCMS Collaborations - arXiv preprint arXiv:1503.07589, 20151164회 인용 관련 학술자료 전체 50개의 버전 Combined Measurement of the Higgs Boson Mass in p p Collisions at s= 7 and 8 TeV with the ATLAS and CMS ExperimentsG Aad, B Abbott, J Abdallah, R Aben, M Abolins… - Physical review letters, 2015939회 인용 관련 학술자료 전체 157개의 버전 ATLAS and CMS collaborations*G Aad - Phys. Rev. Lett, 2015770회 인용 관련 학술자료 ",
            "호": "19"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Combined Measurement of the Higgs Boson Mass in  Collisions at  and 8 TeV with the ATLAS and CMS Experiments",
        "year": null
    },
    "Electron efficiency measurements with the ATLAS detector using 2012 LHC proton–proton collision data": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/3",
            "게시자": "Springer Berlin Heidelberg",
            "권": "77",
            "설명": " This paper describes the algorithms for the reconstruction and identification of electrons in the central region of the ATLAS detector at the Large Hadron Collider (LHC). These algorithms were used for all ATLAS results with electrons in the final state that are based on the 2012 pp collision data produced by the LHC at  = 8 . The efficiency of these algorithms, together with the charge misidentification rate, is measured in data and evaluated in simulated samples using electrons from ,  and  decays. For these efficiency measurements, the full recorded data set, corresponding to an integrated luminosity of 20.3 fb, is used. Based on a new reconstruction algorithm used in 2012, the electron reconstruction efficiency is 97% for electrons with   and 99% at  . Combining this with the efficiency of additional selection criteria to reject electrons from background …",
            "저널": "The European Physical Journal C",
            "저자": "Morad Aaboud, G Aad, B Abbott, J Abdallah, B Abeloos, OS AbouZeid, NL Abraham, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, S Adachi, L Adamczyk, DL Adams, J Adelman, S Adomeit, T Adye, AA Affolder, T Agatonovic-Jovin, JA Aguilar-Saavedra, SP Ahlen, F Ahmadov, G Aielli, H Akerstedt, TPA Åkesson, AV Akimov, GL Alberghi, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, T Alexopoulos, M Alhroob, B Ali, M Aliev, G Alimonti, J Alison, SP Alkire, BMM Allbrooke, BW Allen, PP Allport, A Aloisio, A Alonso, F Alonso, C Alpigiani, AA Alshehri, M Alstaty, B Alvarez Gonzalez, D Álvarez Piqueras, MG Alviggi, BT Amadio, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, JK Anders, KJ Anderson, A Andreazza, V Andrei, S Angelidakis, I Angelozzi, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, C Antel, M Antonelli, A Antonov, DJ Antrim, F Anulli, M Aoki, L Aperio Bella, G Arabidze, Y Arai, JP Araque, ATH Arce, FA Arduh, J-F Arguin, S Argyropoulos, M Arik, AJ Armbruster, LJ Armitage, O Arnaez, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Artz, S Asai, N Asbah, A Ashkenazi, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, K Augsten, G Avolio, B Axen, MK Ayoub, G Azuelos, MA Baak, AE Baas, MJ Baca, H Bachacou, K Bachas, M Backes, M Backhaus, P Bagiacchi, P Bagnaia, Y Bai, JT Baines, M Bajic, OK Baker, EM Baldin, P Balek, T Balestri, F Balli, WK Balunas, E Banas, Sw Banerjee, AAE Bannoura, L Barak, EL Barberio, D Barberis, M Barbero, T Barillari, M-S Barisits, T Barklow, N Barlow, SL Barnes, BM Barnett, RM Barnett, Z Barnovska-Blenessy, A Baroncelli",
            "전체 인용횟수": "2500회 인용20162017201820192020202120222023355599692349108714439",
            "페이지": "1-45",
            "학술 문서": "Electron efficiency measurements with the ATLAS detector using 2012 LHC proton–proton collision dataM Aaboud, G Aad, B Abbott, J Abdallah, O Abdinov… - The European Physical Journal C, 20172499회 인용 관련 학술자료 전체 119개의 버전 Electron efficiency measurements with the ATLAS detector using 2012 LHC proton–proton collision dataH Arnold - 20171회 인용 관련 학술자료 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Electron efficiency measurements with the ATLAS detector using 2012 LHC proton–proton collision data",
        "year": null
    },
    "Performance of missing transverse momentum reconstruction in proton-proton collisions at  with ATLAS": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/1",
            "게시자": "Springer-Verlag",
            "권": "72",
            "설명": " The measurement of missing transverse momentum in the ATLAS detector, described in this paper, makes use of the full event reconstruction and a calibration based on reconstructed physics objects. The performance of the missing transverse momentum reconstruction is evaluated using data collected in pp collisions at a centre-of-mass energy of 7 TeV in 2010. Minimum bias events and events with jets of hadrons are used from data samples corresponding to an integrated luminosity of about 0.3 nb−1 and 600 nb−1 respectively, together with events containing a Z boson decaying to two leptons (electrons or muons) or a W boson decaying to a lepton (electron or muon) and a neutrino, from a data sample corresponding to an integrated luminosity of about 36 pb−1. An estimate of the systematic uncertainty on the missing transverse momentum scale is presented.",
            "저널": "The European Physical Journal C",
            "저자": "Georges Aad, B Abbott, J Abdallah, AA Abdelalim, A Abdesselam, B Abi, M Abolins, H Abramowicz, H Abreu, E Acerbi, BS Acharya, DL Adams, TN Addy, J Adelman, M Aderholz, S Adomeit, P Adragna, T Adye, S Aefsky, JA Aguilar-Saavedra, M Aharrouche, SP Ahlen, F Ahles, A Ahmad, M Ahsan, G Aielli, T Akdogan, TPA Åkesson, G Akimoto, AV Akimov, A Akiyama, MS Alam, MA Alam, J Albert, S Albrand, M Aleksa, IN Aleksandrov, F Alessandria, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, M Aliev, G Alimonti, J Alison, M Aliyev, PP Allport, SE Allwood-Spiers, J Almond, A Aloisio, R Alon, A Alonso, MG Alviggi, K Amako, P Amaral, C Amelung, VV Ammosov, A Amorim, G Amorós, N Amram, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, ML Andrieux, XS Anduaga, A Angerami, F Anghinolfi, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, S Aoun, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, ATH Arce, JP Archambault, S Arfaoui, JF Arguin, E Arik, M Arik, AJ Armbruster, O Arnaez, C Arnault, A Artamonov, G Artoni, D Arutinov, S Asai, R Asfandiyarov, S Ask, B Åsman, L Asquith, K Assamagan, A Astbury, A Astvatsatourov, G Atoian, B Aubert, B Auerbach, E Auge, K Augsten, M Aurousseau, N Austin, G Avolio, R Avramidou, D Axen, C Ay, G Azuelos, Y Azuma, MA Baak, G Baccaglioni, C Bacci, AM Bach, H Bachacou, K Bachas, G Bachy, M Backes, M Backhaus, E Badescu, P Bagnaia, S Bahinipati, Y Bai, DC Bailey, T Bain, JT Baines, OK Baker, MD Baker, S Baker, E Banas, P Banerjee, Sw Banerjee, D Banfi, A Bangert, V Bansal, HS Bansil, L Barak, SP Baranov, A Barashkou, A Barbaro Galtieri",
            "전체 인용횟수": "2383회 인용201120122013201420152016201720182019202020212022202319416316366485347241793818162312",
            "페이지": "1-35",
            "학술 문서": "Performance of missing transverse momentum reconstruction in proton-proton collisions at 7 TeV with ATLAS*ATLAS collaboration - arXiv preprint arXiv:1108.5602, 20111988회 인용 관련 학술자료 전체 16개의 버전 Electron performance measurements with the ATLAS detector using the 2010 LHC proton-proton collision data*ATLAS Collaboration atlas. publications@ cern. ch… - The European Physical Journal C, 2012317회 인용 관련 학술자료 전체 76개의 버전 Performance of missing transverse momentum reconstruction in proton-proton collisions at s=7~TeV with ATLASATLAS Collaboration atlas. publications@ cern. ch… - The European Physical Journal C, 2012230회 인용 관련 학술자료 전체 71개의 버전 Performance of missing transverse momentum reconstruction in proton-proton collisions at√ s= 7 TeV with ATLAS*G Aad, B Abbott, J Abdallah, AA Abdelalim… - 2012Performance of missing transverse momentum reconstruction in proton-proton collisions at√ s= 7 TeV with atlasMS Alam, J Ernst, V Rojo, S Bahinipati, NJ Buchanan… - The European Physical Journal C-Particles and Fields, 2012전체 2개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Performance of missing transverse momentum reconstruction in proton-proton collisions at  with ATLAS",
        "year": null
    },
    "Electron performance measurements with the ATLAS detector using the 2010 LHC proton-proton collision data": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/3",
            "게시자": "Springer-Verlag",
            "권": "72",
            "설명": "Detailed measurements of the electron performance of the ATLAS detector at the LHC are reported, using decays of the Z, W and J/psi particles. Data collected in 2010 at sqrt(s)=7 TeV are used, corresponding to an integrated luminosity of almost 40 pb^-1. The inter-alignment of the inner detector and the electromagnetic calorimeter, the determination of the electron energy scale and resolution, and the performance in terms of response uniformity and linearity are discussed. The electron identification, reconstruction and trigger efficiencies, as well as the charge misidentification probability, are also presented.",
            "저널": "The European Physical Journal C",
            "저자": "Georges Aad, B Abbott, J Abdallah, AA Abdelalim, A Abdesselam, B Abi, M Abolins, H Abramowicz, H Abreu, E Acerbi, BS Acharya, DL Adams, TN Addy, J Adelman, M Aderholz, S Adomeit, P Adragna, T Adye, S Aefsky, JA Aguilar-Saavedra, M Aharrouche, SP Ahlen, F Ahles, A Ahmad, M Ahsan, G Aielli, T Akdogan, TPA Åkesson, G Akimoto, AV Akimov, A Akiyama, MS Alam, MA Alam, J Albert, S Albrand, M Aleksa, IN Aleksandrov, F Alessandria, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, M Aliev, G Alimonti, J Alison, M Aliyev, PP Allport, SE Allwood-Spiers, J Almond, A Aloisio, R Alon, A Alonso, MG Alviggi, K Amako, P Amaral, C Amelung, VV Ammosov, A Amorim, G Amorós, N Amram, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, ML Andrieux, XS Anduaga, A Angerami, F Anghinolfi, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, S Aoun, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, ATH Arce, JP Archambault, S Arfaoui, JF Arguin, E Arik, M Arik, AJ Armbruster, O Arnaez, C Arnault, A Artamonov, G Artoni, D Arutinov, S Asai, R Asfandiyarov, S Ask, B Åsman, L Asquith, K Assamagan, A Astbury, A Astvatsatourov, G Atoian, B Aubert, B Auerbach, E Auge, K Augsten, M Aurousseau, N Austin, G Avolio, R Avramidou, D Axen, C Ay, G Azuelos, Y Azuma, MA Baak, G Baccaglioni, C Bacci, AM Bach, H Bachacou, K Bachas, G Bachy, M Backes, M Backhaus, E Badescu, P Bagnaia, S Bahinipati, Y Bai, DC Bailey, T Bain, JT Baines, OK Baker, MD Baker, S Baker, E Banas, P Banerjee, Sw Banerjee, D Banfi, A Bangert, V Bansal, HS Bansil, L Barak, SP Baranov, A Barashkou, A Barbaro Galtieri",
            "전체 인용횟수": "2109회 인용2011201220132014201520162017201820192020202120222023315014643862432091224451237155",
            "페이지": "1-46",
            "학술 문서": "Electron performance measurements with the ATLAS detector using the 2010 LHC proton-proton collision dataATLAS collaboration - arXiv preprint arXiv:1110.3174, 20111815회 인용 관련 학술자료 전체 15개의 버전 Electron performance measurements with the ATLAS detector using the 2010 LHC proton-proton collision dataATLAS Collaboration atlas. publications@ cern. ch… - The European Physical Journal C, 2012317회 인용 관련 학술자료 전체 76개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Electron performance measurements with the ATLAS detector using the 2010 LHC proton-proton collision data",
        "year": null
    },
    "Jet energy scale measurements and their systematic uncertainties in proton-proton collisions at  with the ATLAS detector": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/10/13",
            "게시자": "American Physical Society",
            "권": "96",
            "설명": "Jet energy scale measurements and their systematic uncertainties are reported for jets measured with the ATLAS detector using proton-proton collision data with a center-of-mass energy of  TeV, corresponding to an integrated luminosity of 3.2 fb collected during 2015 at the LHC. Jets are reconstructed from energy deposits forming topological clusters of calorimeter cells, using the anti- algorithm with radius parameter . Jets are calibrated with a series of simulation-based corrections and in situ techniques. In situ techniques exploit the transverse momentum balance between a jet and a reference object such as a photon,  boson, or multijet system for jets with  GeV and pseudorapidities of , using both data and simulation. An uncertainty in the jet energy scale of less than 1% is found in the central calorimeter region () for jets with  GeV. An uncertainty of about 4.5% is found for low- jets with  GeV in the central region, dominated by uncertainties in the corrections for multiple proton-proton interactions. The calibration of forward jets () is derived from dijet  balance measurements. For jets of  GeV, the additional uncertainty for the forward jet calibration reaches its largest value of about 2% in the range  and in a narrow slice of .",
            "저널": "Physical Review D",
            "저자": "Morad Aaboud, Georges Aad, B Abbott, J Abdallah, B Abeloos, SH Abidi, OS AbouZeid, NL Abraham, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, S Adachi, L Adamczyk, J Adelman, M Adersberger, T Adye, AA Affolder, T Agatonovic-Jovin, C Agheorghiesei, JA Aguilar-Saavedra, SP Ahlen, F Ahmadov, G Aielli, S Akatsuka, H Akerstedt, TPA Åkesson, AV Akimov, GL Alberghi, J Albert, P Albicocco, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, T Alexopoulos, M Alhroob, B Ali, M Aliev, G Alimonti, J Alison, SP Alkire, BMM Allbrooke, BW Allen, PP Allport, A Aloisio, A Alonso, F Alonso, C Alpigiani, AA Alshehri, M Alstaty, B Alvarez Gonzalez, D Álvarez Piqueras, MG Alviggi, BT Amadio, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, JK Anders, KJ Anderson, A Andreazza, V Andrei, S Angelidakis, I Angelozzi, A Angerami, AV Anisenkov, N Anjos, A Annovi, C Antel, M Antonelli, A Antonov, DJ Antrim, F Anulli, M Aoki, L Aperio Bella, G Arabidze, Y Arai, JP Araque, V Araujo Ferraz, ATH Arce, RE Ardell, FA Arduh, JF Arguin, S Argyropoulos, M Arik, AJ Armbruster, LJ Armitage, O Arnaez, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Artz, S Asai, N Asbah, A Ashkenazi, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, K Augsten, G Avolio, B Axen, MK Ayoub, G Azuelos, AE Baas, MJ Baca, H Bachacou, K Bachas, M Backes, M Backhaus, P Bagnaia, H Bahrasemani, JT Baines, M Bajic, OK Baker, EM Baldin, P Balek, F Balli, WK Balunas, E Banas, Sw Banerjee, AAE Bannoura, L Barak, EL Barberio, D Barberis, M Barbero, T Barillari, MS Barisits, T Barklow, N Barlow, SL Barnes, BM Barnett, RM Barnett, Z Barnovska-Blenessy, A Baroncelli, G Barone",
            "전체 인용횟수": "2089회 인용201620172018201920202021202220237182507463359241146179",
            "페이지": "072002",
            "학술 문서": "Jet energy scale measurements and their systematic uncertainties in proton-proton collisions at $\\sqrt {s}= 13$ TeV with the ATLAS detectorATLAS collaboration - arXiv preprint arXiv:1703.09665, 20171920회 인용 관련 학술자료 전체 27개의 버전 Jet energy scale measurements and their systematic uncertainties in proton-proton collisions at s= 13 TeV with the ATLAS detectorM Aaboud, G Aad, B Abbott, J Abdallah, B Abeloos… - Physical Review D, 2017217회 인용 관련 학술자료 전체 64개의 버전 Jet energy scale measurements and their systematic uncertainties in proton-proton collisions at root s= 13 TeV with the ATLAS detectorATLA Collaboration - Physical review D, 2017관련 학술자료 전체 8개의 버전 ",
            "호": "7"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Jet energy scale measurements and their systematic uncertainties in proton-proton collisions at  with the ATLAS detector",
        "year": null
    },
    "Jet energy measurement and its systematic uncertainty in proton–proton collisions at                                                        s                        =            7  TeV with the …": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/1",
            "게시자": "Springer Berlin Heidelberg",
            "권": "75",
            "설명": " The jet energy scale (JES) and its systematic uncertainty are determined for jets measured with the ATLAS detector using proton–proton collision data with a centre-of-mass energy of  TeV corresponding to an integrated luminosity of  . Jets are reconstructed from energy deposits forming topological clusters of calorimeter cells using the anti- algorithm with distance parameters  or , and are calibrated using MC simulations. A residual JES correction is applied to account for differences between data and MC simulations. This correction and its systematic uncertainty are estimated using a combination of in situ techniques exploiting the transverse momentum balance between a jet and a reference object such as a photon or a  boson, for  and pseudorapidities . The effect of multiple proton–proton interactions is corrected for, and an uncertainty is evaluated using in situ techniques …",
            "저널": "The European Physical Journal C",
            "저자": "Georges Aad, Tatevik Abajyan, Brad Abbott, J Abdallah, S Abdel Khalek, Rosemarie Aben, Babak Abi, Maris Abolins, OS AbouZeid, Halina Abramowicz, H Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, TN Addy, J Adelman, S Adomeit, T Adye, S Aefsky, T Agatonovic-Jovin, JA Aguilar-Saavedra, M Agustoni, SP Ahlen, A Ahmad, F Ahmadov, G Aielli, TPA Åkesson, G Akimoto, AV Akimov, MA Alam, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, F Alessandria, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, M Aliev, G Alimonti, L Alio, J Alison, BMM Allbrooke, LJ Allison, PP Allport, SE Allwood-Spiers, J Almond, A Aloisio, R Alon, A Alonso, F Alonso, A Altheimer, B Alvarez Gonzalez, MG Alviggi, K Amako, Y Amaral Coutinho, C Amelung, VV Ammosov, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, S Angelidakis, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, ATH Arce, S Arfaoui, JF Arguin, S Argyropoulos, E Arik, M Arik, AJ Armbruster, O Arnaez, V Arnal, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, S Ask, B Åsman, L Asquith, K Assamagan, R Astalos, A Astbury, M Atkinson, NB Atlay, B Auerbach, E Auge, K Augsten, M Aurousseau, G Avolio, G Azuelos, Y Azuma, MA Baak, C Bacci, AM Bach, H Bachacou, K Bachas, M Backes, M Backhaus, J Backus Mayes, E Badescu, P Bagiacchi, P Bagnaia, Y Bai, DC Bailey, T Bain, JT Baines, OK Baker, S Baker, P Balek, F Balli, E Banas, Sw Banerjee, D Banfi, A Bangert, V Bansal, HS Bansil, L Barak",
            "전체 인용횟수": "2063회 인용20142015201620172018201920202021202220237650445945224713052603232",
            "페이지": "1-101",
            "학술 문서": "Jet energy measurement and its systematic uncertainty in proton–proton collisions at s= 7 s= 7 TeV with the ATLAS detectorAtlas Collaboration atlas. publications@ cern. ch… - The European Physical Journal C, 20152063회 인용 관련 학술자료 전체 118개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Jet energy measurement and its systematic uncertainty in proton–proton collisions at                                                        s                        =            7  TeV with the …",
        "year": null
    },
    "Measurement of the muon reconstruction performance of the ATLAS detector using 2011 and 2012 LHC proton–proton collision data": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/11",
            "게시자": "Springer Berlin Heidelberg",
            "권": "74",
            "설명": " This paper presents the performance of the ATLAS muon reconstruction during the LHC run with  collisions at –8 TeV in 2011–2012, focusing mainly on data collected in 2012. Measurements of the reconstruction efficiency and of the momentum scale and resolution, based on large reference samples of ,  and  decays, are presented and compared to Monte Carlo simulations. Corrections to the simulation, to be used in physics analysis, are provided. Over most of the covered phase space (muon  and  GeV) the efficiency is above  and is measured with per-mille precision. The momentum resolution ranges from  at central rapidity and for transverse momentum  GeV, to  at large rapidity and  GeV. The momentum scale is known with an uncertainty of  to  depending on rapidity. A method for the recovery of final state …",
            "저널": "The European Physical Journal C",
            "저자": "ATLAS Collaboration atlas. publications@ cern. ch, Georges Aad, B Abbott, J Abdallah, S Abdel Khalek, O Abdinov, R Aben, B Abi, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, J Adelman, S Adomeit, T Adye, T Agatonovic-Jovin, JA Aguilar-Saavedra, M Agustoni, SP Ahlen, F Ahmadov, G Aielli, H Akerstedt, TPA Åkesson, G Akimoto, AV Akimov, GL Alberghi, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, G Alimonti, L Alio, J Alison, BMM Allbrooke, LJ Allison, PP Allport, J Almond, A Aloisio, A Alonso, F Alonso, C Alpigiani, A Altheimer, B Alvarez Gonzalez, MG Alviggi, K Amako, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, S Angelidakis, I Angelozzi, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, JP Araque, ATH Arce, J-F Arguin, S Argyropoulos, M Arik, AJ Armbruster, O Arnaez, V Arnal, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, A Ashkenazi, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, B Auerbach, K Augsten, M Aurousseau, G Avolio, G Azuelos, Y Azuma, MA Baak, A Baas, C Bacci, H Bachacou, K Bachas, M Backes, M Backhaus, J Backus Mayes, E Badescu, P Bagiacchi, P Bagnaia, Y Bai, T Bain, JT Baines, OK Baker, P Balek, F Balli, E Banas, Sw Banerjee, AAE Bannoura, V Bansal, HS Bansil, L Barak, SP Baranov, EL Barberio, D Barberis, M Barbero, T Barillari, M Barisonzi",
            "전체 인용횟수": "1941회 인용2013201420152016201720182019202020212022202356256051237018310955262819",
            "페이지": "1-34",
            "학술 문서": "Measurement of the muon reconstruction performance of the ATLAS detector using 2011 and 2012 LHC proton–proton collision dataATLAS Collaboration atlas. publications@ cern. ch… - The European Physical Journal C, 20141941회 인용 관련 학술자료 전체 118개의 버전 Measurement of the muon reconstruction performance of the ATLAS detector using 2011 and 2012 LHC proton–proton collision dataMJ Alconada Verzini, F Alonso, XS Anduaga, MT Dova… - The European Physical Journal C, 2014관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Measurement of the muon reconstruction performance of the ATLAS detector using 2011 and 2012 LHC proton–proton collision data",
        "year": null
    },
    "Electron reconstruction and identification efficiency measurements with the ATLAS detector using the 2011 LHC proton–proton collision data": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/7",
            "게시자": "Springer Berlin Heidelberg",
            "권": "74",
            "설명": " Many of the interesting physics processes to be measured at the LHC have a signature involving one or more isolated electrons. The electron reconstruction and identification efficiencies of the ATLAS detector at the LHC have been evaluated using proton–proton collision data collected in 2011 at  TeV and corresponding to an integrated luminosity of 4.7 fb. Tag-and-probe methods using events with leptonic decays of  and  bosons and  mesons are employed to benchmark these performance parameters. The combination of all measurements results in identification efficiencies determined with an accuracy at the few per mil level for electron transverse energy greater than 30 GeV.",
            "저널": "The European Physical Journal C",
            "저자": "ATLAS Collaboration atlas. publications@ cern. ch, Georges Aad, T Abajyan, B Abbott, J Abdallah, S Abdel Khalek, O Abdinov, R Aben, B Abi, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, TN Addy, J Adelman, S Adomeit, T Adye, T Agatonovic-Jovin, JA Aguilar-Saavedra, M Agustoni, SP Ahlen, A Ahmad, F Ahmadov, G Aielli, TPA Åkesson, G Akimoto, AV Akimov, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, G Alimonti, L Alio, J Alison, BMM Allbrooke, LJ Allison, PP Allport, SE Allwood-Spiers, J Almond, A Aloisio, R Alon, A Alonso, F Alonso, C Alpigiani, A Altheimer, B Alvarez Gonzalez, MG Alviggi, K Amako, Y Amaral Coutinho, C Amelung, D Amidei, VV Ammosov, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, S Angelidakis, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, JP Araque, ATH Arce, JF Arguin, S Argyropoulos, M Arik, AJ Armbruster, O Arnaez, V Arnal, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, A Ashkenazi, S Ask, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, B Auerbach, E Auge, K Augsten, M Aurousseau, G Avolio, G Azuelos, Y Azuma, MA Baak, C Bacci, AM Bach, H Bachacou, K Bachas, M Backes, M Backhaus, J Backus Mayes, E Badescu, P Bagiacchi, P Bagnaia, Y Bai, DC Bailey, T Bain, JT Baines, OK Baker, S Baker, P Balek, F Balli, E Banas, Sw Banerjee, D Banfi, A Bangert, AAE Bannoura, V Bansal, HS Bansil, L Barak",
            "전체 인용횟수": "1883회 인용201320142015201620172018201920202021202220238219560430354159901818912",
            "페이지": "1-38",
            "학술 문서": "Electron reconstruction and identification efficiency measurements with the ATLAS detector using the 2011 LHC proton–proton collision dataATLAS Collaboration atlas. publications@ cern. ch… - The European Physical Journal C, 20141881회 인용 관련 학술자료 전체 97개의 버전 Electron reconstruction and identification efficiency measurements with the ATLAS detector using the 2011 LHC proton-proton collision dataG Aad, B Abbott, J Abdallah, S Abdel Khalek, R Aben… - The European physical journal. C, Particles and fields, 20143회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Electron reconstruction and identification efficiency measurements with the ATLAS detector using the 2011 LHC proton–proton collision data",
        "year": null
    },
    "Performance of missing transverse momentum reconstruction with the ATLAS detector using proton–proton collisions at                                                        s …": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/11",
            "게시자": "Springer Berlin Heidelberg",
            "권": "78",
            "설명": "The performance of the missing transverse momentum () reconstruction with the ATLAS detector is evaluated using data collected in proton–proton collisions at the LHC at a centre-of-mass energy of 13 TeV in 2015. To reconstruct, fully calibrated electrons, muons, photons, hadronically decaying, and jets reconstructed from calorimeter energy deposits and charged-particle tracks are used. These are combined with the soft hadronic activity measured by reconstructed charged-particle tracks not associated with the hard objects. Possible double counting of contributions from reconstructed charged-particle tracks from the inner detector, energy deposits in the calorimeter, and reconstructed muons from the muon spectrometer is avoided by applying a signal ambiguity resolution procedure which rejects already used signals when combining the various contributions. The individual terms as well as the overall …",
            "저널": "The European Physical Journal C",
            "저자": "Morad Aaboud, Georges Aad, Brad Abbott, B Abeloos, SH Abidi, OS AbouZeid, NL Abraham, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, S Adachi, L Adamczyk, J Adelman, M Adersberger, T Adye, AA Affolder, Y Afik, T Agatonovic-Jovin, C Agheorghiesei, JA Aguilar-Saavedra, F Ahmadov, G Aielli, S Akatsuka, H Akerstedt, TPA Åkesson, E Akilli, AV Akimov, GL Alberghi, J Albert, P Albicocco, MJ Alconada Verzini, S Alderweireldt, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, T Alexopoulos, M Alhroob, B Ali, G Alimonti, J Alison, SP Alkire, BMM Allbrooke, BW Allen, PP Allport, A Aloisio, A Alonso, F Alonso, C Alpigiani, AA Alshehri, MI Alstaty, B Alvarez Gonzalez, D Álvarez Piqueras, MG Alviggi, BT Amadio, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, S Amoroso, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, JK Anders, KJ Anderson, A Andreazza, V Andrei, S Angelidakis, I Angelozzi, A Angerami, AV Anisenkov, N Anjos, A Annovi, C Antel, M Antonelli, Alexey Antonov, DJA Antrim, F Anulli, M Aoki, L Aperio Bella, G Arabidze, Y Arai, JP Araque, V Araujo Ferraz, ATH Arce, RE Ardell, FA Arduh, JF Arguin, S Argyropoulos, M Arik, AJ Armbruster, LJ Armitage, O Arnaez, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Artz, S Asai, N Asbah, A Ashkenazi, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, K Augsten, G Avolio, B Axen, MK Ayoub, G Azuelos, AE Baas, MJ Baca, H Bachacou, K Bachas, M Backes, P Bagnaia, M Bahmani, H Bahrasemani, JT Baines, M Bajic, OK Baker, EM Baldin, P Balek, F Balli, WK Balunas, E Banas, A Bandyopadhyay, S Banerjee, AAE Bannoura, L Barak, EL Barberio, D Barberis, M Barbero, T Barillari, MS Barisits, J Barkeloo, T Barklow, N Barlow, SL Barnes, BM Barnett, RM Barnett, Z Barnovska-Blenessy, A Baroncelli",
            "전체 인용횟수": "1881회 인용2017201820192020202120222023171333339260254193222",
            "페이지": "1-46",
            "학술 문서": "Performance of missing transverse momentum reconstruction with the ATLAS detector using proton–proton collisions at $$\\sqrt {s}= 13~\\hbox {TeV} $$ s= 13 TeVM Aaboud, G Aad, B Abbott, B Abeloos, SH Abidi… - The European Physical Journal C, 20181881회 인용 관련 학술자료 전체 86개의 버전 ",
            "호": "11"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Performance of missing transverse momentum reconstruction with the ATLAS detector using proton–proton collisions at                                                        s …",
        "year": null
    },
    "Electron and photon energy calibration with the ATLAS detector using LHC Run 1 data": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/10",
            "게시자": "Springer Berlin Heidelberg",
            "권": "74",
            "설명": " This paper presents the electron and photon energy calibration achieved with the ATLAS detector using about 25 fb of LHC proton–proton collision data taken at centre-of-mass energies of  and 8 TeV. The reconstruction of electron and photon energies is optimised using multivariate algorithms. The response of the calorimeter layers is equalised in data and simulation, and the longitudinal profile of the electromagnetic showers is exploited to estimate the passive material in front of the calorimeter and reoptimise the detector simulation. After all corrections, the  resonance is used to set the absolute energy scale. For electrons from  decays, the achieved calibration is typically accurate to 0.05 % in most of the detector acceptance, rising to 0.2 % in regions with large amounts of passive material. The remaining inaccuracy is less than 0.2–1 % for electrons with a transverse energy of 10 GeV, and is …",
            "저널": "The European Physical Journal C",
            "저자": "ATLAS Collaboration atlas. publications@ cern. ch, Georges Aad, B Abbott, J Abdallah, S Abdel Khalek, O Abdinov, R Aben, B Abi, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, J Adelman, S Adomeit, T Adye, T Agatonovic-Jovin, JA Aguilar-Saavedra, M Agustoni, SP Ahlen, F Ahmadov, G Aielli, H Akerstedt, TPA Åkesson, G Akimoto, AV Akimov, GL Alberghi, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, G Alimonti, L Alio, J Alison, BMM Allbrooke, LJ Allison, PP Allport, J Almond, A Aloisio, A Alonso, F Alonso, C Alpigiani, A Altheimer, B Alvarez Gonzalez, MG Alviggi, K Amako, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, S Angelidakis, I Angelozzi, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, JP Araque, ATH Arce, JF Arguin, S Argyropoulos, M Arik, AJ Armbruster, O Arnaez, V Arnal, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, A Ashkenazi, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, B Auerbach, K Augsten, M Aurousseau, G Avolio, G Azuelos, Y Azuma, MA Baak, A Baas, C Bacci, H Bachacou, K Bachas, M Backes, M Backhaus, J Backus Mayes, E Badescu, P Bagiacchi, P Bagnaia, Y Bai, T Bain, JT Baines, OK Baker, P Balek, F Balli, E Banas, Sw Banerjee, AAE Bannoura, V Bansal, HS Bansil, L Barak, SP Baranov, EL Barberio, D Barberis, M Barbero, T Barillari, M Barisonzi",
            "전체 인용횟수": "1767회 인용2013201420152016201720182019202020212022202364324237539534319664362727",
            "페이지": "1-48",
            "학술 문서": "Electron and photon energy calibration with the ATLAS detector using LHC Run 1 dataATLAS Collaboration atlas. publications@ cern. ch… - The European Physical Journal C, 20141767회 인용 관련 학술자료 전체 87개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Electron and photon energy calibration with the ATLAS detector using LHC Run 1 data",
        "year": null
    },
    "Measurement of the Z/ γ* boson transverse momentum distribution in pp collisions at  TeV with the ATLAS detector": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/9/1",
            "게시자": "Springer Berlin Heidelberg",
            "권": "2014",
            "설명": "This paper describes a measurement of the Z/γ* boson transverse momentum spectrum using ATLAS proton-proton collision data at a centre-of-mass energy of TeV at the LHC. The measurement is performed in the Z/γ*→ e+ e− and Z/γ*→ μ+ μ− channels, using data corresponding to an integrated luminosity of 4.7 fb− 1. Normalized differential cross sections as a function of the Z/γ* boson transverse momentum are measured for transverse momenta up to 800 GeV. The measurement is performed inclusively for Z/γ* rapidities up to 2.4, as well as in three rapidity bins. The channel results are combined, compared to perturbative and resummed QCD calculations and used to constrain the parton shower parameters of Monte Carlo generators.",
            "저널": "Journal of High Energy Physics",
            "저자": "Georges Aad, Brad Abbott, Jalal Abdallah, S Abdel Khalek, R Aben, B Abi, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, J Adelman, Stefanie Adomeit, T Adye, T Agatonovic-Jovin, JA Aguilar-Saavedra, M Agustoni, SP Ahlen, F Ahmadov, G Aielli, H Akerstedt, TPA Åkesson, G Akimoto, AV Akimov, GL Alberghi, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, G Alimonti, L Alio, J Alison, BMM Allbrooke, LJ Allison, PP Allport, J Almond, A Aloisio, A Alonso, F Alonso, C Alpigiani, A Altheimer, B Alvarez Gonzalez, MG Alviggi, K Amako, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, S Angelidakis, I Angelozzi, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, JP Araque, ATH Arce, JF Arguin, S Argyropoulos, M Arik, AJ Armbruster, O Arnaez, V Arnal, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, A Ashkenazi, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, B Auerbach, K Augsten, M Aurousseau, G Avolio, G Azuelos, Y Azuma, MA Baak, C Bacci, H Bachacou, K Bachas, M Backes, M Backhaus, J Backus Mayes, E Badescu, P Bagiacchi, P Bagnaia, Y Bai, T Bain, JT Baines, OK Baker, S Baker, P Balek, F Balli, E Banas, Sw Banerjee, AAE Bannoura, V Bansal, HS Bansil, L Barak, SP Baranov, EL Barberio, D Barberis, M Barbero, T Barillari, M Barisonzi, T Barklow, N Barlow",
            "전체 인용횟수": "1704회 인용20142015201620172018201920202021202220231364154246328247187192106158",
            "페이지": "145",
            "학술 문서": "Measurement of the Z/γ* boson transverse momentum distribution in pp collisions at s= 7$$\\sqrt {s}= 7$$ TeV with the ATLAS detectorG Aad, B Abbott, J Abdallah, S Abdel Khalek, R Aben… - Journal of High Energy Physics, 20141692회 인용 관련 학술자료 전체 87개의 버전 Measurement of the Z/γ* boson transverse momentum distribution in pp collisions at√ s= 7 TeV with the ATLAS detectorG Aad, B Abbott, J Abdallah, S Abdel Khalek, R Aben… - 201420회 인용 관련 학술자료 Measurement of the $ Z/\\gamma^* $ boson transverse momentum distribution in $ pp $ collisions at $\\sqrt {s} $= 7 TeV with the ATLAS detectorY Peters, G Aad, A Pilkington, B Cox, A Oh, C Da Via… - JHEP, 2014전체 2개의 버전 ",
            "호": "9"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Measurement of the Z/ γ* boson transverse momentum distribution in pp collisions at  TeV with the ATLAS detector",
        "year": null
    },
    "Evidence for the spin-0 nature of the Higgs boson using ATLAS data": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/7/4",
            "설명": "Studies of the spin and parity quantum numbers of the Higgs boson are presented, based on proton-proton collision data collected by the ATLAS experiment at the LHC. The Standard Model spin-parity JP = 0+ hypothesis is compared with alternative hypotheses using the Higgs boson decays H->gamma gamma, H -> ZZ -> 4 leptons and H->WW -> l nu l nu, as well as the combination of these channels. The analysed dataset corresponds to an integrated luminosity of 20.7 fb-1 collected at a centre-of-mass energy of sqrt(s) = 8 TeV. For the H -> ZZ -> 4-lepton decay mode the dataset corresponding to an integrated luminosity of 4.6 fb-1 collected at sqrt(s) = 7 TeV is added. The data are compatible with the Standard Model JP = 0+ quantum numbers for the Higgs boson, whereas all alternative hypotheses studied in this letter, namely some specific JP = 0-; 1+; 1-; 2+ models, are excluded at confidence levels above 97.8%. This exclusion holds independently of the assumptions on the coupling strengths to the Standard Model particles and in the case of the JP = 2+ model, of the relative fractions of gluon-fusion and quark-antiquark production of the spin-2 particle. The data thus provide evidence for the spin-0 nature of the Higgs boson, with positive parity being strongly preferred.",
            "저널": "arXiv preprint arXiv:1307.1432",
            "저자": "Atlas Collaboration",
            "전체 인용횟수": "1678회 인용201320142015201620172018201920202021202220238333934518115915511381866447",
            "학술 문서": "Evidence for the spin-0 nature of the Higgs boson using ATLAS dataAtlas Collaboration - arXiv preprint arXiv:1307.1432, 20131107회 인용 관련 학술자료 전체 40개의 버전 Evidence for the spin-0 nature of the Higgs boson using ATLAS dataG Aad, T Abajyan, B Abbott, J Abdallah, SA Khalek… - Physics Letters B, 2013506회 인용 관련 학술자료 전체 67개의 버전 ATLAS collaboration*G Aad, T Abajyan, B Abbott, J Abdallah… - Nuclear Physics. A, 2014129회 인용 관련 학술자료 전체 33개의 버전 Evidence for the spin-0 nature of the Higgs boson using ATLAS dataP Jackson, N Soni, W Edson, J Ernst, S Guindon… - Physics Letters. Section B: Nuclear, Elementary …, 20131회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Evidence for the spin-0 nature of the Higgs boson using ATLAS data",
        "year": null
    },
    "Jet energy resolution in proton-proton collisions at  recorded in 2010 with the ATLAS detector": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/3",
            "게시자": "Springer-Verlag",
            "권": "73",
            "설명": " The measurement of the jet energy resolution is presented using data recorded with the ATLAS detector in proton-proton collisions at . The sample corresponds to an integrated luminosity of 35 pb−1. Jets are reconstructed from energy deposits measured by the calorimeters and calibrated using different jet calibration schemes. The jet energy resolution is measured with two different in situ methods which are found to be in agreement within uncertainties. The total uncertainties on these measurements range from 20 % to 10 % for jets within |y|<2.8 and with transverse momenta increasing from 30 GeV to 500 GeV. Overall, the Monte Carlo simulation of the jet energy resolution agrees with the data within 10 %.",
            "저널": "The European Physical Journal C",
            "저자": "ATLAS Collaboration atlas. publications@ cern. ch, Georges Aad, T Abajyan, B Abbott, J Abdallah, S Abdel Khalek, Ahmed Ali Abdelalim, O Abdinov, R Aben, B Abi, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, E Acerbi, BS Acharya, L Adamczyk, DL Adams, TN Addy, J Adelman, S Adomeit, P Adragna, T Adye, S Aefsky, JA Aguilar-Saavedra, M Agustoni, Mohamed Aharrouche, SP Ahlen, F Ahles, A Ahmad, M Ahsan, G Aielli, T Akdogan, TPA Åkesson, G Akimoto, AV Akimov, MS Alam, MA Alam, J Albert, S Albrand, M Aleksa, IN Aleksandrov, F Alessandria, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, M Aliev, G Alimonti, J Alison, BMM Allbrooke, PP Allport, SE Allwood-Spiers, J Almond, A Aloisio, R Alon, A Alonso, F Alonso, B Alvarez Gonzalez, MG Alviggi, K Amako, C Amelung, VV Ammosov, A Amorim, N Amram, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, P Anger, A Angerami, F Anghinolfi, A Anisenkov, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, S Aoun, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, ATH Arce, S Arfaoui, JF Arguin, E Arik, M Arik, AJ Armbruster, O Arnaez, V Arnal, C Arnault, A Artamonov, G Artoni, D Arutinov, S Asai, R Asfandiyarov, S Ask, B Åsman, L Asquith, K Assamagan, A Astbury, M Atkinson, B Aubert, E Auge, K Augsten, M Aurousseau, G Avolio, R Avramidou, D Axen, G Azuelos, Y Azuma, MA Baak, G Baccaglioni, C Bacci, AM Bach, H Bachacou, K Bachas, M Backes, M Backhaus, E Badescu, P Bagnaia, S Bahinipati, Y Bai, DC Bailey, T Bain, JT Baines, OK Baker, MD Baker, S Baker, E Banas, P Banerjee, Sw Banerjee, D Banfi, A Bangert, V Bansal, HS Bansil, L Barak, SP Baranov",
            "전체 인용횟수": "1429회 인용2012201320142015201620172018201920202021202220236712143091912221499779441923",
            "페이지": "1-27",
            "학술 문서": "Jet energy resolution in proton-proton collisions at s=7TeV recorded in 2010 with the ATLAS detectorATLAS Collaboration atlas. publications@ cern. ch… - The European Physical Journal C, 20131429회 인용 관련 학술자료 전체 97개의 버전 Jet energy resolution in proton-proton collisions at [... formula...] recorded in 2010 with the ATLAS detectorG Aad, T Abajyan, B Abbott, J Abdallah, SA Khalek… - The European Physical Journal. C, Particles and Fields, 20131회 인용 관련 학술자료 전체 4개의 버전 Jet energy resolution in proton-proton collisions at\\documentclass [12pt]{minimal}\\usepackage {amsmath}\\usepackage {wasysym}\\usepackage {amsfonts}\\usepackage {amssymb}\\usepackage {amsbsy}\\usepackage {mathrsfs}\\usepackage {upgreek}\\setlength {\\oddsidemargin}{-69pt}\\begin {document} $\\sqrt {\\mathrm {s}}= 7\\mbox {TeV} $\\end {document} recorded in 2010 with the ATLAS detector*G Aad, T Abajyan, B Abbott, J Abdallah… - 2013"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Jet energy resolution in proton-proton collisions at  recorded in 2010 with the ATLAS detector",
        "year": null
    },
    "Observation of a Centrality-Dependent Dijet Asymmetry in Lead-Lead Collisions at  with the ATLAS Detector at the LHC": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/12/13",
            "게시자": "American Physical Society",
            "권": "105",
            "설명": "By using the ATLAS detector, observations have been made of a centrality-dependent dijet asymmetry in the collisions of lead ions at the Large Hadron Collider. In a sample of lead-lead events with a per-nucleon center of mass energy of 2.76 TeV, selected with a minimum bias trigger, jets are reconstructed in fine-grained, longitudinally segmented electromagnetic and hadronic calorimeters. The transverse energies of dijets in opposite hemispheres are observed to become systematically more unbalanced with increasing event centrality leading to a large number of events which contain highly asymmetric dijets. This is the first observation of an enhancement of events with such large dijet asymmetries, not observed in proton-proton collisions, which may point to an interpretation in terms of strong jet energy loss in a hot, dense medium.",
            "저널": "Physical review letters",
            "저자": "Georges Aad, B Abbott, J Abdallah, AA Abdelalim, Abdelmalek Abdesselam, B Abi, M Abolins, H Abramowicz, H Abreu, E Acerbi, BS Acharya, M Ackers, DL Adams, TN Addy, J Adelman, M Aderholz, S Adomeit, P Adragna, T Adye, S Aefsky, JA Aguilar-Saavedra, M Aharrouche, SP Ahlen, F Ahles, A Ahmad, M Ahsan, G Aielli, T Akdogan, TPA Åkesson, G Akimoto, AV Akimov, MS Alam, MA Alam, S Albrand, M Aleksa, IN Aleksandrov, M Aleppo, F Alessandria, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, M Aliev, G Alimonti, J Alison, M Aliyev, PP Allport, SE Allwood-Spiers, J Almond, A Aloisio, R Alon, A Alonso, J Alonso, MG Alviggi, K Amako, P Amaral, C Amelung, VV Ammosov, A Amorim, G Amoros, N Amram, C Anastopoulos, T Andeen, CF Anders, KJ Anderson, A Andreazza, V Andrei, ML Andrieux, XS Anduaga, A Angerami, F Anghinolfi, N Anjos, A Annovi, A Antonaki, M Antonelli, S Antonelli, J Antos, F Anulli, S Aoun, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, ATH Arce, JP Archambault, S Arfaoui, JF Arguin, E Arik, M Arik, AJ Armbruster, KE Arms, SR Armstrong, O Arnaez, C Arnault, A Artamonov, G Artoni, D Arutinov, S Asai, J Silva, R Asfandiyarov, S Ask, B Åsman, L Asquith, K Assamagan, A Astbury, A Astvatsatourov, G Atoian, B Aubert, B Auerbach, E Auge, K Augsten, M Aurousseau, N Austin, R Avramidou, D Axen, C Ay, G Azuelos, Y Azuma, MA Baak, G Baccaglioni, C Bacci, AM Bach, H Bachacou, K Bachas, G Bachy, M Backes, E Badescu, P Bagnaia, S Bahinipati, Y Bai, DC Bailey, T Bain, JT Baines, OK Baker, S Baker, F Baltasar Dos Santos Pedrosa, E Banas, P Banerjee, Sw Banerjee, D Banfi, A Bangert, V Bansal, HS Bansil, L Barak, SP Baranov, A Barashkou, A Barbaro Galtieri, T Barber",
            "전체 인용횟수": "1410회 인용20112012201320142015201620172018201920202021202220239717115414114415310511111454486055",
            "페이지": "252303",
            "학술 문서": "Observation of a centrality-dependent dijet asymmetry in lead-lead collisions at s NN= 2.76 TeV with the ATLAS detector at the LHCG Aad, B Abbott, J Abdallah, AA Abdelalim… - Physical review letters, 20101407회 인용 관련 학술자료 전체 104개의 버전 Observation of a centrality-dependent dijet asymmetry in lead-lead collisions at vsNN= 2.76 Tev with the ATLASO detector at the LHCG Aad, A De Santo, F Salvatore, C ATLAS - Physical Review Letters, 20104회 인용 관련 학술자료 전체 2개의 버전 Observation of a centrality-dependent dijet asymmetry in lead-lead collisions at√ sNN= 2.76 Tev with the ATLASO detector at the LHCATLAS Collaboration - 2010관련 학술자료 전체 19개의 버전 Observation of a Centrality‐Dependent Dijet Asymmetry in Lead‐Lead Collisions at√ SNN= 2.76 TeV with the ATLAS Detector at LHCJ Salt Cairols, JA Valls Ferrer, S Cabrera Urbán… - Physical Review Letters, 2010, vol. 105, p. 252303-1 …, 2010관련 학술자료 전체 2개의 버전 ",
            "호": "25"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Observation of a Centrality-Dependent Dijet Asymmetry in Lead-Lead Collisions at  with the ATLAS Detector at the LHC",
        "year": null
    },
    "Observation of a centrality-dependent dijet asymmetry in lead-lead collisions at √sNN=2.76 Tev with the ATLASO detector at the LHC": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010",
            "게시자": "American Physical Society",
            "권": "105",
            "설명": "By using the ATLAS detector, observations have been made of a centrality-dependent dijet asymmetry in the collisions of lead ions at the Large Hadron Collider. In a sample of lead-lead events with a per-nucleon center of mass energy of 2.76 TeV, selected with a minimum bias trigger, jets are reconstructed in fine-grained, longitudinally segmented electromagnetic and hadronic calorimeters. The transverse energies of dijets in opposite hemispheres are observed to become systematically more unbalanced with increasing event centrality leading to a large number of events which contain highly asymmetric dijets. This is the first observation of an enhancement of events with such large dijet asymmetries, not observed in proton-proton collisions, which may point to an interpretation in terms of strong jet energy loss in a hot, dense medium.",
            "저널": "Physical Review Letters",
            "저자": "MS Alam, J Ernst, V Rojo, S Bahinipati, NJ Buchanan, K Chan, L Chen, DM Gingrich, MS Kim, S Liu, J Lu, RW Moore, JL Pinfold, N Soni, S Subramania, O Cakir, AK Ciftci, R Ciftci, S Persembe, Yildiz H Duran, M Yilmaz, S Sultansoy, Cakir I Turk, Bella L Aperio, O Arnaez, B Aubert, M Aurousseau, N Berger, J Colas, L Di Ciaccio, TKO Doan, M El Kacimi, S Elles, P Ghez, M Gouanère, C Goy, T Guillemin, L Helary, T Hryn'ova, P Iengo, G Ionescu, A Jeremie, S Jézéquel, M Kataoka, J Labbe, R Lafaye, S Laplace, N Massol, P Perrodo, H Przysiezniak, G Sauvage, T Todorov, I Wingerter-Seez, R Zitoun, Y Zolnierowski, L Asquith, RE Blair, S Chekanov, JW Dawson, D Fellmann, GF Gieraltowski, VJ Guarino, D Hill, N Hill, K Karr, T Lecompte, D Malon, EN May, L Nodulman, A Paramonov, LE Price, J Proudfoot, Ferrando BM Salvachua, JL Schlereth, RW Stanek, DG Underwood, P Van Gemmeren, A Vaniachine, R Yoshida, J Zhang, E Cheu, KA Johns, V Kaushik, CL Lampen, W Lampl, X Lei, P Loch, P Mal, F Rühr, JP Rutherfoord, L Shaver, MA Shupe, EW Varnes, A Brandt, K De, A Farbin, H Kim, P Nilsson, N Ozturk, R Pravahan, E Sarkisyan-Grinbaum, M Sosebee, B Spurlock, AR Stradling, G Usai, A Vartapetian, A White, J Yu, A Antonaki, D Fassouliotis, V Giakoumopoulou, N Giokaris, P Ioannou, C Kourkoumelis, A Manousakis-Katsikakis, G Tzanakos, C Vellidis, T Alexopoulos, R Avramidou, M Dris, A Filippas, M Fokitis, EN Gazis, F Georgatos, G Iakovidis, E Katsoufis, S Maltezos, E Mountricha, E Panagiotopoulou, TD Papadopoulou, P Savva, C Tsarouchas, G Tsipolitis, S Vlachos, L Xaplanteris, M Aliyev, N Huseynov, F Khalil-Zada, S Rzaeva, J Abdallah, M Bosman, MP Casado, M Cavalli-Sforza, MC Conidi, B Demirkoz, M Dosil, Curull X Espinal, L Fiorini, S Grinstein, C Helsens",
            "전체 인용횟수": "1410회 인용20112012201320142015201620172018201920202021202220239717115414114415310511111454486055",
            "페이지": "252303-252303",
            "학술 문서": "Observation of a centrality-dependent dijet asymmetry in lead-lead collisions at s NN= 2.76 TeV with the ATLAS detector at the LHCG Aad, B Abbott, J Abdallah, AA Abdelalim… - Physical review letters, 20101407회 인용 관련 학술자료 전체 104개의 버전 Observation of a centrality-dependent dijet asymmetry in lead-lead collisions at vsNN= 2.76 Tev with the ATLASO detector at the LHCG Aad, A De Santo, F Salvatore, C ATLAS - Physical Review Letters, 20104회 인용 관련 학술자료 전체 2개의 버전 Observation of a centrality-dependent dijet asymmetry in lead-lead collisions at√ sNN= 2.76 Tev with the ATLASO detector at the LHCATLAS Collaboration - 2010관련 학술자료 전체 19개의 버전 ",
            "호": "25"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Observation of a centrality-dependent dijet asymmetry in lead-lead collisions at √sNN=2.76 Tev with the ATLASO detector at the LHC",
        "year": null
    },
    "Observation of a new particle in the search for the Standard Model Higgs boson with the ATLAS detector at the LHC": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/9/17",
            "게시자": "North-Holland",
            "권": "716",
            "설명": "A search for the Standard Model Higgs boson in proton–proton collisions with the ATLAS detector at the LHC is presented. The datasets used correspond to integrated luminosities of approximately 4.8 fb−1 collected at s=7 TeV in 2011 and 5.8 fb−1 at s=8 TeV in 2012. Individual searches in the channels H→ZZ(⁎)→4ℓ, H→γγ and H→WW(⁎)→eνμν in the 8 TeV data are combined with previously published results of searches for H→ZZ(⁎), WW(⁎), bb¯ and τ+τ− in the 7 TeV data and results from improved analyses of the H→ZZ(⁎)→4ℓ and H→γγ channels in the 7 TeV data. Clear evidence for the production of a neutral boson with a measured mass of 126.0±0.4(stat)±0.4(sys) GeV is presented. This observation, which has a significance of 5.9 standard deviations, corresponding to a background fluctuation probability of 1.7×10−9, is compatible with the production and decay of the Standard Model Higgs boson.",
            "저널": "Physics Letters B",
            "저자": "Georges Aad, Tatevik Abajyan, B Abbott, J Abdallah, S Abdel Khalek, Ahmed Ali Abdelalim, R Aben, B Abi, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, BS Acharya, L Adamczyk, DL Adams, TN Addy, J Adelman, S Adomeit, P Adragna, T Adye, S Aefsky, JA Aguilar-Saavedra, M Agustoni, M Aharrouche, SP Ahlen, F Ahles, A Ahmad, M Ahsan, G Aielli, T Akdogan, TPA Åkesson, G Akimoto, AV Akimov, MS Alam, MA Alam, J Albert, S Albrand, M Aleksa, IN Aleksandrov, F Alessandria, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, M Aliev, G Alimonti, J Alison, BMM Allbrooke, PP Allport, SE Allwood-Spiers, J Almond, A Aloisio, R Alon, Alejandro Alonso, F Alonso, A Altheimer, B Alvarez Gonzalez, MG Alviggi, K Amako, C Amelung, VV Ammosov, SP Amor Dos Santos, A Amorim, N Amram, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, M-L Andrieux, XS Anduaga, S Angelidakis, P Anger, A Angerami, F Anghinolfi, A Anisenkov, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, S Aoun, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, ATH Arce, S Arfaoui, J-F Arguin, E Arik, M Arik, AJ Armbruster, O Arnaez, V Arnal, C Arnault, A Artamonov, G Artoni, D Arutinov, S Asai, S Ask, B Åsman, L Asquith, K Assamagan, A Astbury, M Atkinson, B Aubert, E Auge, K Augsten, M Aurousseau, G Avolio, R Avramidou, D Axen, G Azuelos, Y Azuma, MA Baak, G Baccaglioni, C Bacci, AM Bach, H Bachacou, K Bachas, M Backes, M Backhaus, J Backus Mayes, E Badescu, P Bagnaia, S Bahinipati, Y Bai, DC Bailey, T Bain, JT Baines, OK Baker, MD Baker, S Baker, P Balek, E Banas, P Banerjee, Sw Banerjee, D Banfi, A Bangert, V Bansal, HS Bansil",
            "전체 인용횟수": "21838회 인용20122013201420152016201720182019202020212022202354225002569256222752013209516931474144913831134",
            "페이지": "1-29",
            "학술 문서": "Observation of a new particle in the search for the Standard Model Higgs boson with the ATLAS detector at the LHCG Aad, T Abajyan, B Abbott, J Abdallah, SA Khalek… - Physics Letters B, 201212890회 인용 관련 학술자료 전체 121개의 버전 Observation of a new particle in the search for the Standard Model Higgs boson with the ATLAS detector at the LHCAtlas Collaboration - arXiv preprint arXiv:1207.7214, 201210725회 인용 관련 학술자료 전체 13개의 버전 Search for extra dimensions using diphoton events in 7 TeV proton-proton collisions with the ATLAS detector ATLAS Collaboration*V Mitsou, JA Fuster Verdú, A Ferrer Soria… - Physics Letters B, 2012, vol. 710, num. 4-5, p. 538-556, 20125회 인용 관련 학술자료 전체 2개의 버전 PLB 716, 1 (2012)*G Aad, ATLAS Collaboration - arXiv preprint arXiv:1207.72143회 인용 관련 학술자료 ATLAS Collaboration*G Aad - 2012관련 학술자료 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Observation of a new particle in the search for the Standard Model Higgs boson with the ATLAS detector at the LHC",
        "year": null
    },
    "Oral rivaroxaban for symptomatic venous thromboembolism": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/12/23",
            "게시자": "Massachusetts Medical Society",
            "권": "363",
            "설명": " Background Rivaroxaban, an oral factor Xa inhibitor, may provide a simple, fixed-dose regimen for treating acute deep-vein thrombosis (DVT) and for continued treatment, without the need for laboratory monitoring. Methods We conducted an open-label, randomized, event-driven, noninferiority study that compared oral rivaroxaban alone (15 mg twice daily for 3 weeks, followed by 20 mg once daily) with subcutaneous enoxaparin followed by a vitamin K antagonist (either warfarin or acenocoumarol) for 3, 6, or 12 months in patients with acute, symptomatic DVT. In parallel, we carried out a double-blind, randomized, event-driven superiority study that compared rivaroxaban alone (20 mg once daily) with placebo for an additional 6 or 12 months in patients who had completed 6 to 12 months of treatment for venous thromboembolism. The primary efficacy outcome for both studies was recurrent venous …",
            "저널": "New England Journal of Medicine",
            "저자": "Einstein Investigators",
            "전체 인용횟수": "3768회 인용2011201220132014201520162017201820192020202120222023122240332375397399334324263289242184160",
            "페이지": "2499-2510",
            "학술 문서": "Oral rivaroxaban for symptomatic venous thromboembolismEinstein Investigators - New England Journal of Medicine, 20103744회 인용 관련 학술자료 전체 16개의 버전 Oral rivaroxaban for symptomatic venous thromboembolism*GW Landman, ROB Gans - New England Journal of Medicine, 201132회 인용 관련 학술자료 전체 3개의 버전 ",
            "호": "26"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Oral rivaroxaban for symptomatic venous thromboembolism",
        "year": null
    },
    "Observation of electron-antineutrino disappearance at Daya Bay": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/4/23",
            "게시자": "American Physical Society",
            "권": "108",
            "설명": "The Daya Bay Reactor Neutrino Experiment has measured a nonzero value for the neutrino mixing angle θ 13 with a significance of 5.2 standard deviations. Antineutrinos from six 2.9 G W th reactors were detected in six antineutrino detectors deployed in two near (flux-weighted baseline 470 m and 576 m) and one far (1648 m) underground experimental halls. With a 43 000 ton–G W th–day live-time exposure in 55 days, 10 416 (80 376) electron-antineutrino candidates were detected at the far hall (near halls). The ratio of the observed to expected number of antineutrinos at the far hall is R= 0.940±0.011 (stat.)±0.004 (syst.). A rate-only analysis finds sin﻿ 2 2 θ 13= 0.092±0.016 (stat.)±0.005 (syst.) in a three-neutrino framework.",
            "저널": "Physical Review Letters",
            "저자": "FP An, JZ Bai, AB Balantekin, HR Band, D Beavis, W Beriguete, M Bishai, S Blyth, K Boddy, RL Brown, B Cai, GF Cao, J Cao, R Carr, WT Chan, JF Chang, Y Chang, C Chasman, HS Chen, HY Chen, SJ Chen, SM Chen, XC Chen, XH Chen, XS Chen, Y Chen, YX Chen, JJ Cherwinka, MC Chu, JP Cummings, ZY Deng, YY Ding, MV Diwan, L Dong, E Draeger, XF Du, DA Dwyer, WR Edwards, SR Ely, SD Fang, JY Fu, ZW Fu, LQ Ge, V Ghazikhanian, RL Gill, J Goett, M Gonchar, GH Gong, H Gong, YA Gornushkin, LS Greenler, WQ Gu, MY Guan, XH Guo, RW Hackenburg, RL Hahn, S Hans, M He, Q He, WS He, KM Heeger, YK Heng, P Hinrichs, TH Ho, YK Hor, YB Hsiung, BZ Hu, T Hu, HX Huang, HZ Huang, PW Huang, X Huang, XT Huang, Patrick Huber, Z Isvan, DE Jaffe, S Jetter, XL Ji, XP Ji, HJ Jiang, WQ Jiang, JB Jiao, RA Johnson, L Kang, SH Kettell, M Kramer, KK Kwan, MW Kwok, T Kwok, CY Lai, WC Lai, WH Lai, K Lau, L Lebanowski, J Lee, MKP Lee, R Leitner, JKC Leung, KY Leung, CA Lewis, B Li, F Li, GS Li, J Li, QJ Li, SF Li, WD Li, XB Li, XN Li, XQ Li, Y Li, ZB Li, H Liang, J Liang, CJ Lin, Guey-Lin Lin, SK Lin, SX Lin, YC Lin, JJ Ling, Jonathan M Link, L Littenberg, BR Littlejohn, BJ Liu, C Liu, DW Liu, H Liu, JC Liu, JL Liu, S Liu, X Liu, YB Liu, C Lu, HQ Lu, A Luk, KB Luk, T Luo, XL Luo, LH Ma, QM Ma, XB Ma, XY Ma, YQ Ma, B Mayes, KT McDonald, MC McFarlane, RD McKeown, Y Meng, D Mohapatra, JE Morgan",
            "전체 인용횟수": "3296회 인용201220132014201520162017201820192020202120222023366573429374321230225176172142133126",
            "페이지": "171803",
            "학술 문서": "Observation of electron-antineutrino disappearance at Daya BayFP An, JZ Bai, AB Balantekin, HR Band, D Beavis… - Physical Review Letters, 20123296회 인용 관련 학술자료 전체 31개의 버전 ",
            "호": "17"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Observation of electron-antineutrino disappearance at Daya Bay",
        "year": null
    },
    "The BABAR detector": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2002/2/21",
            "게시자": "North-Holland",
            "권": "479",
            "설명": "BABAR, the detector for the SLAC PEP-II asymmetric e+e− B Factory operating at the ϒ(4S) resonance, was designed to allow comprehensive studies of CP-violation in B-meson decays. Charged particle tracks are measured in a multi-layer silicon vertex tracker surrounded by a cylindrical wire drift chamber. Electromagnetic showers from electrons and photons are detected in an array of CsI crystals located just inside the solenoidal coil of a superconducting magnet. Muons and neutral hadrons are identified by arrays of resistive plate chambers inserted into gaps in the steel flux return of the magnet. Charged hadrons are identified by dE/dx measurements in the tracking detectors and by a ring-imaging Cherenkov detector surrounding the drift chamber. The trigger, data acquisition and data-monitoring systems, VME- and network-based, are controlled by custom-designed online software. Details of the layout and …",
            "저자": "Bernard Aubert, A Bazan, A Boucham, D Boutigny, I De Bonis, J Favier, J-M Gaillard, A Jeremie, Y Karyotakis, T Le Flour, JP Lees, S Lieunard, P Petitpas, P Robbe, V Tisserand, K Zachariadou, Antimo Palano, GP Chen, JC Chen, ND Qi, G Rong, P Wang, YS Zhu, G Eigen, PL Reinertsen, B Stugu, B Abbott, GS Abrams, L Amerman, AW Borgland, AB Breon, DN Brown, J Button-Shafer, AR Clark, S Dardin, C Day, SF Dow, Q Fan, I Gaponenko, MS Gill, FR Goozen, SJ Gowdy, A Gritsan, Y Groysman, C Hernikl, RG Jacobsen, RC Jared, RW Kadel, J Kadyk, A Karcher, LT Kerth, I Kipnis, S Kluth, JF Kral, R Lafever, C LeClerc, ME Levi, SA Lewis, C Lionberger, T Liu, M Long, L Luo, G Lynch, P Luft, E Mandelli, M Marino, K Marks, C Matuk, AB Meyer, R Minor, A Mokhtarani, M Momayezi, M Nyman, PJ Oddone, J Ohnemus, D Oshatz, S Patton, M Pedrali-Noy, A Perazzo, C Peters, W Pope, M Pripstein, DR Quarrie, JE Rasson, NA Roe, A Romosan, MT Ronan, VG Shelkov, R Stone, PD Strother, AV Telnov, H von der Lippe, TF Weber, WA Wenzel, G Zizka, PG Bright-Thomas, CM Hawkes, A Kirk, DJ Knowles, SW O'Neale, AT Watson, NK Watson, T Deppermann, H Koch, J Krug, M Kunze, B Lewandowski, K Peters, H Schmuecker, M Steinke, JC Andress, NR Barlow, W Bhimji, N Chevalier, PJ Clark, WN Cottingham, N De Groot, N Dyce, B Foster, A Mass, JD McFall, D Wallom, FF Wilson, K Abe, C Hearty, JA McKenna, D Thiessen, B Camanzi, TJ Harrison, AK McKemey, J Tinslay, EI Antohin, VE Blinov, AD Bukin, DA Bukin, AR Buzykaev, MS Dubrovin, VB Golubev, VN Ivanchenko, GM Kolachev, AA Korol, EA Kravchenko, SF Mikhailov, AP Onuchin, AA Salnikov, SI Serednyakov, Yu I Skovpen, VI Telnov, AN Yushkov, J Booth",
            "전체 인용횟수": "3249회 인용200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220233110719326126427125026028921818215815982886454655348303360",
            "출처": "Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment",
            "페이지": "1-116",
            "학술 문서": "The BABAR detectorB Aubert, A Bazan, A Boucham, D Boutigny, I De Bonis… - Nuclear Instruments and Methods in Physics Research …, 20023249회 인용 관련 학술자료 전체 50개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The BABAR detector",
        "year": null
    },
    "The belle detector": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2002/2/21",
            "게시자": "North-Holland",
            "권": "479",
            "설명": "The Belle detector was designed and constructed to carry out quantitative studies of rare B-meson decay modes with very small branching fractions using an asymmetric e+e− collider operating at the ϒ(4S) resonance, the KEK-B-factory. Such studies require data samples containing ∼107 B-meson decays. The Belle detector is configured around a 1.5 T  superconducting solenoid and iron structure surrounding the KEK-B beams at the Tsukuba interaction region. B-meson decay vertices are measured by a silicon vertex detector situated just outside of a cylindrical beryllium beam pipe. Charged particle tracking is performed by a wire drift chamber (CDC). Particle identification is provided by dE/dx measurements in CDC, aerogel threshold Cherenkov counter and time-of-flight counter placed radially outside of CDC. Electromagnetic showers are detected in an array of CsI(Tl) crystals located inside the solenoid coil …",
            "저자": "A Abashian, K Gotow, N Morgan, L Piilonen, S Schrenk, K Abe, I Adachi, JP Alexander, K Aoki, S Behari, Y Doi, R Enomoto, H Fujii, Y Fujita, Y Funahashi, J Haba, H Hamasaki, T Haruyama, K Hayashi, Y Higashi, N Hitomi, S Igarashi, Y Igarashi, T Iijima, Hirokazu Ikeda, Hitomi Ikeda, R Itoh, M Iwai, H Iwasaki, Y Iwasaki, KK Joo, K Kasami, N Katayama, M Kawai, H Kichimi, T Kobayashi, S Koike, Y Kondo, MH Lee, Y Makida, A Manabe, T Matsuda, T Murakami, S Nagayama, M Nakao, T Nozaki, K Ogawa, R Ohkubo, Y Ohnishi, H Ozaki, H Sagawa, M Saito, Y Sakai, T Sasaki, N Sato, T Sumiyoshi, J Suzuki, JI Suzuki, S Suzuki, F Takasaki, K Tamai, M Tanaka, T Tatomi, T Tsuboyama, K Tsukada, T Tsukamoto, S Uehara, N Ujiie, S Uno, B Yabsley, Y Yamada, H Yamaguchi, H Yamaoka, Y Yamaoka, M Yamauchi, Y Yoshimura, H Zhao, R Abe, G Iwai, T Kawasaki, H Miyata, K Shimada, S Takahashi, N Tamura, H Hanada, T Nagamine, M Nakajima, T Nakajima, S Narita, M Sanpei, T Takayama, M Ueki, M Yamaga, A Yamaguchi, BS Ahn, JS Kang, Hyunwoo Kim, CW Park, H Park, HS Ahn, HK Jang, CH Kim, SK Kim, SH Lee, CS Park, E Won, H Aihara, T Higuchi, H Kawai, T Matsubara, T Nakadaira, H Tajima, J Tanaka, T Tomura, M Yokoyama, M Akatsu, K Fujimoto, M Hirose, K Inami, A Ishikawa, S Itami, T Kani, T Matsumoto, I Nagai, T Okabe, T Oshima, K Senyo, A Sugi, A Sugiyama, S Suitoh, M Tomoto, K Yoshida, R Akhmetshin, P Chang, Y Chao, YQ Chen, WS Hou, SC Hsu, HC Huang, TJ Huang, MC Lee, RS Lu, JC Peng, KC Peng, S Sahu, HF Sung, KL Tsai, K Ueno, CC Wang, MZ Wang",
            "전체 인용횟수": "2291회 인용200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220239428299133113135153128108102102118911141201151258761928765",
            "출처": "Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment",
            "페이지": "117-232",
            "학술 문서": "The belle detectorA Abashian, K Gotow, N Morgan, L Piilonen, S Schrenk… - Nuclear Instruments and Methods in Physics Research …, 20022291회 인용 관련 학술자료 전체 12개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The belle detector",
        "year": null
    },
    "Observation of  Resonances Consistent with Pentaquark States in  Decays": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/8/12",
            "게시자": "American Physical Society",
            "권": "115",
            "설명": "Observations of exotic structures in the J/ψ p channel, which we refer to as charmonium-pentaquark states, in Λ b 0→ J/ψ K− p decays are presented. The data sample corresponds to an integrated luminosity of 3 fb− 1 acquired with the LHCb detector from 7 and 8 TeV p p collisions. An amplitude analysis of the three-body final state reproduces the two-body mass and angular distributions. To obtain a satisfactory fit of the structures seen in the J/ψ p mass spectrum, it is necessary to include two Breit-Wigner amplitudes that each describe a resonant state. The significance of each of these resonances is more than 9 standard deviations. One has a mass of 4380±8±29 MeV and a width of 205±18±86 MeV, while the second is narrower, with a mass of 4449.8±1.7±2.5 MeV and a width of 39±5±19 MeV. The preferred J P assignments are of opposite parity, with one state having spin 3/2 and the other 5/2.",
            "저널": "Physical review letters",
            "저자": "Roel Aaij, B Adeva, M Adinolfi, A Affolder, Z Ajaltouni, S Akar, J Albrecht, F Alessio, M Alexander, S Ali, G Alkhazov, P Alvarez Cartelle, AA Alves Jr, S Amato, S Amerio, Y Amhis, L An, L Anderlini, J Anderson, G Andreassi, M Andreotti, JE Andrews, RB Appleby, O Aquines Gutierrez, F Archilli, P d’Argent, A Artamonov, M Artuso, E Aslanides, G Auriemma, M Baalouch, S Bachmann, JJ Back, A Badalov, C Baesso, W Baldini, RJ Barlow, C Barschel, S Barsuk, W Barter, V Batozskaya, V Battista, A Bay, L Beaucourt, J Beddow, F Bedeschi, I Bediaga, LJ Bel, V Bellee, N Belloli, I Belyaev, E Ben-Haim, G Bencivenni, S Benson, J Benton, A Berezhnoy, R Bernet, A Bertolin, M-O Bettler, M Van Beuzekom, A Bien, S Bifani, P Billoir, T Bird, A Birnkraut, A Bizzeti, T Blake, F Blanc, J Blouw, S Blusk, V Bocci, A Bondar, N Bondar, W Bonivento, S Borghi, M Borsato, TJV Bowcock, E Bowen, C Bozzi, S Braun, M Britsch, T Britton, J Brodzicka, NH Brook, A Bursche, J Buytaert, S Cadeddu, R Calabrese, M Calvi, M Calvo Gomez, P Campana, D Campora Perez, L Capriotti, Angelo Carbone, G Carboni, R Cardinale, A Cardini, P Carniti, L Carson, K Carvalho Akiba, G Casse, L Cassina, L Castillo Garcia, M Cattaneo, Ch Cauet, G Cavallero, R Cenci, M Charles, Ph Charpentier, M Chefdeville, S Chen, S-F Cheung, N Chiapolini, M Chrzaszcz, X Cid Vidal, G Ciezarek, PEL Clarke, M Clemencic, HV Cliff, J Closier, V Coco, J Cogan, E Cogneras, V Cogoni, L Cojocariu, G Collazuol, P Collins, A Comerma-Montells, A Contu, A Cook, M Coombes, S Coquereau, G Corti, M Corvo, B Couturier, GA Cowan, DC Craik, A Crocombe, M Cruz Torres, S Cunliffe, R Currie, C D’Ambrosio, E Dall’Occo, J Dalseno, PNY David, A Davis, K De Bruyn, S De Capua, M De Cian, JM De Miranda",
            "전체 인용횟수": "2132회 인용20152016201720182019202020212022202388341302267284225245192170",
            "페이지": "072001",
            "학술 문서": "Observation of J/ψ p Resonances Consistent with Pentaquark States in Λ b 0→ J/ψ K− p DecaysR Aaij, B Adeva, M Adinolfi, A Affolder, Z Ajaltouni… - Physical review letters, 20152132회 인용 관련 학술자료 전체 71개의 버전 ",
            "호": "7"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Observation of  Resonances Consistent with Pentaquark States in  Decays",
        "year": null
    },
    "Jet energy scale and resolution in the CMS experiment in pp collisions at 8 TeV": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "게시자": "IOP Publishing",
            "설명": "The state-of-the-art techniques used in the CMS experiment at the CERN LHC for jet energy scale (JES) and jet energy resolution (JER) calibration are presented, based on a data sample corresponding to an integrated luminosity of 19.7 fb− 1 collected in proton-proton collisions at a center-of-mass energy of 8 TeV. Jets are the experimental signatures of energetic quarks and gluons produced in high-energy processes. Like all experimentally-reconstructed objects, jets need to be calibrated in order to have the correct energy scale: this is the aim of the jet energy corrections (JEC). The detailed understanding of both the energy scale and the transverse momentum resolution of the jets is of crucial importance for many physics analyses, and a leading component of their associated systematic uncertainties. Improvements made in understanding the JES in the recent years have resulted in very precise measurements …",
            "저자": "Vardan Khachatryan, Albert M Sirunyan, Armen Tumasyan, Wolfgang Adam, E Asilar, Thomas Bergauer, Johannes Brandstetter, Erica Brondolin, Marko Dragicevic, J Ero, Martin Flechl, G Gomez, C Silkworth, Y Iiyama, E Laird, S Undleeb, B Stieger, P Turner, S Jindariani, H Brun, N Varelas, J Varela, P Giacomelli, J Tuominiemi, MC Fouz, Y Eshaq, M Trovato, Z Wu, S Thuer, P Petkov, M Zakaria, B Bilki, S Basegmez, MF Sevilla, F Ball, E Tuovinen, I Volobouev, R Dell'Orso, W Clarida, N Heracleous, S Carrillo Moreno, S Wasserbaech, K Dilsiz, G Landsberg, S Brandt, I Ahmed, S Durgut, L Wendland, L Beck, RP Gandrajula, JW Gary, M Haytmyradov, G Rolandi, M Naimuddin, JG Bian, MABM Ali, V Palichik, S Liu, S Lacaprara, E Kwon, AT Serban, H Stadie, V Khristenko, S Sarkar, JP Merlo, H Mermerkaya, B Dahmes, M Ahmad, A Dierlamm, A Safonov, Z Mao, G Quast, A Mestvirishvili, A Escalante Del Valle, JJ Brooke, V Cherepanov, A Grebenyuk, D Sabes, P Spagnolo, M Velasco, Y Choi, L Uvarov, S Jabeen, S Wilbur, F Pauss, G Masetti, RJ Wang, O Bouhali, A Custodio, A Brinkerhoff, N Dev, M Hildreth, C Jessop, DJ Karmgard, L Mundim, J Zhang, S Schael, N Tosi, F Romeo, A Lopez Virto, N Kellams, N Mccoll, Y Erdogan, G Majumder, J Wetzel, N De Filippis, M Stoye, M Kirakosyan, K Lannon, S Lynch, J Marco, N Marinelli, Y Mao, S Fink, M Bluj, E Halkiadakis, M Friedl, M Kovac, M Johnson, A Belloni, F Meng, M Finger, S Elgammal, C Mueller, M Narain, W Waltenberger, G Flugge, Y Takahashi, PR Dudero, F Frensch, Y Musienko, T Pearson, JR Vlimant, R Marco, GSF Stephans, M Planer, A Meyer, M De Palma, M Sharan, E Appelt, K Yi, E Clement, A Reinsvold, M Giffels, C Martinez Rivero, A Levine, J Talvitie, R Ruchti, G Smith, S Taroni",
            "전체 인용횟수": "2013회 인용2016201720182019202020212022202318240443385260252238171",
            "학술 문서": "Jet energy scale and resolution in the CMS experiment in pp collisions at 8 TeVV Khachatryan, AM Sirunyan, A Tumasyan, W Adam… - 20171852회 인용 관련 학술자료 전체 93개의 버전 The CMS trigger system*V Khachatryan, AM Sirunyan, A Tumasyan, W Adam… - Journal of Instrumentation, 2017277회 인용 관련 학술자료 전체 50개의 버전 Jet energy scale and resolution in the CMS experiment in pp collisions at 8 TeVA Apyan, RA Barbieri, AA Baty, K Bierwagen… - 2017관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Jet energy scale and resolution in the CMS experiment in pp collisions at 8 TeV",
        "year": null
    },
    "Measurement of the Z/γ * boson transverse momentum distribution in pp collisions at s = 7  TeV with the ATLAS detector": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/9",
            "게시자": "Springer Berlin Heidelberg",
            "권": "2014",
            "설명": "This paper describes a measurement of the Z/γ* boson transverse momentum spectrum using ATLAS proton-proton collision data at a centre-of-mass energy of TeV at the LHC. The measurement is performed in the Z/γ*→ e+ e− and Z/γ*→ μ+ μ− channels, using data corresponding to an integrated luminosity of 4.7 fb− 1. Normalized differential cross sections as a function of the Z/γ* boson transverse momentum are measured for transverse momenta up to 800 GeV. The measurement is performed inclusively for Z/γ* rapidities up to 2.4, as well as in three rapidity bins. The channel results are combined, compared to perturbative and resummed QCD calculations and used to constrain the parton shower parameters of Monte Carlo generators.",
            "저널": "Journal of High Energy Physics",
            "저자": "Georges Aad, Brad Abbott, Jalal Abdallah, Samah Abdel Khalek, R Aben, B Abi, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, J Adelman, Stefanie Adomeit, T Adye, T Agatonovic-Jovin, JA Aguilar-Saavedra, M Agustoni, SP Ahlen, F Ahmadov, G Aielli, H Akerstedt, TPA Åkesson, G Akimoto, AV Akimov, GIAN LUIGI Alberghi, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, G Alimonti, L Alio, J Alison, BMM Allbrooke, LJ Allison, PP Allport, J Almond, A Aloisio, A Alonso, F Alonso, C Alpigiani, A Altheimer, B Alvarez Gonzalez, MG Alviggi, K Amako, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, S Angelidakis, I Angelozzi, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, JP Araque, ATH Arce, JF Arguin, S Argyropoulos, M Arik, AJ Armbruster, O Arnaez, V Arnal, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, A Ashkenazi, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, B Auerbach, K Augsten, M Aurousseau, G Avolio, G Azuelos, Y Azuma, MA Baak, C Bacci, H Bachacou, K Bachas, M Backes, M Backhaus, J Backus Mayes, E Badescu, P Bagiacchi, P Bagnaia, Y Bai, T Bain, JT Baines, OK Baker, S Baker, P Balek, F Balli, E Banas, Sw Banerjee, AAE Bannoura, V Bansal, HS Bansil, L Barak, SP Baranov, EL Barberio, D Barberis, M Barbero, T Barillari, M Barisonzi, T Barklow, N Barlow",
            "전체 인용횟수": "1704회 인용20142015201620172018201920202021202220231364154246328247187192106158",
            "페이지": "1-47",
            "학술 문서": "Measurement of the Z/γ* boson transverse momentum distribution in pp collisions at s= 7$$\\sqrt {s}= 7$$ TeV with the ATLAS detectorG Aad, B Abbott, J Abdallah, S Abdel Khalek, R Aben… - Journal of High Energy Physics, 20141692회 인용 관련 학술자료 전체 87개의 버전 Measurement of the Z/γ* boson transverse momentum distribution in pp collisions at√ s= 7 TeV with the ATLAS detector*G Aad, B Abbott, J Abdallah, S Abdel Khalek, R Aben… - 201420회 인용 관련 학술자료 ",
            "호": "9"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Measurement of the Z/γ * boson transverse momentum distribution in pp collisions at s = 7  TeV with the ATLAS detector",
        "year": null
    },
    "Design and construction of the BESIII detector": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/3/11",
            "게시자": "North-Holland",
            "권": "614",
            "설명": "This paper will discuss the design and construction of BESIII, which is designed to study physics in the τ-charm energy region utilizing the new high luminosity BEPCII double ring e+e− collider. The expected performance will be given based on Monte Carlo simulations and results of cosmic ray and beam tests. In BESIII, tracking and momentum measurements for charged particles are made by a cylindrical multilayer drift chamber in a 1T superconducting solenoid. Charged particles are identified with a time-of-flight system based on plastic scintillators in conjunction with dE/dx (energy loss per unit pathlength) measurements in the drift chamber. Energies of electromagnetic showers are measured by a CsI(Tl) crystal calorimeter located inside the solenoid magnet. Muons are identified by arrays of resistive plate chambers in a steel magnetic yoke for the flux return. The level 1 trigger system, data acquisition system …",
            "저널": "Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment",
            "저자": "Medina Ablikim, ZH An, JZ Bai, Niklaus Berger, JM Bian, X Cai, GF Cao, XX Cao, JF Chang, C Chen, G Chen, HC Chen, HX Chen, J Chen, JC Chen, LP Chen, P Chen, XH Chen, YB Chen, ML Chen, YP Chu, XZ Cui, HL Dai, ZY Deng, MY Dong, SX Du, ZZ Du, J Fang, CD Fu, CS Gao, MY Gong, WX Gong, SD Gu, BJ Guan, J Guan, YN Guo, JF Han, KL He, M He, X He, YK Heng, ZL Hou, HM Hu, T Hu, B Huang, J Huang, SK Huang, YP Huang, Q Ji, XB Ji, XL Ji, LK Jia, LL Jiang, XS Jiang, DP Jin, S Jin, Y Jin, YF Lai, GK Lei, F Li, G Li, HB Li, HS Li, J Li, JC Li, QJ Li, L Li, RB Li, RY Li, WD Li, WG Li, XN Li, XP Li, XR Li, YR Li, W Li, DX Lin, BJ Liu, CX Liu, F Liu, GM Liu, H Liu, HM Liu, HW Liu, JB Liu, LF Liu, Q Liu, QG Liu, SD Liu, WJ Liu, X Liu, XZ Liu, Y Liu, YJ Liu, ZA Liu, ZQ Liu, ZX Liu, JG Lu, T Lu, YP Lu, XL Luo, HL Ma, QM Ma, X Ma, XY Ma, ZP Mao, J Min, XH Mo, J Nie, ZD Nie, RG Ping, S Qian, Q Qiao, G Qin, ZH Qin, JF Qiu, RG Liu, ZY Ren, G Rong, L Shang, DL Shen, XY Shen, HY Sheng, YF Shi, LW Song, WY Song, DH Sun, GX Sun, HS Sun, LJ Sun, SS Sun, XD Sun, YZ Sun, ZJ Sun, JP Tan, SQ Tang, X Tang, N Tao, HL Tian, YR Tian, X Wan, DY Wang, JK Wang, JZ Wang, K Wang, KX Wang, L Wang, LJ Wang, LS Wang, M Wang",
            "전체 인용횟수": "1697회 인용2010201120122013201420152016201720182019202020212022202322417896106150142194128152101147173160",
            "페이지": "345-399",
            "학술 문서": "Design and construction of the BESIII detectorM Ablikim, ZH An, JZ Bai, N Berger, JM Bian, X Cai… - Nuclear Instruments and Methods in Physics Research …, 20101697회 인용 관련 학술자료 전체 18개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Design and construction of the BESIII detector",
        "year": null
    },
    "Observation of Large  Violation in the Neutral  Meson System": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2001/8/14",
            "게시자": "American Physical Society",
            "권": "87",
            "설명": "We present a measurement of the standard model CP violation parameter sin 2 φ 1 based on a 29.1 fb− 1 data sample collected at the ϒ (4 S) resonance with the Belle detector at the KEKB asymmetric-energy e+ e− collider. One neutral B meson is fully reconstructed as a J/ψ K S, ψ (2 S) K S, χ c 1 K S, η c K S, J/ψ K L, or J/ψ K* 0 decay and the flavor of the accompanying B meson is identified from its decay products. From the asymmetry in the distribution of the time intervals between the two B meson decay points, we determine sin 2 φ 1= 0.99±0.14 (stat)±0.06 (syst). We conclude that we have observed CP violation in the neutral B meson system.",
            "저널": "Physical review letters",
            "저자": "Kazuo Abe, R Abe, I Adachi, Byoung Sup Ahn, H Aihara, M Akatsu, G Alimonti, K Asai, M Asai, Y Asano, T Aso, V Aulchenko, T Aushev, AM Bakich, E Banas, S Behari, PK Behera, D Beiline, A Bondar, A Bozek, TE Browder, BCK Casey, P Chang, Y Chao, K-F Chen, BG Cheon, R Chistov, S-K Choi, Y Choi, LY Dong, J Dragic, A Drutskoy, S Eidelman, V Eiges, Y Enari, R Enomoto, CW Everton, F Fang, H Fujii, C Fukunaga, M Fukushima, N Gabyshev, A Garmash, TJ Gershon, A Gordon, K Gotow, H Guler, R Guo, J Haba, H Hamasaki, K Hanagaki, F Handa, K Hara, T Hara, NC Hastings, H Hayashii, M Hazumi, EM Heenan, Y Higasino, I Higuchi, T Higuchi, T Hirai, H Hirano, T Hojo, T Hokuue, Y Hoshi, K Hoshina, SR Hou, W-S Hou, S-C Hsu, H-C Huang, Y Igarashi, T Iijima, H Ikeda, K Ikeda, K Inami, A Ishikawa, H Ishino, R Itoh, G Iwai, H Iwasaki, Y Iwasaki, DJ Jackson, P Jalocha, HK Jang, M Jones, R Kagan, H Kakuno, J Kaneko, JH Kang, JS Kang, P Kapusta, N Katayama, H Kawai, Y Kawakami, N Kawamura, T Kawasaki, H Kichimi, DW Kim, Heejong Kim, HJ Kim, Hyunwoo Kim, SK Kim, TH Kim, K Kinoshita, S Kobayashi, S Koishi, H Konishi, K Korotushenko, P Krokovny, R Kulasiri, S Kumar, T Kuniya, E Kurihara, A Kuzmin, Y-J Kwon, JS Lange, G Leder, MH Lee, SH Lee, C Leonidopoulos, Y-S Lin, D Liventsev, R-S Lu, J MacNaughton, D Marlow, T Matsubara, S Matsui, S Matsumoto, T Matsumoto, Y Mikami, K Misono, K Miyabayashi, H Miyake, H Miyata, LC Moffitt, GR Moloney, GF Moorhead, S Mori, T Mori, A Murakami, T Nagamine, Y Nagasaka, Y Nagashima, T Nakadaira, T Nakamura, E Nakano, M Nakao, H Nakazawa, JW Nam",
            "전체 인용횟수": "1640회 인용20012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023551981429258705770513739576168566367615667516260",
            "페이지": "091802",
            "학술 문서": "Observation of large CP violation in the neutral B meson systemK Abe, R Abe, I Adachi, BS Ahn, H Aihara, M Akatsu… - Physical review letters, 20011640회 인용 관련 학술자료 전체 24개의 버전 ",
            "호": "9"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Observation of Large  Violation in the Neutral  Meson System",
        "year": null
    },
    "Observation of  Violation in the  Meson System": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2001/8/14",
            "게시자": "American Physical Society",
            "권": "87",
            "설명": "We present an updated measurement of time-dependent CP-violating asymmetries in neutral B decays with the BABAR detector at the PEP-II asymmetric B Factory at SLAC. This result uses an additional sample of ϒ (4 S) decays collected in 2001, bringing the data available to 32× 10 6 B B pairs. We select events in which one neutral B meson is fully reconstructed in a final state containing charmonium and the flavor of the other neutral B meson is determined from its decay products. The amplitude of the CP-violating asymmetry, which in the standard model is proportional to sin 2 β, is derived from the decay time distributions in such events. The result sin 2 β= 0.59±0.14 (stat)±0.05 (syst) establishes CP violation in the B 0 meson system. We also determine| λ|= 0.93±0.09 (stat)±0.03 (syst), consistent with no direct CP violation.",
            "저널": "Physical review letters",
            "저자": "Bernard Aubert, D Boutigny, J-M Gaillard, A Hicheur, Y Karyotakis, JP Lees, P Robbe, V Tisserand, Antimo Palano, GP Chen, JC Chen, ND Qi, G Rong, P Wang, YS Zhu, G Eigen, PL Reinertsen, B Stugu, B Abbott, GS Abrams, AW Borgland, AB Breon, DN Brown, J Button-Shafer, RN Cahn, AR Clark, MS Gill, AV Gritsan, Y Groysman, RG Jacobsen, RW Kadel, J Kadyk, LT Kerth, S Kluth, Yu G Kolomensky, JF Kral, C LeClerc, ME Levi, T Liu, G Lynch, AB Meyer, M Momayezi, PJ Oddone, A Perazzo, M Pripstein, NA Roe, A Romosan, MT Ronan, VG Shelkov, AV Telnov, WA Wenzel, MS Zisman, PG Bright-Thomas, TJ Harrison, CM Hawkes, DJ Knowles, SW O'Neale, RC Penny, AT Watson, NK Watson, T Deppermann, K Goetzen, H Koch, J Krug, M Kunze, B Lewandowski, K Peters, H Schmuecker, M Steinke, JC Andress, NR Barlow, W Bhimji, N Chevalier, PJ Clark, WN Cottingham, N De Groot, N Dyce, B Foster, JD McFall, D Wallom, FF Wilson, K Abe, C Hearty, TS Mattison, JA McKenna, D Thiessen, S Jolly, AK McKemey, J Tinslay, VE Blinov, AD Bukin, DA Bukin, AR Buzykaev, VB Golubev, VN Ivanchenko, AA Korol, EA Kravchenko, AP Onuchin, AA Salnikov, SI Serednyakov, Yu I Skovpen, VI Telnov, AN Yushkov, D Best, AJ Lankford, M Mandelkern, S McMahon, DP Stoker, A Ahsan, K Arisaka, C Buchanan, S Chun, JG Branson, DB MacFarlane, S Prell, Sh Rahatlou, G Raven, V Sharma, C Campagnari, B Dahmes, PA Hart, N Kuznetsova, SL Levy, O Long, A Lu, JD Richman, W Verkerke, M Witherell, S Yellin, J Beringer, DE Dorfan, AM Eisner, A Frey, AA Grillo, M Grothe, CA Heusch, RP Johnson, W Kroeger, William S Lockman, T Pulliam, H Sadrozinski, T Schalk, RE Schmitz, BA Schumm, A Seiden, M Turri, W Walkowiak, DC Williams, MG Wilson, E Chen",
            "전체 인용횟수": "1483회 인용20012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023612061478456635170472833555659454947544050435544",
            "페이지": "091801",
            "학술 문서": "Observation of CP violation in the B 0 meson systemB Aubert, D Boutigny, JM Gaillard, A Hicheur… - Physical review letters, 20011476회 인용 관련 학술자료 전체 43개의 버전 Observation of CP violation in the B0 meson systemC BABAR, JC Andress, NR Barlow, W Bhimji… - Physical Review Letters, 200111회 인용 관련 학술자료 ",
            "호": "9"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Observation of  Violation in the  Meson System",
        "year": null
    },
    "Neutrino physics with JUNO": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/2/10",
            "게시자": "IOP Publishing",
            "권": "43",
            "설명": "The Jiangmen Underground Neutrino Observatory (JUNO), a 20 kton multi-purpose underground liquid scintillator detector, was proposed with the determination of the neutrino mass hierarchy (MH) as a primary physics goal. The excellent energy resolution and the large fiducial volume anticipated for the JUNO detector offer exciting opportunities for addressing many important topics in neutrino and astro-particle physics. In this document, we present the physics motivations and the anticipated performance of the JUNO detector for various proposed measurements. Following an introduction summarizing the current status and open issues in neutrino physics, we discuss how the detection of antineutrinos generated by a cluster of nuclear power plants allows the determination of the neutrino MH at a 3–4σ significance with six years of running of JUNO. The measurement of antineutrino spectrum with excellent energy resolution will also lead to …",
            "저널": "Journal of Physics G: Nuclear and Particle Physics",
            "저자": "Fengpeng An, Guangpeng An, Qi An, Vito Antonelli, Eric Baussan, John Beacom, Leonid Bezrukov, Simon Blyth, Riccardo Brugnera, Margherita Buizza Avanzini, Jose Busto, Anatael Cabrera, Hao Cai, Xiao Cai, Antonio Cammi, Guofu Cao, Jun Cao, Yun Chang, Shaomin Chen, Shenjian Chen, Yixue Chen, Davide Chiesa, Massimiliano Clemenza, Barbara Clerbaux, Janet Conrad, Davide D’Angelo, Herve De Kerret, Zhi Deng, Ziyan Deng, Yayun Ding, Zelimir Djurcic, Damien Dornic, Marcos Dracos, Olivier Drapier, Stefano Dusini, Stephen Dye, Timo Enqvist, Donghua Fan, Jian Fang, Laurent Favart, Richard Ford, Marianne Goeger-Neff, Haonan Gan, Alberto Garfagnini, Marco Giammarchi, Maxim Gonchar, Guanghua Gong, Hui Gong, Michel Gonin, Marco Grassi, Christian Grewing, Mengyun Guan, Vic Guarino, Gang Guo, Wanlei Guo, Xin-Heng Guo, Caren Hagner, Ran Han, Miao He, Yuekun Heng, Yee Hsiung, Jun Hu, Shouyang Hu, Tao Hu, Hanxiong Huang, Xingtao Huang, Lei Huo, Ara Ioannisian, Manfred Jeitler, Xiangdong Ji, Xiaoshan Jiang, Cecile Jollet, Li Kang, Michael Karagounis, Narine Kazarian, Zinovy Krumshteyn, Andre Kruth, Pasi Kuusiniemi, Tobias Lachenmaier, Rupert Leitner, Chao Li, Jiaxing Li, Weidong Li, Weiguo Li, Xiaomei Li, Xiaonan Li, Yi Li, Yufeng Li, Zhi-Bing Li, Hao Liang, Guey-Lin Lin, Tao Lin, Yen-Hsun Lin, Jiajie Ling, Ivano Lippi, Dawei Liu, Hongbang Liu, Hu Liu, Jianglai Liu, Jianli Liu, Jinchang Liu, Qian Liu, Shubin Liu, Shulin Liu, Paolo Lombardi, Yongbing Long, Haoqi Lu, Jiashu Lu, Jingbin Lu, Junguang Lu, Bayarto Lubsandorzhiev, Livia Ludhova, Shu Luo, Vladimir Lyashuk, Randolph Moellenberg, Xubo Ma, Fabio Mantovani, Yajun Mao, Stefano M Mari, William F McDonough, Guang Meng, Anselmo Meregaglia, Emanuela Meroni, Mauro Mezzetto, Lino Miramonti, Thomas Mueller, Dmitry Naumov, Lothar Oberauer, Juan Pedro Ochoa-Ricoux, Alexander Olshevskiy, Fausto Ortica, Alessandro Paoloni, Haiping Peng, Jen-Chieh Peng, Ezio Previtali, Ming Qi, Sen Qian, Xin Qian, Yongzhong Qian, Zhonghua Qin, Georg Raffelt, Gioacchino Ranucci, Barbara Ricci, Markus Robens, Aldo Romani, Xiangdong Ruan, Xichao Ruan, Giuseppe Salamanna, Mike Shaevitz, Valery Sinev",
            "전체 인용횟수": "1336회 인용2015201620172018201920202021202220231380130171147185210209182",
            "페이지": "030401",
            "학술 문서": "Neutrino physics with JUNOF An, G An, Q An, V Antonelli, E Baussan, J Beacom… - Journal of Physics G: Nuclear and Particle Physics, 20161336회 인용 관련 학술자료 전체 29개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Neutrino physics with JUNO",
        "year": null
    },
    "Observation of a Broad Structure in the  Mass Spectrum around ": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/9/28",
            "게시자": "American Physical Society",
            "권": "95",
            "설명": "We study initial-state radiation events, e+ e−→ γ ISR π+ π− J/ψ, with data collected with the BABAR detector. We observe an accumulation of events near 4.26 GeV/c 2 in the invariant-mass spectrum of π+ π− J/ψ. Fits to the mass spectrum indicate that a broad resonance with a mass of about 4.26 GeV/c 2 is required to describe the observed structure. The presence of additional narrow resonances cannot be excluded. The fitted width of the broad resonance is 50 to 90 MeV/c 2, depending on the fit hypothesis.",
            "저널": "Physical review letters",
            "저자": "Bernard Aubert, R Barate, D Boutigny, F Couderc, Y Karyotakis, JP Lees, V Poireau, V Tisserand, A Zghiche, E Grauges, A Palano, M Pappagallo, A Pompili, JC Chen, ND Qi, G Rong, P Wang, YS Zhu, G Eigen, I Ofte, B Stugu, GS Abrams, M Battaglia, AB Breon, DN Brown, J Button-Shafer, RN Cahn, E Charles, CT Day, MS Gill, AV Gritsan, Y Groysman, RG Jacobsen, RW Kadel, J Kadyk, LT Kerth, Yu G Kolomensky, G Kukartsev, G Lynch, LM Mir, PJ Oddone, TJ Orimoto, M Pripstein, NA Roe, MT Ronan, WA Wenzel, M Barrett, KE Ford, TJ Harrison, AJ Hart, CM Hawkes, SE Morgan, AT Watson, M Fritsch, K Goetzen, T Held, H Koch, B Lewandowski, M Pelizaeus, K Peters, T Schroeder, M Steinke, JT Boyd, JP Burke, N Chevalier, WN Cottingham, T Cuhadar-Donszelmann, BG Fulsom, C Hearty, NS Knecht, TS Mattison, JA McKenna, A Khan, P Kyberd, M Saleem, L Teodorescu, AE Blinov, VE Blinov, AD Bukin, VP Druzhinin, VB Golubev, EA Kravchenko, AP Onuchin, SI Serednyakov, Yu I Skovpen, EP Solodov, AN Yushkov, D Best, M Bondioli, M Bruinsma, M Chao, S Curry, I Eschrich, D Kirkby, AJ Lankford, P Lund, M Mandelkern, RK Mommsen, W Roethel, DP Stoker, C Buchanan, BL Hartfiel, AJR Weinstein, SD Foulkes, JW Gary, O Long, BC Shen, K Wang, L Zhang, D Del Re, HK Hadavand, EJ Hill, DB MacFarlane, HP Paar, S Rahatlou, V Sharma, JW Berryhill, C Campagnari, A Cunha, B Dahmes, TM Hong, MA Mazur, JD Richman, W Verkerke, TW Beck, AM Eisner, CJ Flacco, CA Heusch, J Kroseberg, WS Lockman, G Nesom, T Schalk, BA Schumm, A Seiden, P Spradlin, DC Williams, MG Wilson, J Albert, E Chen, GP Dubois-Felsmann, A Dvoretskii, DG Hitlin, I Narsky, T Piatenko, FC Porter, A Ryd, A Samuel, R Andreassen, S Jayatilleke, G Mancinelli",
            "전체 인용횟수": "1233회 인용2005200620072008200920102011201220132014201520162017201820192020202120222023107676901067747607065845283586458584948",
            "페이지": "142001",
            "학술 문서": "Observation of a broad structure in the π+ π− J/ψ mass spectrum around 4.26 GeV/c 2B Aubert, R Barate, D Boutigny, F Couderc… - Physical review letters, 20051233회 인용 관련 학술자료 전체 39개의 버전 ",
            "호": "14"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Observation of a Broad Structure in the  Mass Spectrum around ",
        "year": null
    },
    "Observation of a Charged Charmoniumlike Structure in  at ": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/6/17",
            "게시자": "American Physical Society",
            "권": "110",
            "설명": "We study the process e+ e−→ π+ π− J/ψ at a center-of-mass energy of 4.260 GeV using a 525 pb− 1 data sample collected with the BESIII detector operating at the Beijing Electron Positron Collider. The Born cross section is measured to be (62.9±1.9±3.7) pb, consistent with the production of the Y (4260). We observe a structure at around 3.9 GeV/c 2 in the π±J/ψ mass spectrum, which we refer to as the Z c (3900). If interpreted as a new particle, it is unusual in that it carries an electric charge and couples to charmonium. A fit to the π±J/ψ invariant mass spectrum, neglecting interference, results in a mass of (3899.0±3.6±4.9) MeV/c 2 and a width of (46±10±20) MeV. Its production ratio is measured to be R=(σ (e+ e−→ π±Z c (3900)∓→ π+ π− J/ψ)/σ (e+ e−→ π+ π− J/ψ))=(21.5±3.3±7.5)%. In all measurements the first errors are statistical and the second are systematic.",
            "저널": "Physical review letters",
            "저자": "M Ablikim, MN Achasov, XC Ai, O Albayrak, DJ Ambrose, FF An, Q An, JZ Bai, R Baldini Ferroli, Y Ban, J Becker, JV Bennett, M Bertani, JM Bian, E Boger, O Bondarenko, I Boyko, RA Briere, V Bytev, H Cai, X Cai, O Cakir, A Calcaterra, GF Cao, SA Cetin, JF Chang, G Chen, HS Chen, JC Chen, ML Chen, SJ Chen, X Chen, YB Chen, HP Cheng, YP Chu, D Cronin-Hennessy, HL Dai, JP Dai, D Dedovich, ZY Deng, A Denig, I Denysenko, M Destefanis, WM Ding, Y Ding, LY Dong, MY Dong, SX Du, J Fang, SS Fang, L Fava, CQ Feng, P Friedel, CD Fu, JL Fu, O Fuks, Q Gao, Y Gao, C Geng, K Goetzen, WX Gong, W Gradl, Michela Greco, MH Gu, YT Gu, YH Guan, AQ Guo, LB Guo, T Guo, YP Guo, YL Han, FA Harris, KL He, M He, ZY He, T Held, YK Heng, ZL Hou, C Hu, HM Hu, JF Hu, T Hu, GM Huang, GS Huang, JS Huang, L Huang, XT Huang, Y Huang, YP Huang, T Hussain, CS Ji, Q Ji, QP Ji, XB Ji, XL Ji, LL Jiang, XS Jiang, JB Jiao, Z Jiao, DP Jin, S Jin, FF Jing, N Kalantar-Nayestanaki, M Kavatsyuk, B Kopf, M Kornicer, W Kühn, W Lai, JS Lange, M Lara, P Larin, M Leyhe, CH Li, Cheng Li, Cui Li, DM Li, F Li, G Li, HB Li, JC Li, K Li, Lei Li, QJ Li, SL Li, WD Li, WG Li, XL Li, XN Li, XQ Li, XR Li, ZB Li, H Liang, YF Liang, YT Liang, GR Liao, XT Liao, D Lin, BJ Liu, CL Liu, CX Liu, FH Liu, Fang Liu, Feng Liu, H Liu, HB Liu, HH Liu, HM Liu, HW Liu, JP Liu, K Liu",
            "전체 인용횟수": "1226회 인용201320142015201620172018201920202021202220238015116713911211799861039272",
            "페이지": "252001",
            "학술 문서": "Observation of a Charged Charmoniumlike Structure in e+ e−→ π+ π− J/ψ at s= 4.26 GeVM Ablikim, MN Achasov, XC Ai, O Albayrak… - Physical review letters, 20131215회 인용 관련 학술자료 전체 19개의 버전 Observation of a Charged Charmoniumlike Structure in [e. sup.+][e. sup.-][right arrow][[pi]. sup.+][[pi]. sup.-] J/[psi] at [square root of s]= 4.26 GeV*M Ablikim - Phys. Rev. Lett, 201321회 인용 관련 학술자료 ",
            "호": "25"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Observation of a Charged Charmoniumlike Structure in  at ",
        "year": null
    },
    "Observation of a Narrow Meson State Decaying to  at a Mass of ": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2003/6/17",
            "게시자": "American Physical Society",
            "권": "90",
            "설명": "We have observed a narrow state near 2.32 G e V/c 2 in the inclusive D s+ π 0 invariant mass distribution from e+ e− annihilation data at energies near 10.6 GeV. The observed width is consistent with the experimental resolution. The small intrinsic width and the quantum numbers of the final state indicate that the decay violates isospin conservation. The state has natural spin-parity and the low mass suggests a J P= 0+ assignment. The data sample corresponds to an integrated luminosity of 91 f b− 1 recorded by the BABAR detector at the SLAC PEP-II asymmetric-energy e+ e− storage ring.",
            "저널": "Physical Review Letters",
            "저자": "Bernard Aubert, R Barate, D Boutigny, J-M Gaillard, A Hicheur, Y Karyotakis, JP Lees, P Robbe, V Tisserand, A Zghiche, Antimo Palano, A Pompili, JC Chen, ND Qi, G Rong, P Wang, YS Zhu, G Eigen, I Ofte, B Stugu, GS Abrams, AW Borgland, AB Breon, DN Brown, J Button-Shafer, RN Cahn, E Charles, CT Day, MS Gill, AV Gritsan, Y Groysman, RG Jacobsen, RW Kadel, J Kadyk, LT Kerth, Yu G Kolomensky, JF Kral, G Kukartsev, C LeClerc, ME Levi, G Lynch, LM Mir, PJ Oddone, TJ Orimoto, M Pripstein, NA Roe, A Romosan, MT Ronan, VG Shelkov, AV Telnov, WA Wenzel, K Ford, TJ Harrison, CM Hawkes, DJ Knowles, SE Morgan, RC Penny, AT Watson, NK Watson, T Deppermann, K Goetzen, H Koch, B Lewandowski, M Pelizaeus, K Peters, H Schmuecker, M Steinke, NR Barlow, JT Boyd, N Chevalier, WN Cottingham, MP Kelly, TE Latham, C Mackay, FF Wilson, K Abe, T Cuhadar-Donszelmann, C Hearty, TS Mattison, JA McKenna, D Thiessen, P Kyberd, AK McKemey, VE Blinov, AD Bukin, VB Golubev, VN Ivanchenko, EA Kravchenko, AP Onuchin, SI Serednyakov, Yu I Skovpen, EP Solodov, AN Yushkov, D Best, M Chao, D Kirkby, AJ Lankford, M Mandelkern, S McMahon, RK Mommsen, W Roethel, DP Stoker, C Buchanan, Daniele del Re, HK Hadavand, EJ Hill, DB MacFarlane, HP Paar, Sh Rahatlou, U Schwanke, V Sharma, JW Berryhill, C Campagnari, B Dahmes, N Kuznetsova, SL Levy, O Long, A Lu, MA Mazur, JD Richman, W Verkerke, TW Beck, J Beringer, AM Eisner, CA Heusch, WS Lockman, T Schalk, RE Schmitz, BA Schumm, A Seiden, M Turri, W Walkowiak, DC Williams, MG Wilson, J Albert, E Chen, GP Dubois-Felsmann, A Dvoretskii, DG Hitlin, I Narsky, FC Porter, A Ryd, A Samuel, S Yang, S Jayatilleke, G Mancinelli, BT Meadows, MD Sokoloff, T Abe, T Barillari",
            "전체 인용횟수": "1103회 인용20032004200520062007200820092010201120122013201420152016201720182019202020212022202362142110768366474638442336325037392925353239",
            "페이지": "242001",
            "학술 문서": "Observation of a Narrow Meson State Decaying to D s+ π 0 at a Mass of 2.32 G e V/c 2B Aubert, R Barate, D Boutigny, JM Gaillard, A Hicheur… - Physical Review Letters, 20031103회 인용 관련 학술자료 전체 43개의 버전 ",
            "호": "24"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Observation of a Narrow Meson State Decaying to  at a Mass of ",
        "year": null
    },
    "Measurement of the ratio of branching fractions B (B¯ 0→ D*+ τ− ν¯ τ)/B (B¯ 0→ D*+ μ− ν¯ μ)": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/9/9",
            "게시자": "American Physical Society",
            "권": "115",
            "설명": "The branching fraction ratio R (D*)≡ B (B 0→ D*+ τ− ν τ)/B (B 0→ D*+ μ− ν μ) is measured using a sample of proton-proton collision data corresponding to 3.0 fb− 1 of integrated luminosity recorded by the LHCb experiment during 2011 and 2012. The tau lepton is identified in the decay mode τ−→ μ− ν μ ν τ. The semitauonic decay is sensitive to contributions from non-standard-model particles that preferentially couple to the third generation of fermions, in particular, Higgs-like charged scalars. A multidimensional fit to kinematic distributions of the candidate B 0 decays gives R (D*)= 0.336±0.027 (stat)±0.030 (syst). This result, which is the first measurement of this quantity at a hadron collider, is 2.1 standard deviations larger than the value expected from lepton universality in the standard model.",
            "저널": "Physical review letters",
            "저자": "Roel Aaij, B Adeva, M Adinolfi, A Affolder, Z Ajaltouni, S Akar, J Albrecht, F Alessio, M Alexander, S Ali, G Alkhazov, P Alvarez Cartelle, AA Alves Jr, S Amato, S Amerio, Y Amhis, L An, L Anderlini, J Anderson, G Andreassi, M Andreotti, JE Andrews, RB Appleby, O Aquines Gutierrez, F Archilli, P d’Argent, A Artamonov, M Artuso, E Aslanides, Giulio Auriemma, M Baalouch, S Bachmann, JJ Back, A Badalov, C Baesso, W Baldini, RJ Barlow, C Barschel, S Barsuk, W Barter, V Batozskaya, V Battista, A Bay, L Beaucourt, J Beddow, F Bedeschi, I Bediaga, LJ Bel, V Bellee, I Belyaev, E Ben-Haim, G Bencivenni, S Benson, J Benton, A Berezhnoy, R Bernet, A Bertolin, M-O Bettler, Martin Van Beuzekom, A Bien, S Bifani, T Bird, A Birnkraut, A Bizzeti, T Blake, F Blanc, Johan Blouw, S Blusk, V Bocci, A Bondar, N Bondar, W Bonivento, S Borghi, M Borsato, TJV Bowcock, E Bowen, C Bozzi, S Braun, D Brett, Markward Britsch, T Britton, J Brodzicka, NH Brook, A Bursche, J Buytaert, S Cadeddu, R Calabrese, M Calvi, M Calvo Gomez, P Campana, D Campora Perez, L Capriotti, A Carbone, G Carboni, R Cardinale, A Cardini, P Carniti, L Carson, K Carvalho Akiba, G Casse, L Cassina, L Castillo Garcia, M Cattaneo, Ch Cauet, G Cavallero, R Cenci, M Charles, Ph Charpentier, M Chefdeville, S Chen, S-F Cheung, N Chiapolini, M Chrzaszcz, X Cid Vidal, G Ciezarek, PEL Clarke, M Clemencic, HV Cliff, J Closier, V Coco, J Cogan, E Cogneras, V Cogoni, L Cojocariu, G Collazuol, P Collins, A Comerma-Montells, A Contu, A Cook, M Coombes, S Coquereau, G Corti, M Corvo, B Couturier, GA Cowan, DC Craik, A Crocombe, M Cruz Torres, S Cunliffe, R Currie, C D’Ambrosio, E Dall’Occo, J Dalseno, PNY David, A Davis, K De Bruyn, S De Capua, M De Cian, JM De Miranda, L De Paula",
            "전체 인용횟수": "1099회 인용2015201620172018201920202021202220232110616818318012010312585",
            "페이지": "111803",
            "학술 문서": "Measurement of the ratio of branching fractions B (B¯ 0→ D*+ τ− ν¯ τ)/B (B¯ 0→ D*+ μ− ν¯ μ)R Aaij, B Adeva, M Adinolfi, A Affolder, Z Ajaltouni… - Physical review letters, 20151097회 인용 관련 학술자료 전체 41개의 버전 Measurement of the Ratio of Branching Fractions B (anti-B0→ D*+ τ− anti-ντ)/B (anti-B0→ D*+ μ− anti-νμ)*R Aaij, B Adeva, M Adinolfi, A Affolder, Z Ajaltouni… - PHYSICAL REVIEW LETTERS, 20152회 인용 관련 학술자료 전체 2개의 버전 ",
            "호": "11"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Measurement of the ratio of branching fractions B (B¯ 0→ D*+ τ− ν¯ τ)/B (B¯ 0→ D*+ μ− ν¯ μ)",
        "year": null
    },
    "Measurement of the Inelastic Proton-Proton Cross Section at  with the ATLAS Detector at the LHC": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/10/26",
            "게시자": "American Physical Society",
            "권": "117",
            "설명": "This Letter presents a measurement of the inelastic proton-proton cross section using 60 μ b− 1 of p p collisions at a center-of-mass energy s of 13 TeV with the ATLAS detector at the LHC. Inelastic interactions are selected using rings of plastic scintillators in the forward region (2.07<| η|< 3.86) of the detector. A cross section of 68.1±1.4 mb is measured in the fiducial region ξ= M X 2/s> 10− 6, where M X is the larger invariant mass of the two hadronic systems separated by the largest rapidity gap in the event. In this ξ range the scintillators are highly efficient. For diffractive events this corresponds to cases where at least one proton dissociates to a system with M X> 13 GeV. The measured cross section is compared with a range of theoretical predictions. When extrapolated to the full phase space, a cross section of 78.1±2.9 mb is measured, consistent with the inelastic cross section increasing with center-of-mass energy.",
            "저널": "Physical review letters",
            "저자": "Morad Aaboud, G Aad, B Abbott, J Abdallah, Baptiste Abeloos, Rosemarie Aben, OS AbouZeid, NL Abraham, Halina Abramowicz, Henso Abreu, Ricardo Abreu, Yiming Abulaiti, Bobby Sami Acharya, Leszek Adamczyk, DL Adams, Jahred Adelman, Stefanie Adomeit, Tim Adye, AA Affolder, Tatjana Agatonovic-Jovin, Juan Anton Aguilar-Saavedra, SP Ahlen, Faig Ahmadov, Giulio Aielli, Henrik Akerstedt, TPA Åkesson, AV Akimov, Gian Luigi Alberghi, Justin Albert, Solveig Albrand, MJ Alconada Verzini, Martin Aleksa, IN Aleksandrov, Calin Alexa, Gideon Alexander, Theodoros Alexopoulos, Muhammad Alhroob, Babar Ali, Malik Aliev, Gianluca Alimonti, John Alison, Steven Pat Alkire, BMM Allbrooke, Benjamin W Allen, PP Allport, Alberto Aloisio, Alejandro Alonso, Francisco Alonso, Cristiano Alpigiani, Mahmoud Alstaty, B Alvarez Gonzalez, D Álvarez Piqueras, MG Alviggi, Brian Thom Amadio, Katsuya Amako, Y Amaral Coutinho, Christoph Amelung, Dante Amidei, SP Amor Dos Santos, Antonio Amorim, Simone Amoroso, Glenn Amundsen, Christos Anastopoulos, Lucian Ste Ancu, Nansi Andari, Timothy Andeen, CF Anders, Gabriel Anders, John Kenne Anders, KJ Anderson, Attilio Andreazza, V Andrei, Stylianos Angelidakis, Ivan Angelozzi, Philipp Anger, Aaron Angerami, Francis Anghinolfi, AV Anisenkov, Nuno Anjos, Alberto Annovi, Claire Antel, Mario Antonelli, Alexey Antonov, Fabio Anulli, Masato Aoki, L Aperio Bella, Giorgi Arabidze, Yasuo Arai, Juan Pedro Araque, ATH Arce, FA Arduh, Jean-Franc Arguin, Spyridon Argyropoulos, Metin Arik, Aaron Jame Armbruster, Lewis Jame Armitage, Olivier Arnaez, Hannah Arnold, Miguel Arratia, Ozan Arslan, Andrei Artamonov, Giacomo Artoni, Sebastian Artz, Shoji Asai, Nedaa Asbah, Adi Ashkenazi, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, K Augsten, G Avolio, B Axen, MK Ayoub, G Azuelos, MA Baak, AE Baas, MJ Baca, H Bachacou, K Bachas, M Backes, M Backhaus, P Bagiacchi, Paolo Bagnaia, Y Bai, JT Baines, OK Baker, EM Baldin, P Balek, T Balestri, F Balli, WK Balunas, E Banas, Sw Banerjee, AAE Bannoura, L Barak, EL Barberio, D Barberis, M Barbero, T Barillari, MS Barisits, T Barklow, N Barlow, SL Barnes, BM Barnett, RM Barnett, Z Barnovska, A Baroncelli",
            "전체 인용횟수": "1060회 인용20162017201820192020202120222023221282501771711356898",
            "페이지": "182002",
            "학술 문서": "Measurement of the Inelastic Proton-Proton Cross Section at s= 13 TeV with the ATLAS Detector at the LHCM Aaboud, G Aad, B Abbott, J Abdallah, B Abeloos… - Physical review letters, 20161060회 인용 관련 학술자료 전체 95개의 버전 Measurement of the Inelastic Proton-Proton Cross Section at√ s= 13 TeV with the ATLAS Detector at the LHCMJ Alconada Verzini, F Alonso, FA Arduh, MT Dova… - Physical Review Letters, 2016전체 5개의 버전 ",
            "호": "18"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Measurement of the Inelastic Proton-Proton Cross Section at  with the ATLAS Detector at the LHC",
        "year": null
    },
    "Study of the  decay and measurement of the  branching fraction": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/4/22",
            "게시자": "American Physical Society",
            "권": "71",
            "설명": "We study the decay B−→ J/ψ K− π+ π− using 117× 10 6 B B events collected at the Y (4 S) resonance with the BABAR detector at the PEP-II e+ e− asymmetric-energy storage ring. We measure the branching fractions B (B−→ J/ψ K− π+ π−)=(116±7 (stat.)±9 (syst.))× 10− 5 and B (B−→ X (3872) K−)× B (X (3872)→ J/ψ π+ π−)=(1.28±0.41)× 10− 5 and find the mass of the X (3872) to be 3873.4±1.4 MeV/c 2. We search for the h c narrow state in the decay B−→ h c K−, h c→ J/ψ π+ π− and for the decay B−→ J/ψ D 0 π−, with D 0→ K− π+. We set the 90% CL limits B (B−→ h c K−)× B (h c→ J/ψ π+ π−)< 3.4× 10− 6 and B (B−→ J/ψ D 0 π−)< 5.2× 10− 5.",
            "저널": "Physical Review D",
            "저자": "Bernard Aubert, R Barate, D Boutigny, F Couderc, J-M Gaillard, A Hicheur, Y Karyotakis, JP Lees, V Tisserand, A Zghiche, A Palano, A Pompili, JC Chen, ND Qi, G Rong, P Wang, YS Zhu, G Eigen, I Ofte, B Stugu, GS Abrams, AW Borgland, AB Breon, DN Brown, J Button-Shafer, RN Cahn, E Charles, CT Day, MS Gill, AV Gritsan, Y Groysman, RG Jacobsen, RW Kadel, J Kadyk, LT Kerth, Yu G Kolomensky, G Kukartsev, C LeClerc, G Lynch, AM Merchant, LM Mir, PJ Oddone, TJ Orimoto, M Pripstein, NA Roe, MT Ronan, VG Shelkov, WA Wenzel, K Ford, TJ Harrison, CM Hawkes, SE Morgan, AT Watson, M Fritsch, K Goetzen, T Held, H Koch, B Lewandowski, M Pelizaeus, M Steinke, JT Boyd, N Chevalier, WN Cottingham, MP Kelly, TE Latham, FF Wilson, T Cuhadar-Donszelmann, C Hearty, NS Knecht, TS Mattison, JA McKenna, D Thiessen, A Khan, P Kyberd, L Teodorescu, VE Blinov, AD Bukin, VP Druzhinin, VB Golubev, VN Ivanchenko, EA Kravchenko, AP Onuchin, SI Serednyakov, Yu I Skovpen, EP Solodov, AN Yushkov, D Best, M Bruinsma, M Chao, I Eschrich, D Kirkby, AJ Lankford, M Mandelkern, RK Mommsen, W Roethel, DP Stoker, C Buchanan, BL Hartfiel, JW Gary, BC Shen, K Wang, Daniele del Re, HK Hadavand, EJ Hill, DB MacFarlane, HP Paar, Sh Rahatlou, V Sharma, JW Berryhill, C Campagnari, B Dahmes, SL Levy, O Long, A Lu, MA Mazur, JD Richman, W Verkerke, TW Beck, AM Eisner, CA Heusch, WS Lockman, T Schalk, RE Schmitz, BA Schumm, A Seiden, P Spradlin, DC Williams, MG Wilson, J Albert, E Chen, GP Dubois-Felsmann, A Dvoretskii, DG Hitlin, I Narsky, T Piatenko, FC Porter, A Ryd, A Samuel, S Yang, S Jayatilleke, G Mancinelli, BT Meadows, MD Sokoloff, T Abe, F Blanc, P Bloom, S Chen, WT Ford, U Nauenberg, A Olivas",
            "전체 인용횟수": "896회 인용200520062007200820092010201120122013201420152016201720182019202020212022202318606759596649386366594750343623403225",
            "페이지": "071103",
            "학술 문서": "Study of the B−→ J/ψ K− π+ π− decay and measurement of the B−→ X (3872) K− branching fractionB Aubert, R Barate, D Boutigny, F Couderc, JM Gaillard… - Physical Review D, 2005896회 인용 관련 학술자료 전체 46개의 버전 ",
            "호": "7"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Study of the  decay and measurement of the  branching fraction",
        "year": null
    },
    "Measurement of Higgs boson production in the diphoton decay channel in  collisions at center-of-mass energies of 7 and 8 TeV with the ATLAS detector": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/12/24",
            "게시자": "American Physical Society",
            "권": "90",
            "설명": "A measurement of the production processes of the recently discovered Higgs boson is performed in the two-photon final state using 4.5 fb− 1 of proton-proton collisions data at s= 7 TeV and 20.3 fb− 1 at s= 8 TeV collected by the ATLAS detector at the Large Hadron Collider. The number of observed Higgs boson decays to diphotons divided by the corresponding Standard Model prediction, called the signal strength, is found to be μ= 1.17±0.27 at the value of the Higgs boson mass measured by ATLAS, m H= 125.4 GeV. The analysis is optimized to measure the signal strengths for individual Higgs boson production processes at this value of m H. They are found to be μ ggF= 1.32±0.38, μ VBF= 0.8±0.7, μ W H= 1.0±1.6, μ Z H= 0.1− 0.1+ 3.7, and μ t t H= 1.6− 1.8+ 2.7, for Higgs boson production through gluon fusion, vector-boson fusion, and in association with a W or Z boson or a top-quark pair, respectively …",
            "저널": "Physical Review D",
            "저자": "Georges Aad, Brad Abbott, Jalal Abdallah, S Abdel Khalek, R Aben, B Abi, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, J Adelman, S Adomeit, T Adye, T Agatonovic-Jovin, JA Aguilar-Saavedra, M Agustoni, SP Ahlen, F Ahmadov, G Aielli, H Akerstedt, TPA Åkesson, G Akimoto, AV Akimov, GL Alberghi, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, G Alimonti, L Alio, J Alison, BMM Allbrooke, LJ Allison, PP Allport, A Aloisio, A Alonso, F Alonso, C Alpigiani, A Altheimer, B Alvarez Gonzalez, MG Alviggi, K Amako, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, S Angelidakis, I Angelozzi, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, JP Araque, ATH Arce, JF Arguin, S Argyropoulos, M Arik, AJ Armbruster, O Arnaez, V Arnal, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, A Ashkenazi, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, B Auerbach, K Augsten, M Aurousseau, G Avolio, G Azuelos, Y Azuma, MA Baak, AE Baas, C Bacci, H Bachacou, K Bachas, M Backes, M Backhaus, J Backus Mayes, E Badescu, P Bagiacchi, Paolo Bagnaia, Y Bai, T Bain, JT Baines, OK Baker, P Balek, F Balli, E Banas, Sw Banerjee, AAE Bannoura, V Bansal, HS Bansil, L Barak, SP Baranov, EL Barberio, D Barberis, M Barbero, T Barillari, M Barisonzi, T Barklow, N Barlow, BM Barnett",
            "전체 인용횟수": "872회 인용201420152016201720182019202020212022202323237178123865345393547",
            "페이지": "112015",
            "학술 문서": "Measurement of Higgs boson production in the diphoton decay channel in p p collisions at center-of-mass energies of 7 and 8 TeV with the ATLAS detectorG Aad, B Abbott, J Abdallah, SA Khalek, R Aben, B Abi… - Physical Review D, 2014868회 인용 관련 학술자료 전체 92개의 버전 Measurement of Higgs boson production in the diphoton decay channel in pp collisions at center-of-mass energies of 7 and 8 TeV with the ATLAS detectorG Aad, B Abbott, J Abdallah, S Abdel Khalek, R Aben… - 20145회 인용 관련 학술자료 Measurement of Higgs boson production in the diphoton decay channel in pp collisions at center-of-mass energies of 7 and 8 TeV with the ATLAS detectorATLAS Collaboration - 2014관련 학술자료 전체 2개의 버전 ",
            "호": "11"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Measurement of Higgs boson production in the diphoton decay channel in  collisions at center-of-mass energies of 7 and 8 TeV with the ATLAS detector",
        "year": null
    },
    "Observation of  Production": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/6/4",
            "게시자": "American Physical Society",
            "권": "120",
            "설명": "The observation of Higgs boson production in association with a top quark-antiquark pair is reported, based on a combined analysis of proton-proton collision data at center-of-mass energies of s= 7, 8, and 13 TeV, corresponding to integrated luminosities of up to 5.1, 19.7, and 35.9 fb− 1, respectively. The data were collected with the CMS detector at the CERN LHC. The results of statistically independent searches for Higgs bosons produced in conjunction with a top quark-antiquark pair and decaying to pairs of W bosons, Z bosons, photons, τ leptons, or bottom quark jets are combined to maximize sensitivity. An excess of events is observed, with a significance of 5.2 standard deviations, over the expectation from the background-only hypothesis. The corresponding expected significance from the standard model for a Higgs boson mass of 125.09 GeV is 4.2 standard deviations. The combined best fit signal strength …",
            "저널": "Physical review letters",
            "저자": "Albert M Sirunyan, Armen Tumasyan, Wolfgang Adam, Federico Ambrogi, Ece Asilar, Thomas Bergauer, Johannes Brandstetter, Marko Dragicevic, Janos Erö, A Escalante Del Valle, Martin Flechl, Rudolf Fruehwirth, Vasile Mihai Ghete, Josef Hrubec, Manfred Jeitler, Natascha Krammer, Ilse Krätschmer, Dietrich Liko, Thomas Madlener, Ivan Mikulec, Navid Rad, Herbert Rohringer, J Schieck, R Schoefbeck, M Spanring, D Spitzbart, A Taurok, W Waltenberger, J Wittmann, C-E Wulz, M Zarucki, J Suarez Gonzalez, EA De Wolf, D Di Croce, X Janssen, J Lauwers, M Pieters, M Van De Klundert, H Van Haevermaet, P Van Mechelen, S Abu Zeid, F Blekman, J D’Hondt, I De Bruyn, J De Clercq, K Deroover, G Flouris, D Lontkovskyi, S Lowette, I Marchesini, S Moortgat, L Moreels, Q Python, K Skovpen, S Tavernier, W Van Doninck, P Van Mulders, I Van Parijs, D Beghin, B Bilin, H Brun, B Clerbaux, G De Lentdecker, H Delannoy, B Dorney, G Fasanella, L Favart, R Goldouzian, A Grebenyuk, AK Kalsi, T Lenzi, J Luetic, N Postiau, E Starling, L Thomas, C Vander Velde, Pascal Vanlaer, David Vannerom, Qun Wang, Tom Cornelis, Didar Dobur, Alexis Fagot, Muhammad Gul, Illia Khvastunov, Deniz Poyraz, Christos Roskas, Daniele Trocino, Michael Tytgat, Willem Verbeke, Basile Vermassen, Martina Vit, Nikolaos Zaganidis, Hamed Bakhshiansohi, Olivier Bondu, Sébastien Brochet, Giacomo Bruno, Claudio Caputo, Pieter David, Christophe Delaere, Martin Delcourt, Brieuc Francois, Andrea Giammanco, Georgios Krintiras, Vincent Lemaitre, Alessio Magitteri, Alexandre Mertens, Marco Musich, Krzysztof Piotrzkowski, Alessia Saggio, M Vidal Marono, Sébastien Wertz, Joze Zobec, Fábio Lúcio Alves, GA Alves, Lucas Brito, M Correa Martins Junior, G Correia Silva, Carsten Hensel, Arthur Moraes, Maria Elena Pol, P Rebello Teles, E Belchior Batista Das Chagas, Wagner Carvalho, Jose Chinellato, Eduardo Coelho, EM Da Costa, Gustavo Gil Da Silveira, D De Jesus Damiao, C De Oliveira Martins, S Fonseca De Souza, Helena Malbouisson, D Matos Figueiredo, M Melo De Almeida, C Mora Herrera, Luiz Mundim, Helio Nogima, WL Prado Da Silva, LJ Sanchez Rosas, Alberto Santoro, Andre Sznajder, Mauricio Thiel, EJ Tonelli Manganote, F Torres Da Silva De Araujo, A Vilela Pereira, Sudha Ahuja, Cesar Augusto Bernardes, Luigi Calligaris, TR Fernandez Perez Tomei, EM Gregores, Pedro G Mercadante",
            "전체 인용횟수": "859회 인용201720182019202020212022202331352571581459463",
            "페이지": "231801",
            "학술 문서": "Observation of t t¯ H productionAM Sirunyan, A Tumasyan, W Adam, F Ambrogi… - Physical review letters, 2018856회 인용 관련 학술자료 전체 111개의 버전 Observation of (tt) over-barH productionAM Sirunyan, A Tumasyan, W Adam, F Ambrogi… - 20183회 인용 관련 학술자료 Observation of t¯ t H Production*SA Zeid, F Blekman, J D'Hondt, IH De Bruyn… - Phys. Rev. Lett., 2018",
            "호": "23"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Observation of  Production",
        "year": null
    },
    "Glove: Global vectors for word representation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/10",
            "설명": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
            "저자": "Jeffrey Pennington, Richard Socher, Christopher D Manning",
            "전체 인용횟수": "37826회 인용20152016201720182019202020212022202339510872053371455966469673663585071",
            "컨퍼런스": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)",
            "페이지": "1532-1543",
            "학술 문서": "Glove: Global vectors for word representationJ Pennington, R Socher, CD Manning - Proceedings of the 2014 conference on empirical …, 201437726회 인용 관련 학술자료 전체 27개의 버전 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)*K Cho, B van Merrienboer, C Gulcehre, D Bahdanau… - Association for Computational Linguistics. https://doi …, 2014104회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Glove: Global vectors for word representation",
        "year": null
    },
    "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013",
            "설명": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.",
            "저자": "Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, Christopher Potts Potts",
            "전체 인용횟수": "8191회 인용20142015201620172018201920202021202220231944375857338709441021108611611056",
            "컨퍼런스": "EMNLP",
            "학술 문서": "Recursive deep models for semantic compositionality over a sentiment treebankR Socher, A Perelygin, J Wu, J Chuang, CD Manning… - Proceedings of the 2013 conference on empirical …, 20138191회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
        "year": null
    },
    "Improved semantic representations from tree-structured long short-term memory networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/2/28",
            "설명": "Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).",
            "저널": "arXiv preprint arXiv:1503.00075",
            "저자": "Kai Sheng Tai, Richard Socher, Christopher D Manning",
            "전체 인용횟수": "3788회 인용20152016201720182019202020212022202364192326523629636556465362",
            "학술 문서": "Improved semantic representations from tree-structured long short-term memory networksKS Tai, R Socher, CD Manning - arXiv preprint arXiv:1503.00075, 20153788회 인용 관련 학술자료 전체 20개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Improved semantic representations from tree-structured long short-term memory networks",
        "year": null
    },
    "Reasoning with neural tensor networks for knowledge base completion": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013",
            "권": "26",
            "설명": "A common problem in knowledge representation and related fields is reasoning over a large joint knowledge graph, represented as triples of a relation between two entities. The goal of this paper is to develop a more powerful neural network model suitable for inference over these relationships. Previous models suffer from weak interaction between entities or simple linear projection of the vector space. We address these problems by introducing a neural tensor network (NTN) model which allow the entities and relations to interact multiplicatively. Additionally, we observe that such knowledge base models can be further improved by representing each entity as the average of vectors for the words in the entity name, giving an additional dimension of similarity by which entities can share statistical strength. We assess the model by considering the problem of predicting additional true relations between entities given a partial knowledge base. Our model outperforms previous models and can classify unseen relationships in WordNet and FreeBase with an accuracy of 86.2% and 90.0%, respectively.",
            "저널": "Advances in neural information processing systems",
            "저자": "Richard Socher, Danqi Chen, Christopher D Manning, Andrew Ng",
            "전체 인용횟수": "2303회 인용201420152016201720182019202020212022202336116176213272312361341270194",
            "학술 문서": "Reasoning with neural tensor networks for knowledge base completionR Socher, D Chen, CD Manning, A Ng - Advances in neural information processing systems, 20132303회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Reasoning with neural tensor networks for knowledge base completion",
        "year": null
    },
    "Parsing natural scenes and natural language with recursive neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011",
            "설명": "Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algorithm can be used both to provide a competitive syntactic parser for natural language sentences from the Penn Treebank and to outperform alternative approaches for semantic scene segmentation, annotation and classification. For segmentation and annotation our algorithm obtains a new level of state-of-theart performance on the Stanford background dataset (78.1%). The features from the image parse tree outperform Gist descriptors for scene classification by 4%.",
            "저자": "Richard Socher, Cliff Chiung-Yu Lin, Andrew Y Ng, Christopher D Manning",
            "전체 인용횟수": "1779회 인용2011201220132014201520162017201820192020202120222023833639917819219524822117014112276",
            "학술 문서": "Parsing natural scenes and natural language with recursive neural networksR Socher, CC Lin, C Manning, AY Ng - Proceedings of the 28th international conference on …, 20111779회 인용 관련 학술자료 전체 22개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Parsing natural scenes and natural language with recursive neural networks",
        "year": null
    },
    "Semantic compositionality through recursive matrix-vector spaces": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/7",
            "설명": "Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.",
            "저자": "Richard Socher, Brody Huval, Christopher D Manning, Andrew Y Ng",
            "전체 인용횟수": "1737회 인용201220132014201520162017201820192020202120222023670121173194200223197179138110100",
            "컨퍼런스": "Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning",
            "페이지": "1201-1211",
            "학술 문서": "Semantic compositionality through recursive matrix-vector spacesR Socher, B Huval, CD Manning, AY Ng - Proceedings of the 2012 joint conference on empirical …, 20121737회 인용 관련 학술자료 전체 19개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Semantic compositionality through recursive matrix-vector spaces",
        "year": null
    },
    "A deep reinforced model for abstractive summarization": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "저널": "International Conference on Learning Representations",
            "저자": "Paulus Romain, Xiong Caiming, Socher Richard",
            "전체 인용횟수": "1716회 인용201720182019202020212022202319220280336329295227",
            "학술 문서": "A deep reinforced model for abstractive summarization*R Paulus, C Xiong, R Socher - arXiv preprint arXiv:1705.04304, 20171683회 인용 관련 학술자료 전체 4개의 버전 A deep reinforced model for abstractive summarizationP Romain, X Caiming, S Richard - International Conference on Learning Representations, 201832회 인용 관련 학술자료 A deep reinforced model for abstractive summarization. arXiv 2017*R Paulus, C Xiong, R Socher - arXiv preprint arXiv:1705.0430419회 인용 관련 학술자료 A deep reinforced model for abstractive summarization (2017)*R Paulus, C Xiong, R Socher - arXiv preprint arXiv:1705.04304, 201718회 인용 관련 학술자료 A deep reinforced model for abstractive summarization. CoRR abs/1705.04304 (2017)*R Paulus, C Xiong, R Socher - arXiv preprint arXiv:1705.04304, 20177회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A deep reinforced model for abstractive summarization",
        "year": null
    },
    "Semi-supervised recursive autoencoders for predicting sentiment distributions": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/7",
            "설명": "We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model’s ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.",
            "저자": "Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, Christopher D Manning",
            "전체 인용횟수": "1695회 인용2011201220132014201520162017201820192020202120222023527591221871912302412021411179056",
            "컨퍼런스": "Proceedings of the 2011 conference on empirical methods in natural language processing",
            "페이지": "151-161",
            "학술 문서": "Semi-supervised recursive autoencoders for predicting sentiment distributionsR Socher, J Pennington, EH Huang, AY Ng… - Proceedings of the 2011 conference on empirical …, 20111695회 인용 관련 학술자료 전체 21개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Semi-supervised recursive autoencoders for predicting sentiment distributions",
        "year": null
    },
    "Pointer sentinel mixture models": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/9/26",
            "설명": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.",
            "저널": "arXiv preprint arXiv:1609.07843",
            "저자": "Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher",
            "전체 인용횟수": "1690회 인용20162017201820192020202120222023958135219255302338369",
            "학술 문서": "Pointer sentinel mixture modelsS Merity, C Xiong, J Bradbury, R Socher - arXiv preprint arXiv:1609.07843, 20161690회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pointer sentinel mixture models",
        "year": null
    },
    "Zero-shot learning through cross-modal transfer": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013",
            "권": "26",
            "설명": "This work introduces a model that can recognize objects in images even if no training data is available for the object class. The only necessary knowledge about unseen categories comes from unsupervised text corpora. Unlike previous zero-shot learning models, which can only differentiate between unseen classes, our model can operate on a mixture of objects, simultaneously obtaining state of the art performance on classes with thousands of training images and reasonable performance on unseen classes. This is achieved by seeing the distributions of words in texts as a semantic space for understanding what objects look like. Our deep learning model does not require any manually defined semantic or visual features for either words or images. Images are mapped to be close to semantic word vectors corresponding to their classes, and the resulting image embeddings can be used to distinguish whether an image is of a seen or unseen class. Then, a separate recognition model can be employed for each type. We demonstrate two strategies, the first gives high accuracy on unseen classes, while the second is conservative in its prediction of novelty and keeps the seen classes' accuracy high.",
            "저널": "Advances in neural information processing systems",
            "저자": "Richard Socher, Milind Ganjoo, Christopher D Manning, Andrew Ng",
            "전체 인용횟수": "1632회 인용20142015201620172018201920202021202220233371109140192234255242189134",
            "학술 문서": "Zero-shot learning through cross-modal transferR Socher, M Ganjoo, CD Manning, A Ng - Advances in neural information processing systems, 20131632회 인용 관련 학술자료 전체 19개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Zero-shot learning through cross-modal transfer",
        "year": null
    },
    "Knowing when to look: Adaptive attention via a visual sentinel for image captioning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "Attention-based neural encoder-decoder frameworks have been widely adopted for image captioning. Most methods force visual attention to be active for every generated word. However, the decoder likely requires little to no visual information from the image to predict non-visual words such as\" the\" and\" of\". Other words that may seem visual can often be predicted reliably just from the language model eg,\" sign\" after\" behind a red stop\" or\" phone\" following\" talking on a cell\". In this paper, we propose a novel adaptive attention model with a visual sentinel. At each time step, our model decides whether to attend to the image (and if so, to which regions) or to the visual sentinel. The model decides whether to attend to the image and where, in order to extract meaningful information for sequential word generation. We test our method on the COCO image captioning 2015 challenge dataset and Flickr30K. Our approach sets the new state-of-the-art by a significant margin.",
            "저자": "Jiasen Lu, Caiming Xiong, Devi Parikh, Richard Socher",
            "전체 인용횟수": "1608회 인용201720182019202020212022202349174268264298314232",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "375-383",
            "학술 문서": "Knowing when to look: Adaptive attention via a visual sentinel for image captioningJ Lu, C Xiong, D Parikh, R Socher - Proceedings of the IEEE conference on computer …, 20171608회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Knowing when to look: Adaptive attention via a visual sentinel for image captioning",
        "year": null
    },
    "Improving word representations via global context and multiple word prototypes": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/7",
            "설명": "Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models. 1",
            "저자": "Eric H Huang, Richard Socher, Christopher D Manning, Andrew Y Ng",
            "전체 인용횟수": "1586회 인용2012201320142015201620172018201920202021202220237361061892452061951671571187664",
            "컨퍼런스": "Proceedings of the 50th annual meeting of the association for computational linguistics (Volume 1: Long papers)",
            "페이지": "873-882",
            "학술 문서": "Improving word representations via global context and multiple word prototypesEH Huang, R Socher, CD Manning, AY Ng - Proceedings of the 50th annual meeting of the …, 20121586회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Improving word representations via global context and multiple word prototypes",
        "year": null
    },
    "Ask me anything: Dynamic memory networks for natural language processing": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/6/24",
            "설명": "Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook’s bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.",
            "저널": "arXiv preprint arXiv:1506.07285",
            "저자": "Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, Richard Socher",
            "전체 인용횟수": "1476회 인용20152016201720182019202020212022202318108190225248228184136118",
            "학술 문서": "Ask me anything: Dynamic memory networks for natural language processingA Kumar, O Irsoy, P Ondruska, M Iyyer, J Bradbury… - International conference on machine learning, 20161476회 인용 관련 학술자료 전체 15개의 버전 Ask Me Anything: Dynamic Memory Networks for Natural Language ProcessingAKOIP Ondruska, MIJBI Gulrajani, R Socher - arXiv preprint arXiv:1506.07285, 2015관련 학술자료 Ask Me Anything: Dynamic Memory Networks for Natural Language ProcessingAKOIJ Su, JBR English, BPPOM Iyyer, IGR Socher - arXiv preprint arXiv:1506.07285, 2015관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Ask me anything: Dynamic memory networks for natural language processing",
        "year": null
    },
    "Regularizing and optimizing LSTM language models": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/8/7",
            "설명": "Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.",
            "저널": "arXiv preprint arXiv:1708.02182",
            "저자": "Stephen Merity, Nitish Shirish Keskar, Richard Socher",
            "전체 인용횟수": "1208회 인용201720182019202020212022202312127240309227182106",
            "학술 문서": "Regularizing and optimizing LSTM language modelsS Merity, NS Keskar, R Socher - arXiv preprint arXiv:1708.02182, 20171208회 인용 관련 학술자료 전체 5개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Regularizing and optimizing LSTM language models",
        "year": null
    },
    "Parsing with compositional vector grammars": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/8",
            "설명": "Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness. Instead, we introduce a Compositional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations. The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%. It is fast to train and implemented approximately as an efficient reranker it is about 20% faster than the current Stanford factored parser. The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments.",
            "저자": "Richard Socher, John Bauer, Christopher D Manning, Andrew Y Ng",
            "전체 인용횟수": "1195회 인용201220132014201520162017201820192020202120222023322116187165156133112103686942",
            "컨퍼런스": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "페이지": "455-465",
            "학술 문서": "Parsing with compositional vector grammarsR Socher, J Bauer, CD Manning, AY Ng - Proceedings of the 51st Annual Meeting of the …, 20131195회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Parsing with compositional vector grammars",
        "year": null
    },
    "Learned in translation: Contextualized word vectors": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "권": "30",
            "설명": "Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.",
            "저널": "Advances in neural information processing systems",
            "저자": "Bryan McCann, James Bradbury, Caiming Xiong, Richard Socher",
            "전체 인용횟수": "1155회 인용201720182019202020212022202311123256228233177119",
            "학술 문서": "Learned in translation: Contextualized word vectorsB McCann, J Bradbury, C Xiong, R Socher - Advances in neural information processing systems, 20171155회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learned in translation: Contextualized word vectors",
        "year": null
    },
    "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011",
            "권": "24",
            "설명": "Paraphrase detection is the task of examining two sentences and determining whether they have the same meaning. In order to obtain high accuracy on this task, thorough syntactic and semantic analysis of the two statements is needed. We introduce a method for paraphrase detection based on recursive autoencoders (RAE). Our unsupervised RAEs are based on a novel unfolding objective and learn feature vectors for phrases in syntactic trees. These features are used to measure the word-and phrase-wise similarity between two sentences. Since sentences may be of arbitrary length, the resulting matrix of similarity measures is of variable size. We introduce a novel dynamic pooling layer which computes a fixed-sized representation from the variable-sized matrices. The pooled representation is then used as input to a classifier. Our method outperforms other state-of-the-art approaches on the challenging MSRP paraphrase corpus.",
            "저널": "Advances in neural information processing systems",
            "저자": "Richard Socher, Eric Huang, Jeffrey Pennin, Christopher D Manning, Andrew Ng",
            "전체 인용횟수": "1128회 인용2011201220132014201520162017201820192020202120222023420547913914313714712194727033",
            "학술 문서": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detectionR Socher, E Huang, J Pennin, CD Manning, A Ng - Advances in neural information processing systems, 20111128회 인용 관련 학술자료 전체 23개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection",
        "year": null
    },
    "Better Word Representations with Recursive Neural Networks for Morphology": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013",
            "저자": "Minh-Thang Luong, Richard Socher, Christopher D Manning",
            "전체 인용횟수": "1061회 인용20132014201520162017201820192020202120222023539681071341451801351247638",
            "학술 문서": "The Arms, morphology*AH Toha, M Dailami - 20131061회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Better Word Representations with Recursive Neural Networks for Morphology",
        "year": null
    },
    "Grounded compositional semantics for finding and describing images with sentences": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/4/1",
            "게시자": "MIT Press",
            "권": "2",
            "설명": " Previous work on Recursive Neural Networks (RNNs) shows that these models can                     produce compositional feature vectors for accurately representing and                     classifying sentences or images. However, the sentence vectors of previous                     models cannot accurately represent visually grounded meaning. We introduce the                     DT-RNN model which uses dependency trees to embed sentences into a vector space                     in order to retrieve images that are described by those sentences. Unlike                     previous RNN-based models which use constituency trees, DT-RNNs naturally focus                     on the action and agents in a sentence. They are better able to abstract from                     the details of word order and syntactic expression. DT-RNNs outperform other                     recursive and recurrent neural networks, kernelized CCA and a bag-of-words                     baseline on …",
            "저널": "Transactions of the Association for Computational Linguistics",
            "저자": "Richard Socher, Andrej Karpathy, Quoc V Le, Christopher D Manning, Andrew Y Ng",
            "전체 인용횟수": "1005회 인용201420152016201720182019202020212022202338110122125140123108997154",
            "페이지": "207-218",
            "학술 문서": "Grounded compositional semantics for finding and describing images with sentencesR Socher, A Karpathy, QV Le, CD Manning, AY Ng - Transactions of the Association for Computational …, 20141005회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Grounded compositional semantics for finding and describing images with sentences",
        "year": null
    },
    "Image super-resolution via sparse representation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/5/18",
            "게시자": "IEEE",
            "권": "19",
            "설명": "This paper presents a new approach to single-image superresolution, based upon sparse signal representation. Research on image statistics suggests that image patches can be well-represented as a sparse linear combination of elements from an appropriately chosen over-complete dictionary. Inspired by this observation, we seek a sparse representation for each patch of the low-resolution input, and then use the coefficients of this representation to generate the high-resolution output. Theoretical results from compressed sensing suggest that under mild conditions, the sparse representation can be correctly recovered from the downsampled signals. By jointly training two dictionaries for the low- and high-resolution image patches, we can enforce the similarity of sparse representations between the low-resolution and high-resolution image patch pair with respect to their own dictionaries. Therefore, the sparse …",
            "저널": "IEEE transactions on image processing",
            "저자": "Jianchao Yang, John Wright, Thomas S Huang, Yi Ma",
            "전체 인용횟수": "6070회 인용201020112012201320142015201620172018201920202021202220232494209310398496600609703672622501425324",
            "페이지": "2861-2873",
            "학술 문서": "Image super-resolution via sparse representationJ Yang, J Wright, TS Huang, Y Ma - IEEE transactions on image processing, 20106070회 인용 관련 학술자료 전체 28개의 버전 ",
            "호": "11"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Image super-resolution via sparse representation",
        "year": null
    },
    "Least-squares fitting of two 3-D point sets": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1987/9",
            "게시자": "IEEE",
            "설명": "Two point sets {pi} and {p'i}; i = 1, 2,..., N are related by p'i = Rpi + T + Ni, where R is a rotation matrix, T a translation vector, and Ni a noise vector. Given {pi} and {p'i}, we present an algorithm for finding the least-squares solution of R and T, which is based on the singular value decomposition (SVD) of a 3 × 3 matrix. This new algorithm is compared to two earlier algorithms with respect to computer time requirements.",
            "저널": "IEEE Transactions on pattern analysis and machine intelligence",
            "저자": "K Somani Arun, Thomas S Huang, Steven D Blostein",
            "전체 인용횟수": "5430회 인용1989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202320212929443962626578778910580122130152159197187155202191226231249277228234249201254296326318",
            "페이지": "698-700",
            "학술 문서": "Least-squares fitting of two 3-D point setsKS Arun, TS Huang, SD Blostein - IEEE Transactions on pattern analysis and machine …, 19875430회 인용 관련 학술자료 전체 23개의 버전 ",
            "호": "5"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Least-squares fitting of two 3-D point sets",
        "year": null
    },
    "Locality-constrained linear coding for image classification": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/6/13",
            "게시자": "IEEE",
            "설명": "The traditional SPM approach based on bag-of-features (BoF) requires nonlinear classifiers to achieve good image classification performance. This paper presents a simple but effective coding scheme called Locality-constrained Linear Coding (LLC) in place of the VQ coding in traditional SPM. LLC utilizes the locality constraints to project each descriptor into its local-coordinate system, and the projected coordinates are integrated by max pooling to generate the final representation. With linear classifier, the proposed approach performs remarkably better than the traditional nonlinear SPM, achieving state-of-the-art performance on several benchmarks. Compared with the sparse coding strategy [22], the objective function used by LLC has an analytical solution. In addition, the paper proposes a fast approximated LLC method by first performing a K-nearest-neighbor search and then solving a constrained least …",
            "저자": "Jinjun Wang, Jianchao Yang, Kai Yu, Fengjun Lv, Thomas Huang, Yihong Gong",
            "전체 인용횟수": "4023회 인용20092010201120122013201420152016201720182019202020212022202313171092224044985775554683472532041337957",
            "컨퍼런스": "2010 IEEE computer society conference on computer vision and pattern recognition",
            "페이지": "3360-3367",
            "학술 문서": "Locality-constrained linear coding for image classificationJ Wang, J Yang, K Yu, F Lv, T Huang, Y Gong - 2010 IEEE computer society conference on computer …, 20104023회 인용 관련 학술자료 전체 25개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Locality-constrained linear coding for image classification",
        "year": null
    },
    "Linear spatial pyramid matching using sparse coding for image classification": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/6/20",
            "게시자": "IEEE",
            "설명": "Recently SVMs using spatial pyramid matching (SPM) kernel have been highly successful in image classification. Despite its popularity, these nonlinear SVMs have a complexity O(n 2  ∼ n 3 ) in training and O(n) in testing, where n is the training size, implying that it is nontrivial to scaleup the algorithms to handlemore than thousands of training images. In this paper we develop an extension of the SPM method, by generalizing vector quantization to sparse coding followed by multi-scale spatial max pooling, and propose a linear SPM kernel based on SIFT sparse codes. This new approach remarkably reduces the complexity of SVMs to O(n) in training and a constant in testing. In a number of image categorization experiments, we find that, in terms of classification accuracy, the suggested linear SPM based on sparse coding of SIFT descriptors always significantly outperforms the linear SPM kernel on histograms, and …",
            "저자": "Jianchao Yang, Kai Yu, Yihong Gong, Thomas Huang",
            "전체 인용횟수": "3955회 인용20092010201120122013201420152016201720182019202020212022202314791532854275195285374152872171641307946",
            "컨퍼런스": "2009 IEEE Conference on computer vision and pattern recognition",
            "페이지": "1794-1801",
            "학술 문서": "Linear spatial pyramid matching using sparse coding for image classificationJ Yang, K Yu, Y Gong, T Huang - 2009 IEEE Conference on computer vision and pattern …, 20093955회 인용 관련 학술자료 전체 21개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Linear spatial pyramid matching using sparse coding for image classification",
        "year": null
    },
    "Image retrieval: Current techniques, promising directions, and open issues": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1999/3/1",
            "게시자": "Academic Press",
            "권": "10",
            "설명": "This paper provides a comprehensive survey of the technical achievements in the research area of image retrieval, especially content-based image retrieval, an area that has been so active and prosperous in the past few years. The survey includes 100+ papers covering the research aspects of image feature representation and extraction, multidimensional indexing, and system design, three of the fundamental bases of content-based image retrieval. Furthermore, based on the state-of-the-art technology available now and the demand from real-world applications, open research issues are identified and future promising research directions are suggested.",
            "저널": "Journal of visual communication and image representation",
            "저자": "Yong Rui, Thomas S Huang, Shih-Fu Chang",
            "전체 인용횟수": "3727회 인용19992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220233179137180210231224213213206165174192189180177160152120959675635240",
            "페이지": "39-62",
            "학술 문서": "Image retrieval: Current techniques, promising directions, and open issuesY Rui, TS Huang, SF Chang - Journal of visual communication and image …, 19993212회 인용 관련 학술자료 전체 22개의 버전 Image retrieval: Past, present, and future*Y Rui, TS Huang, SF Chang - Journal of Visual Communication and Image …, 1999564회 인용 관련 학술자료 전체 5개의 버전 Image Retrieval: Past*Y Rui, SH Thomas, SF Chang - Present, and Future, 19988회 인용 관련 학술자료 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Image retrieval: Current techniques, promising directions, and open issues",
        "year": null
    },
    "A survey of affect recognition methods: audio, visual and spontaneous expressions": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2007/11/12",
            "설명": "Automated analysis of human affective behavior has attracted increasing attention from researchers in psychology, computer science, linguistics, neuroscience, and related disciplines. Promising approaches have been reported, including automatic methods for facial and vocal affect recognition. However, the existing methods typically handle only deliberately displayed and exaggerated expressions of prototypical emotions-despite the fact that deliberate behavior differs in visual and audio expressions from spontaneously occurring behavior. Recently efforts to develop algorithms that can process naturally occurring human affective behavior have emerged. This paper surveys these efforts. We first discuss human emotion perception from a psychological perspective. Next, we examine the available approaches to solving the problem of machine understanding of human affective behavior occurring in real-world …",
            "저자": "Zhihong Zeng, Maja Pantic, Glenn I Roisman, Thomas S Huang",
            "전체 인용횟수": "3555회 인용20082009201020112012201320142015201620172018201920202021202220233788190228245317327342302311264204198182156103",
            "출처": "Proceedings of the 9th international conference on Multimodal interfaces",
            "페이지": "126-133",
            "학술 문서": "A survey of affect recognition methods: audio, visual and spontaneous expressionsZ Zeng, M Pantic, GI Roisman, TS Huang - Proceedings of the 9th international conference on …, 20073550회 인용 관련 학술자료 전체 28개의 버전 TS A Survey of Affect Recognition Methods: Audio*Z Zeng, M Pantic, G Roisman, TS Huang - Visual, and ICMI2007, 20076회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A survey of affect recognition methods: audio, visual and spontaneous expressions",
        "year": null
    },
    "Visual interpretation of hand gestures for human-computer interaction: A review": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1997/7",
            "게시자": "IEEE",
            "권": "19",
            "설명": "The use of hand gestures provides an attractive alternative to cumbersome interface devices for human-computer interaction (HCI). In particular, visual interpretation of hand gestures can help in achieving the ease and naturalness desired for HCI. This has motivated a very active research area concerned with computer vision-based analysis and interpretation of hand gestures. We survey the literature on visual interpretation of hand gestures in the context of its role in HCI. This discussion is organized on the basis of the method used for modeling, analyzing, and recognizing gestures. Important differences in the gesture interpretation approaches arise depending on whether a 3D model of the human hand or an image appearance model of the human hand is used. 3D hand models offer a way of more elaborate modeling of hand gestures but lead to computational hurdles that have not been overcome given the real …",
            "저자": "Vladimir I Pavlovic, Rajeev Sharma, Thomas S.  Huang",
            "전체 인용횟수": "2899회 인용1998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202333427376751041211141291251261211331571671951811471591318211280546747",
            "출처": "IEEE Transactions on pattern analysis and machine intelligence",
            "페이지": "677-695",
            "학술 문서": "Visual interpretation of hand gestures for human-computer interaction: A reviewVI Pavlovic, R Sharma, TS Huang - IEEE Transactions on pattern analysis and machine …, 19972899회 인용 관련 학술자료 전체 10개의 버전 ",
            "호": "7"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Visual interpretation of hand gestures for human-computer interaction: A review",
        "year": null
    },
    "Relevance feedback: a power tool for interactive content-based image retrieval": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1998/9",
            "게시자": "IEEE",
            "권": "8",
            "설명": "Content-based image retrieval (CBIR) has become one of the most active research areas in the past few years. Many visual feature representations have been explored and many systems built. While these research efforts establish the basis of CBIR, the usefulness of the proposed approaches is limited. Specifically, these efforts have relatively ignored two distinct characteristics of CBIR systems: (1) the gap between high-level concepts and low-level features, and (2) the subjectivity of human perception of visual content. This paper proposes a relevance feedback based interactive retrieval approach, which effectively takes into account the above two characteristics in CBIR. During the retrieval process, the user's high-level query and perception subjectivity are captured by dynamically updated weights based on the user's feedback. The experimental results over more than 70000 images show that the proposed …",
            "저널": "IEEE Transactions on circuits and systems for video technology",
            "저자": "Yong Rui, Thomas S Huang, Michael Ortega, Sharad Mehrotra",
            "전체 인용횟수": "2746회 인용19992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220233168105101161159190173177167159123124148122104939590706147613231",
            "페이지": "644-655",
            "학술 문서": "Relevance feedback: a power tool for interactive content-based image retrievalY Rui, TS Huang, M Ortega, S Mehrotra - IEEE Transactions on circuits and systems for video …, 19982746회 인용 관련 학술자료 전체 16개의 버전 ",
            "호": "5"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Relevance feedback: a power tool for interactive content-based image retrieval",
        "year": null
    },
    "Multiframe image restoration and registration": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1984",
            "권": "1",
            "설명": "A new method for multiframe image restoration and registration is presented. The observations are sequences of low-resolution, undersampled, discrete frames. The result is a restored high-resolution image. The restoration part is suitable for real-time implementation since the computation consists of only a few complex operations.",
            "저널": "Multiframe image restoration and registration",
            "저자": "Roger Y Tsai, Thomas S Huang",
            "전체 인용횟수": "2562회 인용1992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202376109121519152726425660861161351001481351351401421471331401421346992868463",
            "페이지": "317-339",
            "학술 문서": "Multiframe image restoration and registrationRY Tsai, TS Huang - Multiframe image restoration and registration, 19842562회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Multiframe image restoration and registration",
        "year": null
    },
    "Graph regularized nonnegative matrix factorization for data representation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/12/23",
            "게시자": "IEEE",
            "권": "33",
            "설명": "Matrix factorization techniques have been frequently applied in information retrieval, computer vision, and pattern recognition. Among them, Nonnegative Matrix Factorization (NMF) has received considerable attention due to its psychological and physiological interpretation of naturally occurring data whose representation may be parts based in the human brain. On the other hand, from the geometric perspective, the data is usually sampled from a low-dimensional manifold embedded in a high-dimensional ambient space. One then hopes to find a compact representation,which uncovers the hidden semantics and simultaneously respects the intrinsic geometric structure. In this paper, we propose a novel algorithm, called Graph Regularized Nonnegative Matrix Factorization (GNMF), for this purpose. In GNMF, an affinity graph is constructed to encode the geometrical information and we seek a matrix factorization …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Deng Cai, Xiaofei He, Jiawei Han, Thomas S Huang",
            "전체 인용횟수": "2485회 인용2011201220132014201520162017201820192020202120222023166198123174218238277248279265235226",
            "페이지": "1548-1560",
            "학술 문서": "Graph regularized nonnegative matrix factorization for data representationD Cai, X He, J Han, TS Huang - IEEE transactions on pattern analysis and machine …, 20102485회 인용 관련 학술자료 전체 21개의 버전 ",
            "호": "8"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Graph regularized nonnegative matrix factorization for data representation",
        "year": null
    },
    "Ccnet: Criss-cross attention for semantic segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020",
            "설명": "Full-image dependencies provide useful contextual information to benefit visual understanding problems. In this work, we propose a Criss-Cross Network (CCNet) for obtaining such contextual information in a more effective and efficient way. Concretely, for each pixel, a novel criss-cross attention module in CCNet harvests the contextual information of all the pixels on its criss-cross path. By taking a further recurrent operation, each pixel can finally capture the full-image dependencies from all pixels. Overall, CCNet is with the following merits: 1) GPU memory friendly. Compared with the non-local block, the proposed recurrent criss-cross attention module requires 11x less GPU memory usage. 2) High computational efficiency. The recurrent criss-cross attention significantly reduces FLOPs by about 85% of the non-local block in computing full-image dependencies. 3) The state-of-the-art performance. We conduct extensive experiments on popular semantic segmentation benchmarks including Cityscapes, ADE20K, and instance segmentation benchmark COCO. In particular, our CCNet achieves the mIoU score of 81.4 and 45.22 on Cityscapes test set and ADE20K validation set, respectively, which are the new state-of-the-art results. The source code is available at https://github. com/speedinghzl/CCNet.",
            "저자": "Zilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi, Wenyu Liu, Thomas S. Huang",
            "전체 인용횟수": "2443회 인용2019202020212022202340248565775805",
            "컨퍼런스": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)",
            "학술 문서": "Ccnet: Criss-cross attention for semantic segmentationZ Huang, X Wang, L Huang, C Huang, Y Wei, W Liu - Proceedings of the IEEE/CVF international conference …, 20192443회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Ccnet: Criss-cross attention for semantic segmentation",
        "year": null
    },
    "Generative image inpainting with contextual attention": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "Recent deep learning based approaches have shown promising results for the challenging task of inpainting large missing regions in an image. These methods can generate visually plausible image structures and textures, but often create distorted structures or blurry textures inconsistent with surrounding areas. This is mainly due to ineffectiveness of convolutional neural networks in explicitly borrowing or copying information from distant spatial locations. On the other hand, traditional texture and patch synthesis approaches are particularly suitable when it needs to borrow textures from the surrounding regions. Motivated by these observations, we propose a new deep generative model-based approach which can not only synthesize novel image structures but also explicitly utilize surrounding image features as references during network training to make better predictions. The model is a feed-forward, fully convolutional neural network which can process images with multiple holes at arbitrary locations and with variable sizes during the test time. Experiments on multiple datasets including faces (CelebA, CelebA-HQ), textures (DTD) and natural images (ImageNet, Places2) demonstrate that our proposed approach generates higher-quality inpainting results than existing ones. Code, demo and models are available at: https://github. com/JiahuiYu/generative_inpainting.",
            "저자": "Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, Thomas S Huang",
            "전체 인용횟수": "2366회 인용20182019202020212022202339260442573557480",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "5505-5514",
            "학술 문서": "Generative image inpainting with contextual attentionJ Yu, Z Lin, J Yang, X Shen, X Lu, TS Huang - Proceedings of the IEEE conference on computer …, 20182366회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Generative image inpainting with contextual attention",
        "year": null
    },
    "Sparse representation for computer vision and pattern recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/4/29",
            "게시자": "IEEE",
            "권": "98",
            "설명": "Techniques from sparse signal representation are beginning to see significant impact in computer vision, often on nontraditional applications where the goal is not just to obtain a compact high-fidelity representation of the observed signal, but also to extract semantic information. The choice of dictionary plays a key role in bridging this gap: unconventional dictionaries consisting of, or learned from, the training samples themselves provide the key to obtaining state-of-the-art results and to attaching semantic meaning to sparse signal representations. Understanding the good performance of such unconventional dictionaries in turn demands new algorithmic and analytical techniques. This review paper highlights a few representative examples of how the interaction between sparse signal representation and computer vision can enrich both fields, and raises a number of open questions for further study.",
            "저자": "John Wright, Yi Ma, Julien Mairal, Guillermo Sapiro, Thomas S Huang, Shuicheng Yan",
            "전체 인용횟수": "2250회 인용2010201120122013201420152016201720182019202020212022202332771451912482692482362071641151148882",
            "출처": "Proceedings of the IEEE",
            "페이지": "1031-1044",
            "학술 문서": "Sparse representation for computer vision and pattern recognitionJ Wright, Y Ma, J Mairal, G Sapiro, TS Huang, S Yan - Proceedings of the IEEE, 20102250회 인용 관련 학술자료 전체 15개의 버전 ",
            "호": "6"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Sparse representation for computer vision and pattern recognition",
        "year": null
    },
    "Image super-resolution as sparse representation of raw image patches": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008/6/23",
            "게시자": "IEEE",
            "설명": "This paper addresses the problem of generating a super-resolution (SR) image from a single low-resolution input image. We approach this problem from the perspective of compressed sensing. The low-resolution image is viewed as downsampled version of a high-resolution image, whose patches are assumed to have a sparse representation with respect to an over-complete dictionary of prototype signal-atoms. The principle of compressed sensing ensures that under mild conditions, the sparse representation can be correctly recovered from the downsampled signal. We will demonstrate the effectiveness of sparsity as a prior for regularizing the otherwise ill-posed super-resolution problem. We further show that a small set of randomly chosen raw patches from training images of similar statistical nature to the input image generally serve as a good dictionary, in the sense that the computed representation is sparse …",
            "저자": "Jianchao Yang, John Wright, Thomas Huang, Yi Ma",
            "전체 인용횟수": "2122회 인용20092010201120122013201420152016201720182019202020212022202323689713714017820218720219216615615410983",
            "컨퍼런스": "2008 IEEE conference on computer vision and pattern recognition",
            "페이지": "1-8",
            "학술 문서": "Image super-resolution as sparse representation of raw image patchesJ Yang, J Wright, T Huang, Y Ma - 2008 IEEE conference on computer vision and pattern …, 20082122회 인용 관련 학술자료 전체 22개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Image super-resolution as sparse representation of raw image patches",
        "year": null
    },
    "A fast two-dimensional median filtering algorithm": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1979/2",
            "게시자": "IEEE",
            "권": "27",
            "설명": "We present a fast algorithm for two-dimensional median filtering. It is based on storing and updating the gray level histogram of the picture elements in the window. The algorithm is much faster than conventional sorting methods. For a window size of m × n, the computer time required is 0(n).",
            "저널": "IEEE transactions on acoustics, speech, and signal processing",
            "저자": "Thomas Huang, GJTGY Yang, Greory Tang",
            "전체 인용횟수": "1838회 인용19841985198619871988198919901991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220231613122227272721222021162624219211923212619313932414560645877988211397102919810681",
            "페이지": "13-18",
            "학술 문서": "A fast two-dimensional median filtering algorithmT Huang, G Yang, G Tang - IEEE transactions on acoustics, speech, and signal …, 19791838회 인용 관련 학술자료 전체 6개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A fast two-dimensional median filtering algorithm",
        "year": null
    },
    "Free-form image inpainting with gated convolution": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "We present a generative image inpainting system to complete images with free-form mask and guidance. The system is based on gated convolutions learned from millions of images without additional labelling efforts. The proposed gated convolution solves the issue of vanilla convolution that treats all input pixels as valid ones, generalizes partial convolution by providing a learnable dynamic feature selection mechanism for each channel at each spatial location across all layers. Moreover, as free-form masks may appear anywhere in images with any shape, global and local GANs designed for a single rectangular mask are not applicable. Thus, we also present a patch-based GAN loss, named SN-PatchGAN, by applying spectral-normalized discriminator on dense image patches. SN-PatchGAN is simple in formulation, fast and stable in training. Results on automatic image inpainting and user-guided extension demonstrate that our system generates higher-quality and more flexible results than previous methods. Our system helps user quickly remove distracting objects, modify image layouts, clear watermarks and edit faces. Code, demo and models are available at: https://github. com/JiahuiYu/generative_inpainting.",
            "저자": "Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, Thomas S Huang",
            "전체 인용횟수": "1652회 인용201820192020202120222023786222405481440",
            "컨퍼런스": "Proceedings of the IEEE/CVF international conference on computer vision",
            "페이지": "4471-4480",
            "학술 문서": "Free-form image inpainting with gated convolutionJ Yu, Z Lin, J Yang, X Shen, X Lu, TS Huang - Proceedings of the IEEE/CVF international conference …, 20191652회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Free-form image inpainting with gated convolution",
        "year": null
    },
    "Human face detection in a complex background": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1994/1/1",
            "게시자": "Pergamon",
            "권": "27",
            "설명": "The human face is a complex pattern. Finding human faces automatically in a scene is a difficult yet significant problem. It is the first important step in a fully automatic human face recognition system. In this paper a new method to locate human faces in a complex background is proposed. This system utilizes a hierarchical knowledge-based method and consists of three levels. The higher two levels are based on mosaic images at different resolutions. In the lower level, an improved edge detection method is proposed. In this research the problem of scale is dealt with, so that the system can locate unknown human faces spanning a wide range of sizes in a complex black-and-white picture. Some experimental results are given.",
            "저널": "Pattern recognition",
            "저자": "Guangzheng Yang, Thomas S Huang",
            "전체 인용횟수": "1368회 인용1994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220234825303031554464666882666449696762765849374931323227361911",
            "페이지": "53-63",
            "학술 문서": "Human face detection in a complex backgroundG Yang, TS Huang - Pattern recognition, 19941368회 인용 관련 학술자료 전체 7개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Human face detection in a complex background",
        "year": null
    },
    "Uniqueness and estimation of three-dimensional motion parameters of rigid objects with curved surfaces": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1984/1",
            "게시자": "IEEE",
            "설명": "Two main results are established in this paper. First, we show that seven point correspondences are sufficient to uniquely determine from two perspective views the three-dimensional motion parameters (within a scale factor for the translations) of a rigid object with curved surfaces. The seven points should not be traversed by two planes with one plane containing the origin, nor by a cone containing the origin. Second, a set of ``essential parameters'' are introduced which uniquely determine the motion parameters up to a scale factor for the translations, and can be estimated by solving a set of linear equations which are derived from the correspondences of eight image points. The actual motion parameters can subsequently be determined by computing the singular value decomposition (SVD) of a 3×3 matrix containing the essential parameters.",
            "저널": "IEEE Transactions on pattern analysis and machine intelligence",
            "저자": "Roger Y Tsai, Thomas S Huang",
            "전체 인용횟수": "1310회 인용198419851986198719881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202316324248556385656768515051575453643422292324272311161581619151569455545",
            "페이지": "13-27",
            "학술 문서": "Uniqueness and estimation of three-dimensional motion parameters of rigid objects with curved surfacesRY Tsai, TS Huang - IEEE Transactions on pattern analysis and machine …, 19841310회 인용 관련 학술자료 전체 23개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Uniqueness and estimation of three-dimensional motion parameters of rigid objects with curved surfaces",
        "year": null
    },
    "Relevance feedback in image retrieval: A comprehensive review": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2003/4",
            "게시자": "Springer-Verlag",
            "권": "8",
            "설명": "  We analyze the nature of the relevance feedback problem in a continuous representation space in the context of content-based image retrieval. Emphasis is put on exploring the uniqueness of the problem and comparing the assumptions, implementations, and merits of various solutions in the literature. An attempt is made to compile a list of critical issues to consider when designing a relevance feedback algorithm. With a comprehensive review as the main portion, this paper also offers some novel solutions and perspectives throughout the discussion.",
            "저자": "Xiang Sean Zhou, Thomas S Huang",
            "전체 인용횟수": "1234회 인용20032004200520062007200820092010201120122013201420152016201720182019202020212022202312325641851027795105829082686650553529271010",
            "출처": "Multimedia systems",
            "페이지": "536-544",
            "학술 문서": "Relevance feedback in image retrieval: A comprehensive reviewXS Zhou, TS Huang - Multimedia systems, 20031234회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Relevance feedback in image retrieval: A comprehensive review",
        "year": null
    },
    "Going deeper with convolutions": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation of this architecture, GoogLeNet, a 22 layers deep network, was used to assess its quality in the context of object detection and classification.",
            "저자": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich",
            "전체 인용횟수": "54980회 인용20152016201720182019202020212022202363919003835576475128253904690637059",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "1-9",
            "학술 문서": "Going deeper with convolutionsC Szegedy, W Liu, Y Jia, P Sermanet, S Reed… - Proceedings of the IEEE conference on computer …, 201554979회 인용 관련 학술자료 전체 57개의 버전 Going deeper with convolutions in 2015 IEEE Conf*C Szegedy, L Wei, J Yangqing - Comput. Vis. Pattern Recognit8회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Going deeper with convolutions",
        "year": null
    },
    "Rethinking the inception architecture for computer vision": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible. We benchmark our methods on the ILSVRC 2012 classification challenge validation set and demonstrate substantial gains over the state of the art via to carefully factorized convolutions and aggressive regularization: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters.",
            "저자": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna",
            "전체 인용횟수": "29851회 인용20162017201820192020202120222023164810215936764910598264115476",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "2818-2826",
            "학술 문서": "Rethinking the inception architecture for computer visionC Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna - Proceedings of the IEEE conference on computer …, 201629851회 인용 관련 학술자료 전체 28개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Rethinking the inception architecture for computer vision",
        "year": null
    },
    "Tensorflow: Large-scale machine learning on heterogeneous distributed systems": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/3/14",
            "설명": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.",
            "저널": "arXiv preprint arXiv:1603.04467",
            "저자": "Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Józefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, Xiaoqiang Zheng",
            "전체 인용횟수": "29330회 인용201620172018201920202021202220236182286382541084603503547873745",
            "학술 문서": "Tensorflow: Large-scale machine learning on heterogeneous distributed systemsM Abadi, A Agarwal, P Barham, E Brevdo, Z Chen… - arXiv preprint arXiv:1603.04467, 201628299회 인용 관련 학술자료 전체 4개의 버전 TensorFlow: Large-scale machine learning on heterogeneous systems, software available from tensorflow. org (2015)*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen… - URL https://www. tensorflow. org, 2015723회 인용 관련 학술자료 TensorFlow: large-scale machine learning on heterogeneous distributed systems. 2015*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen… - URL http://download. tensorflow. org/paper …, 2015373회 인용 관련 학술자료 TensorFlow: Large-scale machine learning on heterogeneous systems (2015), software available from tensorflow. org*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen… - URL https://www. tensorflow. org, 2019252회 인용 관련 학술자료 Tensor ow: Large-scale machine learning on heterogeneous distributed systemsM Abadi, A Agarwal, P Barham, E Brevdo, Z Chen… - arXiv preprint arXiv:1603.04467, 2016156회 인용 관련 학술자료 TensorFlow: Large-scale machine learning on heterogeneous systems. tensorflow. org*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen… - Accessed May, 201533회 인용 관련 학술자료 i Xiaoqiang Zheng*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen… - TensorFlow: Large-scale machine learning on …, 201533회 인용 관련 학술자료 TensorFlow: Large-scale machine learning on heterogeneous systems [Internet]. 2015*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen… - Available from: tensorflow. org, 201932회 인용 관련 학술자료 & Zheng, X.(2015)*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen… - TensorFlow: Large-scale machine learning on …, 201723회 인용 관련 학술자료 Martin wa enberg, martin wicke, yuan yu, and xiaoqiang zheng. 2015. tensorflow: Large-scale machine learning on heterogeneous systems.(2015). hp*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen… - tensor ow. org/So ware available from tensor ow. org, 201519회 인용 관련 학술자료 TensorFlow: large-scale machine learning on heterogeneous distributed systems. arXiv [cs. DC]M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen… - arXiv preprint arXiv:1603.04467, 201616회 인용 관련 학술자료 Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint: 160304467*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen… - Retrieved from arxiv. org/abs/1603.04467, 201611회 인용 관련 학술자료 Tensorflow: Large-scale machine learning on heterogeneous systems, 2015. url h ttp. tensorflow. org*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen… - Software available from tensorflow. org11회 인용 관련 학술자료 TensorFlow: Large-Scale machine learning on heterogeneous distributed systems. ArXiv e-prints, March 2016M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen… - arXiv preprint arXiv:1603.04467, 20169회 인용 관련 학술자료 TensorFlow: Large-scale machine learning on heterogeneous systems, 2015*AA Mart'in Abadi, P Barham, E Brevdo, Z Chen, C Citro… - Software available from tensorflow. org, 20159회 인용 관련 학술자료 Google Research*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen… - TensorFlow: Large-Scale Machine Learning on …, 20158회 인용 관련 학술자료 see tensorflow. org for “*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen… - TensorFlow: Large-scale machine learning on …, 20158회 인용 관련 학술자료 TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. 2015*R JOZEFOWICZ, L KAISER, M KUDLUR… - Available also from: https://www. tensorflow. org …7회 인용 관련 학술자료 TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems.(2015). tensorflow. org*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen… - 20155회 인용 관련 학술자료 TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL http://tensorflow. org/. Software available from tensorflow. org. in electricity markets: The Australian experience*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen… - 28th USENIX Security Symposium (USENIX Security …, 20055회 인용 관련 학술자료 Yuan Yu i Xiaoqiang Zheng, TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems, 2015*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen…4회 인용 관련 학술자료 TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems. arXiv: 160304467. 2016*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen…3회 인용 관련 학술자료 TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems. CoRR Vol. abs/1603.04467 (2016)*AA Mart'ın Abadi, P Barham, E Brevdo, Z Chen, C Citro… - 20162회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems",
        "year": null
    },
    "Inception-v4, inception-resnet and the impact of residual connections on learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/2/12",
            "권": "31",
            "설명": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge.",
            "저널": "Proceedings of the AAAI conference on artificial intelligence",
            "저자": "Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alexander Alemi",
            "전체 인용횟수": "15599회 인용2016201720182019202020212022202397505128021302731311731112476",
            "학술 문서": "Inception-v4, inception-resnet and the impact of residual connections on learningC Szegedy, S Ioffe, V Vanhoucke, A Alemi - Proceedings of the AAAI conference on artificial …, 201715599회 인용 관련 학술자료 전체 22개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Inception-v4, inception-resnet and the impact of residual connections on learning",
        "year": null
    },
    "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/10/18",
            "게시자": "IEEE",
            "권": "29",
            "설명": "Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.",
            "저널": "IEEE Signal processing magazine",
            "저자": "Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, Brian Kingsbury",
            "전체 인용횟수": "13099회 인용201320142015201620172018201920202021202220232054157671113134217761955175515081226910",
            "페이지": "82-97",
            "학술 문서": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groupsG Hinton, L Deng, D Yu, GE Dahl, A Mohamed, N Jaitly… - IEEE Signal processing magazine, 201212792회 인용 관련 학술자료 전체 42개의 버전 rahman Mohamed*G Hinton, L Deng, D Yu, G Dahl - A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P …, 2012241회 인용 관련 학술자료 TN Sainath, and B*G Hinton, L Deng, D Yu, GE Dahl, A Mohamed, N Jaitly… - Kingsbury,“Deep neural networks for acoustic …, 2012170회 인용 관련 학술자료 Deep neural networks for acoustic modeling in speech recognition*A Senior, V Vanhoucke, P Nguyen, T Sainath - IEEE Signal processing magazine, 2012105회 인용 관련 학술자료 Deep Neural Networks forG Hinton, L Deng, D Yu, G Dahl - IEEE Signal processing magazine, 201211회 인용 관련 학술자료 Abdel rahman Mohamed, Navdecp Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury. Deep neural networks for acoustic modeling in speech recognition*G Hinton, L Deng, D Yu, G Dahl - Signal Processing Magazine, 20125회 인용 관련 학술자료 Deep neural networks for acoustic modelling in speech recognition, IEEE Signal Process. Magazine, 29, 6. 82–97*G Hinton, L Deng, D Yu, G Dahl, A Mohamed, N Jaitly… - 20122회 인용 관련 학술자료 Deep neural networks 382 for acoustic modeling in speech recognition: The shared views of four research groups*A Senior, V Vanhoucke, P Nguyen, TN Sainath - IEEE2회 인용 관련 학술자료 Deep Neural Networks for Acoustic Modeling*RCR Counterclockwise, G Hinton, L Deng, D Yu…관련 학술자료 ",
            "호": "6"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
        "year": null
    },
    "Scalable deep reinforcement learning for vision-based robotic manipulation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/6/27",
            "게시자": "PMLR",
            "설명": "In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2 M parameters to perform closed-loop, real-world grasping that generalizes to 96% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.",
            "저자": "Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, Sergey Levine",
            "전체 인용횟수": "1301회 인용20182019202020212022202316113210315358278",
            "컨퍼런스": "2nd Conference on Robot Learning (CoRL)",
            "페이지": "651-673",
            "학술 문서": "Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation*D Kalashnikov, A Irpan, P Pastor, J Ibarz, A Herzog… - arXiv preprint arXiv:1806.10293, 2018780회 인용 관련 학술자료 전체 6개의 버전 Scalable deep reinforcement learning for vision-based robotic manipulationD Kalashnikov, A Irpan, P Pastor, J Ibarz, A Herzog… - Conference on Robot Learning, 2018559회 인용 관련 학술자료 전체 3개의 버전 ",
            "호": "87"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Scalable deep reinforcement learning for vision-based robotic manipulation",
        "year": null
    },
    "Improving the speed of neural networks on CPUs": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011",
            "설명": "Recent advances in deep learning have made the use of large, deep neural networks with tens of millions of parameters suitable for a number of applications that require real-time processing. The sheer size of these networks can represent a challenging computational burden, even for modern CPUs. For this reason, GPUs are routinely used instead to train and run such networks. This paper is a tutorial for students and researchers on some of the techniques that can be used to reduce this computational cost considerably on modern x86 CPUs. We emphasize data layout, batching of the computation, the use of SSE2 instructions, and particularly leverage SSSE3 and SSE4 fixed-point instructions which provide a 3× improvement over an optimized floating-point baseline. We use speech recognition as an example task, and show that a real-time hybrid hidden Markov model/neural network (HMM/NN) large vocabulary system can be built with a 10× speedup over an unoptimized baseline and a 4× speedup over an aggressively optimized floating-point baseline at no cost in accuracy. The techniques described extend readily to neural network training and provide an effective alternative to the use of specialized hardware.",
            "저자": "Vincent Vanhoucke, Andrew Senior, Mark Z Mao",
            "전체 인용횟수": "1020회 인용201220132014201520162017201820192020202120222023752541809913716513111812180",
            "컨퍼런스": "2011 NIPS Deep Learning and Unsupervised Feature Learning Workshop",
            "학술 문서": "Improving the speed of neural networks on CPUsV Vanhoucke, A Senior, MZ Mao - 20111020회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Improving the speed of neural networks on CPUs",
        "year": null
    },
    "Sim-to-real: Learning agile locomotion for quadruped robots": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/4/27",
            "설명": "Designing agile locomotion for quadruped robots often requires extensive expertise and tedious manual tuning. In this paper, we present a system to automate this process by leveraging deep reinforcement learning techniques. Our system can learn quadruped locomotion from scratch using simple reward signals. In addition, users can provide an open loop reference to guide the learning process when more control over the learned gait is needed. The control policies are learned in a physics simulator and then deployed on real robots. In robotics, policies trained in simulation often do not transfer to the real world. We narrow this reality gap by improving the physics simulator and learning robust policies. We improve the simulation using system identification, developing an accurate actuator model and simulating latency. We learn robust controllers by randomizing the physical environments, adding perturbations and designing a compact observation space. We evaluate our system on two agile locomotion gaits: trotting and galloping. After learning in simulation, a quadruped robot can successfully perform both gaits in the real world.",
            "저자": "Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, Vincent Vanhoucke",
            "전체 인용횟수": "734회 인용20182019202020212022202320100132163166147",
            "컨퍼런스": "Robotics: Science and Systems (RSS) XIV",
            "학술 문서": "Sim-to-real: Learning agile locomotion for quadruped robotsJ Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner… - arXiv preprint arXiv:1804.10332, 2018734회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Sim-to-real: Learning agile locomotion for quadruped robots",
        "year": null
    },
    "System and method for enabling the use of captured images through recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "전체 인용횟수": "721회 인용20082009201020112012201320142015201620172018201920202021202220239113938656967745846385143414424",
            "특허 번호": "20060251339",
            "특허청": "US",
            "학술 문서": "System and method for enabling the use of captured images through recognition*SB Gokturk, D Anguelov, V Vanhoucke, K Lee, D Vu… - US Patent 7,519,200, 2009597회 인용 관련 학술자료 전체 4개의 버전 System and method for enabling the use of captured images through recognition*SB Gokturk, D Anguelov, V Vanhoucke, K Lee, D Vu… - US Patent 8,649,572, 2014145회 인용 관련 학술자료 전체 4개의 버전 System and method for enabling the use of captured images through recognition*SB Gokturk, D Anguelov, V Vanhoucke, K Lee, D Vu… - US Patent 8,897,505, 201434회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "System and method for enabling the use of captured images through recognition",
        "year": null
    },
    "On rectified linear units for speech processing": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/5/26",
            "설명": "Deep neural networks have recently become the gold standard for acoustic modeling in speech recognition systems. The key computational unit of a deep network is a linear projection followed by a point-wise non-linearity, which is typically a logistic function. In this work, we show that we can improve generalization and make training of deep networks faster and simpler by substituting the logistic units with rectified linear units. These units are linear when their input is positive and zero otherwise. In a supervised setting, we can successfully train very deep nets from random initialization on a large vocabulary speech recognition task achieving lower word error rates than using a logistic network with the same topology. Similarly in an unsupervised setting, we show how we can learn sparse features that can be useful for discriminative tasks. All our experiments are executed in a distributed environment using several …",
            "저자": "Matthew D Zeiler, M Ranzato, Rajat Monga, Min Mao, Kun Yang, Quoc Viet Le, Patrick Nguyen, Alan Senior, Vincent Vanhoucke, Jeffrey Dean, Geoffrey E Hinton",
            "전체 인용횟수": "712회 인용201320142015201620172018201920202021202220231045797785766871746950",
            "컨퍼런스": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "페이지": "3517-3521",
            "학술 문서": "On rectified linear units for speech processingMD Zeiler, M Ranzato, R Monga, M Mao, K Yang… - 2013 IEEE International Conference on Acoustics …, 2013712회 인용 관련 학술자료 전체 21개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "On rectified linear units for speech processing",
        "year": null
    },
    "Using simulation and domain adaptation to improve efficiency of deep robotic grasping": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/5/21",
            "설명": "Instrumenting and collecting annotated visual grasping datasets to train modern machine learning algorithms can be extremely time-consuming and expensive. An appealing alternative is to use off-the-shelf simulators to render synthetic data for which ground-truth annotations are generated automatically. Unfortunately, models trained purely on simulated data often fail to generalize to the real world. We study how randomized simulated environments and domain adaptation methods can be extended to train a grasping system to grasp novel objects from raw monocular RGB images. We extensively evaluate our approaches with a total of more than 25,000 physical test grasps, studying a range of simulation conditions and domain adaptation methods, including a novel extension of pixel-level domain adaptation that we term the GraspGAN. We show that, by using synthetic data and domain adaptation, we are able to …",
            "저자": "Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mrinal Kalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, Sergey Levine, Vincent Vanhoucke",
            "전체 인용횟수": "677회 인용201720182019202020212022202376011714313312785",
            "컨퍼런스": "2018 IEEE International Conference on Robotics and Automation (ICRA)",
            "페이지": "4243-4250",
            "학술 문서": "Using simulation and domain adaptation to improve efficiency of deep robotic graspingK Bousmalis, A Irpan, P Wohlhart, Y Bai, M Kelcey… - 2018 IEEE international conference on robotics and …, 2018677회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Using simulation and domain adaptation to improve efficiency of deep robotic grasping",
        "year": null
    },
    "Do as I can, not as I say: Grounding language in robotic affordances": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2022/4/4",
            "설명": "Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's \"hands and eyes,\" while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at https …",
            "저널": "arXiv preprint arXiv:2204.01691",
            "저자": "Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan",
            "전체 인용횟수": "632회 인용2022202397532",
            "학술 문서": "Do as i can, not as i say: Grounding language in robotic affordancesM Ahn, A Brohan, N Brown, Y Chebotar, O Cortes… - arXiv preprint arXiv:2204.01691, 2022560회 인용 관련 학술자료 전체 2개의 버전 Do as i can, not as i say: Grounding language in robotic affordances*A Brohan, Y Chebotar, C Finn, K Hausman, A Herzog… - Conference on Robot Learning, 202376회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Do as I can, not as I say: Grounding language in robotic affordances",
        "year": null
    },
    "YouTube-BoundingBoxes: A large high-precision human-annotated data set for object detection in video": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "We introduce a new large-scale data set of video URLs with densely-sampled object bounding box annotations called YouTube-BoundingBoxes (YT-BB). The data set consists of approximately 380,000 video segments about 19s long, automatically selected to feature objects in natural settings without editing or post-processing, with a recording quality often akin to that of a hand-held cell phone camera. The objects represent a subset of the COCO label set. All video segments were human-annotated with high-precision classification labels and bounding boxes at 1 frame per second. The use of a cascade of increasingly precise human annotations ensures a label accuracy above 95% for every class and tight bounding boxes. Finally, we train and evaluate well-known deep network architectures and report baseline figures for per-frame classification and localization. We also demonstrate how the temporal contiguity of video can potentially be used to improve such inferences. The data set can be found at https://research. google. com/youtube-bb. We hope the availability of such large curated corpus will spur new advances in video object detection and tracking.",
            "저자": "Esteban Real, Jonathon Shlens, Stefano Mazzocchi, Xin Pan, Vincent Vanhoucke",
            "전체 인용횟수": "611회 인용201720182019202020212022202313266799145138119",
            "컨퍼런스": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "페이지": "5296-5305",
            "학술 문서": "Youtube-boundingboxes: A large high-precision human-annotated data set for object detection in videoE Real, J Shlens, S Mazzocchi, X Pan, V Vanhoucke - proceedings of the IEEE Conference on Computer …, 2017611회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "YouTube-BoundingBoxes: A large high-precision human-annotated data set for object detection in video",
        "year": null
    },
    "System and method for enabling search and retrieval from image files based on recognized information": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/10/5",
            "발명자": "Salih Burak Gokturk, Dragomir Anguelov, Vincent Vanhoucke, Kuang-chih Lee, Diem Vu, Danny Yang, Munjal Shah, Azhar Khan",
            "설명": "An embodiment provides for enabling retrieval of a collection of captured images that form at least a portion of a library of images. For each image in the collection, a captured image may be analyzed to recognize information from image data contained in the captured image, and an index may be generated, where the index data is based on the recognized information. Using the index, functionality such as search and retrieval is enabled. Various recognition techniques, including those that use the face, clothing, apparel, and combinations of characteristics may be utilized. Recognition may be performed on, among other things, persons and text carried on objects.",
            "전체 인용횟수": "514회 인용20092010201120122013201420152016201720182019202020212022202371632474557694941333325201614",
            "출원번호": "11246741",
            "특허 번호": "7809722",
            "특허청": "US",
            "학술 문서": "System and method for enabling search and retrieval from image files based on recognized informationSB Gokturk, D Anguelov, V Vanhoucke, K Lee, D Vu… - US Patent 7,809,722, 2010514회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "System and method for enabling search and retrieval from image files based on recognized information",
        "year": null
    },
    "System and method for recognizing objects from images and identifying relevancy amongst images and information": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/10/5",
            "발명자": "Salih Burak Gokturk, Dragomir Anguelov, Vincent Vanhoucke, Kuang-chih Lee, Diem Vu, Danny Yang, Munjal Shah, Azhar Khan",
            "설명": "An embodiment provides for enabling retrieval of a collection of captured images that form at least a portion of a library of images. For each image in the collection, a captured image may be analyzed to recognize information from image data contained in the captured image, and an index may be generated, where the index data is based on the recognized information. Using the index, functionality such as search and retrieval is enabled. Various recognition techniques, including those that use the face, clothing, apparel, and combinations of characteristics may be utilized. Recognition may be performed on, among other things, persons and text carried on objects.",
            "전체 인용횟수": "455회 인용20092010201120122013201420152016201720182019202020212022202391424353845514731223931272212",
            "출원번호": "11246589",
            "특허 번호": "7809192",
            "특허청": "US",
            "학술 문서": "System and method for recognizing objects from images and identifying relevancy amongst images and informationSB Gokturk, D Anguelov, V Vanhoucke, K Lee, D Vu… - US Patent 7,809,192, 2010455회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "System and method for recognizing objects from images and identifying relevancy amongst images and information",
        "year": null
    },
    "System and method for providing objectified image renderings using recognition information from images": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/8/24",
            "발명자": "Salih Burak Gokturk, Dragomir Anguelov, Vincent Vanhoucke, Kuang-chih Lee, Diem Vu, Danny Yang, Munjal Shah, Azhar Khan",
            "설명": "An embodiment provides for enabling retrieval of a collection of captured images that form at least a portion of a library of images. For each image in the collection, a captured image may be analyzed to recognize information from image data contained in the captured image, and an index may be generated, where the index data is based on the recognized information. Using the index, functionality such as search and retrieval is enabled. Various recognition techniques, including those that use the face, clothing, apparel, and combinations of characteristics may be utilized. Recognition may be performed on, among other things, persons and text carried on objects.",
            "전체 인용횟수": "453회 인용20092010201120122013201420152016201720182019202020212022202361611221835413842372949512922",
            "출원번호": "11246434",
            "특허 번호": "7783135",
            "특허청": "US",
            "학술 문서": "System and method for providing objectified image renderings using recognition information from imagesSB Gokturk, D Anguelov, V Vanhoucke, K Lee, D Vu… - US Patent 7,783,135, 2010366회 인용 관련 학술자료 전체 4개의 버전 System and method for providing objectified image renderings using recognition information from imagesSB Gokturk, D Anguelov, V Vanhoucke, K Lee, D Vu… - US Patent 8,139,900, 201259회 인용 관련 학술자료 전체 4개의 버전 System and method for providing objectified image renderings using recognition information from images*SB Gokturk, D Anguelov, V Vanhoucke, K Lee, D Vu… - US Patent 8,630,513, 201438회 인용 관련 학술자료 전체 4개의 버전 System and method for providing objectified image renderings using recognition information from images*SB Gokturk, D Anguelov, VO Vanhoucke, K Lee, DT Vu… - US Patent 9,430,719, 201618회 인용 관련 학술자료 전체 4개의 버전 System and method for providing objectified image renderings using recognition information from images*SB Gokturk, D Anguelov, V Vanhoucke, K Lee, D Vu… - US Patent 9,171,013, 201515회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "System and method for providing objectified image renderings using recognition information from images",
        "year": null
    },
    "System and method for enabling image recognition and searching of images": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/2/2",
            "발명자": "Salih Burak Gokturk, Baris Sumengen, Diem Vu, Navneet Dalal, Danny Yang, Xiaofan Lin, Azhar Khan, Munjal Shah, Dragomir Anguelov, Lorenzo Torresani, Vincent Vanhoucke",
            "설명": "Embodiments programmatically analyze each of a plurality of images in order to determine one or more visual characteristics about an item shown in each of the plurality of images. Data is stored corresponding to the one or more visual characteristics. An interface in is provided for which a user is able to specify one or more search criteria. In response to receiving the one or more search criteria, a search operation is performed to identify one or more items that have a visual characteristic that satisfies at least some of the one or more search criteria.",
            "전체 인용횟수": "430회 인용2009201020112012201320142015201620172018201920202021202220232106182753362746273346354518",
            "출원번호": "11936705",
            "특허 번호": "7657100",
            "특허청": "US",
            "학술 문서": "System and method for enabling image recognition and searching of imagesSB Gokturk, B Sumengen, D Vu, N Dalal, D Yang, X Lin… - US Patent 7,657,100, 2010357회 인용 관련 학술자료 전체 4개의 버전 System and method for enabling image recognition and searching of imagesSB Gokturk, B Sumengen, D Vu, N Dalal, D Yang, X Lin… - US Patent App. 12/648,245, 201091회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "System and method for enabling image recognition and searching of images",
        "year": null
    },
    "System and method for search portions of objects in images and features thereof": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/2/2",
            "발명자": "Salih Burak Gokturk, Baris Sumengen, Diem Vu, Navneet Dalal, Danny Yang, Xiaofan Lin, Azhar Khan, Munjal Shah, Dragomir Anguelov, Lorenzo Torresani, Vincent Vanhoucke",
            "설명": "Embodiments enable searching of portions of objects in images, including programmatically analyzing each image in a collection in order to determine image data that, for individual images in the collection, represents one or more visual characteristics of a portion of an object shown in that image. A user is enabled to specify one or more search criteria that includes image data, and a search result may be determined based on one or more images in the collection that show a corresponding object that has a portion that satisfies a threshold. The threshold is defined at least in part by the one or more search criteria.",
            "전체 인용횟수": "394회 인용200920102011201220132014201520162017201820192020202120222023596192635312740183139483624",
            "출원번호": "11936734",
            "특허 번호": "7657126",
            "특허청": "US",
            "학술 문서": "System and method for search portions of objects in images and features thereofSB Gokturk, B Sumengen, D Vu, N Dalal, D Yang, X Lin… - US Patent 7,657,126, 2010221회 인용 관련 학술자료 전체 4개의 버전 System and method for search portions of objects in images and features thereofSB Gokturk, B Sumengen, D Vu, N Dalal, D Yang, X Lin… - US Patent 8,345,982, 2013172회 인용 관련 학술자료 전체 4개의 버전 System and method for search portions of objects in images and features thereof*SB Gokturk, B Sumengen, D Vu, N Dalal, D Yang, X Lin… - US Patent 9,008,435, 201558회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "System and method for search portions of objects in images and features thereof",
        "year": null
    },
    "Multilingual acoustic models using distributed deep neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/5/26",
            "설명": "Today's speech recognition technology is mature enough to be useful for many practical applications. In this context, it is of paramount importance to train accurate acoustic models for many languages within given resource constraints such as data, processing power, and time. Multilingual training has the potential to solve the data issue and close the performance gap between resource-rich and resource-scarce languages. Neural networks lend themselves naturally to parameter sharing across languages, and distributed implementations have made it feasible to train large networks. In this paper, we present experimental results for cross- and multi-lingual network training of eleven Romance languages on 10k hours of data in total. The average relative gains over the monolingual baselines are 4%/2% (data-scarce/data-rich languages) for cross- and 7%/2% for multi-lingual training. However, the additional gain …",
            "저자": "Georg Heigold, Vincent Vanhoucke, Alan Senior, Patrick Nguyen, Marc’Aurelio Ranzato, Matthieu Devin, Jeffrey Dean",
            "전체 인용횟수": "375회 인용20132014201520162017201820192020202120222023936414839434136382121",
            "컨퍼런스": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "페이지": "8619-8623",
            "학술 문서": "Multilingual acoustic models using distributed deep neural networksG Heigold, V Vanhoucke, A Senior, P Nguyen… - 2013 IEEE international conference on acoustics …, 2013375회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Multilingual acoustic models using distributed deep neural networks",
        "year": null
    },
    "System and method for enabling image recognition and searching of remote content on display": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "전체 인용횟수": "361회 인용2008200920102011201220132014201520162017201820192020202120222023181510162430402429182631472913",
            "특허 번호": "20080082426",
            "특허청": "US",
            "학술 문서": "System and method for enabling image recognition and searching of remote content on display*SB Gokturk, D Chiao, J Phillips, M Moran, V Vanhoucke… - US Patent 8,732,025, 2014321회 인용 관련 학술자료 전체 4개의 버전 System and method for enabling image recognition and searching of remote content on display*SB Gokturk, D Chiao, J Phillips, M Moran, V Vanhoucke… - US Patent 8,712,862, 201450회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "System and method for enabling image recognition and searching of remote content on display",
        "year": null
    },
    "Arbitrary style transfer in real-time with adaptive instance normalization": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition, our approach allows flexible user controls such as content-style trade-off, style interpolation, color & spatial controls, all using a single feed-forward neural network.",
            "저자": "Xun Huang, Serge Belongie",
            "전체 인용횟수": "3895회 인용20172018201920202021202220231812129454684110321026",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "1501-1510",
            "학술 문서": "Arbitrary style transfer in real-time with adaptive instance normalizationX Huang, S Belongie - Proceedings of the IEEE international conference on …, 20173895회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Arbitrary style transfer in real-time with adaptive instance normalization",
        "year": null
    },
    "The caltech-ucsd birds-200-2011 dataset": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011",
            "게시자": "California Institute of Technology",
            "설명": "CUB-200-2011 is an extended version of CUB-200 [7], a challenging dataset of 200 bird species. The extended version roughly doubles the number of images per category and adds new part localization annotations. All images are annotated with bounding boxes, part locations, and at- tribute labels. Images and annotations were filtered by mul- tiple users of Mechanical Turk. We introduce benchmarks and baseline experiments for multi-class categorization and part localization.",
            "저자": "Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, Serge Belongie",
            "전체 인용횟수": "3646회 인용20132014201520162017201820192020202120222023204075107168269390440690732658",
            "학술 문서": "The caltech-ucsd birds-200-2011 datasetC Wah, S Branson, P Welinder, P Perona, S Belongie - 20113646회 인용 관련 학술자료 전체 8개의 버전 ",
            "호": "2010-001"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The caltech-ucsd birds-200-2011 dataset",
        "year": null
    },
    "Robust object tracking with online multiple instance learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/12/23",
            "게시자": "IEEE",
            "권": "33",
            "설명": "In this paper, we address the problem of tracking an object in a video given its location in the first frame and no other information. Recently, a class of tracking techniques called “tracking by detection” has been shown to give promising results at real-time speeds. These methods train a discriminative classifier in an online manner to separate the object from the background. This classifier bootstraps itself by using the current tracker state to extract positive and negative examples from the current frame. Slight inaccuracies in the tracker can therefore lead to incorrectly labeled training examples, which degrade the classifier and can cause drift. In this paper, we show that using Multiple Instance Learning (MIL) instead of traditional supervised learning avoids these problems and can therefore lead to a more robust tracker with fewer parameter tweaks. We propose a novel online MIL algorithm for object tracking that …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Boris Babenko, Ming-Hsuan Yang, Serge Belongie",
            "전체 인용횟수": "2650회 인용201120122013201420152016201720182019202020212022202313541852903873523163162521721249149",
            "페이지": "1619-1632",
            "학술 문서": "Robust object tracking with online multiple instance learningB Babenko, MH Yang, S Belongie - IEEE transactions on pattern analysis and machine …, 20102650회 인용 관련 학술자료 전체 13개의 버전 ",
            "호": "8"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Robust object tracking with online multiple instance learning",
        "year": null
    },
    "Visual tracking with online multiple instance learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009",
            "설명": "In this paper, we address the problem of learning an adaptive appearance model for object tracking. In particular, a class of tracking techniques called “tracking by detection” have been shown to give promising results at real-time speeds. These methods train a discriminative classifier in an online manner to separate the object from the background. This classifier bootstraps itself by using the current tracker state to extract positive and negative examples from the current frame. Slight inaccuracies in the tracker can therefore lead to incorrectly labeled training examples, which degrades the classifier and can cause further drift. In this paper we show that using Multiple Instance Learning (MIL) instead of traditional supervised learning avoids these problems, and can therefore lead to a more robust tracker with fewer parameter tweaks. We present a novel online MIL algorithm for object tracking that achieves superior …",
            "저자": "B. Babenko, M.-H. Yang, S. Belongie",
            "전체 인용횟수": "2539회 인용2009201020112012201320142015201620172018201920202021202220237781321741942562963242412181911311068248",
            "컨퍼런스": "IEEE Conference on Computer Vision and Pattern Recognition",
            "페이지": "983-990",
            "학술 문서": "Visual tracking with online multiple instance learningB Babenko, MH Yang, S Belongie - 2009 IEEE Conference on computer vision and Pattern …, 20092537회 인용 관련 학술자료 전체 25개의 버전 Visual tracking with online multiple instance learning in CVPR*B Babenko, MH Yang, S Belongi - 20095회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Visual tracking with online multiple instance learning",
        "year": null
    },
    "Multimodal unsupervised image-to-image translation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "Unsupervised image-to-image translation is an important and challenging problem in computer vision. Given an image in the source domain, the goal is to learn the conditional distribution of corresponding images in the target domain, without seeing any pairs of corresponding images. While this conditional distribution is inherently multimodal, existing approaches make an overly simplified assumption, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse outputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework. We assume that the image representation can be decomposed into a content code that is domain-invariant, and a style code that captures domain-specific properties. To translate an image to another domain, we recombine its content code with a random style code sampled from the style space of the target domain. We analyze the proposed framework and establish several theoretical results. Extensive experiments with comparisons to the state-of-the-art approaches further demonstrates the advantage of the proposed framework. Moreover, our framework allows users to control the style of translation outputs by providing an example style image.",
            "저자": "Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz",
            "전체 인용횟수": "2512회 인용20182019202020212022202348274473615589500",
            "컨퍼런스": "Proceedings of the European conference on computer vision (ECCV)",
            "페이지": "172-189",
            "학술 문서": "Multimodal unsupervised image-to-image translationX Huang, MY Liu, S Belongie, J Kautz - Proceedings of the European conference on computer …, 20182512회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Multimodal unsupervised image-to-image translation",
        "year": null
    },
    "DOTA: A large-scale dataset for object detection in aerial images": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "Object detection is an important and challenging problem in computer vision. Although the past decade has witnessed major advances in object detection in natural scenes, such successes have been slow to aerial imagery, not only because of the huge variation in the scale, orientation and shape of the object instances on the earth's surface, but also due to the scarcity of well-annotated datasets of objects in aerial scenes. To advance object detection research in Earth Vision, also known as Earth Observation and Remote Sensing, we introduce a large-scale Dataset for Object deTection in Aerial images (DOTA). To this end, we collect 2806 aerial images from different sensors and platforms. Each image is of the size about 4000-by-4000 pixels and contains objects exhibiting a wide variety of scales, orientations, and shapes. These DOTA images are then annotated by experts in aerial image interpretation using 15 common object categories. The fully annotated DOTA images contains 188,282 instances, each of which is labeled by an arbitrary (8 dof) quadrilateral. To build a baseline for object detection in Earth Vision, we evaluate state-of-the-art object detection algorithms on DOTA. Experiments demonstrate that DOTA well represents real Earth Vision applications and are quite challenging.",
            "저자": "Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, Liangpei Zhang",
            "전체 인용횟수": "2014회 인용20182019202020212022202334132266460568545",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "3974-3983",
            "학술 문서": "DOTA: A large-scale dataset for object detection in aerial imagesGS Xia, X Bai, J Ding, Z Zhu, S Belongie, J Luo… - Proceedings of the IEEE conference on computer …, 20182014회 인용 관련 학술자료 전체 20개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "DOTA: A large-scale dataset for object detection in aerial images",
        "year": null
    },
    "Class-balanced loss based on effective number of samples": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "With the rapid increase of large-scale, real-world datasets, it becomes critical to address the problem of long-tailed data distribution (ie, a few classes account for most of the data, while most classes are under-represented). Existing solutions typically adopt class re-balancing strategies such as re-sampling and re-weighting based on the number of observations for each class. In this work, we argue that as the number of samples increases, the additional benefit of a newly added data point will diminish. We introduce a novel theoretical framework to measure data overlap by associating with each sample a small neighboring region rather than a single point. The effective number of samples is defined as the volume of samples and can be calculated by a simple formula (1-b^ n)/(1-b), where n is the number of samples and b\\in [0, 1) is a hyperparameter. We design a re-weighting scheme that uses the effective number of samples for each class to re-balance the loss, thereby yielding a class-balanced loss. Comprehensive experiments are conducted on artificially induced long-tailed CIFAR datasets and large-scale datasets including ImageNet and iNaturalist. Our results show that when trained with the proposed class-balanced loss, the network is able to achieve significant performance gains on long-tailed datasets.",
            "저자": "Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, Serge Belongie",
            "전체 인용횟수": "1980회 인용2019202020212022202333187446623682",
            "컨퍼런스": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "페이지": "9268-9277",
            "학술 문서": "Class-balanced loss based on effective number of samplesY Cui, M Jia, TY Lin, Y Song, S Belongie - Proceedings of the IEEE/CVF conference on computer …, 20191980회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Class-balanced loss based on effective number of samples",
        "year": null
    },
    "Contour and texture analysis for image segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2001/6",
            "게시자": "Kluwer Academic Publishers",
            "권": "43",
            "설명": " This paper provides an algorithm for partitioning grayscale images into disjoint regions of coherent brightness and texture. Natural images contain both textured and untextured regions, so the cues of contour and texture differences are exploited simultaneously. Contours are treated in the intervening contour framework, while texture is analyzed using textons. Each of these cues has a domain of applicability, so to facilitate cue combination we introduce a gating operator based on the texturedness of the neighborhood at a pixel. Having obtained a local measure of how likely two nearby pixels are to belong to the same region, we use the spectral graph theoretic framework of normalized cuts to find partitions of the image into regions of coherent texture and brightness. Experimental results on a wide range of images are shown.",
            "저널": "International journal of computer vision",
            "저자": "Jitendra Malik, Serge Belongie, Thomas Leung, Jianbo Shi",
            "전체 인용횟수": "1670회 인용200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220232024575567858670751008911310113511710680585351324422",
            "페이지": "7-27",
            "학술 문서": "Contour and texture analysis for image segmentationJ Malik, S Belongie, T Leung, J Shi - International journal of computer vision, 20011670회 인용 관련 학술자료 전체 24개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Contour and texture analysis for image segmentation",
        "year": null
    },
    "Caltech-UCSD birds 200": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/9/29",
            "게시자": "California Institute of Technology",
            "설명": "Caltech-UCSD Birds 200 (CUB-200) is a challenging image dataset annotated with 200 bird species. It was created to enable the study of subordinate categorization, which is not possible with other popular datasets that focus on basic level categories (such as PASCAL VOC, Caltech-101, etc). The images were downloaded from the website Flickr and filtered by workers on Amazon Mechanical Turk. Each image is annotated with a bounding box, a rough bird segmentation, and a set of attribute labels.",
            "저자": "Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, Pietro Perona",
            "전체 인용횟수": "1536회 인용20112012201320142015201620172018201920202021202220232122314149675996125204283280241",
            "학술 문서": "Caltech-UCSD birds 200P Welinder, S Branson, T Mita, C Wah, F Schroff… - 20101536회 인용 관련 학술자료 전체 9개의 버전 ",
            "호": "2010-001"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Caltech-UCSD birds 200",
        "year": null
    },
    "End-to-end scene text recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/11/6",
            "게시자": "IEEE",
            "설명": "This paper focuses on the problem of word detection and recognition in natural images. The problem is significantly more challenging than reading text in scanned documents, and has only recently gained attention from the computer vision community. Sub-components of the problem, such as text detection and cropped image word recognition, have been studied in isolation [7, 4, 20]. However, what is unclear is how these recent approaches contribute to solving the end-to-end problem of word recognition. We fill this gap by constructing and evaluating two systems. The first, representing the de facto state-of-the-art, is a two stage pipeline consisting of text detection followed by a leading OCR engine. The second is a system rooted in generic object recognition, an extension of our previous work in [20]. We show that the latter approach achieves superior performance. While scene text recognition has generally been …",
            "저자": "Kai Wang, Boris Babenko, Serge Belongie",
            "전체 인용횟수": "1378회 인용20122013201420152016201720182019202020212022202332527999116104113142134174158127",
            "컨퍼런스": "2011 International conference on computer vision",
            "페이지": "1457-1464",
            "학술 문서": "End-to-end scene text recognitionK Wang, B Babenko, S Belongie - 2011 International conference on computer vision, 20111372회 인용 관련 학술자료 전체 19개의 버전 , 2013 IEEE Int. Conf. on Computer Vision (ICCV)*K Wang, B Babenko, S Belongie - 20115회 인용 관련 학술자료 IEEE Int. Conf. on Computer Vision (ICCV)*K Wang, B Babenko, S Belongie - IEEE, Piscataway, NJ,(2011), 20113회 인용 관련 학술자료 ICCV 2011*K Wang, B Babenko, S Belongie - 20112회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "End-to-end scene text recognition",
        "year": null
    },
    "Residual networks behave like ensembles of relatively shallow networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "권": "29",
            "설명": "In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.",
            "저널": "Advances in neural information processing systems",
            "저자": "Andreas Veit, Michael J Wilber, Serge Belongie",
            "전체 인용횟수": "1262회 인용201620172018201920202021202220232199159194207207209149",
            "학술 문서": "Residual networks behave like ensembles of relatively shallow networksA Veit, MJ Wilber, S Belongie - Advances in neural information processing systems, 20161153회 인용 관련 학술자료 전체 12개의 버전 Residual networks are exponential ensembles of relatively shallow networks*A Veit, M Wilber, S Belongie - arXiv preprint arXiv:1605.06431, 2016120회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Residual networks behave like ensembles of relatively shallow networks",
        "year": null
    },
    "Blobworld: A system for region-based image indexing and retrieval": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1999",
            "게시자": "Springer Berlin Heidelberg",
            "설명": " Blobworld is a system for image retrieval based on finding coherent image regions which roughly correspond to objects. Each image is automatically segmented into regions (“blobs”) with associated color and texture descriptors. Queryingi s based on the attributes of one or two regions of interest, rather than a description of the entire image. In order to make large-scale retrieval feasible, we index the blob descriptions using a tree. Because indexing in the high-dimensional feature space is computationally prohibitive, we use a lower-rank approximation to the high-dimensional distance. Experiments show encouraging results for both querying and indexing.",
            "저자": "Chad Carson, Megan Thomas, Serge Belongie, Joseph M Hellerstein, Jitendra Malik",
            "전체 인용횟수": "1259회 인용199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202311316288103105105969266706751583833313727171914583",
            "컨퍼런스": "Visual Information and Information Systems: Third International Conference, VISUAL’99 Amsterdam, The Netherlands, June 2–4, 1999 Proceedings 3",
            "페이지": "509-517",
            "학술 문서": "Blobworld: A system for region-based image indexing and retrievalC Carson, M Thomas, S Belongie, JM Hellerstein… - Visual Information and Information Systems: Third …, 19991257회 인용 관련 학술자료 전체 17개의 버전 Blobworld: A System for Region-Based Image Indexing and Retrieval (long version)?*C Carson, M Thomas, S Belongie, JM Hellerstein… - Retrieved from the Internet Archive3회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Blobworld: A system for region-based image indexing and retrieval",
        "year": null
    },
    "An iterative image registration technique with an application to stereo vision": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1981/8/24",
            "권": "2",
            "설명": "Image registration finds a variety of applications in computer vision. Unfortunately, traditional image registration techniques tend to be costly. We present a new image registration technique that makes use of the spatial intensity gradient of the images to find a good match using a type of Newton-Raphson iteration. Our technique is faster because it examines far fewer potential matches between the images than existing techniques. Furthermore, this registration technique can be generalized to handle rotation, scaling and shearing. We show show our technique can be adapted for use in a stereo vision system.",
            "저자": "Bruce D Lucas, Takeo Kanade",
            "전체 인용횟수": "19046회 인용19941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202358497576111991281581972743664375527188468779681009100110951157115212191061949879861872878617",
            "컨퍼런스": "IJCAI'81: 7th international joint conference on Artificial intelligence",
            "페이지": "674-679",
            "학술 문서": "An iterative image registration technique with an application to stereo visionBD Lucas, T Kanade - IJCAI'81: 7th international joint conference on Artificial …, 198119046회 인용 관련 학술자료 전체 37개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "An iterative image registration technique with an application to stereo vision",
        "year": null
    },
    "Neural network-based face detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1998/1",
            "게시자": "IEEE",
            "권": "20",
            "설명": "We present a neural network-based upright frontal face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We present a straightforward procedure for aligning positive face examples for training. To collect negative examples, we use a bootstrap algorithm, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting nonface training examples, which must be chosen to span the entire space of nonface images. Simple heuristics, such as using the fact that faces rarely overlap in images, can further improve the accuracy. Comparisons with several other state-of-the-art face detection systems are presented, showing that our system has comparable performance in …",
            "저널": "IEEE Transactions on pattern analysis and machine intelligence",
            "저자": "Henry A Rowley, Shumeet Baluja, Takeo Kanade",
            "전체 인용횟수": "6343회 인용199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023245310816920726928832239936032134731132431630425630324621624620817315514312255",
            "페이지": "23-38",
            "학술 문서": "Neural network-based face detectionHA Rowley, S Baluja, T Kanade - IEEE Transactions on pattern analysis and machine …, 19986343회 인용 관련 학술자료 전체 54개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Neural network-based face detection",
        "year": null
    },
    "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/6/13",
            "게시자": "IEEE",
            "설명": "In 2000, the Cohn-Kanade (CK) database was released for the purpose of promoting research into automatically detecting individual facial expressions. Since then, the CK database has become one of the most widely used test-beds for algorithm development and evaluation. During this period, three limitations have become apparent: 1) While AU codes are well validated, emotion labels are not, as they refer to what was requested rather than what was actually performed, 2) The lack of a common performance metric against which to evaluate new algorithms, and 3) Standard protocols for common databases have not emerged. As a consequence, the CK database has been used for both AU and emotion detection (even though labels for the latter have not been validated), comparison with benchmark algorithms is missing, and use of random subsets of the original database makes meta-analyses difficult. To …",
            "저자": "Patrick Lucey, Jeffrey F Cohn, Takeo Kanade, Jason Saragih, Zara Ambadar, Iain Matthews",
            "전체 인용횟수": "4648회 인용20112012201320142015201620172018201920202021202220235275143200258325376461508538570564496",
            "컨퍼런스": "2010 ieee computer society conference on computer vision and pattern recognition-workshops",
            "페이지": "94-101",
            "학술 문서": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expressionP Lucey, JF Cohn, T Kanade, J Saragih, Z Ambadar… - 2010 ieee computer society conference on computer …, 20104648회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
        "year": null
    },
    "Shape and motion from image streams under orthography: a factorization method": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1992/11",
            "게시자": "Kluwer Academic Publishers",
            "권": "9",
            "설명": " Inferring scene geometry and camera motion from a stream of images is possible in principle, but is an ill-conditioned problem when the objects are distant with respect to their size. We have developed a factorization method that can overcome this difficulty by recovering shape and motion under orthography without computing depth as an intermediate step. An image stream can be represented by the 2F×P measurement matrix of the image coordinates of P points tracked through F frames. We show that under orthographic projection this matrix is of rank 3. Based on this observation, the factorization method uses the singular-value decomposition technique to factor the measurement matrix into two matrices which represent object shape and camera rotation respectively. Two of the three translation components are computed in a preprocessing stage. The method can also handle and obtain a …",
            "저널": "International journal of computer vision",
            "저자": "Carlo Tomasi, Takeo Kanade",
            "전체 인용횟수": "4304회 인용19921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023111942415987105135131146153174177195205198187172175189160161172158207177130121123917569",
            "페이지": "137-154",
            "학술 문서": "Shape and motion from image streams under orthography: a factorization methodC Tomasi, T Kanade - International journal of computer vision, 19924094회 인용 관련 학술자료 전체 51개의 버전 Shape and motion from image streams: a factorization method.*C Tomasi, T Kanade - Proceedings of the National Academy of Sciences, 1993231회 인용 관련 학술자료 전체 26개의 버전 Shape and Motion from Image Streams: A Factorization Method. Detection and Tracking of Point Features*C Tomasi, T Kanade - 19916회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Shape and motion from image streams under orthography: a factorization method",
        "year": null
    },
    "Detection and tracking of point": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1991/4",
            "권": "9",
            "설명": "The factorization method described in this series of reports requires an algorithm to track the motion of features in an image stream. Given the small inter-frame displacement made possible by the factorization approach, the best tracking method turns out to be the one proposed by Lucas and Kanade in 1981.The method defines the measure of match between fixed-size feature windows in the past and current frame as the sum of squared intensity di erences over the windows. The displacement is then defined as the one that minimizes this sum. For small motions, a linearization of the image intensities leads to a Newton-Raphson style minimization. In this report, after rederiving the method in a physically intuitive way, we answer the crucial question of how to choose the feature windows that are best suited for tracking. Our selection criterion is based directly on the definition of the tracking algorithm, and expresses how well a feature can be tracked. As a result, the criterion is optimal by construction. We show by experiment that the performance of both the selection and the tracking algorithm are adequate for our factorization method, and we address the issue of how to detect occlusions. In the conclusion, we point out specific open questions for future research.",
            "저널": "Int J Comput Vis",
            "저자": "Carlo Tomasi, Takeo Kanade",
            "전체 인용횟수": "3953회 인용199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220231311818162144658811711616218120520222920717921218823724420820616515314616998",
            "페이지": "3",
            "학술 문서": "Detection and tracking of pointC Tomasi, T Kanade - Int J Comput Vis, 19913888회 인용 관련 학술자료 전체 30개의 버전 Tracking of point features*C Tomasi, TK Detection - Int. J. Comput. Vis, 199170회 인용 관련 학술자료 전체 4개의 버전 ",
            "호": "137-154"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Detection and tracking of point",
        "year": null
    },
    "Comprehensive database for facial expression analysis": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2000/3/28",
            "게시자": "IEEE",
            "설명": "Within the past decade, significant effort has occurred in developing methods of facial expression analysis. Because most investigators have used relatively limited data sets, the generalizability of these various methods remains unknown. We describe the problem space for facial expression analysis, which includes level of description, transitions among expressions, eliciting conditions, reliability and validity of training and test data, individual differences in subjects, head orientation and scene complexity image characteristics, and relation to non-verbal behavior. We then present the CMU-Pittsburgh AU-Coded Face Expression Image Database, which currently includes 2105 digitized image sequences from 182 adult subjects of varying ethnicity, performing multiple tokens of most primary FACS action units. This database is the most comprehensive testbed to date for comparative studies of facial expression analysis.",
            "저자": "Takeo Kanade, Jeffrey F Cohn, Yingli Tian",
            "전체 인용횟수": "3525회 인용2000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220239192134668698121130164208222192210233223237232246204161151117107",
            "컨퍼런스": "Proceedings fourth IEEE international conference on automatic face and gesture recognition (cat. No. PR00580)",
            "페이지": "46-53",
            "학술 문서": "Comprehensive database for facial expression analysisT Kanade, JF Cohn, Y Tian - Proceedings fourth IEEE international conference on …, 20003525회 인용 관련 학술자료 전체 23개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Comprehensive database for facial expression analysis",
        "year": null
    },
    "Convolutional pose machines": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.",
            "저자": "Shih-En Wei, Varun Ramakrishna, Takeo Kanade, Yaser Sheikh",
            "전체 인용횟수": "3390회 인용2016201720182019202020212022202337206477594579552517376",
            "컨퍼런스": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition",
            "페이지": "4724-4732",
            "학술 문서": "Convolutional pose machinesSE Wei, V Ramakrishna, T Kanade, Y Sheikh - Proceedings of the IEEE conference on Computer …, 20163390회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Convolutional pose machines",
        "year": null
    },
    "Multi-pie": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/5/1",
            "게시자": "Elsevier",
            "권": "28",
            "설명": "A close relationship exists between the advancement of face recognition algorithms and the availability of face databases varying factors that affect facial appearance in a controlled manner. The CMU PIE database has been very influential in advancing research in face recognition across pose and illumination. Despite its success the PIE database has several shortcomings: a limited number of subjects, a single recording session and only few expressions captured. To address these issues we collected the CMU Multi-PIE database. It contains 337 subjects, imaged under 15 view points and 19 illumination conditions in up to four recording sessions. In this paper we introduce the database and describe the recording procedure. We furthermore present results from baseline experiments using PCA and LDA classifiers to highlight similarities and differences between PIE and Multi-PIE.",
            "저널": "Image and vision computing",
            "저자": "Ralph Gross, Iain Matthews, Jeffrey Cohn, Takeo Kanade, Simon Baker",
            "전체 인용횟수": "2566회 인용200920102011201220132014201520162017201820192020202120222023193464101115166221245228265254231237176147",
            "페이지": "807-813",
            "학술 문서": "Multi-pieR Gross, I Matthews, J Cohn, T Kanade, S Baker - Image and vision computing, 20102566회 인용 관련 학술자료 전체 16개의 버전 ",
            "호": "5"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Multi-pie",
        "year": null
    },
    "Recognizing action units for facial expression analysis": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2001/2",
            "게시자": "IEEE",
            "권": "23",
            "설명": "Most automatic expression analysis systems attempt to recognize a small set of prototypic expressions, such as happiness, anger, surprise, and fear. Such prototypic expressions, however, occur rather infrequently. Human emotions and intentions are more often communicated by changes in one or a few discrete facial features. In this paper, we develop an automatic face analysis (AFA) system to analyze facial expressions based on both permanent facial features (brows, eyes, mouth) and transient facial features (deepening of facial furrows) in a nearly frontal-view face image sequence. The AFA system recognizes fine-grained changes in facial expression into action units (AU) of the Facial Action Coding System (FACS), instead of a few prototypic expressions. Multistate face and facial component models are proposed for tracking and modeling the various facial features, including lips, eyes, brows, cheeks, and …",
            "저널": "IEEE Transactions on pattern analysis and machine intelligence",
            "저자": "Y-I Tian, Takeo Kanade, Jeffrey F Cohn",
            "전체 인용횟수": "2295회 인용20012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023297560748891871041038899931161121281331119913710312810992",
            "페이지": "97-115",
            "학술 문서": "Recognizing action units for facial expression analysisYI Tian, T Kanade, JF Cohn - IEEE Transactions on pattern analysis and machine …, 20012285회 인용 관련 학술자료 전체 16개의 버전 Recognizing action units for facial expression analysisYL Tian, T Kanade, JF Colin - Multimodal interface for human-machine …, 200212회 인용 관련 학술자료 전체 10개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Recognizing action units for facial expression analysis",
        "year": null
    },
    "A statistical method for 3D object detection applied to faces and cars": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2000/6/15",
            "게시자": "IEEE",
            "권": "1",
            "설명": "In this paper, we describe a statistical method for 3D object detection. We represent the statistics of both object appearance and \"non-object\" appearance using a product of histograms. Each histogram represents the joint statistics of a subset of wavelet coefficients and their position on the object. Our approach is to use many such histograms representing a wide variety of visual attributes. Using this method, we have developed the first algorithm that can reliably detect human faces with out-of-plane rotation and the first algorithm that can reliably detect passenger cars over a wide range of viewpoints.",
            "저자": "Henry Schneiderman, Takeo Kanade",
            "전체 인용횟수": "1924회 인용2000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220236616410215415515313514013112110096968260655030212421205",
            "컨퍼런스": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No. PR00662)",
            "페이지": "746-751",
            "학술 문서": "A statistical method for 3D object detection applied to faces and carsH Schneiderman, T Kanade - Proceedings IEEE Conference on Computer Vision …, 20001909회 인용 관련 학술자료 전체 25개의 버전 A statistical approcah to 3d object detection applied to faces and cars*H Schneiderman, T Kanade - Proceedings of the Eighth IEEE International …, 200018회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A statistical method for 3D object detection applied to faces and cars",
        "year": null
    },
    "A stereo matching algorithm with an adaptive window: Theory and experiment": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1994/9",
            "게시자": "IEEE",
            "권": "16",
            "설명": "A central problem in stereo matching by computing correlation or sum of squared differences (SSD) lies in selecting an appropriate window size. The window size must be large enough to include enough intensity variation for reliable matching, but small enough to avoid the effects of projective distortion. If the window is too small and does not cover enough intensity variation, it gives a poor disparity estimate, because the signal (intensity variation) to noise ratio is low. If, on the other hand, the window is too large and covers a region in which the depth of scene points (i.e., disparity) varies, then the position of maximum correlation or minimum SSD may not represent correct matching due to different projective distortions in the left and right images. For this reason, a window size must be selected adaptively depending on local variations of intensity and disparity. The authors present a method to select an appropriate …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Takeo Kanade, Masatoshi Okutomi",
            "전체 인용횟수": "1919회 인용199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202371218263347525570769210911197857397901111148468565552484729253116",
            "페이지": "920-932",
            "학술 문서": "A stereo matching algorithm with an adaptive window: Theory and experimentT Kanade, M Okutomi - IEEE transactions on pattern analysis and machine …, 19941919회 인용 관련 학술자료 전체 17개의 버전 ",
            "호": "9"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A stereo matching algorithm with an adaptive window: Theory and experiment",
        "year": null
    },
    "Limits on super-resolution and how to break them": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2002/9",
            "게시자": "IEEE",
            "권": "24",
            "설명": "Nearly all super-resolution algorithms are based on the fundamental constraints that the super-resolution image should generate low resolution input images when appropriately warped and down-sampled to model the image formation process. (These reconstruction constraints are normally combined with some form of smoothness prior to regularize their solution.) We derive a sequence of analytical results which show that the reconstruction constraints provide less and less useful information as the magnification factor increases. We also validate these results empirically and show that, for large enough magnification factors, any smoothness prior leads to overly smooth results with very little high-frequency content. Next, we propose a super-resolution algorithm that uses a different kind of constraint in addition to the reconstruction constraints. The algorithm attempts to recognize local features in the low-resolution …",
            "저널": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "저자": "Simon Baker, Takeo Kanade",
            "전체 인용횟수": "1916회 인용2002200320042005200620072008200920102011201220132014201520162017201820192020202120222023541677092949213910412115112813810711191719456494630",
            "페이지": "1167-1183",
            "학술 문서": "Limits on super-resolution and how to break themS Baker, T Kanade - IEEE Transactions on Pattern Analysis and Machine …, 20021916회 인용 관련 학술자료 전체 7개의 버전 ",
            "호": "9"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Limits on super-resolution and how to break them",
        "year": null
    },
    "A system for video surveillance and monitoring": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2000/5",
            "게시자": "CMU-RI-TR-00-12, Carnegie Melon University, Pittsburgh, Peen, America",
            "권": "2000",
            "설명": "Under the three-year Video Surveillance and Monitoring (VSAM) project (1997–1999), the Robotics Institute at Carnegie Mellon University (CMU) and the Sarnoff Corporation developed a system for autonomous Video Surveillance and Monitoring. The technical approach uses multiple, cooperative video sensors to provide continuous coverage of people and vehicles in a cluttered environment. This final report presents an overview of the system, and of the technical accomplishments that have been achieved. c",
            "저널": "VSAM final report",
            "저자": "Robert T Collins, Alan J Lipton, Takeo Kanade, Hironobu Fujiyoshi, David Duggins, Yanghai Tsin, David Tolliver, Nobuyoshi Enomoto, Osamu Hasegawa, Peter Burt, Lambert Wixson",
            "전체 인용횟수": "1773회 인용20002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202361636396711310211810212713412211811693646762565442423522",
            "페이지": "1",
            "학술 문서": "A system for video surveillance and monitoringRT Collins, AJ Lipton, T Kanade, H Fujiyoshi… - VSAM final report, 20001773회 인용 관련 학술자료 전체 15개의 버전 ",
            "호": "1-68"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A system for video surveillance and monitoring",
        "year": null
    },
    "Stereo by intra-and inter-scanline search using dynamic programming": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1985/3",
            "게시자": "IEEE",
            "설명": "This paper presents a stereo matching algorithm using the dynamic programming technique. The stereo matching problem, that is, obtaining a correspondence between right and left images, can be cast as a search problem. When a pair of stereo images is rectified, pairs of corresponding points can be searched for within the same scanlines. We call this search intra-scanline search. This intra-scanline search can be treated as the problem of finding a matching path on a two-dimensional (2D) search plane whose axes are the right and left scanlines. Vertically connected edges in the images provide consistency constraints across the 2D search planes. Inter-scanline search in a three-dimensional (3D) search space, which is a stack of the 2D search planes, is needed to utilize this constraint. Our stereo matching algorithm uses edge-delimited intervals as elements to be matched, and employs the above mentioned …",
            "저널": "IEEE Transactions on pattern analysis and machine intelligence",
            "저자": "Yuichi Ohta, Takeo Kanade",
            "전체 인용횟수": "1581회 인용198419851986198719881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202341020323839413737393844384253595047514165786269645950475245473225241614129206",
            "페이지": "139-154",
            "학술 문서": "Stereo by intra-and inter-scanline search using dynamic programmingY Ohta, T Kanade - IEEE Transactions on pattern analysis and machine …, 19851581회 인용 관련 학술자료 전체 17개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Stereo by intra-and inter-scanline search using dynamic programming",
        "year": null
    },
    "Color information for region segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1980/7/1",
            "게시자": "Academic Press",
            "권": "13",
            "설명": "In color image processing various kinds of color features can be calculated from the tristimuli R, G, and B. We attempt to derive a set of effective color features by systematic experiments of region segmentation. An Ohlander-type segmentation algorithm by recursive thresholding is employed as a tool for the experiment. At each step of segmenting a region, new color features are calculated for the pixels in that region by the Karhunen Loeve transformation of R, G, and B data. By analyzing more than 100 color features which are thus obtained during segmenting eight kinds of color pictures, we have found that a set of color features,(R+ G+ B) 3, R− B, and (2G− R− B) 2, are effective. These three features are significant in this order and in many cases a good segmentation can be achieved by using only the first two. The effectiveness of our color feature set is discussed by a comparative study with various other sets of …",
            "저널": "Computer graphics and image processing",
            "저자": "Yu-Ichi Ohta, Takeo Kanade, Toshiyuki Sakai",
            "전체 인용횟수": "1548회 인용19851986198719881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202383715112692437273038414727515150506882456053826476585644514732383217201711",
            "페이지": "222-241",
            "학술 문서": "Color information for region segmentationYI Ohta, T Kanade, T Sakai - Computer graphics and image processing, 19801548회 인용 관련 학술자료 전체 5개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Color information for region segmentation",
        "year": null
    },
    "A multiple-baseline stereo": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1993/4",
            "게시자": "IEEE",
            "권": "15",
            "설명": "A stereo matching method that uses multiple stereo pairs with various baselines generated by a lateral displacement of a camera to obtain precise distance estimates without suffering from ambiguity is presented. Matching is performed simply by computing the sum of squared-difference (SSD) values. The SSD functions for individual stereo pairs are represented with respect to the inverse distance and are then added to produce the sum of SSDs. This resulting function is called the SSSD-in-inverse-distance. It is shown that the SSSD-in-inverse-distance function exhibits a unique and clear minimum at the correct matching position, even when the underlying intensity patterns of the scene include ambiguities or repetitive patterns. The authors first define a stereo algorithm based on the SSSD-in-inverse-distance and present a mathematical analysis to show how the algorithm can remove ambiguity and increase …",
            "저널": "IEEE Transactions on pattern analysis and machine intelligence",
            "저자": "Masatoshi Okutomi, Takeo Kanade",
            "전체 인용횟수": "1534회 인용19931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220238231828416138796673848788876669544949665542454044363228262013",
            "페이지": "353-363",
            "학술 문서": "A multiple-baseline stereoM Okutomi, T Kanade - IEEE Transactions on pattern analysis and machine …, 19931528회 인용 관련 학술자료 전체 12개의 버전 A Multiple-Baseline Stereo.M OKUTOMI, T KANADE - 電子情報通信学会論文誌 D-2, 19926회 인용 관련 학술자료 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A multiple-baseline stereo",
        "year": null
    },
    "Vision and navigation for the Carnegie-Mellon Navlab": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1988/5",
            "게시자": "IEEE",
            "권": "10",
            "설명": "A distributed architecture articulated around the CODGER (communication database with geometric reasoning) knowledge database is described for a mobile robot system that includes both perception and navigation tools. Results are described for vision and navigation tests using a mobile testbed that integrates perception and navigation capabilities that are based on two types of vision algorithms: color vision for road following, and 3-D vision for obstacle detection and avoidance. The perception modules are integrated into a system that allows the vehicle to drive continuously in an actual outdoor environment. The resulting system is able to navigate continuously on roads while avoiding obstacles.< >",
            "저널": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "저자": "Charles Thorpe, Martial H Hebert, Takeo Kanade, Steven A Shafer",
            "전체 인용횟수": "1126회 인용19871988198919901991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220238204049494634614843293623341426251831282927323527343229182731242628231611",
            "페이지": "362-373",
            "학술 문서": "Vision and navigation for the Carnegie-Mellon NavlabC Thorpe, MH Hebert, T Kanade, SA Shafer - IEEE Transactions on Pattern Analysis and Machine …, 19881006회 인용 관련 학술자료 전체 7개의 버전 Vision and navigation for the Carnegie-Mellon NavlabC Thorpe, M Hebert, T Kanade, S Shafer - Annual Review of Computer Science, 1987149회 인용 관련 학술자료 전체 6개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Vision and navigation for the Carnegie-Mellon Navlab",
        "year": null
    },
    "Kalman filter-based algorithms for estimating depth from image sequences": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1989/9",
            "게시자": "Kluwer Academic Publishers",
            "권": "3",
            "설명": " Using known camera motion to estimate depth from image sequences is an important problem in robot vision. Many applications of depth-from-motion, including navigation and manipulation, require algorithms that can estimate depth in an on-line, incremental fashion. This requires a representation that records the uncertainty in depth estimates and a mechanism that integrates new measurements with existing depth estimates to reduce the uncertainty over time. Kalman filtering provides this mechanism. Previous applications of Kalman filtering to depth-from-motion have been limited to estimating depth at the location of a sparse set of features. In this paper, we introduce a new, pixel-based (iconic) algorithm that estimates depth and depth uncertainty at each pixel and incrementally refines these estimates over time. We describe the algorithm and contrast its formulation and performance to that of a feature …",
            "저널": "International Journal of Computer Vision",
            "저자": "Larry Matthies, Takeo Kanade, Richard Szeliski",
            "전체 인용횟수": "1087회 인용198819891990199119921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023413254261416744303933253435292927222731403335374636293117162621147184",
            "페이지": "209-238",
            "학술 문서": "Kalman filter-based algorithms for estimating depth from image sequencesL Matthies, T Kanade, R Szeliski - International Journal of Computer Vision, 19891087회 인용 관련 학술자료 전체 26개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Kalman filter-based algorithms for estimating depth from image sequences",
        "year": null
    },
    "Computer-assisted surgery planner and intra-operative guidance system": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2001/3/20",
            "발명자": "Anthony M DiGioia III, David A Simon, Branislav Jaramaz, Michael K Blackwell, Frederick M Morgan, Robert V O'toole, Takeo Kanade",
            "설명": "An apparatus for facilitating the implantation of an artificial component in one of a hip joint, a knee joint, a hand and wrist joint, an elbow joint, a shoulder joint, and a foot and ankle joint. The apparatus includes a pre-operative geometric planner and a pre-operative kinematic biomechanical simulator in communication with the pre-operative geometric planner.",
            "전체 인용횟수": "998회 인용2003200420052006200720082009201020112012201320142015201620172018201920202021202220235919162417153844458667687195566952887535",
            "출원번호": "09189914",
            "특허 번호": "6205411",
            "특허청": "US",
            "학술 문서": "Computer-assisted surgery planner and intra-operative guidance systemAM DiGioia III, DA Simon, B Jaramaz, MK Blackwell… - US Patent 6,205,411, 2001998회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Computer-assisted surgery planner and intra-operative guidance system",
        "year": null
    },
    "Surface reflection: physical and geometrical perspectives": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1991",
            "권": "13",
            "설명": "Machine vision can greatly benefit from the development of accurate reflectance models. There are two approaches to the study of reflection: physical and geometrical optics. While geometrical models may be construed as mere approximations to physical models, they possess simpler mathematical forms that often render them more usable than physical models. However, in general, geometrical models are applicable only when the wavelength of incident light is small compared to the dimensions of the surface imperfections. Therefore, it is incorrect to use these models to interpret or predict reflections from smooth surfaces; only physical models are capable of describing the underlying reflection mechanism.In this paper, reflectance models based on physical optics and geometrical optics are studied in detail. More specifically, we consider the Beckmann-Spizzichino (physical optics) model and the Torrance-Sparrow (geometrical optics) model. We have chosen these two particular models as they have been reported to fit experimental data well. Each model is described in detail, and the conditions that determine the validity of the model are clearly stated. By studying reflectance curves predicted by the two models, we propose a reflectance framework comprising three components: the diffuse lobe, the specular lobe, and the specular spike. The effects of surface roughness on the three primary components are analyzed in detail.",
            "저자": "Shree K Nayar, Katsushi Ikeuchi, Takeo Kanade",
            "전체 인용횟수": "969회 인용199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220231614322132222345233645382853474534313231224736302625182215252115",
            "페이지": "611-634",
            "학술 문서": "Surface reflection: physical and geometrical perspectivesSK Nayar, K Ikeuchi, T Kanade - 1991969회 인용 관련 학술자료 전체 13개의 버전 ",
            "호": "7"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Surface reflection: physical and geometrical perspectives",
        "year": null
    },
    "Graph-based visual saliency": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006",
            "권": "19",
            "설명": "A new bottom-up visual saliency model, Graph-Based Visual Saliency (GBVS), is proposed. It consists of two steps: rst forming activation maps on certain feature channels, and then normalizing them in a way which highlights conspicuity and admits combination with other maps. The model is simple, and biologically plausible insofar as it is naturally parallelized. This model powerfully predicts human xations on 749 variations of 108 natural images, achieving 98% of the ROC area of a human-based control, whereas the classical algorithms of Itti & Koch ([2],[3],[4]) achieve only 84%.",
            "저널": "Advances in neural information processing systems",
            "저자": "Jonathan Harel, Christof Koch, Pietro Perona",
            "전체 인용횟수": "4654회 인용2008200920102011201220132014201520162017201820192020202120222023133080157223340440499550481463394297273196146",
            "학술 문서": "Graph-based visual saliencyJ Harel, C Koch, P Perona - Advances in neural information processing systems, 20064654회 인용 관련 학술자료 전체 19개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Graph-based visual saliency",
        "year": null
    },
    "Caltech-256 object category dataset": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2007/3/10",
            "게시자": "California Institute of Technology",
            "설명": "We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 [1] was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) artifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, then benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching [2] algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions.",
            "저자": "Gregory Griffin, Alex Holub, Pietro Perona",
            "전체 인용횟수": "2789회 인용20072008200920102011201220132014201520162017201820192020202120222023216274120135178189204208207190212223225193146146",
            "학술 문서": "Caltech-256 object category datasetG Griffin, A Holub, P Perona - 20072789회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Caltech-256 object category dataset",
        "year": null
    },
    "Self-tuning spectral clustering": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004",
            "권": "17",
            "설명": "We study a number of open issues in spectral clustering:(i) Selecting the appropriate scale of analysis,(ii) Handling multi-scale data,(iii) Clustering with irregular background clutter, and,(iv) Finding automatically the number of groups. We first propose that a ‘local’scale should be used to compute the affinity between each pair of points. This local scaling leads to better clustering especially when the data includes multiple scales and when the clusters are placed within a cluttered background. We further suggest exploiting the structure of the eigenvectors to infer automatically the number of groups. This leads to a new algorithm in which the final randomly initialized k-means stage is eliminated.",
            "저널": "Advances in neural information processing systems",
            "저자": "Lihi Zelnik-Manor, Pietro Perona",
            "전체 인용횟수": "2767회 인용20052006200720082009201020112012201320142015201620172018201920202021202220231636528598140169179191162198194165195201183183150137",
            "학술 문서": "Self-tuning spectral clusteringL Zelnik-Manor, P Perona - Advances in neural information processing systems, 20042767회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Self-tuning spectral clustering",
        "year": null
    },
    "Preattentive texture discrimination with early vision mechanisms": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1990/5/1",
            "게시자": "Optica Publishing Group",
            "권": "7",
            "설명": "We present a model of human preattentive texture perception. This model consists of three stages: (1) convolution of the image with a bank of even-symmetric linear filters followed by half-wave rectification to give a set of responses modeling outputs of V1 simple cells, (2) inhibition, localized in space, within and among the neural-response profiles that results in the suppression of weak responses when there are strong responses at the same or nearby locations, and (3) texture-boundary detection by using wide odd-symmetric mechanisms. Our model can predict the salience of texture boundaries in any arbitrary gray-scale image. A computer implementation of this model has been tested on many of the classic stimuli from psychophysical literature. Quantitative predictions of the degree of discriminability of different texture pairs match well with experimental measurements of discriminability in human observers.",
            "저널": "JOSA A",
            "저자": "Jitendra Malik, Pietro Perona",
            "전체 인용횟수": "1368회 인용19901991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220234183938514952444657544943575860515461514333394251373824211519261317",
            "페이지": "923-932",
            "학술 문서": "Preattentive texture discrimination with early vision mechanismsJ Malik, P Perona - JOSA A, 19901368회 인용 관련 학술자료 전체 23개의 버전 ",
            "호": "5"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Preattentive texture discrimination with early vision mechanisms",
        "year": null
    },
    "The multidimensional wisdom of crowds": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010",
            "설명": "Distributing labeling tasks among hundreds or thousands of annotators is an increasingly important method for annotating large datasets. We present a method for estimating the underlying value (eg the class) of each image from (noisy) annotations provided by multiple annotators. Our method is based on a model of the image formation and annotation process. Each image has different characteristics that are represented in an abstract Euclidean space. Each annotator is modeled as a multidimensional entity with variables representing competence, expertise and bias. This allows the model to discover and represent groups of annotators that have different sets of skills and knowledge, as well as groups of images that differ qualitatively. We find that our model predicts ground truth labels on both synthetic and real data more accurately than state of the art methods. Experiments also show that our model, starting from a set of binary labels, may discover rich information, such as different\" schools of thought\" amongst the annotators, and can group together images belonging to separate categories.",
            "저자": "Peter Welinder, Steve Branson, Serge J. Belongie, Pietro Perona",
            "전체 인용횟수": "1104회 인용20112012201320142015201620172018201920202021202220233342821021081019694871051087360",
            "컨퍼런스": "Advances in Neural Information Processing Systems (NIPS)",
            "페이지": "2424-2432",
            "학술 문서": "The multidimensional wisdom of crowdsP Welinder, S Branson, P Perona, S Belongie - Advances in neural information processing systems, 20101104회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The multidimensional wisdom of crowds",
        "year": null
    },
    "Learning object categories from google's image search": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/10/17",
            "게시자": "IEEE",
            "권": "2",
            "설명": "Current approaches to object category recognition require datasets of training images to be manually prepared, with varying degrees of supervision. We present an approach that can learn an object category from just its name, by utilizing the raw output of image search engines available on the Internet. We develop a new model, TSI-pLSA, which extends pLSA (as applied to visual words) to include spatial information in a translation and scale invariant manner. Our approach can handle the high intra-class variability and large proportion of unrelated images returned by search engines. We evaluate tire models on standard test sets, showing performance competitive with existing methods trained on hand prepared datasets",
            "저자": "Robert Fergus, Li Fei-Fei, Pietro Perona, Andrew Zisserman",
            "전체 인용횟수": "961회 인용2005200620072008200920102011201220132014201520162017201820192020202120222023829608910296868775896241332791014175",
            "컨퍼런스": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1",
            "페이지": "1816-1823",
            "학술 문서": "Learning object categories from google's image searchR Fergus, L Fei-Fei, P Perona, A Zisserman - Tenth IEEE International Conference on Computer …, 2005961회 인용 관련 학술자료 전체 28개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning object categories from google's image search",
        "year": null
    },
    "Unsupervised learning of models for recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2000",
            "게시자": "Springer Berlin Heidelberg",
            "설명": " We present a method to learn object class models from unlabeled and unsegmented cluttered scenes for the purpose of visual object recognition. We focus on a particular type of model where objects are represented as flexible constellations of rigid parts (features). The variability within a class is represented by a joint probability density function (pdf) on the shape of the constellation and the output of part detectors. In a first stage, the method automatically identifies distinctive parts in the training set by applying a clustering algorithm to patterns selected by an interest operator. It then learns the statistical shape model using expectation maximization. The method achieves very good classification results on human faces and rear views of cars.",
            "저자": "Markus Weber, Max Welling, Pietro Perona",
            "전체 인용횟수": "960회 인용200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023419262373758473736772734042362726171823189128",
            "컨퍼런스": "Computer Vision-ECCV 2000: 6th European Conference on Computer Vision Dublin, Ireland, June 26–July 1, 2000 Proceedings, Part I 6",
            "페이지": "18-32",
            "학술 문서": "Unsupervised learning of models for recognitionM Weber, M Welling, P Perona - Computer Vision-ECCV 2000: 6th European …, 2000960회 인용 관련 학술자료 전체 21개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Unsupervised learning of models for recognition",
        "year": null
    },
    "A performance evaluation of local descriptors": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/8/22",
            "게시자": "IEEE",
            "권": "27",
            "설명": "In this paper, we compare the performance of descriptors computed for local interest regions, as, for example, extracted by the Harris-Affine detector [Mikolajczyk, K and Schmid, C, 2004]. Many different descriptors have been proposed in the literature. It is unclear which descriptors are more appropriate and how their performance depends on the interest region detector. The descriptors should be distinctive and at the same time robust to changes in viewing conditions as well as to errors of the detector. Our evaluation uses as criterion recall with respect to precision and is carried out for different image transformations. We compare shape context [Belongie, S, et al., April 2002], steerable filters [Freeman, W and Adelson, E, Setp. 1991], PCA-SIFT [Ke, Y and Sukthankar, R, 2004], differential invariants [Koenderink, J and van Doorn, A, 1987], spin images [Lazebnik, S, et al., 2003], SIFT [Lowe, D. G., 1999], complex filters …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Krystian Mikolajczyk, Cordelia Schmid",
            "전체 인용횟수": "10797회 인용2004200520062007200820092010201120122013201420152016201720182019202020212022202381145339392535690834829894959848793774588473400318310249185",
            "페이지": "1615-1630",
            "학술 문서": "A performance evaluation of local descriptorsK Mikolajczyk, C Schmid - IEEE transactions on pattern analysis and machine …, 200510797회 인용 관련 학술자료 전체 50개의 버전 ",
            "호": "10"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A performance evaluation of local descriptors",
        "year": null
    },
    "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006/6/17",
            "게시자": "IEEE",
            "권": "2",
            "설명": "This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting \"spatial pyramid\" is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralba’s \"gist\" and Lowe’s SIFT descriptors.",
            "저자": "Svetlana Lazebnik, Cordelia Schmid, Jean Ponce",
            "전체 인용횟수": "10607회 인용20062007200820092010201120122013201420152016201720182019202020212022202345127213359501642783993115311831039863653561461346285218",
            "컨퍼런스": "2006 IEEE computer society conference on computer vision and pattern recognition (CVPR'06)",
            "페이지": "2169-2178",
            "학술 문서": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categoriesS Lazebnik, C Schmid, J Ponce - 2006 IEEE computer society conference on computer …, 200610607회 인용 관련 학술자료 전체 41개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories",
        "year": null
    },
    "Scale & affine invariant interest point detectors": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004/10",
            "게시자": "Kluwer Academic Publishers",
            "권": "60",
            "설명": " In this paper we propose a novel approach for detecting interest points invariant to scale and affine transformations. Our scale and affine invariant detectors are based on the following recent results: (1) Interest points extracted with the Harris detector can be adapted to affine transformations and give repeatable results (geometrically stable). (2) The characteristic scale of a local structure is indicated by a local extremum over scale of normalized derivatives (the Laplacian). (3) The affine shape of a point neighborhood is estimated based on the second moment matrix. Our scale invariant detector computes a multi-scale representation for the Harris interest point detector and then selects points at which a local measure (the Laplacian) is maximal over scales. This provides a set of distinctive points which are invariant to scale, rotation and translation as well as robust to illumination changes and limited …",
            "저널": "International journal of computer vision",
            "저자": "Krystian Mikolajczyk, Cordelia Schmid",
            "전체 인용횟수": "5687회 인용20052006200720082009201020112012201320142015201620172018201920202021202220237420327734641347049149748143644035125822019013113310488",
            "페이지": "63-86",
            "학술 문서": "Scale & affine invariant interest point detectorsK Mikolajczyk, C Schmid - International journal of computer vision, 20045687회 인용 관련 학술자료 전체 50개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Scale & affine invariant interest point detectors",
        "year": null
    },
    "Learning realistic human actions from movies": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008/6/23",
            "게시자": "IEEE",
            "설명": "The aim of this paper is to address recognition of natural human actions in diverse and realistic video settings. This challenging but important subject has mostly been ignored in the past due to several problems one of which is the lack of realistic and annotated video datasets. Our first contribution is to address this limitation and to investigate the use of movie scripts for automatic annotation of human actions in videos. We evaluate alternative methods for action retrieval from scripts and show benefits of a text-based classifier. Using the retrieved action samples for visual learning, we next turn to the problem of action classification in video. We present a new method for video classification that builds upon and extends several recent ideas including local space-time features, space-time pyramids and multi-channel non-linear SVMs. The method is shown to improve state-of-the-art results on the standard KTH action …",
            "저자": "Ivan Laptev, Marcin Marszalek, Cordelia Schmid, Benjamin Rozenfeld",
            "전체 인용횟수": "4535회 인용200820092010201120122013201420152016201720182019202020212022202317111159225353449438459469414356276247193173102",
            "컨퍼런스": "2008 IEEE Conference on Computer Vision and Pattern Recognition",
            "페이지": "1-8",
            "학술 문서": "Learning realistic human actions from moviesI Laptev, M Marszalek, C Schmid, B Rozenfeld - 2008 IEEE Conference on Computer Vision and …, 20084535회 인용 관련 학술자료 전체 33개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning realistic human actions from movies",
        "year": null
    },
    "Product quantization for nearest neighbor search": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/3/18",
            "게시자": "IEEE",
            "권": "33",
            "설명": "This paper introduces a product quantization-based approach for approximate nearest neighbor search. The idea is to decompose the space into a Cartesian product of low-dimensional subspaces and to quantize each subspace separately. A vector is represented by a short code composed of its subspace quantization indices. The euclidean distance between two vectors can be efficiently estimated from their codes. An asymmetric version increases precision, as it computes the approximate distance between a vector and a code. Experimental results show that our approach searches for nearest neighbors efficiently, in particular in combination with an inverted file system. Results for SIFT and GIST image descriptors show excellent search accuracy, outperforming three state-of-the-art approaches. The scalability of our approach is validated on a data set of two billion vectors.",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Herve Jegou, Matthijs Douze, Cordelia Schmid",
            "전체 인용횟수": "4136회 인용1988198919901991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220231834383335437156807798821078647171413161324314080130186221244254246259257233291322287",
            "페이지": "117-128",
            "학술 문서": "Product quantization for nearest neighbor searchH Jegou, M Douze, C Schmid - IEEE transactions on pattern analysis and machine …, 20104136회 인용 관련 학술자료 전체 35개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Product quantization for nearest neighbor search",
        "year": null
    },
    "Action recognition with improved trajectories": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013",
            "설명": "Recently dense trajectories were shown to be an efficient video representation for action recognition and achieved state-of-the-art results on a variety of datasets. This paper improves their performance by taking into account camera motion to correct them. To estimate camera motion, we match feature points between frames using SURF descriptors and dense optical flow, which are shown to be complementary. These matches are, then, used to robustly estimate a homography with RANSAC. Human motion is in general different from camera motion and generates inconsistent matches. To improve the estimation, a human detector is employed to remove these matches. Given the estimated camera motion, we remove trajectories consistent with it. We also use this estimation to cancel out camera motion from the optical flow. This significantly improves motion-based descriptors, such as HOF and MBH. Experimental results on four challenging action datasets (ie, Hollywood2, HMDB51, Olympic Sports and UCF50) significantly outperform the current state of the art.",
            "저자": "Heng Wang, Cordelia Schmid",
            "전체 인용횟수": "3902회 인용2013201420152016201720182019202020212022202310113295410491524493474443313257",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "3551-3558",
            "학술 문서": "Action recognition with improved trajectoriesH Wang, C Schmid - Proceedings of the IEEE international conference on …, 20133902회 인용 관련 학술자료 전체 33개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Action recognition with improved trajectories",
        "year": null
    },
    "Aggregating local descriptors into a compact image representation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/6/13",
            "게시자": "IEEE",
            "설명": "We address the problem of image search on a very large scale, where three constraints have to be considered jointly: the accuracy of the search, its efficiency, and the memory usage of the representation. We first propose a simple yet efficient way of aggregating local image descriptors into a vector of limited dimension, which can be viewed as a simplification of the Fisher kernel representation. We then show how to jointly optimize the dimension reduction and the indexing algorithm, so that it best preserves the quality of vector comparison. The evaluation shows that our approach significantly outperforms the state of the art: the search accuracy is comparable to the bag-of-features approach for an image representation that fits in 20 bytes. Searching a 10 million image dataset takes about 50ms.",
            "저자": "Hervé Jégou, Matthijs Douze, Cordelia Schmid, Patrick Pérez",
            "전체 인용횟수": "3102회 인용2009201020112012201320142015201620172018201920202021202220238218288128197260340326327309271262257186",
            "컨퍼런스": "2010 IEEE computer society conference on computer vision and pattern recognition",
            "페이지": "3304-3311",
            "학술 문서": "Aggregating local descriptors into a compact image representationH Jégou, M Douze, C Schmid, P Pérez - 2010 IEEE computer society conference on computer …, 20103029회 인용 관련 학술자료 전체 16개의 버전 Aggregating local descriptors into a compact representation*H Jégou, M Douze, C Schmid, P Pérez - IEEE Conf. Comput. Vis. Pattern Recognit, 2010127회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Aggregating local descriptors into a compact image representation",
        "year": null
    },
    "Aggregating local image descriptors into compact codes": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/12/13",
            "게시자": "IEEE",
            "권": "34",
            "설명": "This paper addresses the problem of large-scale image search. Three constraints have to be taken into account: search accuracy, efficiency, and memory usage. We first present and evaluate different ways of aggregating local image descriptors into a vector and show that the Fisher kernel achieves better performance than the reference bag-of-visual words approach for any given vector dimension. We then jointly optimize dimensionality reduction and indexing in order to obtain a precise vector comparison as well as a compact representation. The evaluation shows that the image representation can be reduced to a few dozen bytes while preserving high accuracy. Searching a 100 million image data set takes about 250 ms on one processor core.",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Hervé Jégou, Florent Perronnin, Matthijs Douze, Jorge Sánchez, Patrick Pérez, Cordelia Schmid",
            "전체 인용횟수": "2924회 인용198819891990199119921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023183435353642695480769682105884917159159223128317013116620223520718417413611910378",
            "페이지": "1704-1716",
            "학술 문서": "Aggregating local image descriptors into compact codesH Jégou, F Perronnin, M Douze, J Sánchez, P Pérez… - IEEE transactions on pattern analysis and machine …, 20112924회 인용 관련 학술자료 전체 27개의 버전 ",
            "호": "9"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Aggregating local image descriptors into compact codes",
        "year": null
    },
    "Local features and kernels for classification of texture and object categories: A comprehensive study": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2007/6",
            "게시자": "Kluwer Academic Publishers",
            "권": "73",
            "설명": " Recently, methods based on local image features have shown promise for texture and object recognition tasks. This paper presents a large-scale evaluation of an approach that represents images as distributions (signatures or histograms) of features extracted from a sparse set of keypoint locations and learns a Support Vector Machine classifier with kernels based on two effective measures for comparing distributions, the Earth Mover’s Distance and the χ2 distance. We first evaluate the performance of our approach with different keypoint detectors and descriptors, as well as different kernels and classifiers. We then conduct a comparative evaluation with several state-of-the-art recognition methods on four texture and five object databases. On most of these databases, our implementation exceeds the best reported results and achieves comparable performance on the rest. Finally, we investigate the …",
            "저널": "International journal of computer vision",
            "저자": "Jianguo Zhang, Marcin Marszałek, Svetlana Lazebnik, Cordelia Schmid",
            "전체 인용횟수": "2610회 인용20062007200820092010201120122013201420152016201720182019202020212022202312411051772562442632492952221891161118958605529",
            "페이지": "213-238",
            "학술 문서": "Local features and kernels for classification of texture and object categories: A comprehensive studyJ Zhang, M Marszałek, S Lazebnik, C Schmid - International journal of computer vision, 20072610회 인용 관련 학술자료 전체 44개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Local features and kernels for classification of texture and object categories: A comprehensive study",
        "year": null
    },
    "Human detection using oriented histograms of flow and appearance": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006",
            "게시자": "Springer Berlin Heidelberg",
            "설명": " Detecting humans in films and videos is a challenging problem owing to the motion of the subjects, the camera and the background and to variations in pose, appearance, clothing, illumination and background clutter. We develop a detector for standing and moving people in videos with possibly moving cameras and backgrounds, testing several different motion coding schemes and showing empirically that orientated histograms of differential optical flow give the best overall performance. These motion-based descriptors are combined with our Histogram of Oriented Gradient appearance descriptors. The resulting detector is tested on several databases including a challenging test set taken from feature films and containing wide ranges of pose, motion and background variations, including moving cameras and backgrounds. We validate our results on two challenging test sets containing more than 4400 …",
            "저자": "Navneet Dalal, Bill Triggs, Cordelia Schmid",
            "전체 인용횟수": "2426회 인용2006200720082009201020112012201320142015201620172018201920202021202220237264167889812618018423022920221717417416010178",
            "컨퍼런스": "Computer Vision–ECCV 2006: 9th European Conference on Computer Vision, Graz, Austria, May 7-13, 2006. Proceedings, Part II 9",
            "페이지": "428-441",
            "학술 문서": "Human detection using oriented histograms of flow and appearanceN Dalal, B Triggs, C Schmid - Computer Vision–ECCV 2006: 9th European …, 20062426회 인용 관련 학술자료 전체 18개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Human detection using oriented histograms of flow and appearance",
        "year": null
    },
    "Evaluation of interest point detectors": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2000/6",
            "게시자": "Kluwer Academic Publishers",
            "권": "37",
            "설명": " Many different low-level feature detectors exist and it is widely agreed that the evaluation of detectors is important. In this paper we introduce two evaluation criteria for interest points' repeatability rate and information content. Repeatability rate evaluates the geometric stability under different transformations. Information content measures the distinctiveness of features. Different interest point detectors are compared using these two criteria. We determine which detector gives the best results and show that it satisfies the criteria well.",
            "저널": "International Journal of computer vision",
            "저자": "Cordelia Schmid, Roger Mohr, Christian Bauckhage",
            "전체 인용횟수": "2415회 인용200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220232031608910412614516715018817816118114311913091716156414326",
            "페이지": "151-172",
            "학술 문서": "Evaluation of interest point detectorsC Schmid, R Mohr, C Bauckhage - International Journal of computer vision, 20002415회 인용 관련 학술자료 전체 32개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Evaluation of interest point detectors",
        "year": null
    },
    "Local grayvalue invariants for image retrieval": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1997/5",
            "게시자": "IEEE",
            "권": "19",
            "설명": "This paper addresses the problem of retrieving images from large image databases. The method is based on local grayvalue invariants which are computed at automatically detected interest points. A voting algorithm and semilocal constraints make retrieval possible. Indexing allows for efficient retrieval from a database of more than 1,000 images. Experimental results show correct retrieval in the case of partial visibility, similarity transformations, extraneous features, and small perspective deformations.",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Cordelia Schmid, Roger Mohr",
            "전체 인용횟수": "2382회 인용1998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202312346062699712715717518116816115713713610091838952423033322314",
            "페이지": "530-535",
            "학술 문서": "Local grayvalue invariants for image retrievalC Schmid, R Mohr - IEEE transactions on pattern analysis and machine …, 19972382회 인용 관련 학술자료 전체 32개의 버전 ",
            "호": "5"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Local grayvalue invariants for image retrieval",
        "year": null
    },
    "Hamming embedding and weak geometric consistency for large scale image search": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008",
            "게시자": "Springer Berlin Heidelberg",
            "설명": " This paper improves recent methods for large scale image search. State-of-the-art methods build on the bag-of-features image representation. We, first, analyze bag-of-features in the framework of approximate nearest neighbor search. This shows the sub-optimality of such a representation for matching descriptors and leads us to derive a more precise representation based on 1) Hamming embedding (HE) and 2) weak geometric consistency constraints (WGC). HE provides binary signatures that refine the matching based on visual words. WGC filters matching descriptors that are not consistent in terms of angle and scale. HE and WGC are integrated within the inverted file and are efficiently exploited for all images, even in the case of very large datasets. Experiments performed on a dataset of one million of images show a significant improvement due to the binary signature and the weak geometric …",
            "저자": "Herve Jegou, Matthijs Douze, Cordelia Schmid",
            "전체 인용횟수": "2359회 인용200920102011201220132014201520162017201820192020202120222023511201161361802282492432172131661431058565",
            "컨퍼런스": "Computer Vision–ECCV 2008: 10th European Conference on Computer Vision, Marseille, France, October 12-18, 2008, Proceedings, Part I 10",
            "페이지": "304-317",
            "학술 문서": "Hamming embedding and weak geometric consistency for large scale image searchH Jegou, M Douze, C Schmid - Computer Vision–ECCV 2008: 10th European …, 20082359회 인용 관련 학술자료 전체 24개의 버전 M. Douze, C. Schmid Hamming Embedding and Weak geometry consistency for large scale image search*H Jegou - Proceedings of the 10th European conference on …, 20081회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Hamming embedding and weak geometric consistency for large scale image search",
        "year": null
    },
    "A spatio-temporal descriptor based on 3d-gradients": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008/9/1",
            "게시자": "British Machine Vision Association",
            "설명": "In this work, we present a novel local descriptor for video sequences. The proposed descriptor is based on histograms of oriented 3D spatio-temporal gradients. Our contribution is four-fold. (i) To compute 3D gradients for arbitrary scales, we develop a memory-efficient algorithm based on integral videos. (ii) We propose a generic 3D orientation quantization which is based on regular polyhedrons. (iii) We perform an in-depth evaluation of all descriptor parameters and optimize them for action recognition. (iv) We apply our descriptor to various action datasets (KTH, Weizmann, Hollywood) and show that we outperform the state-of-the-art.",
            "저자": "Alexander Klaser, Marcin Marszałek, Cordelia Schmid",
            "전체 인용횟수": "2320회 인용2009201020112012201320142015201620172018201920202021202220232858661351841822192562432162241681409769",
            "컨퍼런스": "BMVC 2008-19th British Machine Vision Conference",
            "페이지": "275: 1-10",
            "학술 문서": "A spatio-temporal descriptor based on 3d-gradientsA Klaser, M Marszałek, C Schmid - BMVC 2008-19th British Machine Vision Conference, 20082320회 인용 관련 학술자료 전체 18개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A spatio-temporal descriptor based on 3d-gradients",
        "year": null
    },
    "An affine invariant interest point detector": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2002",
            "게시자": "Springer Berlin Heidelberg",
            "설명": " This paper presents a novel approach for detecting affine invariant interest points. Our method can deal with significant affine transformations including large scale changes. Such transformations introduce significant changes in the point location as well as in the scale and the shape of the neighbourhood of an interest point. Our approach allows to solve for these problems simultaneously. It is based on three key ideas: 1) The second moment matrix computed in a point can be used to normalize a region in an affine invariant way (skew and stretch). 2) The scale of the local structure is indicated by local extrema of normalized derivatives over scale. 3) An affine-adapted Harris detector determines the location of interest points. A multi-scale version of this detector is used for initialization. An iterative algorithm then modifies location, scale and neighbourhood of each point and converges to affine invariant points …",
            "저자": "Krystian Mikolajczyk, Cordelia Schmid",
            "전체 인용횟수": "2229회 인용2003200420052006200720082009201020112012201320142015201620172018201920202021202220233110712013311513813915115316215113213411297758652492629",
            "컨퍼런스": "Computer Vision—ECCV 2002: 7th European Conference on Computer Vision Copenhagen, Denmark, May 28–31, 2002 Proceedings, Part I 7",
            "페이지": "128-142",
            "학술 문서": "An affine invariant interest point detectorK Mikolajczyk, C Schmid - Computer Vision—ECCV 2002: 7th European …, 20022229회 인용 관련 학술자료 전체 30개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "An affine invariant interest point detector",
        "year": null
    },
    "The visual object tracking vot2015 challenge results": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "The Visual Object Tracking challenge 2015, VOT2015, aims at comparing short-term single-object visual trackers that do not apply pre-learned models of object appearance. Results of 62 trackers are presented. The number of tested trackers makes VOT 2015 the largest benchmark on short-term tracking to date. For each participating tracker, a short description is provided in the appendix. Features of the VOT2015 challenge that go beyond its VOT2014 predecessor are:(i) a new VOT2015 dataset twice as large as in VOT2014 with full annotation of targets by rotated bounding boxes and per-frame attribute,(ii) extensions of the VOT2014 evaluation methodology by introduction of a new performance measure. The dataset, the evaluation kit as well as the results are publicly available at the challenge website.",
            "저자": "Matej Kristan, Jiri Matas, Ales Leonardis, Michael Felsberg, Luka Cehovin, Gustavo Fernandez, Tomas Vojir, Gustav Hager, Georg Nebehay, Roman Pflugfelder",
            "전체 인용횟수": "2105회 인용20142015201620172018201920202021202220231762146233332381325263191119",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision workshops",
            "페이지": "1-23",
            "학술 문서": "The visual object tracking vot2015 challenge resultsM Kristan, J Matas, A Leonardis, M Felsberg, L Cehovin… - Proceedings of the IEEE international conference on …, 20152105회 인용 관련 학술자료 전체 102개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The visual object tracking vot2015 challenge results",
        "year": null
    },
    "Description of interest regions with local binary patterns": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/3/1",
            "게시자": "Pergamon",
            "권": "42",
            "설명": "This paper presents a novel method for interest region description. We adopted the idea that the appearance of an interest region can be well characterized by the distribution of its local features. The most well-known descriptor built on this idea is the SIFT descriptor that uses gradient as the local feature. Thus far, existing texture features are not widely utilized in the context of region description. In this paper, we introduce a new texture feature called center-symmetric local binary pattern (CS-LBP) that is a modified version of the well-known local binary pattern (LBP) feature. To combine the strengths of the SIFT and LBP, we use the CS-LBP as the local feature in the SIFT algorithm. The resulting descriptor is called the CS-LBP descriptor. In the matching and object category classification experiments, our descriptor performs favorably compared to the SIFT. Furthermore, the CS-LBP descriptor is computationally …",
            "저널": "Pattern Recognition",
            "저자": "Marko Heikkilä, Matti Pietikäinen, Cordelia Schmid",
            "전체 인용횟수": "1579회 인용20092010201120122013201420152016201720182019202020212022202314639010714314917216013013411187946344",
            "페이지": "425-436",
            "학술 문서": "Description of interest regions with local binary patternsM Heikkilä, M Pietikäinen, C Schmid - Pattern recognition, 20091579회 인용 관련 학술자료 전체 16개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Description of interest regions with local binary patterns",
        "year": null
    },
    "Dense trajectories and motion boundary descriptors for action recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/5",
            "게시자": "Springer US",
            "권": "103",
            "설명": " This paper introduces a video representation based on dense trajectories and motion boundary descriptors. Trajectories capture the local motion information of the video. A dense representation guarantees a good coverage of foreground motion as well as of the surrounding context. A state-of-the-art optical flow algorithm enables a robust and efficient extraction of dense trajectories. As descriptors we extract features aligned with the trajectories to characterize shape (point coordinates), appearance (histograms of oriented gradients) and motion (histograms of optical flow). Additionally, we introduce a descriptor based on motion boundary histograms (MBH) which rely on differential optical flow. The MBH descriptor shows to consistently outperform other state-of-the-art descriptors, in particular on real-world videos that contain a significant amount of camera motion. We evaluate our video representation in the …",
            "저널": "International journal of computer vision",
            "저자": "Heng Wang, Alexander Kläser, Cordelia Schmid, Cheng-Lin Liu",
            "전체 인용횟수": "1990회 인용201320142015201620172018201920202021202220234715120526525824520817817212790",
            "페이지": "60-79",
            "학술 문서": "Dense trajectories and motion boundary descriptors for action recognitionH Wang, A Kläser, C Schmid, CL Liu - International journal of computer vision, 20131990회 인용 관련 학술자료 전체 31개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Dense trajectories and motion boundary descriptors for action recognition",
        "year": null
    },
    "Indexing based on scale invariant interest points": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2001/7/7",
            "게시자": "IEEE",
            "권": "1",
            "설명": "This paper presents a new method for detecting scale invariant interest points. The method is based on two recent results on scale space: (1) Interest points can be adapted to scale and give repeatable results (geometrically stable). (2) Local extrema over scale of normalized derivatives indicate the presence of characteristic local structures. Our method first computes a multi-scale representation for the Harris interest point detector. We then select points at which a local measure (the Laplacian) is maximal over scales. This allows a selection of distinctive points for which the characteristic scale is known. These points are invariant to scale, rotation and translation as well as robust to illumination changes and limited changes of viewpoint. For indexing, the image is characterized by a set of scale invariant points; the scale associated with each point allows the computation of a scale invariant descriptor. Our descriptors …",
            "저자": "Krystian Mikolajczyk, Cordelia Schmid",
            "전체 인용횟수": "1862회 인용200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220239214071831071011261491171261221111289710673654646371922",
            "컨퍼런스": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001",
            "페이지": "525-531",
            "학술 문서": "Indexing based on scale invariant interest pointsK Mikolajczyk, C Schmid - Proceedings Eighth IEEE International Conference on …, 20011862회 인용 관련 학술자료 전체 28개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Indexing based on scale invariant interest points",
        "year": null
    },
    "Pattern classification": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2000",
            "게시자": "Wiley",
            "설명": "Pattern Classification Page 1 Pattern Classification All materials in these slides were taken \nfrom Pattern Classification (2nd ed) by RO Duda, PE Hart and DG Stork, John Wiley & Sons, \n2000 with the permission of the authors and the publisher Page 2 Pattern Classification, \nChapter 2 (Part 1) 1 Page 3 Pattern Classification, Chapter 2 (Part 1) 2 • Posterior, likelihood, \nevidence • P(ωj | x) = P(x | ωj ) . P (ωj ) / P(x) • Where in case of two categories • Posterior = (Likelihood. \nPrior) / Evidence ∑ = = = 2j 1j j j )(P)|x(P )x(P ω ω Page 4 Pattern Classification, Chapter 2 (Part \n1) 3 Page 5 Pattern Classification, Chapter 2 (Part 1) 4 • Decision given the posterior \nprobabilities X is an observation for which: if P(ω1 | x) > P(ω2 | x) True state of nature = ω1 if \nP(ω1 | x) < P(ω2 | x) True state of nature = ω2 Therefore: whenever we observe a particular x, \nthe probability of error is : P(error | x) = P(ω1 | x) if we decide ω2 P(error | x) = P(ω2 | x) if …",
            "저자": "Peter E Hart, David G Stork, Richard O Duda",
            "전체 인용횟수": "1070회 인용200220032004200520062007200820092010201120122013201420152016201720182019202020212022202392630614455394255544964544749472926314310994",
            "학술 문서": "Pattern classificationPE Hart, DG Stork, RO Duda - 20001070회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pattern classification",
        "year": null
    },
    "Pattern classification and scene analysis": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1973/1",
            "게시자": "Wiley",
            "권": "3",
            "설명": "Until now we have assumed that the training samples used to design a classifier were labelled by their category membership. Procedures that use labelled samples are said to be supervised. Now we shall investigate a number of unsupervised procedures, which use unlabelled samples. That is, we shall see what can be done when all one has is a collection of samples without being told their category. One might wonder why anyone is interested in such an unpromising problem, and whether or not it is even possible in principle to learn anything of value from unlabelled samples. There are at least five basic reasons for interest in unsupervised procedures. First, the collection and labelling of a large set of sample patterns can be surprisingly costly. For instance, recorded speech is virtually free, but labelling the speech| marking what word or phoneme is being uttered at each instant| can be very expensive and time consuming. If a classifier can be crudely designed on a small, labelled set of samples, and then\\tuned up\" by allowing it to run without supervision on a large, unlabelled set, much time and trouble can be saved. Second, one might wish to proceed in the reverse direction: train with large amounts of (less expensive) unlabelled data, and only then use supervision to label the groupings found. This may be appropriate for large\\data mining\" applications where the contents of a large database are not known beforehand. Third, in many applications the characteristics of the patterns can change slowly with time, for example in automated food classification as the seasons change. If these changes can be tracked by a classifier running in an …",
            "저자": "Richard O Duda, Peter E Hart",
            "전체 인용횟수": "26829회 인용198419851986198719881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202315916617425027833443948355958870870777986497197010099899109381005989949965865813744703739679643591601502442444407371371258",
            "페이지": "731-739",
            "학술 문서": "Pattern classification and scene analysisRO Duda, PE Hart - 197326814회 인용 관련 학술자료 전체 6개의 버전 Pattern classification ans scene analysisRO Duda, PE Hart - 197315회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pattern classification and scene analysis",
        "year": null
    },
    "Nearest neighbor pattern classification": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1967/1",
            "게시자": "IEEE",
            "권": "13",
            "설명": "The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of error  of such a rule must be at least as great as the Bayes probability of error  --the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in the  -category case that  , where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in …",
            "저널": "IEEE transactions on information theory",
            "저자": "Thomas Cover, Peter Hart",
            "전체 인용횟수": "17805회 인용19931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220235381737293109135118125137187241250299346363415529586671742824882917975108312341354134114551362",
            "페이지": "21-27",
            "학술 문서": "Nearest neighbor pattern classificationT Cover, P Hart - IEEE transactions on information theory, 196717801회 인용 관련 학술자료 전체 5개의 버전 Nearest neighbour classification*T Cover, P Hart - IEEE Transactions IT-134회 인용 관련 학술자료 Nearest Pattern Classification*TM Cover, PE Hart - IEEE Trans. on Infor'mation Theory, vol. IT4회 인용 관련 학술자료 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Nearest neighbor pattern classification",
        "year": null
    },
    "A formal basis for the heuristic determination of minimum cost paths": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1968/7",
            "게시자": "IEEE",
            "권": "4",
            "설명": "Although the problem of determining the minimum cost path through a graph arises naturally in a number of interesting applications, there has been no underlying theory to guide the development of efficient search procedures. Moreover, there is no adequate conceptual framework within which the various ad hoc search strategies proposed to date can be compared. This paper describes how heuristic information from the problem domain can be incorporated into a formal mathematical theory of graph searching and demonstrates an optimality property of a class of search strategies.",
            "저널": "IEEE transactions on Systems Science and Cybernetics",
            "저자": "Peter E Hart, Nils J Nilsson, Bertram Raphael",
            "전체 인용횟수": "14444회 인용198819891990199119921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023403942624856586337435755607370831071152012322923864185106067116797408138338659651054126313061121",
            "페이지": "100-107",
            "학술 문서": "A formal basis for the heuristic determination of minimum cost pathsPE Hart, NJ Nilsson, B Raphael - IEEE transactions on Systems Science and …, 196814444회 인용 관련 학술자료 전체 5개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A formal basis for the heuristic determination of minimum cost paths",
        "year": null
    },
    "Use of the Hough transformation to detect lines and curves in pictures": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1972/1/1",
            "게시자": "ACM",
            "권": "15",
            "설명": "Hough has proposed an interesting and computationally efficient procedure for detecting lines in pictures. This paper points out that the use of angle-radius rather than slope-intercept parameters simplifies the computation further. It also shows how the method can be used for more general curve fitting, and gives alternative interpretations that explain the source of its efficiency.",
            "저널": "Communications of the ACM",
            "저자": "Richard O Duda, Peter E Hart",
            "전체 인용횟수": "10123회 인용1985198619871988198919901991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220233436588468987210285116114128989399105111107107109136201260315354379444505551583587510543504482454477446333",
            "페이지": "11-15",
            "학술 문서": "Use of the Hough transformation to detect lines and curves in picturesRO Duda, PE Hart - Communications of the ACM, 197210105회 인용 관련 학술자료 전체 7개의 버전 Communs Ass. comput*RO Duda, PE Hart - Mach, 197532회 인용 관련 학술자료 Use of Hough transformation to detect line and curve in picturesRO Dude, PE Hart - CACM, 19724회 인용 관련 학술자료 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Use of the Hough transformation to detect lines and curves in pictures",
        "year": null
    },
    "The condensed nearest neighbor rule (corresp.)": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1968/5",
            "게시자": "IEEE",
            "권": "14",
            "설명": "The purpose of this note is to introduce the condensed nearest neighbor decision rule (CNN rule) and to pose some unsolved theoretical questions which it raises. The CNN rule, one of a class of ad hoc decision rules which have appeared in the literature in the past few years, was motivated by statistical considerations",
            "저널": "IEEE transactions on information theory",
            "저자": "Peter Hart",
            "전체 인용횟수": "2868회 인용198919901991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220238917172637272342273064585664658071108859010695122108118112128110122143140134166124",
            "페이지": "515-516",
            "학술 문서": "The condensed nearest neighbor rule (corresp.)P Hart - IEEE transactions on information theory, 19682868회 인용 관련 학술자료 전체 7개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The condensed nearest neighbor rule (corresp.)",
        "year": null
    },
    "Pattern classification, chapter nonparametric techniques": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2000",
            "게시자": "Wiley-Interscience Publication,",
            "설명": "In most pattern recognition applications, the common parametric forms of the underlying density function rarely fit the densities actually encountered in practice.",
            "저자": "RO Duda, PE Hart, DG Stork, Alexandru Ionescu",
            "전체 인용횟수": "2659회 인용20022003200420052006200720082009201020112012201320142015201620172018201920202021202220231929479191117199161176176212202181138981331181161011088338",
            "페이지": "177-178",
            "학술 문서": "Pattern classification, chapter nonparametric techniquesRO Duda, PE Hart, DG Stork, A Ionescu - 20002574회 인용 관련 학술자료 Pattern Classification, chapter 10*RO Duda, PE Hart, DG Stork - 200186회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pattern classification, chapter nonparametric techniques",
        "year": null
    },
    "Learning and executing generalized robot plans": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1972/1/1",
            "게시자": "Elsevier",
            "권": "3",
            "설명": "In this paper we describe some major new additions to the STRIPS robot problem-solving system. The first addition is a process for generalizing a plan produced by STRIPS so that problem-specific constants appearing in the plan are replaced by problem-independent parameters.The generalized plan, stored in a convenient format called a triangle table, has two important functions. The more obvious function is as a single macro action that can be used by STRIPS— either in whole or in part—during the solution of a subsequent problem. Perhaps less obviously, the generalized plan also plays a central part in the process that monitors the real-world execution of a plan, and allows the robot to react “intelligently” to unexpected consequences of actions.We conclude with a discussion of experiments with the system on several example problems.",
            "저널": "Artificial intelligence",
            "저자": "Richard E Fikes, Peter E Hart, Nils J Nilsson",
            "전체 인용횟수": "1695회 인용198419851986198719881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202325313644726172586776566249354034253333373032384735392921273334202127242722222521",
            "페이지": "251-288",
            "학술 문서": "Learning and executing generalized robot plansRE Fikes, PE Hart, NJ Nilsson - Artificial intelligence, 19721695회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning and executing generalized robot plans",
        "year": null
    },
    "Subjective Bayesian methods for rule-based inference systems": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1976/6/7",
            "도서": "Proceedings of the June 7-10, 1976, national computer conference and exposition",
            "설명": "The general problem of drawing inferences from uncertain or incomplete evidence has invited a variety of technical approaches, some mathematically rigorous and some largely informal and intuitive. Most current inference systems in artificial intelligence have emphasized intuitive methods, because the absence of adequate statistical samples forces a reliance on the subjective judgment of human experts. We describe in this paper a subjective Bayesian inference method that realizes some of the advantages of both formal and informal approaches. Of particular interest are the modifications needed to deal with the inconsistencies usually found in collections of subjective statements.",
            "저자": "Richard O Duda, Peter E Hart, Nils J Nilsson",
            "전체 인용횟수": "1002회 인용1984198519861987198819891990199119921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023193756497151505044263131212517131315177101518249161123184223108126663105",
            "페이지": "1075-1082",
            "학술 문서": "Subjective Bayesian methods for rule-based inference systemsRO Duda, PE Hart, NJ Nilsson - Proceedings of the June 7-10, 1976, national computer …, 19761002회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Subjective Bayesian methods for rule-based inference systems",
        "year": null
    },
    "Model Design in the Prospector Consultant System for Mineral Exploration,": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1970",
            "게시자": "Edinburgh University Press, Edinburgh",
            "도서": "Expert Systems in the Micro Electronic Age",
            "저자": "J. Gaschnig and P. E. Hart Duda, R. O.",
            "전체 인용횟수": "855회 인용1984198519861987198819891990199119921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023415667606150432630323219238121015136107105513201231913149336108865",
            "페이지": "153-167",
            "학술 문서": "Model design in the PROSPECTOR consultant system for mineral exploration*R Duda, J Gaschnig, P Hart - Readings in Artificial Intelligence, 1981855회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Model Design in the Prospector Consultant System for Mineral Exploration,",
        "year": null
    },
    "Correction to \"A Formal Basis for the Heuristic Determination of Minimum Cost Paths\"": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1972/12/1",
            "게시자": "ACM",
            "설명": "Our paper on the use of heuristic information in graph searching defined a path-finding algorithm, A*, and proved that it had two important properties. In the notation of the paper, we proved that if the heuristic function ñ (n) is a lower bound on the true minimal cost from node n to a goal node, then A* is admissible; i.e., it would find a minimal cost path if any path to a goal node existed. Further, we proved that if the heuristic function also satisfied something called the consistency assumption, then A* was optimal; i.e., it expanded no more nodes than any other admissible algorithm A no more informed than A*. These results were summarized in a book by one of us.",
            "저널": "ACM SIGART Bulletin",
            "저자": "Peter E Hart, Nils J Nilsson, Bertram Raphael",
            "전체 인용횟수": "571회 인용1986198719881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202333566764455143469108333101627283345413220202024272521922",
            "페이지": "28-29",
            "학술 문서": "Correction to\" a formal basis for the heuristic determination of minimum cost paths\"PE Hart, NJ Nilsson, B Raphael - ACM SIGART Bulletin, 1972571회 인용 관련 학술자료 전체 12개의 버전 ",
            "호": "37"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Correction to \"A Formal Basis for the Heuristic Determination of Minimum Cost Paths\"",
        "year": null
    },
    "Pattern Classification. JohnWiley & Sons": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2001",
            "저널": "Inc.,",
            "저자": "Richard O Duda, Peter E Hart, David G Stork",
            "전체 인용횟수": "545회 인용20002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202333916223334253036401629292730342226202014911",
            "학술 문서": "Pattern Classification. JohnWiley & SonsRO Duda, PE Hart, DG Stork - Inc.,, 2001416회 인용 관련 학술자료 Pattern Classification, John Willey & SonsRO Duda, PE Hart, DG Stork - Inc., second edition edition, 2001129회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pattern Classification. JohnWiley & Sons",
        "year": null
    },
    "Image matching and retrieval by multi-access redundant hashing": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1995/11/7",
            "발명자": "Jonathan J Hull, Peter E Hart",
            "설명": "An improved document matching and retrieval system is disclosed where an input document is matched against a database of documents, using a descriptor database which lists descriptors and points to a list of documents containing features from which the descriptor is derived document. The descriptors are selected to be invariant to distortions caused by digitizing the documents or differences between the input document and its match in the document database. An array of accumulators is used to accumulate votes for each document in the document database as the descriptor base is scanned, wherein a vote is added to an accumulator for a document if the document is on the list as having a descriptor which is also found in the input document. The document which accumulates the most votes is returned as the matching document, or the documents with more than a threshold number of votes are returned.",
            "전체 인용횟수": "433회 인용1998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202391121187871611122223363642352221231816161172",
            "출원번호": "08222281",
            "특허 번호": "5465353",
            "특허청": "US",
            "학술 문서": "Image matching and retrieval by multi-access redundant hashingJJ Hull, PE Hart - US Patent 5,465,353, 1995433회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Image matching and retrieval by multi-access redundant hashing",
        "year": null
    },
    "Image database browsing and query using texture analysis": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1999/8/3",
            "발명자": "John Cullen, Jonathan Hull, Peter Hart",
            "설명": "Method and apparatus for querying a document image database based on texture, ie, analytically discernable patterns in the document images of the database. According to the invention, a document image database could be browsed for documents with a particular texture in a variety of ways. For example, a user could input an example document image with a similar appearance to the desired document. Alternatively, the user could employ a simple interface to define a synthetic document based on selection of a few categories. The synthetic document would then serve as an example for search. Or the user may employ a graphical interface to more precisely define an example for search. Thus, the user's knowledge of the general appearance of the desired document or documents provides the basis for the search.",
            "전체 인용횟수": "308회 인용2001200220032004200520062007200820092010201120122013201420152016201720182019202046751111816101692233314527217118",
            "출원번호": "08609641",
            "특허 번호": "5933823",
            "특허청": "US",
            "학술 문서": "Image database browsing and query using texture analysisJ Cullen, J Hull, P Hart - US Patent 5,933,823, 1999308회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Image database browsing and query using texture analysis",
        "year": null
    },
    "Triggering applications based on a captured text in a mixed media environment": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/3/2",
            "발명자": "Jonathan J Hull, Berna Erol, Jamey Graham, Peter E Hart, Dar-Shyang Lee, Kurt Piersol",
            "설명": "A Mixed Media Reality (MMR) system and associated techniques are disclosed. The MMR system provides mechanisms for forming a mixed media document that includes media of at least two types (eg, printed paper as a first medium and digital content and/or web link as a second medium). In one particular embodiment, the MMR system includes an action processor and method, and MMR documents with an associated action. The MMR document structure is particularly advantageous because the ability to specify different actions for different MMR documents, combined with the ability to create any number of MMR documents for a particular location on any media, allows the MMR architecture to serve as a universal trigger or initiator for additional processing. In other words, addition processing or actions can be triggered or initiated based on MMR recognition. The action processor receives the output of the MMR …",
            "전체 인용횟수": "282회 인용20092010201120122013201420152016201720182019202020212022202398192234343629161613101265",
            "출원번호": "11461032",
            "특허 번호": "7672543",
            "특허청": "US",
            "학술 문서": "Triggering applications based on a captured text in a mixed media environmentJJ Hull, B Erol, J Graham, PE Hart, DS Lee, K Piersol - US Patent 7,672,543, 2010282회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Triggering applications based on a captured text in a mixed media environment",
        "year": null
    },
    "Tokens Usable in Value-Based Transactions": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008/10/2",
            "발명자": "Peter E Hart, John W Barrus, Jamey Graham",
            "설명": "Techniques for generating a token that can be used to transfer value. The token may be used to transfer value in a value-based transaction with a vendor in a way that is secure and safe and maintains anonymity of the source of the value and preserves secrecy of information that should preferably not be disclosed to an untrusted third party such as a vendor. The token comprises sufficient information that enables value to be transferred from an account associated with the token to a vendor during a value-based transaction. Such a token may be presented by a user to a vendor in a value-based transaction with the purpose of transferring value involved in the transaction to the vendor in order to complete the transaction.",
            "전체 인용횟수": "268회 인용2012201320142015201620172018201920202021202220232465122031563549369",
            "출원번호": "11694076",
            "특허청": "US",
            "학술 문서": "Tokens Usable in Value-Based TransactionsPE Hart, JW Barrus, J Graham - US Patent App. 11/694,076, 2008268회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Tokens Usable in Value-Based Transactions",
        "year": null
    },
    "Automatic adaptive document help for paper documents": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2002/4/9",
            "발명자": "Jamey Graham, Peter E Hart",
            "설명": "A reader's annotation assistant application for documents in paper form is provided by virtue of the present invention. In certain embodiments, an elongated thumbnail image of all or part of an electronically stored document is imprinted on each page of the printed form of the document. Sections of the document of interest to the reader are emphasized in the elongated thumbnail image. The emphasized area in the elongated thumbnail image assists the user with the selection of sections or pages of the document having particular interest to the user. The operation of the assistant is personalized for a particular user by setting of a sensitivity level and selection of relevant topics of interest. Some embodiments of the assistant are also capable of improved performance over time by both automatic and manual feedback. The assistant is usable with many popular electronic document formats.",
            "전체 인용횟수": "229회 인용200220032004200520062007200820092010201120122013201420152016201720182019202020212022202315711141820333011111816137234221",
            "출원번호": "09149921",
            "특허 번호": "6369811",
            "특허청": "US",
            "학술 문서": "Automatic adaptive document help for paper documentsJ Graham, PE Hart - US Patent 6,369,811, 2002229회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Automatic adaptive document help for paper documents",
        "year": null
    },
    "How the Hough transform was invented [DSP History]": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/10/23",
            "게시자": "IEEE",
            "권": "26",
            "설명": "Where did this transform come from? You may vaguely recall learning that it goes back to a 1962 patent by P.V.C. Hough, though the author suspect very few readers have actually looked at that patent, the title page. If you do, you may be surprised to find that the popular transform used today is not described there. Indeed, today's transform was not a single-step invention but instead took several steps that resulted in Hough's initial idea being combined with an idea from an obscure branch of late 19th century mathematics to produce the familiar sinusoidal transform. The previously untold history of how this came about illustrates how important advances sometime come from combining not-obviously-related ideas. The history perhaps also illustrates that the observation of Louis Pasteur, \"Chance favors the prepared mind,\" remains as apt in the 20th and 21st centuries as it was in the 19th.",
            "저널": "IEEE Signal Processing Magazine",
            "저자": "Peter E Hart",
            "전체 인용횟수": "206회 인용2010201120122013201420152016201720182019202020212022202352112152219241216101015914",
            "페이지": "18-22",
            "학술 문서": "How the Hough transform was invented [DSP History]PE Hart - IEEE Signal Processing Magazine, 2009206회 인용 관련 학술자료 전체 6개의 버전 ",
            "호": "6"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "How the Hough transform was invented [DSP History]",
        "year": null
    },
    "Pattern classification 2nd edition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2001",
            "권": "35",
            "저널": "New York, USA: John Wiley&Sons",
            "저자": "Richard O Duda, Peter E Hart, David G Stork",
            "전체 인용횟수": "206회 인용200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220231393713101713151412111311551458662",
            "학술 문서": "Pattern classification 2nd editionRO Duda, PE Hart, DG Stork - New York, USA: John Wiley&Sons, 2001206회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pattern classification 2nd edition",
        "year": null
    },
    "Ssd: Single shot multibox detector": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "게시자": "Springer International Publishing",
            "설명": " We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component …",
            "저자": "Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C Berg",
            "전체 인용횟수": "35400회 인용2017201820192020202120222023782238444855647722579386585",
            "컨퍼런스": "Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part I 14",
            "페이지": "21-37",
            "학술 문서": "Ssd: Single shot multibox detectorW Liu, D Anguelov, D Erhan, C Szegedy, S Reed… - Computer Vision–ECCV 2016: 14th European …, 201634148회 인용 관련 학술자료 전체 34개의 버전 European conference on computer vision*W Liu, D Anguelov, D Erhan, C Szegedy, S Reed… - 20161518회 인용 관련 학술자료 SSD: Single shot multibox detector. arXiv 2015W Liu, D Anguelov, D Erhan, C Szegedy, S Reed… - arXiv preprint arXiv:1512.02325, 2020132회 인용 관련 학술자료 SSD: single shot multibox detector. CoRR abs/1512.02325 (2015)*W Liu, DE Dragomir Anguelov, C Szegedy, SE Reed… - arXiv preprint arXiv:1512.02325, 201559회 인용 관련 학술자료 SSD: Single shot multibox detector, European Conf*W Liu, D Anguelov, D Erhan, C Szegedy, S Reed… - Computer Vision (Springer, Cham, 2016)10회 인용 관련 학술자료 SSD: Single Shot MultiBox Detector.(dec 2015)*W Liu, DE Dragomir Anguelov, C Szegedy, S Reed… - arXiv preprint arXiv:1512.02325, 20158회 인용 관련 학술자료 SSD ARCHITECTURE*W Liu, D Anguelov, D Erhan, C Szegedy, S Reed… - image관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Ssd: Single shot multibox detector",
        "year": null
    },
    "Intriguing properties of neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/12/21",
            "설명": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
            "저널": "arXiv preprint arXiv:1312.6199",
            "저자": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus",
            "전체 인용횟수": "14704회 인용201520162017201820192020202120222023111210437111917312425282730772661",
            "학술 문서": "Intriguing properties of neural networksC Szegedy, W Zaremba, I Sutskever, J Bruna, D Erhan… - arXiv preprint arXiv:1312.6199, 201314704회 인용 관련 학술자료 전체 20개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Intriguing properties of neural networks",
        "year": null
    },
    "Show and tell: A neural image caption generator": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.",
            "저자": "Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan",
            "전체 인용횟수": "7010회 인용201520162017201820192020202120222023228493680920985960919957728",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "3156-3164",
            "학술 문서": "Show and tell: A neural image caption generatorO Vinyals, A Toshev, S Bengio, D Erhan - Proceedings of the IEEE conference on computer …, 20156982회 인용 관련 학술자료 전체 27개의 버전 Show and tell: A neural image caption generator. CoRR abs/1411.4555 (2014)*O Vinyals, A Toshev, S Bengio, D Erhan - arXiv preprint arXiv:1411.4555, 201425회 인용 관련 학술자료 Show and tell: A neural image caption generator. CoRR abs/1411.4555*O Vinyals, A Toshev, S Bengio, D Erhan - arXiv preprint arXiv:1411.4555, 20148회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Show and tell: A neural image caption generator",
        "year": null
    },
    "Why does unsupervised pre-training help deep learning?": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/3/31",
            "게시자": "JMLR Workshop and Conference Proceedings",
            "설명": "Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants with impressive results being obtained in several areas, mostly on vision and language datasets. The best results obtained on supervised learning tasks often involve an unsupervised learning component, usually in an unsupervised pre-training phase. The main question investigated here is the following: why does unsupervised pre-training work so well? Through extensive experimentation, we explore several possible explanations discussed in the literature including its action as a regularizer (Erhan et al. 2009) and as an aid to optimization (Bengio et al. 2007). Our results build on the work of Erhan et al. 2009, showing that unsupervised pre-training appears to play predominantly a regularization role in subsequent supervised training. However our results in an online setting, with a virtually unlimited data stream, point to a somewhat more nuanced interpretation of the roles of optimization and regularization in the unsupervised pre-training effect.",
            "저자": "Dumitru Erhan, Aaron Courville, Yoshua Bengio, Pascal Vincent",
            "전체 인용횟수": "3403회 인용2010201120122013201420152016201720182019202020212022202315336565159253339353414391384362295240",
            "컨퍼런스": "Proceedings of the thirteenth international conference on artificial intelligence and statistics",
            "페이지": "201-208",
            "학술 문서": "Why does unsupervised pre-training help deep learning?D Erhan, A Courville, Y Bengio, P Vincent - Proceedings of the thirteenth international conference …, 20103403회 인용 관련 학술자료 전체 41개의 버전 Why Does Unsupervised Pre-training Help Deep Discriminant Learning?*D Erhan, Y Bengio, A Courville, PA Manzagol…관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Why does unsupervised pre-training help deep learning?",
        "year": null
    },
    "Deep neural networks for object detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/12",
            "권": "26",
            "설명": "Deep Neural Networks (DNNs) have recently shown outstanding performance on the task of whole image classification. In this paper we go one step further and address the problem of object detection--not only classifying but also precisely localizing objects of various classes using DNNs. We present a simple and yet powerful formulation of object detection as a regression to object masks. We define a multi-scale inference procedure which is able to produce a high-resolution object detection at a low cost by a few network applications. The approach achieves state-of-the-art performance on Pascal 2007 VOC.",
            "저널": "Advances in neural information processing systems",
            "저자": "Christian Szegedy, Alexander Toshev, Dumitru Erhan",
            "전체 인용횟수": "1941회 인용2013201420152016201720182019202020212022202362988152189252264260260230168",
            "페이지": "1-9",
            "학술 문서": "Deep neural networks for object detectionC Szegedy, A Toshev, D Erhan - Advances in neural information processing systems, 20131909회 인용 관련 학술자료 전체 15개의 버전 Deep neural networks for object detection*S Christian, T Alexander, E Dumitru - Advances in neural information processing systems, 201345회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep neural networks for object detection",
        "year": null
    },
    "Challenges in representation learning: A report on three machine learning contests": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013",
            "게시자": "Springer berlin heidelberg",
            "설명": " The ICML 2013 Workshop on Challenges in Representation Learning focused on three challenges: the black box learning challenge, the facial expression recognition challenge, and the multimodal learning challenge. We describe the datasets created for these challenges and summarize the results of the competitions. We provide suggestions for organizers of future challenges and some comments on what kind of knowledge can be gained from machine learning competitions.",
            "저자": "Ian J Goodfellow, Dumitru Erhan, Pierre Luc Carrier, Aaron Courville, Mehdi Mirza, Ben Hamner, Will Cukierski, Yichuan Tang, David Thaler, Dong-Hyun Lee, Yingbo Zhou, Chetan Ramaiah, Fangxiang Feng, Ruifan Li, Xiaojie Wang, Dimitris Athanasakis, John Shawe-Taylor, Maxim Milakov, John Park, Radu Ionescu, Marius Popescu, Cristian Grozea, James Bergstra, Jingjing Xie, Lukasz Romaszko, Bing Xu, Zhang Chuang, Yoshua Bengio",
            "전체 인용횟수": "1731회 인용20142015201620172018201920202021202220231416356399180260322354366",
            "컨퍼런스": "Neural Information Processing: 20th International Conference, ICONIP 2013, Daegu, Korea, November 3-7, 2013. Proceedings, Part III 20",
            "페이지": "117-124",
            "학술 문서": "Challenges in representation learning: A report on three machine learning contestsIJ Goodfellow, D Erhan, PL Carrier, A Courville… - … : 20th International Conference, ICONIP 2013, Daegu …, 20131731회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Challenges in representation learning: A report on three machine learning contests",
        "year": null
    },
    "Unsupervised pixel-level domain adaptation with generative adversarial networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "Collecting well-annotated image datasets to train modern machine learning algorithms is prohibitively expensive for many tasks. One appealing alternative is rendering synthetic data where ground-truth annotations are generated automatically. Unfortunately, models trained purely on rendered images fail to generalize to real images. To address this shortcoming, prior work introduced unsupervised domain adaptation algorithms that have tried to either map representations between the two domains, or learn to extract features that are domain-invariant. In this work, we approach the problem in a new light by learning in an unsupervised manner a transformation in the pixel space from one domain to the other. Our generative adversarial network (GAN)-based method adapts source-domain images to appear as if drawn from the target domain. Our approach not only produces plausible samples, but also outperforms the state-of-the-art on a number of unsupervised domain adaptation scenarios by large margins. Finally, we demonstrate that the adaptation process generalizes to object classes unseen during training.",
            "저자": "Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, Dilip Krishnan",
            "전체 인용횟수": "1727회 인용201720182019202020212022202345200304366358254192",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "3722-3731",
            "학술 문서": "Unsupervised pixel-level domain adaptation with generative adversarial networksK Bousmalis, N Silberman, D Dohan, D Erhan… - Proceedings of the IEEE conference on computer …, 20171727회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Unsupervised pixel-level domain adaptation with generative adversarial networks",
        "year": null
    },
    "Visualizing higher-layer features of a deep network": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/6/9",
            "권": "1341",
            "설명": "Deep architectures have demonstrated state-of-the-art results in a variety of settings, especially with vision datasets. Beyond the model definitions and the quantitative analyses, there is a need for qualitative comparisons of the solutions learned by various deep architectures. The goal of this paper is to find good qualitative interpretations of high level features represented by such models. To this end, we contrast and compare several techniques applied on Stacked Denoising Autoencoders and Deep Belief Networks, trained on several vision datasets. We show that, perhaps counter-intuitively, such interpretation is possible at the unit level, that it is simple to accomplish and that the results are consistent across various techniques. We hope that such techniques will allow researchers in deep architectures to understand more of how and why deep architectures work.",
            "저널": "University of Montreal",
            "저자": "Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pascal Vincent",
            "전체 인용횟수": "1574회 인용2013201420152016201720182019202020212022202312124086100166216245284228162",
            "페이지": "1",
            "학술 문서": "Visualizing higher-layer features of a deep networkD Erhan, Y Bengio, A Courville, P Vincent - University of Montreal, 20091574회 인용 관련 학술자료 전체 2개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Visualizing higher-layer features of a deep network",
        "year": null
    },
    "Domain separation networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "권": "29",
            "설명": "The cost of large scale data collection and annotation often makes the application of machine learning algorithms to new tasks or datasets prohibitively expensive. One approach circumventing this cost is training models on synthetic data where annotations are provided automatically. Despite their appeal, such models often fail to generalize from synthetic to real images, necessitating domain adaptation algorithms to manipulate these models before they can be successfully applied. Existing approaches focus either on mapping representations from one domain to the other, or on learning to extract features that are invariant to the domain from which they were extracted. However, by focusing only on creating a mapping or shared representation between the two domains, they ignore the individual characteristics of each domain. We hypothesize that explicitly modeling what is unique to each domain can improve a model's ability to extract domain-invariant features. Inspired by work on private-shared component analysis, we explicitly learn to extract image representations that are partitioned into two subspaces: one component which is private to each domain and one which is shared across domains. Our model is trained to not only perform the task we care about in the source domain, but also to use the partitioned representation to reconstruct the images from both domains. Our novel architecture results in a model that outperforms the state-of-the-art on a range of unsupervised domain adaptation scenarios and additionally produces visualizations of the private and shared representations enabling interpretation of the domain adaptation process.",
            "저널": "Advances in neural information processing systems",
            "저자": "Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, Dumitru Erhan",
            "전체 인용횟수": "1497회 인용20162017201820192020202120222023660162211283272246250",
            "학술 문서": "Domain separation networksK Bousmalis, G Trigeorgis, N Silberman, D Krishnan… - Advances in neural information processing systems, 20161497회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Domain separation networks",
        "year": null
    },
    "Scalable object detection using deep neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "설명": "Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.",
            "저자": "Dumitru Erhan, Christian Szegedy, Alexander Toshev, Dragomir Anguelov",
            "전체 인용횟수": "1497회 인용20142015201620172018201920202021202220231376135142195253183174184103",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "2147-2154",
            "학술 문서": "Scalable object detection using deep neural networksD Erhan, C Szegedy, A Toshev, D Anguelov - Proceedings of the IEEE conference on computer …, 20141497회 인용 관련 학술자료 전체 21개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Scalable object detection using deep neural networks",
        "year": null
    },
    "An empirical evaluation of deep architectures on problems with many factors of variation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2007/6/20",
            "도서": "Proceedings of the 24th international conference on Machine learning",
            "설명": "Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.",
            "저자": "Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, Yoshua Bengio",
            "전체 인용횟수": "1353회 인용200720082009201020112012201320142015201620172018201920202021202220234172636433948679397119123147146110114109",
            "페이지": "473-480",
            "학술 문서": "An empirical evaluation of deep architectures on problems with many factors of variationH Larochelle, D Erhan, A Courville, J Bergstra… - Proceedings of the 24th international conference on …, 20071353회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "An empirical evaluation of deep architectures on problems with many factors of variation",
        "year": null
    },
    "Theano: A Python framework for fast computation of mathematical expressions": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/5",
            "설명": "Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers-especially in the machine learning community-and has shown steady performance improvements. Theano is being actively and continuously developed since 2008, multiple frameworks have been built on top of it and it has been used to produce many state-of-the-art machine learning models. The present article is structured as follows. Section I provides an overview of the Theano software and its community. Section II presents the principal features of Theano and how to use them, and compares them with other similar projects. Section III focuses on recently-introduced functionalities and improvements. Section IV compares the performance of Theano against Torch7 and …",
            "저널": "arXiv e-prints",
            "저자": "Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Frédéric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, Yoshua Bengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, Nicolas Bouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Brébisson, Olivier Breuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooijmans, Marc-Alexandre Côté, Myriam Côté, Aaron Courville, Yann N Dauphin, Olivier Delalleau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, Mélanie Ducoffe, Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, Mathieu Germain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, Iban Harlouchet, Jean-Philippe Heng, Balázs Hidasi, Sina Honari, Arjun Jain, Sébastien Jean, Kai Jia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, César Laurent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas Léonard, Zhouhan Lin, Jesse A Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropietro, Robert T McGibbon, Roland Memisevic, Bart van Merriënboer, Vincent Michalski, Mehdi Mirza, Alberto Orlandi, Christopher Pal, Razvan Pascanu, Mohammad Pezeshki, Colin Raffel, Daniel Renshaw, Matthew Rocklin, Adriana Romero, Markus Roth, Peter Sadowski, John Salvatier, François Savard, Jan Schlüter, John Schulman, Gabriel Schwartz, Iulian Vlad Serban, Dmitriy Serdyuk, Samira Shabanian, Étienne Simon, Sigurd Spieckermann, S Ramana Subramanyam, Jakub Sygnowski, Jérémie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban, Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J Webb, Matthew Willson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, Ying Zhang",
            "전체 인용횟수": "1075회 인용20162017201820192020202120222023462032322041461108142",
            "페이지": "arXiv: 1605.02688",
            "학술 문서": "Theano: A Python framework for fast computation of mathematical expressionsR Al-Rfou, G Alain, A Almahairi, C Angermueller… - arXiv e-prints, 2016916회 인용 관련 학술자료 Theano: A Python framework for fast computation of mathematical expressions*TTD Team, R Al-Rfou, G Alain, A Almahairi… - arXiv preprint arXiv:1605.02688, 2016199회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Theano: A Python framework for fast computation of mathematical expressions",
        "year": null
    },
    "Training deep neural networks on noisy labels with bootstrapping": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/12/20",
            "설명": "Current state-of-the-art deep learning systems for visual object recognition and detection use purely supervised training with regularization such as dropout to avoid overfitting. The performance depends critically on the amount of labeled examples, and in current practice the labels are assumed to be unambiguous and accurate. However, this assumption often does not hold; e.g. in recognition, class labels may be missing; in detection, objects in the image may not be localized; and in general, the labeling may be subjective. In this work we propose a generic way to handle noisy and incomplete labeling by augmenting the prediction objective with a notion of consistency. We consider a prediction consistent if the same prediction is made given similar percepts, where the notion of similarity is between deep network features computed from the input data. In experiments we demonstrate that our approach yields substantial robustness to label noise on several datasets. On MNIST handwritten digits, we show that our model is robust to label corruption. On the Toronto Face Database, we show that our model handles well the case of subjective labels in emotion recognition, achieving state-of-the- art results, and can also benefit from unlabeled face images with no modification to our method. On the ILSVRC2014 detection challenge data, we show that our approach extends to very deep networks, high resolution images and structured outputs, and results in improved scalable detection.",
            "저널": "arXiv preprint arXiv:1412.6596",
            "저자": "Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, Andrew Rabinovich",
            "전체 인용횟수": "1072회 인용2015201620172018201920202021202220239284480142174227198159",
            "학술 문서": "Training deep neural networks on noisy labels with bootstrappingS Reed, H Lee, D Anguelov, C Szegedy, D Erhan… - arXiv preprint arXiv:1412.6596, 20141060회 인용 관련 학술자료 전체 11개의 버전 Training deep neural networks on noisy labels with bootstrapping. arXivS Reed, H Lee, D Anguelov, C Szegedy, D Erhan… - arXiv preprint arXiv:1412.6596, 201426회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Training deep neural networks on noisy labels with bootstrapping",
        "year": null
    },
    "Show and tell: Lessons learned from the 2015 mscoco image captioning challenge": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/7/7",
            "게시자": "IEEE",
            "권": "39",
            "설명": "Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. Finally, given the recent surge of interest in this task, a competition was organized in 2015 using the newly released COCO dataset. We describe and analyze the various improvements we …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan",
            "전체 인용횟수": "999회 인용20162017201820192020202120222023587143169173137143125",
            "페이지": "652-663",
            "학술 문서": "Show and tell: Lessons learned from the 2015 mscoco image captioning challengeO Vinyals, A Toshev, S Bengio, D Erhan - IEEE transactions on pattern analysis and machine …, 2016999회 인용 관련 학술자료 전체 19개의 버전 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Show and tell: Lessons learned from the 2015 mscoco image captioning challenge",
        "year": null
    },
    "Model-based reinforcement learning for atari": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/3/1",
            "설명": "Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. However, this typically requires very large amounts of interaction -- substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the answer may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, we explore how video prediction models can similarly enable agents to solve Atari games with fewer interactions than model-free methods. We describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present a comparison of several model architectures, including a novel architecture that yields the best results in our setting. Our experiments evaluate SimPLe on a range of Atari games in low data regime of 100k interactions between the agent and the environment, which corresponds to two hours of real-time play. In most games SimPLe outperforms state-of-the-art model-free algorithms, in some games by over an order of magnitude.",
            "저널": "arXiv preprint arXiv:1903.00374",
            "저자": "Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin, Ryan Sepassi, George Tucker, Henryk Michalewski",
            "전체 인용횟수": "783회 인용2019202020212022202360137199212169",
            "학술 문서": "Model-based reinforcement learning for atariL Kaiser, M Babaeizadeh, P Milos, B Osinski… - arXiv preprint arXiv:1903.00374, 2019782회 인용 관련 학술자료 전체 6개의 버전 Model-based reinforcement learning for atari*B Osinski, C Finn, D Erhan, G Tucker, H Michalewski… - 20201회 인용 관련 학술자료 Model Based Reinforcement Learning for Atari*RH Campbell, K Czechowski, D Erhan, C Finn… - Proceedings of the International Conference on …, 20191회 인용 관련 학술자료 전체 4개의 버전 Model Based Reinforcement Learning for Atari*K Czechowski, D Erhan, C Finn, P Kozakowski…관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Model-based reinforcement learning for atari",
        "year": null
    },
    "The (un) reliability of saliency methods": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "게시자": "Springer International Publishing",
            "설명": " Saliency methods aim to explain the predictions of deep neural networks. These methods lack reliability when the explanation is sensitive to factors that do not contribute to the model prediction. We use a simple and common pre-processing step which can be compensated for easily—adding a constant shift to the input data—to show that a transformation with no effect on how the model makes the decision can cause numerous methods to attribute incorrectly. In order to guarantee reliability, we believe that the explanation should not change when we can guarantee that two networks process the images in identical manners. We show, through several examples, that saliency methods that do not satisfy this requirement result in misleading attribution. The approach can be seen as a type of unit test; we construct a narrow ground truth to measure one stated desirable property. As such, we hope the community will …",
            "저널": "Explainable AI: Interpreting, explaining and visualizing deep learning",
            "저자": "Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T Schütt, Sven Dähne, Dumitru Erhan, Been Kim",
            "전체 인용횟수": "621회 인용2018201920202021202220233857101114162144",
            "페이지": "267-280",
            "학술 문서": "The (un) reliability of saliency methodsPJ Kindermans, S Hooker, J Adebayo, M Alber… - Explainable AI: Interpreting, explaining and visualizing …, 2019621회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The (un) reliability of saliency methods",
        "year": null
    },
    "The difficulty of training deep architectures and the effect of unsupervised pre-training": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/4/15",
            "게시자": "PMLR",
            "설명": "Whereas theoretical work suggests that deep architectures might be more efficient at representing highly-varying functions, training deep architectures was unsuccessful until the recent advent of algorithms based on unsupervised pre-training. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. Answering these questions is important if learning in deep architectures is to be further improved. We attempt to shed some light on these questions through extensive simulations. The experiments confirm and clarify the advantage of unsupervised pre-training. They demonstrate the robustness of the training procedure with respect to the random initialization, the positive effect of pre-training in terms of optimization and its role as a kind of regularizer. We show the influence of architecture depth, model capacity, and number of training examples.",
            "저자": "Dumitru Erhan, Pierre-Antoine Manzagol, Yoshua Bengio, Samy Bengio, Pascal Vincent",
            "전체 인용횟수": "616회 인용20092010201120122013201420152016201720182019202020212022202342210191839615564636548515338",
            "컨퍼런스": "Artificial intelligence and statistics",
            "페이지": "153-160",
            "학술 문서": "The difficulty of training deep architectures and the effect of unsupervised pre-trainingD Erhan, PA Manzagol, Y Bengio, S Bengio, P Vincent - Artificial intelligence and statistics, 2009616회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The difficulty of training deep architectures and the effect of unsupervised pre-training",
        "year": null
    },
    "Zero-data learning of new tasks.": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008/7/13",
            "권": "1",
            "설명": "We introduce the problem of zero-data learning, where a model must generalize to classes or tasks for which no training data are available and only a description of the classes or tasks are provided. Zero-data learning is useful for problems where the set of classes to distinguish or tasks to solve is very large and is not entirely covered by the training data. The main contributions of this work lie in the presentation of a general formalization of zero-data learning, in an experimental analysis of its properties and in empirical evidence showing that generalization is possible and significant in this context. The experimental work of this paper addresses two classification problems of character recognition and a multitask ranking problem in the context of drug discovery. Finally, we conclude by discussing how this new framework could lead to a novel perspective on how to extend machine learning towards AI, where an agent can be given a specification for a learning problem before attempting to solve it (with very few or even zero examples).",
            "저널": "AAAI",
            "저자": "Hugo Larochelle, Dumitru Erhan, Yoshua Bengio",
            "전체 인용횟수": "595회 인용200920102011201220132014201520162017201820192020202120222023349111710302951346360899386",
            "페이지": "3",
            "학술 문서": "Zero-data learning of new tasks.H Larochelle, D Erhan, Y Bengio - AAAI, 2008595회 인용 관련 학술자료 전체 12개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Zero-data learning of new tasks.",
        "year": null
    },
    "A benchmark for interpretability methods in deep neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "권": "32",
            "설명": "We propose an empirical measure of the approximate accuracy of feature importance estimates in deep neural networks. Our results across several large-scale image classification datasets show that many popular interpretability methods produce estimates of feature importance that are not better than a random designation of feature importance. Only certain ensemble based approaches---VarGrad and SmoothGrad-Squared---outperform such a random assignment of importance. The manner of ensembling remains critical, we show that some approaches do no better then the underlying method but carry a far higher computational burden.",
            "저널": "Advances in neural information processing systems",
            "저자": "Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, Been Kim",
            "전체 인용횟수": "561회 인용20192020202120222023480115180179",
            "학술 문서": "A benchmark for interpretability methods in deep neural networksS Hooker, D Erhan, PJ Kindermans, B Kim - Advances in neural information processing systems, 2019561회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A benchmark for interpretability methods in deep neural networks",
        "year": null
    },
    "U-net: Convolutional networks for biomedical image segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "게시자": "Springer International Publishing",
            "설명": " There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image …",
            "저자": "Olaf Ronneberger, Philipp Fischer, Thomas Brox",
            "전체 인용횟수": "74112회 인용20172018201920202021202220238483210706010772149881826318095",
            "컨퍼런스": "Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18",
            "페이지": "234-241",
            "학술 문서": "U-net: Convolutional networks for biomedical image segmentationO Ronneberger, P Fischer, T Brox - Medical Image Computing and Computer-Assisted …, 201574109회 인용 관련 학술자료 전체 31개의 버전 May 2015, U-Net: Convolutional networks for biomedical image segmentation*O Ronneberger, P Fischer, T Brox - arXiv preprint arXiv:1505.04597, 184회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "U-net: Convolutional networks for biomedical image segmentation",
        "year": null
    },
    "3D U-Net: learning dense volumetric segmentation from sparse annotation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "게시자": "Springer International Publishing",
            "설명": " This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup, the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup, we assume that a representative, sparsely annotated training set exists. Trained on this data set, the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch, i.e., no pre-trained network is required. We test the performance of the proposed method …",
            "저자": "Özgün Çiçek, Ahmed Abdulkadir, Soeren S Lienkamp, Thomas Brox, Olaf Ronneberger",
            "전체 인용횟수": "6260회 인용2017201820192020202120222023105376691944135813891318",
            "컨퍼런스": "Medical Image Computing and Computer-Assisted Intervention–MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part II 19",
            "페이지": "424-432",
            "학술 문서": "3D U-Net: learning dense volumetric segmentation from sparse annotationÖ Çiçek, A Abdulkadir, SS Lienkamp, T Brox… - Medical Image Computing and Computer-Assisted …, 20166260회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "3D U-Net: learning dense volumetric segmentation from sparse annotation",
        "year": null
    },
    "Striving for simplicity: The all convolutional net": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/12/21",
            "설명": "Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the \"deconvolution approach\" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.",
            "저널": "arXiv preprint arXiv:1412.6806",
            "저자": "Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, Martin Riedmiller",
            "전체 인용횟수": "5297회 인용20152016201720182019202020212022202327181341600747843932861700",
            "학술 문서": "Striving for simplicity: The all convolutional netJT Springenberg, A Dosovitskiy, T Brox, M Riedmiller - arXiv preprint arXiv:1412.6806, 20145297회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Striving for simplicity: The all convolutional net",
        "year": null
    },
    "Flownet: Learning optical flow with convolutional networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/12/7",
            "게시자": "IEEE",
            "설명": "Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks CNNs succeeded at. In this paper we construct CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a large synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.",
            "저자": "Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Caner Hazirbas, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers, Thomas Brox",
            "전체 인용횟수": "4531회 인용2015201620172018201920202021202220231695292453620739822798643",
            "컨퍼런스": "2015 IEEE International Conference on Computer Vision (ICCV)",
            "페이지": "2758-2766",
            "학술 문서": "Flownet: Learning optical flow with convolutional networksA Dosovitskiy, P Fischer, E Ilg, P Hausser, C Hazirbas… - Proceedings of the IEEE international conference on …, 20153906회 인용 관련 학술자료 전체 15개의 버전 Flownet: Learning optical flow with convolutional networks*P Fischer, A Dosovitskiy, E Ilg, P Häusser, C Hazırbaş… - arXiv preprint arXiv:1504.06852, 2015680회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Flownet: Learning optical flow with convolutional networks",
        "year": null
    },
    "High accuracy optical flow estimation based on a theory for warping": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004",
            "게시자": "Springer Berlin Heidelberg",
            "설명": " We study an energy functional for computing optical flow that combines three assumptions: a brightness constancy assumption, a gradient constancy assumption, and a discontinuity-preserving spatio-temporal smoothness constraint. In order to allow for large displacements, linearisations in the two data terms are strictly avoided. We present a consistent numerical scheme based on two nested fixed point iterations. By proving that this scheme implements a coarse-to-fine warping strategy, we give a theoretical foundation for warping which has been used on a mainly experimental basis so far. Our evaluation demonstrates that the novel method gives significantly smaller angular errors than previous techniques for optical flow estimation. We show that it is fairly insensitive to parameter variations, and we demonstrate its excellent robustness under noise.",
            "저자": "Thomas Brox, Andrés Bruhn, Nils Papenberg, Joachim Weickert",
            "전체 인용횟수": "3659회 인용200420052006200720082009201020112012201320142015201620172018201920202021202220232142708295119141202208234233264286277304255237220173145",
            "컨퍼런스": "Computer Vision-ECCV 2004: 8th European Conference on Computer Vision, Prague, Czech Republic, May 11-14, 2004. Proceedings, Part IV 8",
            "페이지": "25-36",
            "학술 문서": "High accuracy optical flow estimation based on a theory for warpingT Brox, A Bruhn, N Papenberg, J Weickert - Computer Vision-ECCV 2004: 8th European …, 20043639회 인용 관련 학술자료 전체 18개의 버전 Computer Vision-ECCV 2004*T Brox, A Bruhn, N Papenberg, J Weickert - Lecture Notes in Computer Science, 200437회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "High accuracy optical flow estimation based on a theory for warping",
        "year": null
    },
    "Flownet 2.0: Evolution of optical flow estimation with deep networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "The FlowNet demonstrated that optical flow estimation can be cast as a learning problem. However, the state of the art with regard to the quality of the flow has still been defined by traditional methods. Particularly on small displacements and real-world data, FlowNet cannot compete with variational methods. In this paper, we advance the concept of end-to-end learning of optical flow and make it work really well. The large improvements in quality and speed are caused by three major contributions: first, we focus on the training data and show that the schedule of presenting data during training is very important. Second, we develop a stacked architecture that includes warping of the second image with intermediate optical flow. Third, we elaborate on small displacements by introducing a subnetwork specializing on small motions. FlowNet 2.0 is only marginally slower than the original FlowNet but decreases the estimation error by more than 50%. It performs on par with state-of-the-art methods, while running at interactive frame rates. Moreover, we present faster variants that allow optical flow computation at up to 140fps with accuracy matching the original FlowNet.",
            "저자": "Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, Thomas Brox",
            "전체 인용횟수": "3317회 인용201720182019202020212022202363326504620645604526",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "2462-2470",
            "학술 문서": "Flownet 2.0: Evolution of optical flow estimation with deep networksE Ilg, N Mayer, T Saikia, M Keuper, A Dosovitskiy… - Proceedings of the IEEE conference on computer …, 20173317회 인용 관련 학술자료 전체 18개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Flownet 2.0: Evolution of optical flow estimation with deep networks",
        "year": null
    },
    "A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "Recent work has shown that optical flow estimation can be formulated as a supervised learning task and can be successfully solved with convolutional networks. Training of the so-called FlowNet was enabled by a large synthetically generated dataset. The present paper extends the concept of optical flow estimation via convolutional networks to disparity and scene flow estimation. To this end, we propose three synthetic stereo video datasets with sufficient realism, variation, and size to successfully train large networks. Our datasets are the first large-scale datasets to enable training and evaluation of scene flow methods. Besides the datasets, we present a convolutional network for real-time disparity estimation that provides state-of-the-art results. By combining a flow and disparity estimation network and training it jointly, we demonstrate the first scene flow estimation with a convolutional network.",
            "저자": "Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, Thomas Brox",
            "전체 인용횟수": "2576회 인용2016201720182019202020212022202322126228348371492527443",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "4040-4048",
            "학술 문서": "A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimationN Mayer, E Ilg, P Hausser, P Fischer, D Cremers… - Proceedings of the IEEE conference on computer …, 20162576회 인용 관련 학술자료 전체 21개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation",
        "year": null
    },
    "Discriminative unsupervised feature learning with convolutional neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "권": "27",
            "설명": "Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training. In this paper we present an approach for training a convolutional neural network using only unlabeled data. We train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled'seed'image patch. We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition. The feature representation learned by our algorithm achieves classification results matching or outperforming the current state-of-the-art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101).",
            "저널": "Advances in neural information processing systems",
            "저자": "Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, Thomas Brox",
            "전체 인용횟수": "1829회 인용201420152016201720182019202020212022202363076102135150300368393242",
            "학술 문서": "Discriminative unsupervised feature learning with convolutional neural networksA Dosovitskiy, JT Springenberg, M Riedmiller, T Brox - Advances in neural information processing systems, 20141771회 인용 관련 학술자료 전체 21개의 버전 Unsupervised feature learning by augmenting single images*A Dosovitskiy, JT Springenberg, T Brox - arXiv preprint arXiv:1312.5242, 201362회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Discriminative unsupervised feature learning with convolutional neural networks",
        "year": null
    },
    "U-Net: deep learning for cell counting, detection, and morphometry": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/1",
            "게시자": "Nature Publishing Group US",
            "권": "16",
            "설명": "U-Net is a generic deep-learning solution for frequently occurring quantification tasks such as cell detection and shape measurements in biomedical image data. We present an ImageJ plugin that enables non-machine-learning experts to analyze their data with U-Net on either a local computer or a remote server/cloud service. The plugin comes with pretrained models for single-cell segmentation and allows for U-Net to be adapted to new tasks on the basis of a few annotated samples.",
            "저널": "Nature methods",
            "저자": "Thorsten Falk, Dominic Mai, Robert Bensch, Özgün Çiçek, Ahmed Abdulkadir, Yassine Marrakchi, Anton Böhm, Jan Deubner, Zoe Jäckel, Katharina Seiwald, Alexander Dovzhenko, Olaf Tietz, Cristina Dal Bosco, Sean Walsh, Deniz Saltukoglu, Tuan Leng Tay, Marco Prinz, Klaus Palme, Matias Simons, Ilka Diester, Thomas Brox, Olaf Ronneberger",
            "전체 인용횟수": "1443회 인용2018201920202021202220234101262361353345",
            "페이지": "67-70",
            "학술 문서": "U-Net: deep learning for cell counting, detection, and morphometryT Falk, D Mai, R Bensch, Ö Çiçek, A Abdulkadir… - Nature methods, 20191428회 인용 관련 학술자료 전체 7개의 버전 Author Correction: U-Net: deep learning for cell counting, detection, and morphometry*T Falk, D Mai, R Bensch, Ö Çiçek, A Abdulkadir… - Nature Methods, 201920회 인용 관련 학술자료 전체 4개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "U-Net: deep learning for cell counting, detection, and morphometry",
        "year": null
    },
    "Generating images with perceptual similarity metrics based on deep networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "권": "29",
            "설명": "We propose a class of loss functions, which we call deep perceptual similarity metrics (DeePSiM), allowing to generate sharp high resolution images from compressed abstract representations. Instead of computing distances in the image space, we compute distances between image features extracted by deep neural networks. This metric reflects perceptual similarity of images much better and, thus, leads to better results. We demonstrate two examples of use cases of the proposed loss:(1) networks that invert the AlexNet convolutional network;(2) a modified version of a variational autoencoder that generates realistic high-resolution random images.",
            "저널": "Advances in neural information processing systems",
            "저자": "Alexey Dosovitskiy, Thomas Brox",
            "전체 인용횟수": "1221회 인용201620172018201920202021202220232274171210230196169128",
            "학술 문서": "Generating images with perceptual similarity metrics based on deep networksA Dosovitskiy, T Brox - Advances in neural information processing systems, 20161221회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Generating images with perceptual similarity metrics based on deep networks",
        "year": null
    },
    "Learning to generate chairs with convolutional neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "We train a generative convolutional neural network which is able to generate images of objects given object type, viewpoint, and color. We train the network in a supervised manner on a dataset of rendered 3D chair models. Our experiments show that the network does not merely learn all images by heart, but rather finds a meaningful representation of a 3D chair model allowing it to assess the similarity of different chairs, interpolate between given viewpoints to generate the missing ones, or invent new chair styles by interpolating between chairs from the training set. We show that the network can be used to find correspondences between different chairs from the dataset, outperforming existing approaches on this task.",
            "저자": "Alexey Dosovitskiy, Jost Tobias Springenberg, Thomas Brox",
            "전체 인용횟수": "1016회 인용20152016201720182019202020212022202337116151184155131817257",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "1538-1546",
            "학술 문서": "Learning to generate chairs with convolutional neural networksA Dosovitskiy, J Tobias Springenberg, T Brox - Proceedings of the IEEE conference on computer …, 20151016회 인용 관련 학술자료 전체 25개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning to generate chairs with convolutional neural networks",
        "year": null
    },
    "Object segmentation by long term analysis of point trajectories": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/9/5",
            "게시자": "Springer Berlin Heidelberg",
            "도서": "European conference on computer vision",
            "설명": " Unsupervised learning requires a grouping step that defines which data belong together. A natural way of grouping in images is the segmentation of objects or parts of objects. While pure bottom-up segmentation from static cues is well known to be ambiguous at the object level, the story changes as soon as objects move. In this paper, we present a method that uses long term point trajectories based on dense optical flow. Defining pair-wise distances between these trajectories allows to cluster them, which results in temporally consistent segmentations of moving objects in a video shot. In contrast to multi-body factorization, points and even whole objects may appear or disappear during the shot. We provide a benchmark dataset and an evaluation method for this so far uncovered setting.",
            "저자": "Thomas Brox, Jitendra Malik",
            "전체 인용횟수": "1000회 인용2009201020112012201320142015201620172018201920202021202220233326527692961151211047669604336",
            "페이지": "282-295",
            "학술 문서": "Object segmentation by long term analysis of point trajectoriesT Brox, J Malik - European conference on computer vision, 20101000회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Object segmentation by long term analysis of point trajectories",
        "year": null
    },
    "Sparsity invariant cnns": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/10/10",
            "게시자": "IEEE",
            "설명": "In this paper, we consider convolutional neural networks operating on sparse inputs with an application to depth completion from sparse laser scan data. First, we show that traditional convolutional networks perform poorly when applied to sparse data even when the location of missing data is provided to the network. To overcome this problem, we propose a simple yet effective sparse convolution layer which explicitly considers the location of missing data during the convolution operation. We demonstrate the benefits of the proposed network architecture in synthetic and real experiments with respect to various baseline approaches. Compared to dense baselines, the proposed sparse convolution network generalizes well to novel datasets and is invariant to the level of sparsity in the data. For our evaluation, we derive a novel dataset from the KITTI benchmark, comprising over 94k depth annotated RGB images. Our …",
            "저자": "Jonas Uhrig, Nick Schneider, Lukas Schneider, Uwe Franke, Thomas Brox, Andreas Geiger",
            "전체 인용횟수": "824회 인용20182019202020212022202341107122185192173",
            "컨퍼런스": "2017 international conference on 3D Vision (3DV)",
            "페이지": "11-20",
            "학술 문서": "Sparsity invariant cnnsJ Uhrig, N Schneider, L Schneider, U Franke, T Brox… - 2017 international conference on 3D Vision (3DV), 2017824회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Sparsity invariant cnns",
        "year": null
    },
    "Inverting visual representations with convolutional networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "Feature representations, both hand-designed and learned ones, are often hard to analyze and interpret, even when they are extracted from visual data. We propose a new approach to study image representations by inverting them with an up-convolutional neural network. We apply the method to shallow representations (HOG, SIFT, LBP), as well as to deep networks. For shallow representations our approach provides significantly better reconstructions than existing methods, revealing that there is surprisingly rich information contained in these features. Inverting a deep network trained on ImageNet provides several insights into the properties of the feature representation learned by the network. Most strikingly, the colors and the rough contours of an image can be reconstructed from activations in higher network layers and even from the predicted class probabilities.",
            "저자": "Alexey Dosovitskiy, Thomas Brox",
            "전체 인용횟수": "796회 인용20152016201720182019202020212022202311636610210512213810375",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "4829-4837",
            "학술 문서": "Inverting visual representations with convolutional networksA Dosovitskiy, T Brox - Proceedings of the IEEE conference on computer …, 2016681회 인용 관련 학술자료 전체 12개의 버전 Inverting convolutional networks with convolutional networks*A Dosovitskiy, T Brox - arXiv preprint arXiv:1506.02753, 2015124회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Inverting visual representations with convolutional networks",
        "year": null
    },
    "Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "We present a deep convolutional decoder architecture that can generate volumetric 3D outputs in a compute-and memory-efficient manner by using an octree representation. The network learns to predict both the structure of the octree, and the occupancy values of individual cells. This makes it a particularly valuable technique for generating 3D shapes. In contrast to standard decoders acting on regular voxel grids, the architecture does not have cubic complexity. This allows representing much higher resolution outputs with a limited memory budget. We demonstrate this in several application domains, including 3D convolutional autoencoders, generation of objects and whole scenes from high-level representations, and shape from a single image.",
            "저자": "Maxim Tatarchenko, Alexey Dosovitskiy, Thomas Brox",
            "전체 인용횟수": "788회 인용20172018201920202021202220231472119166174121114",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "2088-2096",
            "학술 문서": "Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputsM Tatarchenko, A Dosovitskiy, T Brox - Proceedings of the IEEE international conference on …, 2017788회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs",
        "year": null
    },
    "Learning to estimate 3d hand pose from single rgb images": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "Low-cost consumer depth cameras and deep learning have enabled reasonable 3D hand pose estimation from single depth images. In this paper, we present an approach that estimates 3D hand pose from regular RGB images. This task has far more ambiguities due to the missing depth information. To this end, we propose a deep network that learns a network-implicit 3D articulation prior. Together with detected keypoints in the images, this network yields good estimates of the 3D pose. We introduce a large scale 3D hand pose dataset based on synthetic hand models for training the involved networks. Experiments on a variety of test sets, including one on sign language recognition, demonstrate the feasibility of 3D hand pose estimation on single color images.",
            "저자": "Christian Zimmermann, Thomas Brox",
            "전체 인용횟수": "746회 인용2017201820192020202120222023762101138161147124",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "4903-4911",
            "학술 문서": "Learning to estimate 3d hand pose from single rgb imagesC Zimmermann, T Brox - Proceedings of the IEEE international conference on …, 2017746회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning to estimate 3d hand pose from single rgb images",
        "year": null
    },
    "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "권": "29",
            "설명": "Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right---similar to why we study the human brain---and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization, which synthesizes an input (eg an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network. The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real,(2) reveals the features learned by each neuron in an interpretable way,(3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).",
            "저널": "Advances in neural information processing systems",
            "저자": "Anh Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, Jeff Clune",
            "전체 인용횟수": "746회 인용20152016201720182019202020212022202329569999113130126104",
            "학술 문서": "Synthesizing the preferred inputs for neurons in neural networks via deep generator networksA Nguyen, A Dosovitskiy, J Yosinski, T Brox, J Clune - Advances in neural information processing systems, 2016746회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks",
        "year": null
    },
    "Demon: Depth and motion network for learning monocular stereo": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "In this paper we formulate structure from motion as a learning problem. We train a convolutional network end-to-end to compute depth and camera motion from successive, unconstrained image pairs. The architecture is composed of multiple stacked encoder-decoder networks, the core part being an iterative network that is able to improve its own predictions. The network estimates not only depth and motion, but additionally surface normals, optical flow between the images and confidence of the matching. A crucial component of the approach is a training loss based on spatial relative differences. Compared to traditional two-frame structure from motion methods, results are more accurate and more robust. In contrast to the popular depth-from-single-image networks, DeMoN learns the concept of matching and, thus, better generalizes to structures not seen during training.",
            "저자": "Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, Thomas Brox",
            "전체 인용횟수": "745회 인용20172018201920202021202220231710513414215211575",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "5038-5047",
            "학술 문서": "Demon: Depth and motion network for learning monocular stereoB Ummenhofer, H Zhou, J Uhrig, N Mayer, E Ilg… - Proceedings of the IEEE conference on computer …, 2017745회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Demon: Depth and motion network for learning monocular stereo",
        "year": null
    },
    "Segmentation of moving objects by long term video analysis": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/12/17",
            "권": "33",
            "설명": "Motion is a strong cue for unsupervised object-level grouping. In this paper, we demonstrate that motion will be exploited most effectively, if it is regarded over larger time windows. Opposed to classical two-frame optical flow, point trajectories that span hundreds of frames are less susceptible to short-term variations that hinder separating different objects. As a positive side effect, the resulting groupings are temporally consistent over a whole video shot, a property that requires tedious post-processing in the vast majority of existing approaches. We suggest working with a paradigm that starts with semi-dense motion cues first and that fills up textureless areas afterwards based on color. This paper also contributes the Freiburg-Berkeley motion segmentation (FBMS) dataset, a large, heterogeneous benchmark with 59 sequences and pixel-accurate ground truth annotation of moving objects.",
            "저널": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "저자": "Peter Ochs, Jitendra Malik, Thomas Brox",
            "전체 인용횟수": "649회 인용201420152016201720182019202020212022202315417091807960635782",
            "페이지": "500-513",
            "학술 문서": "Segmentation of moving objects by long term video analysisP Ochs, J Malik, T Brox - IEEE transactions on pattern analysis and machine …, 2013649회 인용 관련 학술자료 전체 18개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Segmentation of moving objects by long term video analysis",
        "year": null
    },
    "Image super-resolution using deep convolutional networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/6/1",
            "게시자": "IEEE",
            "권": "38",
            "설명": "We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang",
            "전체 인용횟수": "8622회 인용2016201720182019202020212022202316147084412021429153415521339",
            "페이지": "295-307",
            "학술 문서": "Image super-resolution using deep convolutional networksC Dong, CC Loy, K He, X Tang - IEEE transactions on pattern analysis and machine …, 20158622회 인용 관련 학술자료 전체 13개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Image super-resolution using deep convolutional networks",
        "year": null
    },
    "Single image haze removal using dark channel prior": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/9/9",
            "게시자": "IEEE",
            "권": "33",
            "설명": "In this paper, we propose a simple but effective image prior-dark channel prior to remove haze from a single input image. The dark channel prior is a kind of statistics of outdoor haze-free images. It is based on a key observation-most local patches in outdoor haze-free images contain some pixels whose intensity is very low in at least one color channel. Using this prior with the haze imaging model, we can directly estimate the thickness of the haze and recover a high-quality haze-free image. Results on a variety of hazy images demonstrate the power of the proposed prior. Moreover, a high-quality depth map can also be obtained as a byproduct of haze removal.",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Kaiming He, Jian Sun, Xiaoou Tang",
            "전체 인용횟수": "8333회 인용2010201120122013201420152016201720182019202020212022202376111158242346487581584705720882102312081142",
            "페이지": "2341-2353",
            "학술 문서": "Single image haze removal using dark channel priorK He, J Sun, X Tang - IEEE transactions on pattern analysis and machine …, 20108272회 인용 관련 학술자료 전체 44개의 버전 SingleimagehazeremovalusingdarkchannelpriorM HeK, TXO SUNJ - IEEE Transactionson Pattern Analysis and Machine …, 2011100회 인용 관련 학술자료 ",
            "호": "12"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Single image haze removal using dark channel prior",
        "year": null
    },
    "Guided image filtering": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/10/2",
            "게시자": "IEEE",
            "권": "35",
            "설명": "In this paper, we propose a novel explicit image filter called guided filter. Derived from a local linear model, the guided filter computes the filtering output by considering the content of a guidance image, which can be the input image itself or another different image. The guided filter can be used as an edge-preserving smoothing operator like the popular bilateral filter [1], but it has better behaviors near edges. The guided filter is also a more generic concept beyond smoothing: It can transfer the structures of the guidance image to the filtering output, enabling new filtering applications like dehazing and guided feathering. Moreover, the guided filter naturally has a fast and nonapproximate linear time algorithm, regardless of the kernel size and the intensity range. Currently, it is one of the fastest edge-preserving filters. Experiments show that the guided filter is both effective and efficient in a great variety of computer …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Kaiming He, Jian Sun, Xiaoou Tang",
            "전체 인용횟수": "7752회 인용201120122013201420152016201720182019202020212022202343112210355581811798861829830824790644",
            "페이지": "1397-1409",
            "학술 문서": "Guided image filteringK He, J Sun, X Tang - IEEE transactions on pattern analysis and machine …, 20127752회 인용 관련 학술자료 전체 27개의 버전 ",
            "호": "6"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Guided image filtering",
        "year": null
    },
    "Deep learning face attributes in the wild": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation.(1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies.(2) It reveals that although the filters of LNet are fine-tuned only with image-level attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, but without face bounding boxes or landmarks, which are required by all attribute recognition works.(3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training with massive face identities, and such concepts are significantly enriched after fine-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts.",
            "저자": "Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang",
            "전체 인용횟수": "7733회 인용201620172018201920202021202220231023226659481141140815591518",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "3730-3738",
            "학술 문서": "Deep learning face attributes in the wildZ Liu, P Luo, X Wang, X Tang - Proceedings of the IEEE international conference on …, 20157733회 인용 관련 학술자료 전체 21개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep learning face attributes in the wild",
        "year": null
    },
    "Learning a deep convolutional network for image super-resolution": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "게시자": "Springer International Publishing",
            "설명": " We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) [15] that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage.",
            "저자": "Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang",
            "전체 인용횟수": "5583회 인용20152016201720182019202020212022202396234362588746810914945822",
            "컨퍼런스": "Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part IV 13",
            "페이지": "184-199",
            "학술 문서": "Learning a deep convolutional network for image super-resolutionC Dong, CC Loy, K He, X Tang - Computer Vision–ECCV 2014: 13th European …, 20145583회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning a deep convolutional network for image super-resolution",
        "year": null
    },
    "3d shapenets: A deep representation for volumetric shapes": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "3D shape is a crucial but heavily underutilized cue in today's computer vision systems, mostly due to the lack of a good generic shape representation. With the recent availability of inexpensive 2.5 D depth sensors (eg Microsoft Kinect), it is becoming increasingly important to have a powerful 3D shape representation in the loop. Apart from category recognition, recovering full 3D shapes from view-based 2.5 D depth maps is also a critical part of visual understanding. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model, 3D ShapeNets, learns the distribution of complex 3D shapes across different object categories and arbitrary poses from raw CAD data, and discovers hierarchical compositional part representation automatically. It naturally supports joint object recognition and shape completion from 2.5 D depth maps, and it enables active object recognition through view planning. To train our 3D deep learning model, we construct ModelNet-a large-scale 3D CAD model dataset. Extensive experiments show that our 3D deep representation enables significant performance improvement over the-state-of-the-arts in a variety of tasks.",
            "저자": "Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, Jianxiong Xiao",
            "전체 인용횟수": "5534회 인용2015201620172018201920202021202220232312424246267180696611421040",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "1912-1920",
            "학술 문서": "3d shapenets: A deep representation for volumetric shapesZ Wu, S Song, A Khosla, F Yu, L Zhang, X Tang, J Xiao - Proceedings of the IEEE conference on computer …, 20155534회 인용 관련 학술자료 전체 20개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "3d shapenets: A deep representation for volumetric shapes",
        "year": null
    },
    "Residual attention network for image classification": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "In this work, we propose\" Residual Attention Network\", a convolutional neural network using attention mechanism which can incorporate with state-of-art feed forward network architecture in an end-to-end training fashion. Our Residual Attention Network is built by stacking Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper. Inside each Attention Module, bottom-up top-down feedforward structure is used to unfold the feedforward and feedback attention process into a single feedforward process. Importantly, we propose attention residual learning to train very deep Residual Attention Networks which can be easily scaled up to hundreds of layers. Extensive analyses are conducted on CIFAR-10 and CIFAR-100 datasets to verify the effectiveness of every module mentioned above. Our Residual Attention Network achieves state-of-the-art object recognition performance on three benchmark datasets including CIFAR-10 (3.90% error), CIFAR-100 (20.45% error) and ImageNet (4.8% single model and single crop, top-5 error). Note that, our method achieves 0.6% top-1 accuracy improvement with 46% trunk depth and 69% forward FLOPs comparing to ResNet-200. The experiment also demonstrates that our network is robust against noisy labels.",
            "저자": "Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, Xiaoou Tang",
            "전체 인용횟수": "3853회 인용201720182019202020212022202314147463718943904638",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "3156-3164",
            "학술 문서": "Residual attention network for image classificationF Wang, M Jiang, C Qian, S Yang, C Li, H Zhang… - Proceedings of the IEEE conference on computer …, 20173853회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Residual attention network for image classification",
        "year": null
    },
    "Action recognition with trajectory-pooled deep-convolutional descriptors": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "Visual features are of vital importance for human action understanding in videos. This paper presents a new video representation, called trajectory-pooled deep-convolutional descriptor (TDD), which shares the merits of both hand-crafted features and deep-learned features. Specifically, we utilize deep architectures to learn discriminative convolutional feature maps, and conduct trajectory-constrained pooling to aggregate these convolutional features into effective descriptors. To enhance the robustness of TDDs, we design two normalization methods to transform convolutional feature maps, namely spatiotemporal normalization and channel normalization. The advantages of our features come from (i) TDDs are automatically learned and contain high discriminative capacity compared with those hand-crafted features;(ii) TDDs take account of the intrinsic characteristics of temporal dimension and introduce the strategies of trajectory-constrained sampling and pooling for aggregating deep-learned features. We conduct experiments on two challenging datasets: HMDB51 and UCF101. Experimental results show that TDDs outperform previous hand-crafted features and deep-learned features. Our method also achieves superior performance to the state of the art on these datasets.",
            "저자": "Limin Wang, Yu Qiao, Xiaoou Tang",
            "전체 인용횟수": "3844회 인용20112012201320142015201620172018201920202021202220232194212258313448509488411370306180141",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "4305-4314",
            "학술 문서": "Action recognition with trajectory-pooled deep-convolutional descriptorsL Wang, Y Qiao, X Tang - Proceedings of the IEEE conference on computer …, 20153835회 인용 관련 학술자료 전체 41개의 버전 Action recognition with trajectory-pooled deep-convolutional descriptorsW Limin, Q Yu, T Xiaoou - IEEE Conference on Computer Vision and Pattern …, 201528회 인용 관련 학술자료 Action recognition with trajectory-pooled deep-convolutional descriptors, 4305–4314*L Wang, Y Qiao, X Tang - IEEE Conference on Computer Vision and Pattern …, 20153회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Action recognition with trajectory-pooled deep-convolutional descriptors",
        "year": null
    },
    "Esrgan: Enhanced super-resolution generative adversarial networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN–network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge (region 3) with the best perceptual index. The code is available at https://github. com/xinntao/ESRGAN.",
            "저자": "Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, Chen Change Loy",
            "전체 인용횟수": "3446회 인용20192020202120222023189525788936977",
            "컨퍼런스": "Proceedings of the European conference on computer vision (ECCV) workshops",
            "페이지": "0-0",
            "학술 문서": "Esrgan: Enhanced super-resolution generative adversarial networksX Wang, K Yu, S Wu, J Gu, Y Liu, C Dong, Y Qiao… - Proceedings of the European conference on computer …, 20183446회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Esrgan: Enhanced super-resolution generative adversarial networks",
        "year": null
    },
    "Accelerating the super-resolution convolutional neural network": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "게시자": "Springer International Publishing",
            "설명": " As a successful deep model applied in image super-resolution (SR), the Super-Resolution Convolutional Neural Network (SRCNN) [1, 2] has demonstrated superior performance to the previous hand-crafted models either in speed and restoration quality. However, the high computational cost still hinders it from practical usage that demands real-time performance (24 fps). In this paper, we aim at accelerating the current SRCNN, and propose a compact hourglass-shape CNN structure for faster and better SR. We re-design the SRCNN structure mainly in three aspects. First, we introduce a deconvolution layer at the end of the network, then the mapping is learned directly from the original low-resolution image (without interpolation) to the high-resolution one. Second, we reformulate the mapping layer by shrinking the input feature dimension before mapping and expanding back afterwards. Third, we adopt …",
            "저자": "Chao Dong, Chen Change Loy, Xiaoou Tang",
            "전체 인용횟수": "3267회 인용2017201820192020202120222023101282378549667672589",
            "컨퍼런스": "Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14",
            "페이지": "391-407",
            "학술 문서": "Accelerating the super-resolution convolutional neural networkC Dong, CC Loy, X Tang - Computer Vision–ECCV 2016: 14th European …, 20163267회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Accelerating the super-resolution convolutional neural network",
        "year": null
    },
    "Learning to detect a salient object": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/3/18",
            "게시자": "IEEE",
            "권": "33",
            "설명": "In this paper, we study the salient object detection problem for images. We formulate this problem as a binary labeling task where we separate the salient object from the background. We propose a set of novel features, including multiscale contrast, center-surround histogram, and color spatial distribution, to describe a salient object locally, regionally, and globally. A conditional random field is learned to effectively combine these features for salient object detection. Further, we extend the proposed approach to detect a salient object from sequential images by introducing the dynamic salient features. We collected a large image database containing tens of thousands of carefully labeled images by multiple users and a video segment database, and conducted a set of experiments over them to demonstrate the effectiveness of the proposed approach.",
            "저널": "IEEE Transactions on Pattern analysis and machine intelligence",
            "저자": "Tie Liu, Zejian Yuan, Jian Sun, Jingdong Wang, Nanning Zheng, Xiaoou Tang, Heung-Yeung Shum",
            "전체 인용횟수": "3185회 인용2009201020112012201320142015201620172018201920202021202220234578135169249264307329327313279184182142100",
            "페이지": "353-367",
            "학술 문서": "Learning to detect a salient objectT Liu, Z Yuan, J Sun, J Wang, N Zheng, X Tang… - IEEE Transactions on Pattern analysis and machine …, 20103185회 인용 관련 학술자료 전체 24개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning to detect a salient object",
        "year": null
    },
    "Deep learning face representation by joint identification-verification": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "권": "27",
            "설명": "The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 features extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 features extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset, 99.15% face verification accuracy is achieved. Compared with the best previous deep learning result on LFW, the error rate has been significantly reduced by 67%.",
            "저널": "Advances in neural information processing systems",
            "저자": "Yi Sun, Yuheng Chen, Xiaogang Wang, Xiaoou Tang",
            "전체 인용횟수": "2732회 인용20142015201620172018201920202021202220237100192335398430448322268182",
            "학술 문서": "Deep learning face representation by joint identification-verificationY Sun, Y Chen, X Wang, X Tang - Advances in neural information processing systems, 20142732회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep learning face representation by joint identification-verification",
        "year": null
    },
    "Deep learning face representation from predicting 10,000 classes": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "설명": "This paper proposes to learn a set of high-level feature representations through deep learning, referred to as Deep hidden IDentity features (DeepID), for face verification. We argue that DeepID can be effectively learned through challenging multi-class face identification tasks, whilst they can be generalized to other tasks (such as verification) and new identities unseen in the training set. Moreover, the generalization capability of DeepID increases as more face classes are to be predicted at training. DeepID features are taken from the last hidden layer neuron activations of deep convolutional networks (ConvNets). When learned as classifiers to recognize about 10,000 face identities in the training set and configured to keep reducing the neuron numbers along the feature extraction hierarchy, these deep ConvNets gradually form compact identity-related features in the top layers with only a small number of hidden neurons. The proposed features are extracted from various face regions to form complementary and over-complete representations. Any state-of-the-art classifiers can be learned based on these high-level representations for face verification. 97.45% verification accuracy on LFW is achieved with only weakly aligned faces.",
            "저자": "Yi Sun, Xiaogang Wang, Xiaoou Tang",
            "전체 인용횟수": "2466회 인용201420152016201720182019202020212022202321113240324404340312278227156",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "1891-1898",
            "학술 문서": "Deep learning face representation from predicting 10,000 classesY Sun, X Wang, X Tang - Proceedings of the IEEE conference on computer …, 20142466회 인용 관련 학술자료 전체 22개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep learning face representation from predicting 10,000 classes",
        "year": null
    },
    "Wider face: A face detection benchmark": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "Face detection is one of the most studied topics in the computer vision community. Much of the progresses have been made by the availability of face detection benchmark datasets. We show that there is a gap between current face detection performance and the real world requirements. To facilitate future face detection research, we introduce the WIDER FACE dataset, which is 10 times larger than existing datasets. The dataset contains rich annotations, including occlusions, poses, event categories, and face bounding boxes. Faces in the proposed dataset are extremely challenging due to large variations in scale, pose and occlusion, as shown in Fig. 1. Furthermore, we show that WIDER FACE dataset is an effective training source for face detection. We benchmark several representative detection systems, providing an overview of state-of-the-art performance and propose a solution to deal with large scale variation. Finally, we discuss common failure cases that worth to be further investigated.",
            "저자": "Shuo Yang, Ping Luo, Chen-Change Loy, Xiaoou Tang",
            "전체 인용횟수": "1904회 인용201620172018201920202021202220231591148235303397395301",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "5525-5533",
            "학술 문서": "Wider face: A face detection benchmarkS Yang, P Luo, CC Loy, X Tang - Proceedings of the IEEE conference on computer …, 20161904회 인용 관련 학술자료 전체 18개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Wider face: A face detection benchmark",
        "year": null
    },
    "Deepfashion: Powering robust clothes recognition and retrieval with rich annotations": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "Recent advances in clothes recognition have been driven by the construction of clothes datasets. Existing datasets are limited in the amount of annotations and are difficult to cope with the various challenges in real-world applications. In this work, we introduce DeepFashion, a large-scale clothes dataset with comprehensive annotations. It contains over 800,000 images, which are richly annotated with massive attributes, clothing landmarks, and correspondence of images taken under different scenarios including store, street snapshot, and consumer. Such rich annotations enable the development of powerful algorithms in clothes recognition and facilitating future researches. To demonstrate the advantages of DeepFashion, we propose a new deep model, namely FashionNet, which learns clothing features by jointly predicting clothing attributes and landmarks. The estimated landmarks are then employed to pool or gate the learned features. It is optimized in an iterative manner. Extensive experiments demonstrate the effectiveness of FashionNet and the usefulness of DeepFashion.",
            "저자": "Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, Xiaoou Tang",
            "전체 인용횟수": "1829회 인용20162017201820192020202120222023876158255295338361324",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "1096-1104",
            "학술 문서": "Deepfashion: Powering robust clothes recognition and retrieval with rich annotationsZ Liu, P Luo, S Qiu, X Wang, X Tang - Proceedings of the IEEE conference on computer …, 20161829회 인용 관련 학술자료 전체 18개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deepfashion: Powering robust clothes recognition and retrieval with rich annotations",
        "year": null
    },
    "Deep convolutional network cascade for facial point detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013",
            "설명": "We propose a new approach for estimation of the positions of facial keypoints with three-level carefully designed convolutional networks. At each level, the outputs of multiple networks are fused for robust and accurate estimation. Thanks to the deep structures of convolutional networks, global high-level features are extracted over the whole face region at the initialization stage, which help to locate high accuracy keypoints. There are two folds of advantage for this. First, the texture context information over the entire face is utilized to locate each keypoint. Second, since the networks are trained to predict all the keypoints simultaneously, the geometric constraints among keypoints are implicitly encoded. The method therefore can avoid local minimum caused by ambiguity and data corruption in difficult image samples due to occlusions, large pose variations, and extreme lightings. The networks at the following two levels are trained to locally refine initial predictions and their inputs are limited to small regions around the initial predictions. Several network structures critical for accurate and robust facial point detection are investigated. Extensive experiments show that our approach outperforms state-ofthe-art methods in both detection accuracy and reliability 1.",
            "저자": "Yi Sun, Xiaogang Wang, Xiaoou Tang",
            "전체 인용횟수": "1749회 인용20132014201520162017201820192020202120222023115413421221523323619816416183",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "3476-3483",
            "학술 문서": "Deep convolutional network cascade for facial point detectionY Sun, X Wang, X Tang - Proceedings of the IEEE conference on computer …, 20131749회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep convolutional network cascade for facial point detection",
        "year": null
    },
    "Facial landmark detection by deep multi-task learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "게시자": "Springer International Publishing",
            "설명": " Facial landmark detection has long been impeded by the problems of occlusion and pose variation. Instead of treating the detection task as a single and independent problem, we investigate the possibility of improving detection robustness through multi-task learning. Specifically, we wish to optimize facial landmark detection together with heterogeneous but subtly correlated tasks, e.g. head pose estimation and facial attribute inference. This is non-trivial since different tasks have different learning difficulties and convergence rates. To address this problem, we formulate a novel tasks-constrained deep model, with task-wise early stopping to facilitate learning convergence. Extensive evaluations show that the proposed task-constrained learning (i) outperforms existing methods, especially in dealing with faces with severe occlusion and pose variation, and (ii) reduces model complexity drastically compared to …",
            "저자": "Zhanpeng Zhang, Ping Luo, Chen Change Loy, Xiaoou Tang",
            "전체 인용횟수": "1669회 인용20152016201720182019202020212022202348120186240254255210172146",
            "컨퍼런스": "Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13",
            "페이지": "94-108",
            "학술 문서": "Facial landmark detection by deep multi-task learningZ Zhang, P Luo, CC Loy, X Tang - Computer Vision–ECCV 2014: 13th European …, 20141669회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Facial landmark detection by deep multi-task learning",
        "year": null
    },
    "Deepid3: Face recognition with very deep neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/2/3",
            "설명": "The state-of-the-art of face recognition has been significantly advanced by the emergence of deep learning. Very deep neural networks recently achieved great success on general object recognition because of their superb learning capacity. This motivates us to investigate their effectiveness on face recognition. This paper proposes two very deep neural network architectures, referred to as DeepID3, for face recognition. These two architectures are rebuilt from stacked convolution and inception layers proposed in VGG net and GoogLeNet to make them suitable to face recognition. Joint face identification-verification supervisory signals are added to both intermediate and final feature extraction layers during training. An ensemble of the proposed two architectures achieves 99.53% LFW face verification accuracy and 96.0% LFW rank-1 face identification accuracy, respectively. A further discussion of LFW face verification result is given in the end.",
            "저널": "arXiv preprint arXiv:1502.00873",
            "저자": "Yi Sun, Ding Liang, Xiaogang Wang, Xiaoou Tang",
            "전체 인용횟수": "1170회 인용201520162017201820192020202120222023147614018917520017111274",
            "학술 문서": "Deepid3: Face recognition with very deep neural networksY Sun, D Liang, X Wang, X Tang - arXiv preprint arXiv:1502.00873, 20151170회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deepid3: Face recognition with very deep neural networks",
        "year": null
    },
    "Deeply learned face representations are sparse, selective, and robust": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "This paper designs a high-performance deep convolutional network (DeepID2+) for face recognition. It is learned with the identification-verification supervisory signal. By increasing the dimension of hidden representations and adding supervision to early convolutional layers, DeepID2+ achieves new state-of-the-art on LFW and YouTube Faces benchmarks. Through empirical studies, we have discovered three properties of its deep neural activations critical for the high performance: sparsity, selectiveness and robustness.(1) It is observed that neural activations are moderately sparse. Moderate sparsity maximizes the discriminative power of the deep net as well as the distance between images. It is surprising that DeepID2+ still can achieve high recognition accuracy even after the neural responses are binarized.(2) Its neurons in higher layers are highly selective to identities and identity-related attributes. We can identify different subsets of neurons which are either constantly excited or inhibited when different identities or attributes are present. Although DeepID2+ is not taught to distinguish attributes during training, it has implicitly learned such high-level concepts.(3) It is much more robust to occlusions, although occlusion patterns are not included in the training set.",
            "저자": "Yi Sun, Xiaogang Wang, Xiaoou Tang",
            "전체 인용횟수": "1146회 인용201520162017201820192020202120222023351051741741781491509958",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "2892-2900",
            "학술 문서": "Deeply learned face representations are sparse, selective, and robustY Sun, X Wang, X Tang - Proceedings of the IEEE conference on computer …, 20151146회 인용 관련 학술자료 전체 18개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deeply learned face representations are sparse, selective, and robust",
        "year": null
    },
    "Distinctive image features from scale-invariant keypoints": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004/11/1",
            "게시자": "Springer Netherlands",
            "권": "60",
            "설명": " This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through …",
            "저널": "International journal of computer vision",
            "저자": "David G Lowe",
            "전체 인용횟수": "72715회 인용20042005200620072008200920102011201220132014201520162017201820192020202120222023190505101615802242301038294212463250765408567453304960460645204098416138142971",
            "페이지": "91-110",
            "학술 문서": "Distinctive image features from scale-invariant keypointsDG Lowe - International journal of computer vision, 200472715회 인용 관련 학술자료 전체 141개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Distinctive image features from scale-invariant keypoints",
        "year": null
    },
    "Object recognition from local scale-invariant features": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1999",
            "게시자": "IEEE",
            "설명": "An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object …",
            "저널": "International Conference on Computer Vision, 1999",
            "저자": "David G Lowe",
            "전체 인용횟수": "24872회 인용200320042005200620072008200920102011201220132014201520162017201820192020202120222023832233053934976518981114133414761663178718541886176616651652147314741307993",
            "페이지": "1150-1157",
            "학술 문서": "Object recognition from local scale-invariant featuresDG Lowe - Proceedings of the seventh IEEE international …, 199924872회 인용 관련 학술자료 전체 56개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Object recognition from local scale-invariant features",
        "year": null
    },
    "Fast Approximate Nearest Neighbors with Automatic Algorithm Configuration.": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/2/5",
            "권": "2",
            "설명": "For many computer vision problems, the most time consuming component consists of nearest neighbor matching in high-dimensional spaces. There are no known exact algorithms for solving these high-dimensional problems that are faster than linear search. Approximate algorithms are known to provide large speedups with only minor loss in accuracy, but many such algorithms have been published with only minimal guidance on selecting an algorithm and its parameters for any given problem. In this paper, we describe a system that answers the question,“What is the fastest approximate nearest-neighbor algorithm for my data?” Our system will take any given dataset and desired degree of precision and use these to automatically determine the best algorithm and parameter values. We also describe a new algorithm that applies priority search on hierarchical k-means trees, which we have found to provide the best known performance on many datasets. After testing a range of alternatives, we have found that multiple randomized kd trees provide the best performance for other datasets. We are releasing public domain code that implements these approaches. This library provides about one order of magnitude improvement in query time over the best previously available software and provides fully automated parameter selection.",
            "저널": "VISAPP (1)",
            "저자": "Marius Muja, David G Lowe",
            "전체 인용횟수": "4077회 인용20092010201120122013201420152016201720182019202020212022202340100184224354399441375357331280246240245183",
            "페이지": "331-340",
            "학술 문서": "Fast approximate nearest neighbors with automatic algorithm configuration.M Muja, DG Lowe - VISAPP (1), 20093980회 인용 관련 학술자료 전체 14개의 버전 Flann-fast library for approximate nearest neighbors user manual*M Muja, D Lowe - Computer Science Department, University of British …, 2009117회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Fast Approximate Nearest Neighbors with Automatic Algorithm Configuration.",
        "year": null
    },
    "Automatic panoramic image stitching using invariant features": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2007/8/1",
            "게시자": "Springer Netherlands",
            "권": "74",
            "설명": " This paper concerns the problem of fully automated panoramic image stitching. Though the 1D problem (single axis of rotation) is well studied, 2D or multi-row stitching is more difficult. Previous approaches have used human input or restrictions on the image sequence in order to establish matching images. In this work, we formulate stitching as a multi-image matching problem, and use invariant local features to find matches between all of the images. Because of this our method is insensitive to the ordering, orientation, scale and illumination of the input images. It is also insensitive to noise images that are not part of a panorama, and can recognise multiple panoramas in an unordered image dataset. In addition to providing more detail, this paper extends our previous work in the area (Brown and Lowe, 2003) by introducing gain compensation and automatic straightening steps.",
            "저널": "International Journal of Computer Vision",
            "저자": "Matthew Brown, David G Lowe",
            "전체 인용횟수": "3485회 인용20072008200920102011201220132014201520162017201820192020202120222023174782154161188251250245295302294333255246182136",
            "페이지": "59-73",
            "학술 문서": "Automatic panoramic image stitching using invariant featuresM Brown, DG Lowe - International journal of computer vision, 20073485회 인용 관련 학술자료 전체 27개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Automatic panoramic image stitching using invariant features",
        "year": null
    },
    "Unsupervised learning of depth and ego-motion from video": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "We present an unsupervised learning framework for the task of dense 3D geometry and camera motion estimation from unstructured video sequences. In common with recent work, we use an end-to-end learning approach with view synthesis as the supervisory signal. In contrast to these works, our method is completely unsupervised, requiring only a sequence of images as input. We achieve this with a network that estimates the 6-DoF camera pose parameters of the input set, along with dense depth for a reference view using single-view inference. Our loss is constructed by projecting the nearby posed views into the reference view via the depth map. Results using the KITTI dataset demonstrate the effectiveness of our approach, which performs on par with another deep learning approach that assumes ground-truth pose information at training time.",
            "저자": "Tinghui Zhou, Matthew Brown, Noah Snavely, David G Lowe",
            "전체 인용횟수": "2605회 인용201720182019202020212022202331227392466532509432",
            "컨퍼런스": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "페이지": "1851-1858",
            "학술 문서": "Unsupervised learning of depth and ego-motion from videoT Zhou, M Brown, N Snavely, DG Lowe - Proceedings of the IEEE conference on computer …, 20172605회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Unsupervised learning of depth and ego-motion from video",
        "year": null
    },
    "Perceptual Organization and Visual Recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1985",
            "게시자": "Kluwer Academic Publishers, Boston",
            "저자": "David G Lowe",
            "전체 인용횟수": "2167회 인용19861987198819891990199119921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023729549181871251291081061059884737173697037445256454640394550292534222021161689",
            "페이지": "162",
            "학술 문서": "Perceptual organization and visual recognition*D Lowe - 20122167회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Perceptual Organization and Visual Recognition",
        "year": null
    },
    "Three-dimensional object recognition from single two-dimensional images": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1987/3/31",
            "게시자": "Elsevier",
            "권": "31",
            "설명": "A computer vision system has been implemented that can recognize three-dimensional objects from unknown viewpoints in single gray-scale images. Unlike most other approaches, the recognition is accomplished without any attempt to reconstruct depth information bottom-up from the visual input. Instead, three other mechanisms are used that can bridge the gap between the two-dimensional image and knowledge of three-dimensional objects. First, a process of perceptual organization is used to form groupings and structures in the image that are likely to be invariant over a wide range of viewpoints. Second, a probabilistic ranking method is used to reduce the size of the search space during model-based matching. Finally, a process of spatial correspondence brings the projections of three-dimensional models into direct correspondence with the image by solving for unknown viewpoint and model parameters. A …",
            "저널": "Artificial intelligence",
            "저자": "David G Lowe",
            "전체 인용횟수": "2090회 인용19871988198919901991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220237385552659178867582646773534875487152527167706261618149475034323426221922",
            "페이지": "355-395",
            "학술 문서": "Three-dimensional object recognition from single two-dimensional imagesDG Lowe - Artificial intelligence, 19872090회 인용 관련 학술자료 전체 17개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Three-dimensional object recognition from single two-dimensional images",
        "year": null
    },
    "Scalable Nearest Neighbor Algorithms for High Dimensional Data": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "게시자": "IEEE",
            "권": "36",
            "설명": "For many computer vision and machine learning problems, large training sets are key for good performance. However, the most computationally expensive part of many computer vision and machine learning algorithms consists of finding nearest neighbor matches to high dimensional vectors that represent the training data. We propose new algorithms for approximate nearest neighbor matching and evaluate and compare them with previous algorithms. For matching high dimensional features, we find two algorithms to be the most efficient: the randomized k-d forest and a new algorithm proposed in this paper, the priority search k-means tree. We also propose a new algorithm for matching binary features by searching multiple hierarchical clustering trees and show it outperforms methods typically used in the literature. We show that the optimal nearest neighbor algorithm and its parameters depend on the data set …",
            "저널": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "저자": "Marius Muja, David G. Lowe",
            "전체 인용횟수": "1647회 인용20142015201620172018201920202021202220232113617425726323915816511195",
            "페이지": "2227-40",
            "학술 문서": "Scalable nearest neighbor algorithms for high dimensional dataM Muja, DG Lowe - IEEE transactions on pattern analysis and machine …, 20141647회 인용 관련 학술자료 전체 9개의 버전 ",
            "호": "11"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Scalable Nearest Neighbor Algorithms for High Dimensional Data",
        "year": null
    },
    "Recognising panoramas": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2003",
            "게시자": "IEEE",
            "설명": "The problem considered in this paper is the fully automatic construction of panoramas. Fundamentally, this problem requires recognition, as we need to know which parts of the panorama join up. Previous approaches have used human input or restrictions on the image sequence for the matching step. In this work we use object recognition techniques based on invariant local features to select matching images, and a probabilistic model for verification. Because of this our method is insensitive to the ordering, orientation, scale and illumination of the images. It is also insensitive to ‘noise’images which are not part of the panorama at all, that is, it recognises panoramas. This suggests a useful application for photographers: the system takes as input the images on an entire flash card or film, recognises images that form part of a panorama, and stitches them with no user input whatsoever.",
            "저자": "Matthew Brown, David G Lowe",
            "전체 인용횟수": "1615회 인용200320042005200620072008200920102011201220132014201520162017201820192020202120222023615397911010112413813313312897907165666143382623",
            "컨퍼런스": "International Conference on Computer Vision, 2003",
            "페이지": "1218-1225",
            "학술 문서": "Recognising panoramas.M Brown, DG Lowe - ICCV, 20031615회 인용 관련 학술자료 전체 27개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Recognising panoramas",
        "year": null
    },
    "Shape indexing using approximate nearest-neighbour search in high-dimensional spaces": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1997/6/17",
            "게시자": "IEEE",
            "설명": "Shape indexing is a way of making rapid associations between features detected in an image and object models that could have produced them. When model databases are large, the use of high-dimensional features is critical, due to the improved level of discrimination they can provide. Unfortunately, finding the nearest neighbour to a query point rapidly becomes inefficient as the dimensionality of the feature space increases. Past indexing methods have used hash tables for hypothesis recovery, but only in low-dimensional situations. In this paper we show that a new variant of the k-d tree search algorithm makes indexing in higher-dimensional spaces practical. This Best Bin First, or BBF search is an approximate algorithm which finds the nearest neighbour for a large fraction of the queries, and a very close neighbour in the remaining cases. The technique has been integrated into a fully developed recognition …",
            "저자": "Jeffrey S Beis, David G Lowe",
            "전체 인용횟수": "1565회 인용1998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202357465917314262881091431091211501191119567655232512917",
            "컨퍼런스": "Computer Vision and Pattern Recognition, 1997. Proceedings., 1997 IEEE Computer Society Conference on",
            "페이지": "1000-1006",
            "학술 문서": "Shape indexing using approximate nearest-neighbour search in high-dimensional spacesJS Beis, DG Lowe - Proceedings of IEEE computer society conference on …, 19971565회 인용 관련 학술자료 전체 20개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Shape indexing using approximate nearest-neighbour search in high-dimensional spaces",
        "year": null
    },
    "A boosted particle filter: Multitarget detection and tracking": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004",
            "게시자": "Springer Berlin/Heidelberg",
            "설명": " The problem of tracking a varying number of non-rigid objects has two major difficulties. First, the observation models and target distributions can be highly non-linear and non-Gaussian. Second, the presence of a large, varying number of objects creates complex interactions with overlap and ambiguities. To surmount these difficulties, we introduce a vision system that is capable of learning, detecting and tracking the objects of interest. The system is demonstrated in the context of tracking hockey players using video sequences. Our approach combines the strengths of two successful algorithms: mixture particle filters and Adaboost. The mixture particle filter [17] is ideally suited to multi-target tracking as it assigns a mixture component to each player. The crucial design issues in mixture particle filters are the choice of the proposal distribution and the treatment of objects leaving and entering the scene. Here …",
            "저널": "Computer Vision-ECCV 2004",
            "저자": "Kenji Okuma, Ali Taleghani, Nando de Freitas, James J Little, David G Lowe",
            "전체 인용횟수": "1516회 인용20032004200520062007200820092010201120122013201420152016201720182019202020212022202341053878411911610211497104107998980574946421816",
            "페이지": "28-39",
            "학술 문서": "A boosted particle filter: Multitarget detection and trackingK Okuma, A Taleghani, N De Freitas, JJ Little, DG Lowe - Computer Vision-ECCV 2004: 8th European …, 20041516회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A boosted particle filter: Multitarget detection and tracking",
        "year": null
    },
    "Fitting parameterized three-dimensional models to images": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1991/5/1",
            "권": "13",
            "설명": "Model-based recognition and motion tracking depends upon the ability to solve for projection and model parameters that will best fit a 3-D model to matching 2-D image features. This paper extends current methods of parameter solving to handle objects with arbitrary curved surfaces and with any number of internal parameters representing articulation, variable dimensions, or surface deformations. Numerical stabilization methods are developed that take account of inherent inaccuracies in the image measurements and allow useful solutions to be determined even when there are fewer matches than unknown parameters. The Levenberg-Marquardt method is used to always ensure convergence of the solution. These techniques allow model-based vision to be used for a much wider class of problems than was possible with previous methods. Their application is demonstrated for tracking the motion of curved …",
            "저널": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "저자": "David G.  Lowe",
            "전체 인용횟수": "1420회 인용19911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202362121405255443940455053546765647557585264655334373524212331201411",
            "페이지": "441-450",
            "학술 문서": "Fitting parameterized three-dimensional models to imagesDG Lowe - IEEE transactions on pattern analysis and machine …, 19911420회 인용 관련 학술자료 전체 20개의 버전 ",
            "호": "5"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Fitting parameterized three-dimensional models to images",
        "year": null
    },
    "Invariant features from interest point groups.": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2002/9/2",
            "권": "4",
            "설명": "This paper approaches the problem of finding correspondences between images in which there are large changes in viewpoint, scale and illumination. Recent work has shown that scale-space ‘interest points’ may be found with good repeatability in spite of such changes. Furthermore, the high entropy of the surrounding image regions means that local descriptors are highly discriminative for matching. For descriptors at interest points to be robustly matched between images, they must be as far as possible invariant to the imaging process. In this work we introduce a family of features which use groups of interest points to form geometrically invariant descriptors of image regions. Feature descriptors are formed by resampling the image relative to canonical frames defined by the points. In addition to robust matching, a key advantage of this approach is that each match implies a hypothesis of the local 2D (projective) transformation. This allows us to immediately reject most of the false matches using a Hough transform. We reject remaining outliers using RANSAC and the epipolar constraint. Results show that dense feature matching can be achieved in a few seconds of computation on 1GHz Pentium III machines.",
            "저자": "Matthew Brown, David G Lowe",
            "전체 인용횟수": "1198회 인용2003200420052006200720082009201020112012201320142015201620172018201920202021202220237252447396983991041009881917359503229362412",
            "컨퍼런스": "BMVC",
            "페이지": "398-410",
            "학술 문서": "Invariant features from interest point groups.M Brown, DG Lowe - Bmvc, 20021198회 인용 관련 학술자료 전체 18개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Invariant features from interest point groups.",
        "year": null
    },
    "Mobile robot localization and mapping with uncertainty using scale-invariant visual landmarks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2002/8/1",
            "게시자": "SAGE Publications",
            "권": "21",
            "설명": "A key component of a mobile robot system is the ability to localize itself accurately and, simultaneously, to build a map of the environment. Most of the existing algorithms are based on laser range finders, sonar sensors or artificial landmarks. In this paper, we describe a vision-based mobile robot localization and mapping algorithm, which uses scale-invariant image features as natural landmarks in unmodified environments. The invariance of these features to image translation, scaling and rotation makes them suitable landmarks for mobile robot localization and map building. With our Triclops stereo vision system, these landmarks are localized and robot ego-motion is estimated by least-squares minimization of the matched landmarks. Feature viewpoint variation and occlusion are taken into account by maintaining a view direction for each landmark. Experiments show that these visual landmarks are robustly …",
            "저널": "The international Journal of robotics Research",
            "저자": "Stephen Se, David Lowe, Jim Little",
            "전체 인용횟수": "1198회 인용20032004200520062007200820092010201120122013201420152016201720182019202020212022202374062103113100827881798050414440393828292818",
            "페이지": "735-758",
            "학술 문서": "Mobile robot localization and mapping with uncertainty using scale-invariant visual landmarksS Se, D Lowe, J Little - The international Journal of robotics Research, 20021198회 인용 관련 학술자료 전체 18개의 버전 ",
            "호": "8"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Mobile robot localization and mapping with uncertainty using scale-invariant visual landmarks",
        "year": null
    },
    "Local feature view clustering for 3D object recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2001",
            "게시자": "IEEE",
            "권": "1",
            "설명": "There have been important recent advances in object recognition through the matching of invariant local image features. However, the existing approaches are based on matching to individual training images. This paper presents a method for combining multiple images of a 3D object into a single model representation. This provides for recognition of 3D objects from any viewpoint, the generalization of models to non-rigid changes, and improved robustness through the combination of features acquired under a range of imaging conditions. The decision of whether to cluster a training image into an existing view representation or to treat it as a new view is based on the geometric accuracy of the match to previous model views. A new probabilistic model is developed to reduce the false positive matches that would otherwise arise due to loosened geometric constraints on matching 3D and non-rigid models. A system …",
            "저자": "David G Lowe",
            "전체 인용횟수": "876회 인용200220032004200520062007200820092010201120122013201420152016201720182019202020212022202358252545405255695852615658434940273531198",
            "컨퍼런스": "Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on",
            "페이지": "I-682-I-688 vol. 1",
            "학술 문서": "Local feature view clustering for 3D object recognitionDG Lowe - Proceedings of the 2001 IEEE Computer Society …, 2001876회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Local feature view clustering for 3D object recognition",
        "year": null
    },
    "Vision-based mobile robot localization and mapping using scale-invariant features": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2001",
            "게시자": "IEEE",
            "권": "2",
            "설명": "A key component of a mobile robot system is the ability to localize itself accurately and build a map of the environment simultaneously. In this paper, a vision-based mobile robot localization and mapping algorithm is described which uses scale-invariant image features as landmarks in unmodified dynamic environments. These 3D landmarks are localized and robot ego-motion is estimated by matching them, taking into account the feature viewpoint variation. With our Triclops stereo vision system, experiments show that these features are robustly matched between views, 3D landmarks are tracked, robot pose is estimated and a 3D map is built.",
            "저자": "Stephen Se, David Lowe, Jim Little",
            "전체 인용횟수": "797회 인용200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220233161831273853606776484536505143312817221466",
            "컨퍼런스": "Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No. 01CH37164)",
            "페이지": "2051-2058",
            "학술 문서": "Vision-based mobile robot localization and mapping using scale-invariant featuresS Se, D Lowe, J Little - … 2001 ICRA. IEEE International Conference on …, 2001797회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Vision-based mobile robot localization and mapping using scale-invariant features",
        "year": null
    },
    "Method and apparatus for identifying scale invariant features in an image and use of same for locating an object in an image": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004/3/23",
            "발명자": "David G Lowe",
            "설명": "A method and apparatus for identifying scale invariant features in an image and a further method and apparatus for using such scale invariant features to locate an object in an image are disclosed. The method and apparatus for identifying scale invariant features may involve the use of a processor circuit for producing a plurality of component subregion descriptors for each subregion of a pixel region about pixel amplitude extrema in a plurality of difference images produced from the image. This may involve producing a plurality of difference images by blurring an initial image to produce a blurred image and by subtracting the blurred image from the initial image to produce the difference image. For each difference image, pixel amplitude extrema are located and a corresponding pixel region is defined about each pixel amplitude extremum. Each pixel region is divided into subregions and a plurality of component …",
            "전체 인용횟수": "789회 인용20062007200820092010201120122013201420152016201720182019202020212022202368571521284053797986886479634221",
            "출원번호": "09519893",
            "특허 번호": "6711293",
            "특허청": "US",
            "학술 문서": "Method and apparatus for identifying scale invariant features in an image and use of same for locating an object in an imageDG Lowe - US Patent 6,711,293, 2004789회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Method and apparatus for identifying scale invariant features in an image and use of same for locating an object in an image",
        "year": null
    },
    "Vision-based global localization and mapping for mobile robots": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/6",
            "게시자": "IEEE",
            "권": "21",
            "설명": "We have previously developed a mobile robot system which uses scale-invariant visual landmarks to localize and simultaneously build three-dimensional (3-D) maps of unmodified environments. In this paper, we examine global localization, where the robot localizes itself globally, without any prior location estimate. This is achieved by matching distinctive visual landmarks in the current frame to a database map. A Hough transform approach and a RANSAC approach for global localization are compared, showing that RANSAC is much more efficient for matching specific features, but much worse for matching nonspecific features. Moreover, robust global localization can be achieved by matching a small submap of the local region built from multiple frames. This submap alignment algorithm for global localization can be applied to map building, which can be regarded as alignment of multiple 3-D submaps. A global …",
            "저널": "Robotics, IEEE Transactions on",
            "저자": "Stephen Se, David G Lowe, James J Little",
            "전체 인용횟수": "769회 인용20062007200820092010201120122013201420152016201720182019202020212022202326717770724761576044344129171915174",
            "페이지": "364-375",
            "학술 문서": "Vision-based global localization and mapping for mobile robotsS Se, DG Lowe, JJ Little - IEEE Transactions on robotics, 2005769회 인용 관련 학술자료 전체 16개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Vision-based global localization and mapping for mobile robots",
        "year": null
    },
    "Multiclass object recognition with sparse, localized features": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006/6/17",
            "게시자": "IEEE",
            "권": "1",
            "설명": "We apply a biologically inspired model of visual object recognition to the multiclass object categorization problem. Our model modifies that of Serre, Wolf, and Poggio. As in that work, we first apply Gabor filters at all positions and scales; feature complexity and position/scale invariance are then built up by alternating template matching and max pooling operations. We refine the approach in several biologically plausible ways, using simple versions of sparsification and lateral inhibition. We demonstrate the value of retaining some position and scale information above the intermediate feature level. Using feature selection we arrive at a model that performs better with fewer features. Our final model is tested on the Caltech 101 object categories and the UIUC car localization task, in both cases achieving state-of-the-art performance. The results strengthen the case for using this class of model in computer vision.",
            "저자": "Jim Mutch, David G Lowe",
            "전체 인용횟수": "658회 인용20052006200720082009201020112012201320142015201620172018201920202021202220232126068646851495161333816241971253",
            "컨퍼런스": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)",
            "페이지": "11-18",
            "학술 문서": "Multiclass object recognition with sparse, localized featuresJ Mutch, DG Lowe - 2006 IEEE Computer Society Conference on Computer …, 2006658회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Multiclass object recognition with sparse, localized features",
        "year": null
    },
    "Robust model-based motion tracking through the integration of search and estimation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1992/8/1",
            "게시자": "Springer Netherlands",
            "권": "8",
            "설명": " A computer vision system has been developed for real-time motion tracking of 3-D objects, including those with variable internal parameters. This system provides for the integrated treatment of matching and measurement errors that arise during motion tracking. These two sources of error have very different distributions and are best handled by separate computational mechanisms. These errors can be treated in an integrated way by using the computation of variance in predicted feature measurements to determine the probability of correctness for each potential matching feature. In return, a best-first search procedure uses these probabilities to find consistent sets of matches, which eliminates the need to treat outliers during the analysis of measurement errors. The most reliable initial matches are used to reduce the parameter variance on further iterations, minimizing the amount of search required for …",
            "저널": "International Journal of Computer Vision",
            "저자": "David G Lowe",
            "전체 인용횟수": "561회 인용199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202371222352029322539312822382724241415171715108313442625",
            "페이지": "113-122",
            "학술 문서": "Robust model-based motion tracking through the integration of search and estimationDG Lowe - International Journal of Computer Vision, 1992561회 인용 관련 학술자료 전체 16개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Robust model-based motion tracking through the integration of search and estimation",
        "year": null
    },
    "Visualizing and understanding convolutional networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "게시자": "Springer International Publishing",
            "설명": " Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.",
            "저자": "Matthew D Zeiler, Rob Fergus",
            "전체 인용횟수": "20756회 인용20132014201520162017201820192020202120222023631705409941656248527802927296226902091",
            "컨퍼런스": "Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13",
            "페이지": "818-833",
            "학술 문서": "Visualizing and understanding convolutional networksMD Zeiler, R Fergus - Computer Vision–ECCV 2014: 13th European …, 201420756회 인용 관련 학술자료 전체 23개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Visualizing and understanding convolutional networks",
        "year": null
    },
    "Learning spatiotemporal features with 3d convolutional networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets; 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets; and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8% accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use.",
            "저자": "Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, Manohar Paluri",
            "전체 인용횟수": "9037회 인용2016201720182019202020212022202316954392812491402164315821393",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "4489-4497",
            "학술 문서": "Learning spatiotemporal features with 3d convolutional networksD Tran, L Bourdev, R Fergus, L Torresani, M Paluri - Proceedings of the IEEE international conference on …, 20159037회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning spatiotemporal features with 3d convolutional networks",
        "year": null
    },
    "Overfeat: Integrated recognition, localization and detection using convolutional networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/12/21",
            "설명": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.",
            "저널": "arXiv preprint arXiv:1312.6229",
            "저자": "Pierre Sermanet, David Eigen, Xiang Zhang, Michaël Mathieu, Rob Fergus, Yann LeCun",
            "전체 인용횟수": "7323회 인용19881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202319353634384471568380998210688501418122415243329325046171463667765868856748633504341",
            "학술 문서": "Overfeat: Integrated recognition, localization and detection using convolutional networksP Sermanet, D Eigen, X Zhang, M Mathieu, R Fergus… - arXiv preprint arXiv:1312.6229, 20137323회 인용 관련 학술자료 전체 30개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Overfeat: Integrated recognition, localization and detection using convolutional networks",
        "year": null
    },
    "Indoor segmentation and support inference from rgbd images": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012",
            "게시자": "Springer Berlin Heidelberg",
            "설명": " We present an approach to interpret the major surfaces, objects, and support relations of an indoor scene from an RGBD image. Most existing work ignores physical interactions or is applied only to tidy rooms and hallways. Our goal is to parse typical, often messy, indoor scenes into floor, walls, supporting surfaces, and object regions, and to recover support relationships. One of our main interests is to better understand how 3D cues can best inform a structured 3D interpretation. We also contribute a novel integer programming formulation to infer physical support relations. We offer a new dataset of 1449 RGBD images, capturing 464 diverse indoor scenes, with detailed annotations. Our experiments demonstrate our ability to infer support relations in complex scenes and verify that our 3D scene cues and inferred support lead to better object segmentation.",
            "저자": "Nathan Silberman, Derek Hoiem, Pushmeet Kohli, Rob Fergus",
            "전체 인용횟수": "5597회 인용2013201420152016201720182019202020212022202388145239309385502596667830887838",
            "컨퍼런스": "Computer Vision–ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part V 12",
            "페이지": "746-760",
            "학술 문서": "Indoor segmentation and support inference from rgbd imagesN Silberman, D Hoiem, P Kohli, R Fergus - Computer Vision–ECCV 2012: 12th European …, 20125597회 인용 관련 학술자료 전체 22개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Indoor segmentation and support inference from rgbd images",
        "year": null
    },
    "Depth map prediction from a single image using a multi-scale deep network": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "권": "27",
            "설명": "Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation.",
            "저널": "Advances in neural information processing systems",
            "저자": "David Eigen, Christian Puhrsch, Rob Fergus",
            "전체 인용횟수": "3981회 인용20152016201720182019202020212022202367138219384564601676682591",
            "학술 문서": "Depth map prediction from a single image using a multi-scale deep networkD Eigen, C Puhrsch, R Fergus - Advances in neural information processing systems, 20143981회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Depth map prediction from a single image using a multi-scale deep network",
        "year": null
    },
    "Spectral hashing": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008",
            "권": "21",
            "설명": "Semantic hashing seeks compact binary codes of datapoints so that the Hamming distance between codewords correlates with semantic similarity. Hinton et al. used a clever implementation of autoencoders to find such codes. In this paper, we show that the problem of finding a best code for a given dataset is closely related to the problem of graph partitioning and can be shown to be NP hard. By relaxing the original problem, we obtain a spectral method whose solutions are simply a subset of thresh-olded eigenvectors of the graph Laplacian. By utilizing recent results on convergence of graph Laplacian eigenvectors to the Laplace-Beltrami eigen-functions of manifolds, we show how to efficiently calculate the code of a novel datapoint. Taken together, both learning the code and applying it to a novel point are extremely simple. Our experiments show that our codes significantly outperform the state-of-the art.",
            "저널": "Advances in neural information processing systems",
            "저자": "Yair Weiss, Antonio Torralba, Rob Fergus",
            "전체 인용횟수": "3167회 인용200920102011201220132014201520162017201820192020202120222023275287139184235274342352309294267241186129",
            "학술 문서": "Spectral hashingY Weiss, A Torralba, R Fergus - Advances in neural information processing systems, 20083167회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Spectral hashing",
        "year": null
    },
    "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "In this paper we address three different computer vision tasks using a single basic architecture: depth prediction, surface normal estimation, and semantic labeling. We use a multiscale convolutional network that is able to adapt easily to each task using only small modifications, regressing from the input image to the output map directly. Our method progressively refines predictions using a sequence of scales, and captures many image details without any superpixels or low-level segmentation. We achieve state-of-the-art performance on benchmarks for all three tasks.",
            "저자": "David Eigen, Rob Fergus",
            "전체 인용횟수": "3029회 인용20152016201720182019202020212022202324124257414475495463426293",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "2650-2658",
            "학술 문서": "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architectureD Eigen, R Fergus - Proceedings of the IEEE international conference on …, 20153029회 인용 관련 학술자료 전체 18개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture",
        "year": null
    },
    "End-to-end memory networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "권": "28",
            "설명": "We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.",
            "저널": "Advances in neural information processing systems",
            "저자": "Sainbayar Sukhbaatar, Jason Weston, Rob Fergus",
            "전체 인용횟수": "2942회 인용20152016201720182019202020212022202333173303441488498393349234",
            "학술 문서": "End-to-end memory networksS Sukhbaatar, J Weston, R Fergus - Advances in neural information processing systems, 20152942회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "End-to-end memory networks",
        "year": null
    },
    "Deep generative image models using a￼ laplacian pyramid of adversarial networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "권": "28",
            "설명": "In this paper we introduce a generative model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks (convnets) within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach. Samples drawn from our model are of significantly higher quality than existing models. In a quantitive assessment by human evaluators our CIFAR10 samples were mistaken for real images around 40% of the time, compared to 10% for GAN samples. We also show samples from more diverse datasets such as STL10 and LSUN.",
            "저널": "Advances in neural information processing systems",
            "저자": "Emily L Denton, Soumith Chintala, Rob Fergus",
            "전체 인용횟수": "2802회 인용20152016201720182019202020212022202320129296417441446372357266",
            "학술 문서": "Deep generative image models using a￼ laplacian pyramid of adversarial networksEL Denton, S Chintala, R Fergus - Advances in neural information processing systems, 20152802회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep generative image models using a￼ laplacian pyramid of adversarial networks",
        "year": null
    },
    "Removing camera shake from a single photograph": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006/7/1",
            "도서": "Acm Siggraph 2006 Papers",
            "설명": "Camera shake during exposure leads to objectionable image blur and ruins many photographs. Conventional blind deconvolution methods typically assume frequency-domain constraints on images, or overly simplified parametric forms for the motion path during camera shake. Real camera motions can follow convoluted paths, and a spatial domain prior can better maintain visually salient image characteristics. We introduce a method to remove the effects of camera shake from seriously blurred images. The method assumes a uniform camera blur over the image and negligible in-plane camera rotation. In order to estimate the blur from the camera shake, the user must specify an image region without saturation effects. We show results for a variety of digital photographs taken from personal photo collections.",
            "저자": "Rob Fergus, Barun Singh, Aaron Hertzmann, Sam T Roweis, William T Freeman",
            "전체 인용횟수": "2619회 인용2006200720082009201020112012201320142015201620172018201920202021202220238285197130131186184219199225182172174169150147120",
            "페이지": "787-794",
            "학술 문서": "Removing camera shake from a single photographR Fergus, B Singh, A Hertzmann, ST Roweis… - Acm Siggraph 2006 Papers, 20062619회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Removing camera shake from a single photograph",
        "year": null
    },
    "80 million tiny images: A large data set for nonparametric object and scene recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008/5/30",
            "게시자": "IEEE",
            "권": "30",
            "설명": "With the advent of the Internet, billions of images are now freely available online and constitute a dense sampling of the visual world. Using a variety of non-parametric methods, we explore this world with the aid of a large dataset of 79,302,017 images collected from the Internet. Motivated by psychophysical results showing the remarkable tolerance of the human visual system to degradations in image resolution, the images in the dataset are stored as 32 x 32 color images. Each image is loosely labeled with one of the 75,062 non-abstract nouns in English, as listed in the Wordnet lexical database. Hence the image database gives a comprehensive coverage of all object categories and scenes. The semantic information from Wordnet can be used in conjunction with nearest-neighbor methods to perform object classification over a range of semantic levels minimizing the effects of labeling noise. For certain classes …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Antonio Torralba, Rob Fergus, William T Freeman",
            "전체 인용횟수": "2319회 인용2008200920102011201220132014201520162017201820192020202120222023874113125152174177188163170158149147169165139",
            "페이지": "1958-1970",
            "학술 문서": "80 million tiny images: A large data set for nonparametric object and scene recognitionA Torralba, R Fergus, WT Freeman - IEEE transactions on pattern analysis and machine …, 20082319회 인용 관련 학술자료 전체 19개의 버전 ",
            "호": "11"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "80 million tiny images: A large data set for nonparametric object and scene recognition",
        "year": null
    },
    "Deconvolutional networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/6/13",
            "게시자": "IEEE",
            "설명": "Building robust low and mid-level image representations, beyond edge primitives, is a long-standing goal in vision. Many existing feature detectors spatially pool edge information which destroys cues such as edge intersections, parallelism and symmetry. We present a learning framework where features that capture these mid-level cues spontaneously emerge from image data. Our approach is based on the convolutional decomposition of images under a spar-sity constraint and is totally unsupervised. By building a hierarchy of such decompositions we can learn rich feature sets that are a robust image representation for both the analysis and synthesis of images.",
            "저자": "Matthew D Zeiler, Dilip Krishnan, Graham W Taylor, Rob Fergus",
            "전체 인용횟수": "2132회 인용2010201120122013201420152016201720182019202020212022202392220324473118198286268276278257219",
            "컨퍼런스": "2010 IEEE Computer Society Conference on computer vision and pattern recognition",
            "페이지": "2528-2535",
            "학술 문서": "Deconvolutional networksMD Zeiler, D Krishnan, GW Taylor, R Fergus - 2010 IEEE Computer Society Conference on computer …, 20102132회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deconvolutional networks",
        "year": null
    },
    "Image and depth from a conventional camera with a coded aperture": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2007/7/29",
            "게시자": "ACM",
            "권": "26",
            "설명": "A conventional camera captures blurred versions of scene information away from the plane of focus. Camera systems have been proposed that allow for recording all-focus images, or for extracting depth, but to record both simultaneously has required more extensive hardware and reduced spatial resolution. We propose a simple modification to a conventional camera that allows for the simultaneous recovery of both (a) high resolution image information and (b) depth information adequate for semi-automatic extraction of a layered depth representation of the image. Our modification is to insert a patterned occluder within the aperture of the camera lens, creating a coded aperture. We introduce a criterion for depth discriminability which we use to design the preferred aperture pattern. Using a statistical model of images, we can recover both depth information and an all-focus image from single photographs taken with …",
            "저널": "ACM transactions on graphics (TOG)",
            "저자": "Anat Levin, Rob Fergus, Frédo Durand, William T Freeman",
            "전체 인용횟수": "1943회 인용2006200720082009201020112012201320142015201620172018201920202021202220238742861001091311581741611651461381241051008467",
            "페이지": "70-es",
            "학술 문서": "Image and depth from a conventional camera with a coded apertureA Levin, R Fergus, F Durand, WT Freeman - ACM transactions on graphics (TOG), 20071943회 인용 관련 학술자료 전체 16개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Image and depth from a conventional camera with a coded aperture",
        "year": null
    },
    "Exploiting linear structure within convolutional networks for efficient evaluation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "권": "27",
            "설명": "We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy, but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the redundancy present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2×, while keeping the accuracy within 1% of the original model.",
            "저널": "Advances in neural information processing systems",
            "저자": "Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, Rob Fergus",
            "전체 인용횟수": "1908회 인용201420152016201720182019202020212022202394187153246290310300238211",
            "학술 문서": "Exploiting linear structure within convolutional networks for efficient evaluationEL Denton, W Zaremba, J Bruna, Y LeCun, R Fergus - Advances in neural information processing systems, 20141908회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Exploiting linear structure within convolutional networks for efficient evaluation",
        "year": null
    },
    "Pointnet: Deep learning on point sets for 3d classification and segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.",
            "저자": "Charles R Qi, Hao Su, Kaichun Mo, Leonidas J Guibas",
            "전체 인용횟수": "13223회 인용20172018201920202021202220236448712381938268934293285",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "652-660",
            "학술 문서": "Pointnet: Deep learning on point sets for 3d classification and segmentationCR Qi, H Su, K Mo, LJ Guibas - Proceedings of the IEEE conference on computer …, 201713223회 인용 관련 학술자료 전체 20개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pointnet: Deep learning on point sets for 3d classification and segmentation",
        "year": null
    },
    "Pointnet++: Deep hierarchical feature learning on point sets in a metric space": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "권": "30",
            "설명": "Few prior works study deep learning on point sets. PointNet is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.",
            "저널": "Advances in neural information processing systems",
            "저자": "Charles Ruizhongtai Qi, Li Yi, Hao Su, Leonidas J Guibas",
            "전체 인용횟수": "9238회 인용2018201920202021202220232447261226189425202565",
            "학술 문서": "Pointnet++: Deep hierarchical feature learning on point sets in a metric spaceCR Qi, L Yi, H Su, LJ Guibas - Advances in neural information processing systems, 20179238회 인용 관련 학술자료 전체 22개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pointnet++: Deep hierarchical feature learning on point sets in a metric space",
        "year": null
    },
    "The earth mover's distance as a metric for image retrieval": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2000/11",
            "게시자": "Kluwer Academic Publishers",
            "권": "40",
            "설명": " We investigate the properties of a metric between two distributions, the Earth Mover's Distance (EMD), for content-based image retrieval. The EMD is based on the minimal cost that must be paid to transform one distribution into the other, in a precise sense, and was first proposed for certain vision problems by Peleg, Werman, and Rom. For image retrieval, we combine this idea with a representation scheme for distributions that is based on vector quantization. This combination leads to an image comparison framework that often accounts for perceptual similarity better than other previously proposed methods. The EMD is based on a solution to the transportation problem from linear optimization, for which efficient algorithms are available, and also allows naturally for partial matching. It is more robust than histogram matching techniques, in that it can operate on variable-length representations of the …",
            "저널": "International journal of computer vision",
            "저자": "Yossi Rubner, Carlo Tomasi, Leonidas J Guibas",
            "전체 인용횟수": "6174회 인용1998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202320211225245567114156178207237253268274318403348346321358359424478451409",
            "페이지": "99-121",
            "학술 문서": "The earth mover's distance as a metric for image retrievalY Rubner, C Tomasi, LJ Guibas - International journal of computer vision, 20005783회 인용 관련 학술자료 전체 41개의 버전 The earth mover’s distance, multi-dimensional scaling, and color-based image retrieval*Y Rubner, LJ Guibas, C Tomasi - Proceedings of the ARPA image understanding …, 1997467회 인용 관련 학술자료 전체 19개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The earth mover's distance as a metric for image retrieval",
        "year": null
    },
    "Shapenet: An information-rich 3d model repository": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/12/9",
            "설명": "We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.",
            "저널": "arXiv preprint arXiv:1512.03012",
            "저자": "Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, Fisher Yu",
            "전체 인용횟수": "4526회 인용20162017201820192020202120222023491623304546338029761073",
            "학술 문서": "Shapenet: An information-rich 3d model repositoryAX Chang, T Funkhouser, L Guibas, P Hanrahan… - arXiv preprint arXiv:1512.03012, 20154526회 인용 관련 학술자료 전체 19개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Shapenet: An information-rich 3d model repository",
        "year": null
    },
    "Frustum pointnets for 3d object detection from rgb-d data": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "In this work, we study 3D object detection from RGB-D data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efficiently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efficiency as well as high recall for even small objects. Benefited from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability.",
            "저자": "Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, Leonidas J Guibas",
            "전체 인용횟수": "2351회 인용20182019202020212022202355282429537560472",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "918-927",
            "학술 문서": "Frustum pointnets for 3d object detection from rgb-d dataCR Qi, W Liu, C Wu, H Su, LJ Guibas - Proceedings of the IEEE conference on computer …, 20182351회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Frustum pointnets for 3d object detection from rgb-d data",
        "year": null
    },
    "A metric for distributions with applications to image databases": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1998/1/7",
            "게시자": "IEEE",
            "설명": "We introduce a new distance between two distributions that we call the Earth Mover's Distance (EMD), which reflects the minimal amount of work that must be performed to transform one distribution into the other by moving \"distribution mass\" around. This is a special case of the transportation problem from linear optimization, for which efficient algorithms are available. The EMD also allows for partial matching. When used to compare distributions that have the same overall mass, the EMD is a true metric, and has easy-to-compute lower bounds. In this paper we focus on applications to image databases, especially color and texture. We use the EMD to exhibit the structure of color-distribution and texture spaces by means of Multi-Dimensional Scaling displays. We also propose a novel approach to the problem of navigating through a collection of color images, which leads to a new paradigm for image database search.",
            "저자": "Yossi Rubner, Carlo Tomasi, Leonidas J Guibas",
            "전체 인용횟수": "2284회 인용1998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202313271940414562827678106971107110311611111711110511611613513812495",
            "컨퍼런스": "Sixth international conference on computer vision (IEEE Cat. No. 98CH36271)",
            "페이지": "59-66",
            "학술 문서": "A metric for distributions with applications to image databasesY Rubner, C Tomasi, LJ Guibas - Sixth international conference on computer vision …, 19982284회 인용 관련 학술자료 전체 30개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A metric for distributions with applications to image databases",
        "year": null
    },
    "Primitives for the manipulation of general subdivisions and the computation of Voronoi": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1985/4/1",
            "게시자": "ACM",
            "권": "4",
            "설명": "The following problem is discussed: given n points in the plane (the sites) and an arbitrary query point q, find the site that is closest to q. This problem can be solved by constructing the Voronoi diagram of the griven sites and then locating the query point inone of its regions. Two algorithms are given, one that constructs the Voronoi diagram in O(n log n) time, and another that inserts a new sit on O(n) time. Both are based on the use of the Voronoi dual, or Delaunay triangulation, and are simple enough to be of practical value. the simplicity of both algorithms can be attributed to the separation of the geometrical and topological aspects of the problem and to the use of two simple but powerful primitives,  a geometric predicate and an operator for manipulating the topology of the diagram. The topology is represented by a new data structure for generalized diagrams, that is, embeddings of graphs in two-dimensional …",
            "저널": "ACM transactions on graphics (TOG)",
            "저자": "Leonidas Guibas, Jorge Stolfi",
            "전체 인용횟수": "2094회 인용19851986198719881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202381122342235625441454159635047676167726982928079828157734266556646433530443725",
            "페이지": "74-123",
            "학술 문서": "Primitives for the manipulation of general subdivisions and the computation of VoronoiL Guibas, J Stolfi - ACM transactions on graphics (TOG), 19852094회 인용 관련 학술자료 전체 18개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Primitives for the manipulation of general subdivisions and the computation of Voronoi",
        "year": null
    },
    "Wireless sensor networks: an information processing approach": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004/7/6",
            "게시자": "Morgan Kaufmann",
            "설명": "Designing, implementing, and operating a wireless sensor network involves a wide range of disciplines and many application-specific constraints. To make sense of and take advantage of these systems, a holistic approach is neededand this is precisely what Wireless Sensor Networks delivers. Inside, two eminent researchers review the diverse technologies and techniques that interact in todays wireless sensor networks. At every step, they are guided by the high-level information-processing tasks that determine how these networks are architected and administered. Zhao and Guibas begin with the canonical problem of localizing and tracking moving objects, then systematically examine the many fundamental sensor network issues that spring from it, including network discovery, service establishment, data routing and aggregation, query processing, programming models, and system organization. The understanding gained as a resulthow different layers support the needs of different applications, and how a wireless sensor network should be built to optimize performance and economyis sure to endure as individual component technologies come and go. Features: Written for practitioners, researchers, and students and relevant to all application areas, including environmental monitoring, industrial sensing and diagnostics, automotive and transportation, security and surveillance, military and battlefield uses, and large-scale infrastructural maintenance. Skillfully integrates the many disciplines at work in wireless sensor network design: signal processing and estimation, communication theory and protocols, distributed algorithms and databases …",
            "저자": "Feng Zhao, Leonidas J Guibas",
            "전체 인용횟수": "2063회 인용20042005200620072008200920102011201220132014201520162017201820192020202120222023873168213169181155170147122126969269665342312015",
            "학술 문서": "Wireless sensor networks: an information processing approachF Zhao, LJ Guibas - 20042063회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Wireless sensor networks: an information processing approach",
        "year": null
    },
    "Kpconv: Flexible and deformable convolution for point clouds": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "We present Kernel Point Convolution (KPConv), a new design of point convolution, ie that operates on point clouds without any intermediate representation. The convolution weights of KPConv are located in Euclidean space by kernel points, and applied to the input points close to them. Its capacity to use any number of kernel points gives KPConv more flexibility than fixed grid convolutions. Furthermore, these locations are continuous in space and can be learned by the network. Therefore, KPConv can be extended to deformable convolutions that learn to adapt kernel points to local geometry. Thanks to a regular subsampling strategy, KPConv is also efficient and robust to varying densities. Whether they use deformable KPConv for complex tasks, or rigid KPconv for simpler tasks, our networks outperform state-of-the-art classification and segmentation approaches on several datasets. We also offer ablation studies and visualizations to provide understanding of what has been learned by KPConv and to validate the descriptive power of deformable KPConv.",
            "저자": "Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, François Goulette, Leonidas J Guibas",
            "전체 인용횟수": "2043회 인용2019202020212022202317178439687715",
            "컨퍼런스": "Proceedings of the IEEE/CVF international conference on computer vision",
            "페이지": "6411-6420",
            "학술 문서": "Kpconv: Flexible and deformable convolution for point cloudsH Thomas, CR Qi, JE Deschaud, B Marcotegui… - Proceedings of the IEEE/CVF international conference …, 20192043회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Kpconv: Flexible and deformable convolution for point clouds",
        "year": null
    },
    "A point set generation network for 3d object reconstruction from a single image": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "Generation of 3D data by deep neural network has been attracting increasing attention in the research community. The majority of extant works resort to regular representations such as volumetric grids or collection of images; however, these representations obscure the natural invariance of 3D shapes under geometric transformations, and also suffer from a number of other issues. In this paper we address the problem of 3D reconstruction from a single image, generating a straight-forward form of output--point cloud coordinates. Along with this problem arises a unique and interesting issue, that the groundtruth shape for an input image may be ambiguous. Driven by this unorthordox output form and the inherent ambiguity in groundtruth, we design architecture, loss function and learning paradigm that are novel and effective. Our final solution is a conditional shape sampler, capable of predicting multiple plausible 3D point clouds from an input image. In experiments not only can our system outperform state-of-the-art methods on single image based 3D reconstruction benchmarks; but it also shows strong performance for 3D shape completion and promising ability in making multiple plausible predictions.",
            "저자": "Haoqiang Fan, Hao Su, Leonidas J Guibas",
            "전체 인용횟수": "2036회 인용201720182019202020212022202326113248324413481415",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "605-613",
            "학술 문서": "A point set generation network for 3d object reconstruction from a single imageH Fan, H Su, LJ Guibas - Proceedings of the IEEE conference on computer …, 20172036회 인용 관련 학술자료 전체 18개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A point set generation network for 3d object reconstruction from a single image",
        "year": null
    },
    "A concise and provably informative multi‐scale signature based on heat diffusion": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/7",
            "게시자": "Blackwell Publishing Ltd",
            "권": "28",
            "설명": " We propose a novel point signature based on the properties of the heat diffusion process on a shape. Our signature, called the Heat Kernel Signature (or HKS), is obtained by restricting the well‐known heat kernel to the temporal domain. Remarkably we show that under certain mild assumptions, HKS captures all of the information contained in the heat kernel, and characterizes the shape up to isometry. This means that the restriction to the temporal domain, on the one hand, makes HKS much more concise and easily commensurable, while on the other hand, it preserves all of the information about the intrinsic geometry of the shape. In addition, HKS inherits many useful properties from the heat kernel, which means, in particular, that it is stable under perturbations of the shape. Our signature also provides a natural and efficiently computable multi‐scale way to capture information about neighborhoods of a given …",
            "저널": "Computer graphics forum",
            "저자": "Jian Sun, Maks Ovsjanikov, Leonidas Guibas",
            "전체 인용횟수": "1822회 인용200920102011201220132014201520162017201820192020202120222023124971112134139129179163144156157126110115",
            "페이지": "1383-1392",
            "학술 문서": "A concise and provably informative multi‐scale signature based on heat diffusionJ Sun, M Ovsjanikov, L Guibas - Computer graphics forum, 20091822회 인용 관련 학술자료 전체 19개의 버전 ",
            "호": "5"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A concise and provably informative multi‐scale signature based on heat diffusion",
        "year": null
    },
    "Handbook of discrete and computational geometry": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/11/22",
            "게시자": "CRC press",
            "설명": "The Handbook of Discrete and Computational Geometry is intended as a reference book fully accessible to nonspecialists as well as specialists, covering all major aspects of both fields. The book offers the most important results and methods in discrete and computational geometry to those who use them in their work, both in the academic world—as researchers in mathematics and computer science—and in the professional world—as practitioners in fields as diverse as operations research, molecular biology, and robotics. Discrete geometry has contributed significantly to the growth of discrete mathematics in recent years. This has been fueled partly by the advent of powerful computers and by the recent explosion of activity in the relatively young field of computational geometry. This synthesis between discrete and computational geometry lies at the heart of this Handbook. A growing list of application fields includes combinatorial optimization, computer-aided design, computer graphics, crystallography, data analysis, error-correcting codes, geographic information systems, motion planning, operations research, pattern recognition, robotics, solid modeling, and tomography.",
            "저자": "Csaba D Toth, Joseph O'Rourke, Jacob E Goodman",
            "전체 인용횟수": "1781회 인용19971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202352534403648456678897179859577506474607262738276999678",
            "학술 문서": "Handbook of discrete and computational geometryCD Toth, J O'Rourke, JE Goodman - 20171780회 인용 관련 학술자료 전체 8개의 버전 Handbook of Discrete and Computational Geometry*G Fejes Tóth, JE Goodman, J O'Rourke - 19976회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Handbook of discrete and computational geometry",
        "year": null
    },
    "Volumetric and multi-view cnns for object classification on 3d data": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "3D shape models are becoming widely available and easier to capture, making available 3D information crucial for progress in object classification. Current state-of-the-art methods rely on CNNs to address this problem. Recently, we witness two types of CNNs being developed: CNNs based upon volumetric representations versus CNNs based upon multi-view representations. Empirical results from these two types of CNNs exhibit a large gap, indicating that existing volumetric CNN architectures and approaches are unable to fully exploit the power of 3D representations. In this paper, we aim to improve both volumetric CNNs and multi-view CNNs according to extensive analysis of existing approaches. To this end, we introduce two distinct network architectures of volumetric CNNs. In addition, we examine multi-view CNNs, where we introduce multi-resolution filtering in 3D. Overall, we are able to outperform current state-of-the-art methods for both volumetric CNNs and multi-view CNNs. We provide extensive experiments designed to evaluate underlying design choices, thus providing a better understanding of the space of methods available for object classification on 3D data.",
            "저자": "Charles R Qi, Hao Su, Matthias Nießner, Angela Dai, Mengyuan Yan, Leonidas J Guibas",
            "전체 인용횟수": "1770회 인용201620172018201920202021202220231392218303348331265176",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "5648-5656",
            "학술 문서": "Volumetric and multi-view cnns for object classification on 3d dataCR Qi, H Su, M Nießner, A Dai, M Yan, LJ Guibas - Proceedings of the IEEE conference on computer …, 20161770회 인용 관련 학술자료 전체 22개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Volumetric and multi-view cnns for object classification on 3d data",
        "year": null
    },
    "Deep knowledge tracing": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "권": "28",
            "설명": "Knowledge tracing, where a machine models the knowledge of a student as they interact with coursework, is an established and significantly unsolved problem in computer supported education. In this paper we explore the benefit of using recurrent neural networks to model student learning. This family of models have important advantages over current state of the art methods in that they do not require the explicit encoding of human domain knowledge, and have a far more flexible functional form which can capture substantially more complex student interactions. We show that these neural networks outperform the current state of the art in prediction on real student data, while allowing straightforward interpretation and discovery of structure in the curriculum. These results suggest a promising new line of research for knowledge tracing.",
            "저널": "Advances in neural information processing systems",
            "저자": "Chris Piech, Jonathan Bassen, Jonathan Huang, Surya Ganguli, Mehran Sahami, Leonidas J Guibas, Jascha Sohl-Dickstein",
            "전체 인용횟수": "1225회 인용20162017201820192020202120222023254710399171228272266",
            "학술 문서": "Deep knowledge tracingC Piech, J Bassen, J Huang, S Ganguli, M Sahami… - Advances in neural information processing systems, 20151225회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep knowledge tracing",
        "year": null
    },
    "Learning representations and generative models for 3d point clouds": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/7/3",
            "게시자": "PMLR",
            "설명": "Three-dimensional geometric data offer an excellent domain for studying representation learning and generative modeling. In this paper, we look at geometric data represented as point clouds. We introduce a deep AutoEncoder (AE) network with state-of-the-art reconstruction quality and generalization ability. The learned representations outperform existing methods on 3D recognition tasks and enable shape editing via simple algebraic manipulations, such as semantic part editing, shape analogies and shape interpolation, as well as shape completion. We perform a thorough study of different generative models including GANs operating on the raw point clouds, significantly improved GANs trained in the fixed latent space of our AEs, and Gaussian Mixture Models (GMMs). To quantitatively evaluate generative models we introduce measures of sample fidelity and diversity based on matchings between sets of point clouds. Interestingly, our evaluation of generalization, fidelity and diversity reveals that GMMs trained in the latent space of our AEs yield the best results overall.",
            "저자": "Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, Leonidas Guibas",
            "전체 인용횟수": "1196회 인용20182019202020212022202318113202255293312",
            "컨퍼런스": "International conference on machine learning",
            "페이지": "40-49",
            "학술 문서": "Learning representations and generative models for 3d point cloudsP Achlioptas, O Diamanti, I Mitliagkas, L Guibas - International conference on machine learning, 20181196회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning representations and generative models for 3d point clouds",
        "year": null
    },
    "Taskonomy: Disentangling task transfer learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "Do visual tasks have a relationship, or are they unrelated? For instance, could having surface normals simplify estimating the depth of an image? Intuition answers these questions positively, implying existence of a structure among visual tasks. Knowing this structure has notable uses; it is the concept underlying transfer learning and, for example, can provide a principled way for reusing supervision among related tasks, finding what tasks transfer well to an arbitrary target task, or solving many tasks in one system without piling up the complexity. This paper proposes a fully computational approach for finding the structure of the space of visual tasks. This is done via a sampled dictionary of twenty six 2D, 2.5 D, 3D, and semantic tasks, and modeling their (1st and higher order) transfer dependencies in a latent space. The product can be viewed as a computational taxonomic map for task transfer learning. We study the consequences of this structure, eg the nontrivial emerged relationships, and exploit them to reduce the demand for labeled data. For example, we show that the total number of labeled datapoints needed for solving a set of 10 tasks can be reduced by roughly 2/3 while keeping the performance nearly the same. Users can employ a provided Binary Integer Programming solver that leverages the taxonomy to find efficient supervision policies for their own use cases.",
            "저자": "Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, Silvio Savarese",
            "전체 인용횟수": "1186회 인용20182019202020212022202327177221254257247",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "3712-3722",
            "학술 문서": "Taskonomy: Disentangling task transfer learningAR Zamir, A Sax, W Shen, LJ Guibas, J Malik… - Proceedings of the IEEE conference on computer …, 20181186회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Taskonomy: Disentangling task transfer learning",
        "year": null
    },
    "A dichromatic framework for balanced trees": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1978/10/16",
            "게시자": "IEEE",
            "설명": "In this paper we present a uniform framework for the implementation and study of balanced tree algorithms. We show how to imbed in this framework the best known balanced tree techniques and then use the framework to develop new algorithms which perform the update and rebalancing in one pass, on the way down towards a leaf. We conclude with a study of performance issues and concurrent updating.",
            "저자": "Leo J Guibas, Robert Sedgewick",
            "전체 인용횟수": "1052회 인용1984198519861987198819891990199119921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023714105132021151924192924291921212811182126202729353238313331294027373039393834",
            "컨퍼런스": "19th Annual Symposium on Foundations of Computer Science (sfcs 1978)",
            "페이지": "8-21",
            "학술 문서": "A dichromatic framework for balanced treesLJ Guibas, R Sedgewick - 19th Annual Symposium on Foundations of Computer …, 19781052회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A dichromatic framework for balanced trees",
        "year": null
    },
    "Deep hough voting for 3d object detection in point clouds": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "Current 3D object detection methods are heavily influenced by 2D detectors. In order to leverage architectures in 2D detectors, they often convert 3D point clouds to regular grids (ie, to voxel grids or to bird's eye view images), or rely on detection in 2D images to propose 3D boxes. Few works have attempted to directly detect objects in point clouds. In this work, we return to first principles to construct a 3D detection pipeline for point cloud data and as generic as possible. However, due to the sparse nature of the data--samples from 2D manifolds in 3D space--we face a major challenge when directly predicting bounding box parameters from scene points: a 3D object centroid can be far from any surface point thus hard to regress accurately in one step. To address the challenge, we propose VoteNet, an end-to-end 3D object detection network based on a synergy of deep point set networks and Hough voting. Our model achieves state-of-the-art 3D detection on two large datasets of real 3D scans, ScanNet and SUN RGB-D with a simple design, compact model size and high efficiency. Remarkably, VoteNet outperforms previous methods by using purely geometric information without relying on color images.",
            "저자": "Charles R Qi, Or Litany, Kaiming He, Leonidas J Guibas",
            "전체 인용횟수": "1040회 인용2019202020212022202311113257344311",
            "컨퍼런스": "proceedings of the IEEE/CVF International Conference on Computer Vision",
            "페이지": "9277-9286",
            "학술 문서": "Deep hough voting for 3d object detection in point cloudsCR Qi, O Litany, K He, LJ Guibas - proceedings of the IEEE/CVF International Conference …, 20191040회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep hough voting for 3d object detection in point clouds",
        "year": null
    },
    "A scalable active framework for region annotation in 3d shape collections": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/11/11",
            "게시자": "ACM",
            "권": "35",
            "설명": "Large repositories of 3D shapes provide valuable input for data-driven analysis and modeling tools. They are especially powerful once annotated with semantic information such as salient regions and functional parts. We propose a novel active learning method capable of enriching massive geometric datasets with accurate semantic region annotations. Given a shape collection and a user-specified region label our goal is to correctly demarcate the corresponding regions with minimal manual work. Our active framework achieves this goal by cycling between manually annotating the regions, automatically propagating these annotations across the rest of the shapes, manually verifying both human and automatic annotations, and learning from the verification results to improve the automatic propagation algorithm. We use a unified utility function that explicitly models the time cost of human input across all steps of our …",
            "저널": "ACM Transactions on Graphics (ToG)",
            "저자": "Li Yi, Vladimir G Kim, Duygu Ceylan, I-Chao Shen, Mengyan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Sheffer, Leonidas Guibas",
            "전체 인용횟수": "985회 인용20172018201920202021202220232757100138181264210",
            "페이지": "1-12",
            "학술 문서": "A scalable active framework for region annotation in 3d shape collectionsL Yi, VG Kim, D Ceylan, IC Shen, M Yan, H Su, C Lu… - ACM Transactions on Graphics (ToG), 2016985회 인용 관련 학술자료 전체 11개의 버전 ",
            "호": "6"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A scalable active framework for region annotation in 3d shape collections",
        "year": null
    },
    "Locating and bypassing holes in sensor networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006/4",
            "게시자": "Kluwer Academic Publishers",
            "권": "11",
            "설명": " In real sensor network deployments, spatial distributions of sensors are usually far from being uniform. Such networks often contain regions without enough sensor nodes, which we call holes. In this paper, we show that holes are important topological features that need to be studied. In routing, holes are communication voids that cause greedy forwarding to fail. Holes can also be defined to denote regions of interest, such as the “hot spots” created by traffic congestion or sensor power shortage. In this paper, we define holes to be the regions enclosed by a polygonal cycle which contains all the nodes where local minima can appear. We also propose simple and distributed algorithms, the Tent rule and BoundHole, to identify and build routes around holes. We show that the boundaries of holes marked using BoundHole can be used in many applications such as geographic routing, path migration, information …",
            "저널": "Mobile networks and Applications",
            "저자": "Qing Fang, Jie Gao, Leonidas J Guibas",
            "전체 인용횟수": "928회 인용20052006200720082009201020112012201320142015201620172018201920202021202220233153737395877260655659474333201713163",
            "페이지": "187-200",
            "학술 문서": "Locating and bypassing holes in sensor networksQ Fang, J Gao, LJ Guibas - Mobile networks and Applications, 2006928회 인용 관련 학술자료 전체 34개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Locating and bypassing holes in sensor networks",
        "year": null
    },
    "Yolov3: An incremental improvement": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/4",
            "게시자": "CoRR",
            "저널": "arXiv preprint arXiv:1804.02767",
            "저자": "Redmon Joseph, Farhadi Ali",
            "전체 인용횟수": "387회 인용201820192020202120222023839649510477",
            "페이지": "1-6",
            "학술 문서": "Yolov3: An incremental improvementR Joseph, F Ali - arXiv preprint arXiv:1804.02767, 2018387회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Yolov3: An incremental improvement",
        "year": null
    },
    "YOLO9000: Better, Faster, Stronger": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time.",
            "저널": "CVPR",
            "저자": "Joseph Redmon, Ali Farhadi",
            "전체 인용횟수": "19058회 인용2017201820192020202120222023349139123412986384944493534",
            "학술 문서": "YOLO9000: better, faster, strongerJ Redmon, A Farhadi - Proceedings of the IEEE conference on computer …, 201718997회 인용 관련 학술자료 전체 23개의 버전 ieee 2017 ieee conference on computer Vision and Pattern Recognition (cvpr)-honolulu, hi (2017.7. 21-2017.7. 26)*J Redmon, A Farhadi - 2017 ieee conference on computer vision and pattern …, 2017165회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "YOLO9000: Better, Faster, Stronger",
        "year": null
    },
    "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/3/16",
            "설명": " We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values resulting in 32 memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58 faster convolutional operations (in terms of number of the high precision operations) and 32 memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is the same as the full-precision AlexNet. We …",
            "저널": "ECCV",
            "저자": "Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, Ali Farhadi",
            "전체 인용횟수": "5106회 인용2016201720182019202020212022202355283650805957906819588",
            "학술 문서": "Xnor-net: Imagenet classification using binary convolutional neural networksM Rastegari, V Ordonez, J Redmon, A Farhadi - European conference on computer vision, 20165106회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks",
        "year": null
    },
    "Unsupervised Deep Embedding for Clustering Analysis": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.",
            "저널": "ICML",
            "저자": "Junyuan Xie, Ross Girshick, Ali Farhadi",
            "전체 인용횟수": "2864회 인용20162017201820192020202120222023961150289476586654616",
            "학술 문서": "Unsupervised deep embedding for clustering analysisJ Xie, R Girshick, A Farhadi - International conference on machine learning, 20162864회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Unsupervised Deep Embedding for Clustering Analysis",
        "year": null
    },
    "Describing objects by their attributes": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/6/20",
            "게시자": "IEEE",
            "설명": "We propose to shift the goal of recognition from naming to describing. Doing so allows us not only to name familiar objects, but also: to report unusual aspects of a familiar object (“spotty dog”, not just “dog”); to say something about unfamiliar objects (“hairy and four-legged”, not just “unknown”); and to learn how to recognize new objects with few or no visual examples. Rather than focusing on identity assignment, we make inferring attributes the core problem of recognition. These attributes can be semantic (“spotty”) or discriminative (“dogs have it but sheep do not”). Learning attributes presents a major new challenge: generalization across object categories, not just across instances within a category. In this paper, we also introduce a novel feature selection method for learning attributes that generalize well across categories. We support our claims by thorough evaluation that provides insights into the limitations of …",
            "저자": "Ali Farhadi, Ian Endres, Derek Hoiem, David Forsyth",
            "전체 인용횟수": "2378회 인용20092010201120122013201420152016201720182019202020212022202312487098146203218230210215195200203161118",
            "컨퍼런스": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on",
            "페이지": "1778-1785",
            "학술 문서": "Describing objects by their attributesA Farhadi, I Endres, D Hoiem, D Forsyth - 2009 IEEE conference on computer vision and pattern …, 20092373회 인용 관련 학술자료 전체 25개의 버전 Learning to describe objects*A Farhadi, I Endres, D Hoiem, D Forsyth - IEEE Computer Society Conference on Computer …, 20095회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Describing objects by their attributes",
        "year": null
    },
    "Bidirectional Attention Flow for Machine Comprehension": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.",
            "저널": "ICLR",
            "저자": "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi",
            "전체 인용횟수": "2193회 인용201620172018201920202021202220237131275472398363301227",
            "학술 문서": "Bidirectional attention flow for machine comprehensionM Seo, A Kembhavi, A Farhadi, H Hajishirzi - arXiv preprint arXiv:1611.01603, 20162193회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Bidirectional Attention Flow for Machine Comprehension",
        "year": null
    },
    "Every picture tells a story: Generating sentences from images": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010",
            "게시자": "Springer Berlin Heidelberg",
            "설명": " Humans can prepare concise descriptions of pictures, focusing on what they find important. We demonstrate that automatic methods can do so too. We describe a system that can compute a score linking an image to a sentence. This score can be used to attach a descriptive sentence to a given image, or to obtain images that illustrate a given sentence. The score is obtained by comparing an estimate of meaning obtained from the image to one obtained from the sentence. Each estimate of meaning comes from a discriminative procedure that is learned using data. We evaluate on a novel dataset consisting of human-annotated images. While our underlying estimate of meaning is impoverished, it is sufficient to produce very good quantitative results, evaluated with a novel score that can account for synecdoche.",
            "저자": "Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, David Forsyth",
            "전체 인용횟수": "1474회 인용20112012201320142015201620172018201920202021202220232123467698129159136170143155173112",
            "컨퍼런스": "Computer Vision–ECCV 2010: 11th European Conference on Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part IV 11",
            "페이지": "15-29",
            "학술 문서": "Every picture tells a story: Generating sentences from imagesA Farhadi, M Hejrati, MA Sadeghi, P Young… - Computer Vision–ECCV 2010: 11th European …, 20101474회 인용 관련 학술자료 전체 29개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Every picture tells a story: Generating sentences from images",
        "year": null
    },
    "Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/4/6",
            "설명": " Computer vision has a great potential to help our daily lives by searching for lost keys, watering flowers or reminding us to take a pill. To succeed with such tasks, computer vision methods need to be trained from real and diverse examples of our daily dynamic scenes. While most of such scenes are not particularly exciting, they typically do not appear on YouTube, in movies or TV broadcasts. So how do we collect sufficiently many diverse but boring samples representing our lives? We propose a novel Hollywood in Homes approach to collect such data. Instead of shooting videos in the lab, we ensure diversity by distributing and crowdsourcing the whole process of video creation from script writing to video recording and annotation. Following this procedure we collect a new dataset, Charades, with hundreds of people recording videos in their own homes, acting out casual everyday activities. The dataset is …",
            "저널": "ECCV",
            "저자": "Gunnar A Sigurdsson, Gül Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, Abhinav Gupta",
            "전체 인용횟수": "1163회 인용2016201720182019202020212022202333793124172248251221",
            "학술 문서": "Hollywood in homes: Crowdsourcing data collection for activity understandingGA Sigurdsson, G Varol, X Wang, A Farhadi, I Laptev… - Computer Vision–ECCV 2016: 14th European …, 20161163회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding",
        "year": null
    },
    "Defending against neural fake news": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "권": "32",
            "설명": "Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news.",
            "저널": "Advances in neural information processing systems",
            "저자": "Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi",
            "전체 인용횟수": "824회 인용2019202020212022202328152196192251",
            "학술 문서": "Defending against neural fake newsR Zellers, A Holtzman, H Rashkin, Y Bisk, A Farhadi… - Advances in neural information processing systems, 2019824회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Defending against neural fake news",
        "year": null
    },
    "From recognition to cognition: Visual commonsense reasoning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "Visual understanding goes well beyond object recognition. With one glance at an image, we can effortlessly imagine the world beyond the pixels: for instance, we can infer people's actions, goals, and mental states. While this task is easy for humans, it is tremendously difficult for today's vision systems, requiring higher-order cognition and commonsense reasoning about the world. We formalize this task as Visual Commonsense Reasoning. Given a challenging question about an image, a machine must answer correctly and then provide a rationale justifying its answer. Next, we introduce a new dataset, VCR, consisting of 290k multiple choice QA problems derived from 110k movie scenes. The key recipe for generating non-trivial and high-quality problems at scale is Adversarial Matching, a new approach to transform rich annotations into multiple choice questions with minimal bias. Experimental results show that while humans find VCR easy (over 90% accuracy), state-of-the-art vision models struggle (45%). To move towards cognition-level understanding, we present a new reasoning engine, Recognition to Cognition Networks (R2C), that models the necessary layered inferences for grounding, contextualization, and reasoning. R2C helps narrow the gap between humans and machines (65%); still, the challenge is far from solved, and we provide analysis that suggests avenues for future work.",
            "저자": "Rowan Zellers, Yonatan Bisk, Ali Farhadi, Yejin Choi",
            "전체 인용횟수": "729회 인용201820192020202120222023238106155218208",
            "컨퍼런스": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "페이지": "6720-6731",
            "학술 문서": "From recognition to cognition: Visual commonsense reasoningR Zellers, Y Bisk, A Farhadi, Y Choi - Proceedings of the IEEE/CVF conference on computer …, 2019729회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "From recognition to cognition: Visual commonsense reasoning",
        "year": null
    },
    "Ai2-thor: An interactive 3d environment for visual ai": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/12/14",
            "설명": "We introduce The House Of inteRactions (THOR), a framework for visual AI research, available at http://ai2thor.allenai.org. AI2-THOR consists of near photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes and interact with objects to perform tasks. AI2-THOR enables research in many different domains including but not limited to deep reinforcement learning, imitation learning, learning by interaction, planning, visual question answering, unsupervised representation learning, object detection and segmentation, and learning models of cognition. The goal of AI2-THOR is to facilitate building visually intelligent models and push the research forward in this domain.",
            "저널": "arXiv preprint arXiv:1712.05474",
            "저자": "Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, Aniruddha Kembhavi, Abhinav Gupta, Ali Farhadi",
            "전체 인용횟수": "670회 인용20172018201920202021202220232335985133170184",
            "학술 문서": "Ai2-thor: An interactive 3d environment for visual aiE Kolve, R Mottaghi, W Han, E VanderBilt, L Weihs… - arXiv preprint arXiv:1712.05474, 2017670회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Ai2-thor: An interactive 3d environment for visual ai",
        "year": null
    },
    "Recognition using visual phrases": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/6/20",
            "게시자": "IEEE",
            "설명": "In this paper we introduce visual phrases, complex visual composites like “a person riding a horse”. Visual phrases often display significantly reduced visual complexity compared to their component objects, because the appearance of those objects can change profoundly when they participate in relations. We introduce a dataset suitable for phrasal recognition that uses familiar PASCAL object categories, and demonstrate significant experimental gains resulting from exploiting visual phrases. We show that a visual phrase detector significantly outperforms a baseline which detects component objects and reasons about relations, even though visual phrase training sets tend to be smaller than those for objects. We argue that any multi-class detection system must decode detector outputs to produce final results; this is usually done with non-maximum suppression. We describe a novel decoding procedure that can …",
            "저자": "Mohammad Amin Sadeghi, Ali Farhadi",
            "전체 인용횟수": "538회 인용20112012201320142015201620172018201920202021202220236433847494757525449382911",
            "컨퍼런스": "CVPR 2011",
            "페이지": "1745-1752",
            "학술 문서": "Recognition using visual phrasesMA Sadeghi, A Farhadi - CVPR 2011, 2011538회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Recognition using visual phrases",
        "year": null
    },
    "Yolov3: an incremental improvement. 2018": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1804",
            "권": "20",
            "저널": "arXiv preprint arXiv:1804.02767",
            "저자": "Joseph Redmon, Ali Farhadi",
            "전체 인용횟수": "534회 인용20182019202020212022202352589117176119",
            "학술 문서": "Yolov3: an incremental improvement. 2018J Redmon, A Farhadi - arXiv preprint arXiv:1804.02767, 1804534회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Yolov3: an incremental improvement. 2018",
        "year": null
    },
    "Hellaswag: Can a machine really finish your sentence?": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/5/19",
            "설명": "Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as \"A woman sits at a piano,\" a machine must select the most likely followup: \"She sets her fingers on the keys.\" With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans (>95% accuracy), state-of-the-art models struggle (<48%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical 'Goldilocks' zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.",
            "저널": "arXiv preprint arXiv:1905.07830",
            "저자": "Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi",
            "전체 인용횟수": "522회 인용20192020202120222023207182133214",
            "학술 문서": "Hellaswag: Can a machine really finish your sentence?R Zellers, A Holtzman, Y Bisk, A Farhadi, Y Choi - arXiv preprint arXiv:1905.07830, 2019522회 인용 관련 학술자료 전체 5개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Hellaswag: Can a machine really finish your sentence?",
        "year": null
    },
    "Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": " As 3D movie viewing becomes mainstream and the Virtual Reality (VR) market emerges, the demand for 3D contents is growing rapidly. Producing 3D videos, however, remains challenging. In this paper we propose to use deep neural networks to automatically convert 2D videos and images to a stereoscopic 3D format. In contrast to previous automatic 2D-to-3D conversion algorithms, which have separate stages and need ground truth depth map as supervision, our approach is trained end-to-end directly on stereo pairs extracted from existing 3D movies. This novel training scheme makes it possible to exploit orders of magnitude more data and significantly increases performance. Indeed, Deep3D outperforms baselines in both quantitative and human subject evaluations.",
            "저자": "Junyuan Xie, Ross Girshick, Ali Farhadi",
            "전체 인용횟수": "475회 인용20162017201820192020202120222023820599986876642",
            "컨퍼런스": "ECCV",
            "학술 문서": "Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networksJ Xie, R Girshick, A Farhadi - Computer Vision–ECCV 2016: 14th European …, 2016475회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks",
        "year": null
    },
    "Understanding egocentric activities": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/11/6",
            "게시자": "IEEE",
            "설명": "We present a method to analyze daily activities, such as meal preparation, using video from an egocentric camera. Our method performs inference about activities, actions, hands, and objects. Daily activities are a challenging domain for activity recognition which are well-suited to an egocentric approach. In contrast to previous activity recognition methods, our approach does not require pre-trained detectors for objects and hands. Instead we demonstrate the ability to learn a hierarchical model of an activity by exploiting the consistent appearance of objects, hands, and actions that results from the egocentric context. We show that joint modeling of activities, actions, and objects leads to superior performance in comparison to the case where they are considered independently. We introduce a novel representation of actions based on object-hand interactions and experimentally demonstrate the superior performance …",
            "저자": "Alireza Fathi, Ali Farhadi, James M Rehg",
            "전체 인용횟수": "464회 인용2010201120122013201420152016201720182019202020212022202331141529566353503739482422",
            "컨퍼런스": "2011 international conference on computer vision",
            "페이지": "407-414",
            "학술 문서": "Understanding egocentric activitiesA Fathi, A Farhadi, JM Rehg - 2011 international conference on computer vision, 2011464회 인용 관련 학술자료 전체 18개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Understanding egocentric activities",
        "year": null
    },
    "Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020/2/15",
            "설명": "Fine-tuning pretrained contextual word embedding models to supervised downstream tasks has become commonplace in natural language processing. This process, however, is often brittle: even with the same hyperparameter values, distinct random seeds can lead to substantially different results. To better understand this phenomenon, we experiment with four datasets from the GLUE benchmark, fine-tuning BERT hundreds of times on each while varying only the random seeds. We find substantial performance increases compared to previously reported results, and we quantify how the performance of the best-found model varies as a function of the number of fine-tuning trials. Further, we examine two factors influenced by the choice of random seed: weight initialization and training data order. We find that both contribute comparably to the variance of out-of-sample performance, and that some weight initializations perform well across all tasks explored. On small datasets, we observe that many fine-tuning trials diverge part of the way through training, and we offer best practices for practitioners to stop training less promising runs early. We publicly release all of our experimental data, including training and validation scores for 2,100 trials, to encourage further analysis of training dynamics during fine-tuning.",
            "저널": "arXiv preprint arXiv:2002.06305",
            "저자": "Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, Noah Smith",
            "전체 인용횟수": "442회 인용202020212022202360103136140",
            "학술 문서": "Fine-tuning pretrained language models: Weight initializations, data orders, and early stoppingJ Dodge, G Ilharco, R Schwartz, A Farhadi, H Hajishirzi… - arXiv preprint arXiv:2002.06305, 2020442회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping",
        "year": null
    },
    "Image-to-image translation with conditional adversarial networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/7",
            "설명": "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without handengineering our loss functions either.",
            "저자": "Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A Efros",
            "전체 인용횟수": "20500회 인용2017201820192020202120222023371142725843457425843273907",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on",
            "학술 문서": "Image-to-image translation with conditional adversarial networksP Isola, JY Zhu, T Zhou, AA Efros - Proceedings of the IEEE conference on computer …, 201720393회 인용 관련 학술자료 전체 24개의 버전 Proceedings of the IEEE conference on computer vision and pattern recognition*P Isola, JY Zhu, T Zhou, AA Efros - Proceedings of the IEEE conference on computer …, 2017185회 인용 관련 학술자료 Efros Alexei A*I Phillip, Z Jun-Yan, Z Tinghui - Image-to-image translation with conditional adversarial …, 201771회 인용 관련 학술자료 Image-to-image translation with conditional adversarial networks. arXiv preprint (2017)*P Isola, JY Zhu, T Zhou, AA Efros - arXiv preprint arXiv:1611.07004, 201732회 인용 관련 학술자료 Image-to-image translation with conditional adversarial networks. arXiv e-prints*P Isola, JY Zhu, T Zhou, AA Efros - arXiv preprint arXiv:1611.07004, 201629회 인용 관련 학술자료 Image-to-image translation with conditional adversarial networks arXiv preprint*P Isola, JY Zhu, T Zhou, AA Efros - arXiv preprint arXiv:1611.07004, 201717회 인용 관련 학술자료 CVPR*P Isola, JY Zhu, T Zhou, AA Efros - 201713회 인용 관련 학술자료 Image-to-image translation with conditional adversarial networks. IEEE Conf on Computer Vision and Pattern Recognition*P Isola, JY Zhu, TH Zhou - 20174회 인용 관련 학술자료 Proc. CVPR*P Isola, JY Zhu, T Zhou, AA Efros - 20174회 인용 관련 학술자료 Image-to-image translation with conditional adversarial networks (2016). arXiv preprint*P Isola, JY Zhu, T Zhou, AA Efros - arXiv preprint arXiv:1611.070044회 인용 관련 학술자료 Image-to-image translation with conditional adversarial networks. arXiv. org, November 2016*P Isola, JY Zhu, T Zhou, AA Efros - arXiv preprint arXiv:1611.070044회 인용 관련 학술자료 Image-to-Image Translation with Conditional Adversarial Networks. 5967–5976*P Isola, JY Zhu, T Zhou, AA Efros - 20173회 인용 관련 학술자료 Alexei*P Isola, J Zhu, T Zhou - A. Efros,“Image-to-Image Translation with Conditional …, 20173회 인용 관련 학술자료 i Alexei A Efros*P Isola, JY Zhu, T Zhou - Image-to-image translation with conditional adversarial …, 20172회 인용 관련 학술자료 Jun-Yan. Zhu, Tinghui Zhou, and Alexei Efros. 2017. Image-to-Image Translation with Conditional Adversarial Networks*P Isola - Proceedings of the IEEE Conference on Computer …2회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Image-to-image translation with conditional adversarial networks",
        "year": null
    },
    "Unpaired image-to-image translation using cycle-consistent adversarial networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/10",
            "설명": "Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G: X-> Y such that the distribution of images from G (X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F: Y-> X and introduce a cycle consistency loss to push F (G (X))~ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.",
            "저자": "Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A Efros",
            "전체 인용횟수": "20332회 인용2017201820192020202120222023192115324183357436345404156",
            "컨퍼런스": "The IEEE International Conference on Computer Vision (ICCV), Proceedings of",
            "학술 문서": "Unpaired image-to-image translation using cycle-consistent adversarial networksJY Zhu, T Park, P Isola, AA Efros - Proceedings of the IEEE international conference on …, 201720125회 인용 관련 학술자료 전체 26개의 버전 Unpaired image-to-image translation using cycle-consistent adversarial networks*Z Jun-Yan, P Taesung, I Phillip, AE Alexei - Proceedings of the IEEE international conference on …, 2017237회 인용 관련 학술자료 Proceedings of the IEEE international conference on computer vision*JY Zhu, T Park, P Isola, AA Efros - 2017180회 인용 관련 학술자료 Unpaired image-to-image translation using cycle-consistent adversarial networks. CoRR abs/1703.10593 (2017)*J Zhu, T Park, P Isola, AA Efros - arXiv preprint arXiv:1703.10593, 201734회 인용 관련 학술자료 Research*JY Zhu, T Park, P Isola, AA Efros - Digital Divide” and Social Integration of the Elderly in …, 202121회 인용 관련 학술자료 Unpaired image-to-image translation using cycle-consistent adversarial networks in Proceedings of the IEEE international conference on computer vision. 2017*JY Zhu, T Park, P Isola, AA Efros - Italy2223–2232, 201715회 인용 관련 학술자료 Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. arXiv e-prints, art*JY Zhu, T Park, P Isola, AA Efros - arXiv preprint arXiv:1703.10593, 20179회 인용 관련 학술자료 Unpaired image-to-image translation using cycle-consistent adversarial networks. CoRR abs/1703.10593*J Zhu, T Park, P Isola, AA Efros - arXiv preprint arXiv:1703.10593, 20177회 인용 관련 학술자료 Computer Vision (ICCV), 2017 IEEE International Conference on*JY Zhu, T Park, P Isola, AA Efros - 20177회 인용 관련 학술자료 Proc. ICCV*JY Zhu, T Park, P Isola, AA Efros - Proc. ICCV, 19957회 인용 관련 학술자료 Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. ArXiv e-prints (March 2017)*JY Zhu, T Park, P Isola, AA Efros - arXiv preprint arXiv:1703.10593, 20174회 인용 관련 학술자료 Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint arXiv: 170310593*JY Zhu, T Park, P Isola, AA Efros - 20174회 인용 관련 학술자료 T. park, P*JY Zhu - Isola, and AA Efros,“Unpaired image-to-image …, 20174회 인용 관련 학술자료 CycleGAN*JY Zhu, T Park, P Isola, AA Efros - URL: https://github. com/junyanz/CycleGAN# failure …, 20174회 인용 관련 학술자료 Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv: 170310593 [cs].(2018)*JY Zhu, T Park, P Isola, AA Efros - 20193회 인용 관련 학술자료 CVPR*JY Zhu, T Park, P Isola, AA Efros - 20173회 인용 관련 학술자료 Unpaired image-to-image translation using cycle-consistent adversarial networks. CoRR*J Zhu, T Park, P Isola, AA Efros - arXiv preprint arXiv:1703.10593, 20173회 인용 관련 학술자료 arXiv preprint arXiv: 1703.10593*JY Zhu, T Park, P Isola, AA Efros - 20172회 인용 관련 학술자료 Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint [Google Scholar]*JY Zhu, T Park, P Isola, AA Efros - 20172회 인용 관련 학술자료 Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint, 2017.• Music-CDs & Vinyl, Music, Digital Music, Children’s Music*JY Zhu, T Park, P Isola, AA Efros - World Music, Electronic Music2회 인용 관련 학술자료 Unpaired image-to-image translation using cycle-consistent adversarial networks (2017). arXiv preprint*JY Zhu, T Park, P Isola, AA Efros - arXiv preprint arXiv:1703.105932회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
        "year": null
    },
    "The unreasonable effectiveness of deep features as a perceptual metric": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called``perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",
            "저자": "Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang",
            "전체 인용횟수": "6784회 인용20182019202020212022202369292626127118872610",
            "컨퍼런스": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "학술 문서": "The unreasonable effectiveness of deep features as a perceptual metricR Zhang, P Isola, AA Efros, E Shechtman, O Wang - Proceedings of the IEEE conference on computer …, 20186784회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The unreasonable effectiveness of deep features as a perceptual metric",
        "year": null
    },
    "Context Encoders: Feature Learning by Inpainting": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/6",
            "설명": "We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders--a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part (s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.",
            "저자": "Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, Alexei A Efros",
            "전체 인용횟수": "5776회 인용201620172018201920202021202220232721253073189411121220977",
            "컨퍼런스": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "학술 문서": "Context encoders: Feature learning by inpaintingD Pathak, P Krahenbuhl, J Donahue, T Darrell… - Proceedings of the IEEE conference on computer …, 20165759회 인용 관련 학술자료 전체 14개의 버전 Context encoders: Feature learning by inpainting. CoRR abs/1604.07379 (2016)*D Pathak, P Krähenbühl, J Donahue, T Darrell… - arXiv preprint arXiv:1604.07379, 20168회 인용 관련 학술자료 Proc. IEEE Conf. Computer Vision Pattern Recognition*D Pathak, P Krahenbuhl, J Donahue - 20166회 인용 관련 학술자료 Context encoders: Feature learning by inpainting. arXiv 2016*D Pathak, P Krahenbuhl, J Donahue, T Darrell… - arXiv preprint arXiv:1604.073795회 인용 관련 학술자료 Context Encoders: Feature Learning by Inpainting, CoRR*D Pathak, P Krahenbuhl, J Donahue, T Darrell… - arXiv preprint arXiv:1604.073795회 인용 관련 학술자료 Context encoders: Feature learning by inpainting: IEEE Conference on Computer Vision and Pattern Recognition*D Pathak, P Krahenbuhl, J Donahue, T Darrell… - 20163회 인용 관련 학술자료 Context Encoders: Feature Learning by Inpainting. CoRR, abs/1604.0*D Pathak, P Krahenbuhl, J Donahue, T Darrell… - 20162회 인용 관련 학술자료 Context encoders: feature learning by inpainting*J Donahue, T Darrell, D Pathak, P Krähenbühl… - 20162회 인용 관련 학술자료 l, and Alexei A*D Pathak, P Agrawal - 20162회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Context Encoders: Feature Learning by Inpainting",
        "year": null
    },
    "Texture synthesis by non-parametric sampling": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1999",
            "게시자": "Ieee",
            "권": "2",
            "설명": "A non-parametric method for texture synthesis is proposed. The texture synthesis process grows a new image outward from an initial seed, one pixel at a time. A Markov random field model is assumed, and the conditional distribution of a pixel given all its neighbors synthesized so far is estimated by querying the sample image and finding all similar neighborhoods. The degree of randomness is controlled by a single perceptually intuitive parameter. The method aims at preserving as much local structure as possible and produces good results for a wide variety of synthetic and real-world textures.",
            "저자": "Alexei A Efros, Thomas K Leung",
            "전체 인용횟수": "4632회 인용20002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202326457998150194204235207246237250225221261216226231227242202201173149",
            "컨퍼런스": "IEEE International Conference on Computer Vision (ICCV), 1999",
            "페이지": "1033-1038 vol. 2",
            "학술 문서": "Texture synthesis by non-parametric samplingAA Efros, TK Leung - Proceedings of the seventh IEEE international …, 19994624회 인용 관련 학술자료 전체 49개의 버전 Texture Synthesis by Non-parametric Sampling IEEE International Conference on Computer Vision (ICCV'99)*AA Efros, TK Leung - Corfu, Greece, September, 19996회 인용 관련 학술자료 IEEE Comput*AA Efros, TK Leung - Soc. Los Alamitos, CA, USA, 19996회 인용 관련 학술자료 ICCV*AA Efros, TK Leung - 19993회 인용 관련 학술자료 Th omas K Leung. Texture Synthesis by Non—paramettti Sampling*AA Efros - Computer Science Division University of California, 19993회 인용 관련 학술자료 Computer Vision, 1999. The Proceedings of the Seventh IEEE International Conference On*AA Efros, TK Leung - 19993회 인용 관련 학술자료 Proceedings of ICCV’99*AA Efros, TK Leung - 19992회 인용 관련 학술자료 A., Leung, T., K., 1999.“Texture synthesis by non-parametric sampling”*A Efros - IEEE International Conference on Computer Vision2회 인용 관련 학술자료 &amp; Leung, T.(1999) Texture synthesis by non-parametric sampling*A Efros - Proceedings of the international conference on …2회 인용 관련 학술자료 Texture Synthesis*A Efros1회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Texture synthesis by non-parametric sampling",
        "year": null
    },
    "Colorful Image Colorization": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/9",
            "게시자": "Springer International Publishing",
            "설명": " Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a “colorization Turing test,” asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32 % of the trials, significantly higher than …",
            "저자": "Richard Zhang, Phillip Isola, Alexei A Efros",
            "전체 인용횟수": "3777회 인용2016201720182019202020212022202329164279391573807883614",
            "컨퍼런스": "European Conference on Computer Vision",
            "페이지": "649-666",
            "학술 문서": "Colorful image colorizationR Zhang, P Isola, AA Efros - Computer Vision–ECCV 2016: 14th European …, 20163777회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Colorful Image Colorization",
        "year": null
    },
    "Image quilting for texture synthesis and transfer": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2023/8/1",
            "도서": "Seminal Graphics Papers: Pushing the Boundaries, Volume 2",
            "설명": "We present a simple image-based method of generating novel visual appearance in which a new image is synthesized by stitching together small patches of existing images. We call this process image quilting. First, we use quilting as a fast and very simple texture synthesis algorithm which produces surprisingly good results for a wide range of textures. Second, we extend the algorithm to perform texture transfer - rendering an object with a texture taken from a different object. More generally, we demonstrate how an image can be re-rendered in the style of a different image. The method works directly on the images and does not require 3D information.",
            "저자": "Alexei A Efros, William T Freeman",
            "전체 인용횟수": "3574회 인용20012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023125276126150179173161173185146175149158154148184174191199191192148",
            "페이지": "571-576",
            "학술 문서": "Image quilting for texture synthesis and transferAA Efros, WT Freeman - Seminal Graphics Papers: Pushing the Boundaries …, 20233574회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Image quilting for texture synthesis and transfer",
        "year": null
    },
    "Unsupervised visual representation learning by context prediction": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/12",
            "설명": "This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the R-CNN framework and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.",
            "저자": "Carl Doersch, Abhinav Gupta, Alexei A Efros",
            "전체 인용횟수": "2896회 인용201620172018201920202021202220237385144219440625709568",
            "컨퍼런스": "Proceedings of the IEEE International Conference on Computer Vision",
            "페이지": "1422-1430",
            "학술 문서": "Unsupervised visual representation learning by context predictionC Doersch, A Gupta, AA Efros - Proceedings of the IEEE international conference on …, 20152896회 인용 관련 학술자료 전체 19개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Unsupervised visual representation learning by context prediction",
        "year": null
    },
    "Unbiased look at dataset bias": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/6/20",
            "게시자": "IEEE",
            "설명": "Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech-101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set …",
            "저자": "Antonio Torralba, Alexei A Efros",
            "전체 인용횟수": "2683회 인용201120122013201420152016201720182019202020212022202384375118134150159184261311403423368",
            "컨퍼런스": "CVPR 2011",
            "페이지": "1521-1528",
            "학술 문서": "Unbiased look at dataset biasA Torralba, AA Efros - CVPR 2011, 20112673회 인용 관련 학술자료 전체 12개의 버전 Unbiased look at dataset bias*T Antonio, AA Efros - CVPR, 201124회 인용 관련 학술자료 Unbiased look at dataset bias. 42, 7 (2011), 1521--1528*A Torralba, AA Efros - Google Scholar Google Scholar Digital Library Digital …, 20113회 인용 관련 학술자료 Computer Vision and Pattern Recognition (CVPR)*T Antonio, A Efros Alexei - 2011 IEEE Conference. IEEE, 20112회 인용 관련 학술자료 et almbox. 2011. Unbiased look at dataset bias*A Torralba, AA Efros - IEEE conference on computer vision and pattern …2회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Unbiased look at dataset bias",
        "year": null
    },
    "Discovering objects and their location in images": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/10/17",
            "게시자": "IEEE",
            "권": "1",
            "설명": "We seek to discover the object categories depicted in a set of unlabelled images. We achieve this using a model developed in the statistical text literature: probabilistic latent semantic analysis (pLSA). In text analysis, this is used to discover topics in a corpus using the bag-of-words document representation. Here we treat object categories as topics, so that an image containing instances of several categories is modeled as a mixture of topics. The model is applied to images by using a visual analogue of a word, formed by vector quantizing SIFT-like region descriptors. The topic discovery approach successfully translates to the visual domain: for a small set of objects, we show that both the object categories and their approximate spatial layout are found without supervision. Performance of this unsupervised method is compared to the supervised approach of Fergus et al. (2003) on a set of unseen images containing …",
            "저자": "Josef Sivic, Bryan C Russell, Alexei A Efros, Andrew Zisserman, William T Freeman",
            "전체 인용횟수": "1911회 인용20042005200620072008200920102011201220132014201520162017201820192020202120222023516741221491892021801641521381139272495539271916",
            "컨퍼런스": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1",
            "페이지": "370-377",
            "학술 문서": "Discovering objects and their location in imagesJ Sivic, BC Russell, AA Efros, A Zisserman… - Tenth IEEE International Conference on Computer …, 20051308회 인용 관련 학술자료 전체 39개의 버전 Discovering object categories in image collections*J Sivic, BC Russell, AA Efros, A Zisserman… - 2005629회 인용 관련 학술자료 전체 18개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Discovering objects and their location in images",
        "year": null
    },
    "Toward Multimodal Image-to-Image Translation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/12",
            "설명": "Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a distribution of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.",
            "저자": "Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, Eli Shechtman",
            "전체 인용횟수": "1601회 인용20182019202020212022202390258317369298258",
            "컨퍼런스": "Advances in Neural Information Processing Systems",
            "페이지": "465-476",
            "학술 문서": "Toward multimodal image-to-image translationJY Zhu, R Zhang, D Pathak, T Darrell, AA Efros… - Advances in neural information processing systems, 20171601회 인용 관련 학술자료 전체 7개의 버전 Efros Alexei A, Wang Oliver, and Shechtman Eli*Z Jun-Yan, Z Richard, P Deepak - Toward multimodal image-to-image translation. In …, 20174회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Toward Multimodal Image-to-Image Translation",
        "year": null
    },
    "Generative visual manipulation on the natural image manifold": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "게시자": "Springer International Publishing",
            "설명": " Realistic image manipulation is challenging because it requires modifying the image appearance in a user-controlled way, while preserving the realism of the result. Unless the user has considerable artistic skill, it is easy to “fall off” the manifold of natural images while editing. In this paper, we propose to learn the natural image manifold directly from data using a generative adversarial neural network. We then define a class of image editing operations, and constrain their output to lie on that learned manifold at all times. The model automatically adjusts the output keeping all edits as realistic as possible. All our manipulations are expressed in terms of constrained optimization and are applied in near-real time. We evaluate our algorithm on the task of realistic photo manipulation of shape and color. The presented method can further be used for changing one image to look like the other, as well as generating …",
            "저자": "Jun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman, Alexei A Efros",
            "전체 인용횟수": "1471회 인용2016201720182019202020212022202312101218216246246221193",
            "컨퍼런스": "Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14",
            "페이지": "597-613",
            "학술 문서": "Generative visual manipulation on the natural image manifoldJY Zhu, P Krähenbühl, E Shechtman, AA Efros - Computer Vision–ECCV 2016: 14th European …, 20161468회 인용 관련 학술자료 전체 8개의 버전 Generative Visual Manipulation on the Natural Image Manifold. arXiv 2018*JY Zhu, P Krähenbühl, E Shechtman, AA Efros - arXiv preprint arXiv:1609.035524회 인용 관련 학술자료 ECCV*JY Zhu, P Krähenbühl, E Shechtman, AA Efros - 20163회 인용 관련 학술자료 Generative visual manipulation on the natural image manifold (2016)*JY Zhu, P Krähenbühl, E Shechtman, A Efros - Google Scholar, 20163회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Generative visual manipulation on the natural image manifold",
        "year": null
    },
    "Scene completion using millions of photographs": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2007/7/29",
            "게시자": "ACM",
            "권": "26",
            "설명": "What can you do with a million images? In this paper we present a new image completion algorithm powered by a huge database of photographs gathered from the Web. The algorithm patches up holes in images by finding similar image regions in the database that are not only seamless but also semantically valid. Our chief insight is that while the space of images is effectively infinite, the space of semantically differentiable scenes is actually not that large. For many image completion tasks we are able to find similar scenes which contain image fragments that will convincingly complete the image. Our algorithm is entirely data-driven, requiring no annotations or labelling by the user. Unlike existing image completion methods, our algorithm can generate a diverse set of results for each input image and we allow users to select among them. We demonstrate the superiority of our algorithm over existing image …",
            "저널": "ACM Transactions on Graphics (ToG)",
            "저자": "James Hays, Alexei A Efros",
            "전체 인용횟수": "1215회 인용20062007200820092010201120122013201420152016201720182019202020212022202341051757678897880677265908870608651",
            "페이지": "4-es",
            "학술 문서": "Scene completion using millions of photographsJ Hays, AA Efros - ACM Transactions on Graphics (ToG), 20071215회 인용 관련 학술자료 전체 13개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Scene completion using millions of photographs",
        "year": null
    },
    "Ensemble of Exemplar-SVMs for Object Detection and Beyond": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/11/6",
            "게시자": "IEEE",
            "설명": "This paper proposes a conceptually simple but surprisingly powerful method which combines the effectiveness of a discriminative object detector with the explicit correspondence offered by a nearest-neighbor approach. The method is based on training a separate linear SVM classifier for every exemplar in the training set. Each of these Exemplar-SVMs is thus defined by a single positive instance and millions of negatives. While each detector is quite specific to its exemplar, we empirically observe that an ensemble of such Exemplar-SVMs offers surprisingly good generalization. Our performance on the PASCAL VOC detection task is on par with the much more complex latent part-based model of Felzenszwalb et al., at only a modest computational cost increase. But the central benefit of our approach is that it creates an explicit association between each detection and a single training exemplar. Because most …",
            "저자": "Tomasz Malisiewicz, Abhinav Gupta, Alexei A Efros",
            "전체 인용횟수": "1170회 인용201020112012201320142015201620172018201920202021202220233105497124163142120967056737651",
            "컨퍼런스": "International Conference on Computer Vision (ICCV)",
            "페이지": "89-96",
            "학술 문서": "Ensemble of exemplar-svms for object detection and beyondT Malisiewicz, A Gupta, AA Efros - 2011 International conference on computer vision, 20111168회 인용 관련 학술자료 전체 15개의 버전 a.(2011, novembre). Ensemble of exemplar-SVMs for object detection and beyond*T Malisiewicz, A Gupta, A Efros - Ieee international conference on computer vision2회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Ensemble of Exemplar-SVMs for Object Detection and Beyond",
        "year": null
    },
    "IM2GPS: estimating geographic information from a single image": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008/6/23",
            "게시자": "IEEE",
            "설명": "Estimating geographic information from an image is an excellent, difficult high-level computer vision problem whose time has come. The emergence of vast amounts of geographically-calibrated image data is a great reason for computer vision to start looking globally - on the scale of the entire planet! In this paper, we propose a simple algorithm for estimating a distribution over geographic locations from a single image using a purely data-driven scene matching approach. For this task, we leverage a dataset of over 6 million GPS-tagged images from the Internet. We represent the estimated image location as a probability distribution over the Earthpsilas surface. We quantitatively evaluate our approach in several geolocation tasks and demonstrate encouraging performance (up to 30 times better than chance). We show that geolocation estimates can provide the basis for numerous other image understanding tasks …",
            "저자": "James Hays, Alexei A Efros",
            "전체 인용횟수": "1116회 인용2007200820092010201120122013201420152016201720182019202020212022202348426472801011031479172815348453838",
            "컨퍼런스": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "페이지": "1-8",
            "학술 문서": "Im2gps: estimating geographic information from a single imageJ Hays, AA Efros - 2008 ieee conference on computer vision and pattern …, 20081109회 인용 관련 학술자료 전체 13개의 버전 Im2gps: estimating geographic images from single images*J Hays, A Efros - Computer Vision and Pattern Recognition (CVPR), 20088회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "IM2GPS: estimating geographic information from a single image",
        "year": null
    },
    "Automatic photo pop-up": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/7/1",
            "도서": "ACM SIGGRAPH 2005 Papers",
            "설명": "This paper presents a fully automatic method for creating a 3D model from a single photograph. The model is made up of several texture-mapped planar billboards and has the complexity of a typical children's pop-up book illustration. Our main insight is that instead of attempting to recover precise geometry, we statistically model geometric classes defined by their orientations in the scene. Our algorithm labels regions of the input image into coarse categories: \"ground\", \"sky\", and \"vertical\". These labels are then used to \"cut and fold\" the image into a pop-up model using a set of simple assumptions. Because of the inherent ambiguity of the problem and the statistical nature of the approach, the algorithm is not expected to work on every image. However. it performs surprisingly well for a wide range of scenes taken from a typical person's photo album.",
            "저자": "Derek Hoiem, Alexei A Efros, Martial Hebert",
            "전체 인용횟수": "956회 인용20052006200720082009201020112012201320142015201620172018201920202021202220237283446614964576260606358725238404832",
            "페이지": "577-584",
            "학술 문서": "Automatic photo pop-upD Hoiem, AA Efros, M Hebert - ACM SIGGRAPH 2005 Papers, 2005951회 인용 관련 학술자료 전체 14개의 버전 Automatic Pop Up*D Hoiem, AA Efros, M Hebert - Robotics Institue, Carnegie Mellon University …, 20053회 인용 관련 학술자료 Alexei a*D Hoiem - Efros and Martial Hebert. Automatic Photo Pop-up …, 20052회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Automatic photo pop-up",
        "year": null
    },
    "Geometric context from a single image": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/10/17",
            "게시자": "IEEE",
            "권": "1",
            "설명": "Many computer vision algorithms limit their performance by ignoring the underlying 3D geometric structure in the image. We show that we can estimate the coarse geometric properties of a scene by learning appearance-based models of geometric classes, even in cluttered natural scenes. Geometric classes describe the 3D orientation of an image region with respect to the camera. We provide a multiple-hypothesis framework for robustly estimating scene structure from a single image and obtaining confidences for each geometric label. These confidences can then be used to improve the performance of many other applications. We provide a thorough quantitative evaluation of our algorithm on a set of outdoor images and demonstrate its usefulness in two applications: object detection and automatic single-view reconstruction.",
            "저자": "Derek Hoiem, Alexei A Efros, Martial Hebert",
            "전체 인용횟수": "953회 인용2004200520062007200820092010201120122013201420152016201720182019202020212022202363273251626974766483576654394846293016",
            "컨퍼런스": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1",
            "페이지": "654-661",
            "학술 문서": "Geometric context from a single imageD Hoiem, AA Efros, M Hebert - Tenth IEEE International Conference on Computer …, 2005953회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Geometric context from a single image",
        "year": null
    },
    "Learning deep features for discriminative localization": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them.",
            "저자": "Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba",
            "전체 인용횟수": "9708회 인용20162017201820192020202120222023362295419741454200522862075",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "2921-2929",
            "학술 문서": "Learning deep features for discriminative localizationB Zhou, A Khosla, A Lapedriza, A Oliva, A Torralba - Proceedings of the IEEE conference on computer …, 20169706회 인용 관련 학술자료 전체 23개의 버전 AudeOliva, and Antonio Torralba,“*B Zhou, A Khosla, A Lapedriza - … deep featuresfor discriminative localization,” in CVPR, 20164회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning deep features for discriminative localization",
        "year": null
    },
    "Modeling the shape of the scene: A holistic representation of the spatial envelope": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2001",
            "권": "42",
            "설명": " In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimensional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a …",
            "저널": "International Journal of Computer Vision",
            "저자": "Aude Oliva, Antonio Torralba",
            "전체 인용횟수": "8249회 인용2003200420052006200720082009201020112012201320142015201620172018201920202021202220236149525688123235332417495600716753795734648571424420350227",
            "페이지": "145-175",
            "학술 문서": "Modeling the shape of the scene: A holistic representation of the spatial envelopeA Oliva, A Torralba - International journal of computer vision, 20018249회 인용 관련 학술자료 전체 27개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Modeling the shape of the scene: A holistic representation of the spatial envelope",
        "year": null
    },
    "Labelme: a database and web-based tool for image annotation. Int": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2007/10/31",
            "권": "77",
            "저널": "International Journal of Computer Vision",
            "저자": "B Russell, Antonio Torralba, K Murphy, W Freeman",
            "전체 인용횟수": "4260회 인용200620072008200920102011201220132014201520162017201820192020202120222023224996177210228219276242245275260233271302333398342",
            "페이지": "157–173",
            "학술 문서": "LabelMe: a database and web-based tool for image annotation*BC Russell, A Torralba, KP Murphy, WT Freeman - International journal of computer vision, 20084255회 인용 관련 학술자료 전체 30개의 버전 Labelme: a database and web-based tool for image annotation. IntB Russell, A Torralba, K Murphy, W Freeman - Journal of Computer Vision, 20076회 인용 관련 학술자료 KevinP. Murphy and WilliamT*BC Russell, A Torralba - 20082회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Labelme: a database and web-based tool for image annotation. Int",
        "year": null
    },
    "Places: A 10 million image database for scene recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/7/4",
            "게시자": "IEEE",
            "권": "40",
            "설명": "The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification performance at tasks such as visual object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories, comprising a large and diverse list of the types of environments encountered in the world. Using the state-of-the-art Convolutional Neural Networks (CNNs), we provide scene classification CNNs (Places-CNNs) as baselines, that significantly outperform the previous approaches. Visualization of the CNNs trained on Places shows that object detectors emerge as an intermediate representation of scene classification. With its high-coverage and high-diversity of exemplars, the Places Database along with the Places-CNNs offer a novel resource to guide future progress on scene …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, Antonio Torralba",
            "전체 인용횟수": "3722회 인용201720182019202020212022202337230437551761886802",
            "페이지": "1452-1464",
            "학술 문서": "Places: A 10 million image database for scene recognitionB Zhou, A Lapedriza, A Khosla, A Oliva, A Torralba - IEEE transactions on pattern analysis and machine …, 20173722회 인용 관련 학술자료 전체 11개의 버전 ",
            "호": "6"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Places: A 10 million image database for scene recognition",
        "year": null
    },
    "Learning deep features for scene recognition using places database": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "권": "27",
            "설명": "Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks.",
            "저널": "Advances in neural information processing systems",
            "저자": "Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, Aude Oliva",
            "전체 인용횟수": "3540회 인용201420152016201720182019202020212022202320194419586513507398361266197",
            "학술 문서": "Learning deep features for scene recognition using places databaseB Zhou, A Lapedriza, J Xiao, A Torralba, A Oliva - Advances in neural information processing systems, 20143540회 인용 관련 학술자료 전체 27개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning deep features for scene recognition using places database",
        "year": null
    },
    "Sun database: Large-scale scene recognition from abbey to zoo": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/6/13",
            "게시자": "IEEE",
            "설명": "Scene categorization is a fundamental problem in computer vision. However, scene understanding research has been constrained by the limited scope of currently-used databases which do not capture the full variety of scene categories. Whereas standard databases for object categorization contain hundreds of different classes of objects, the largest available dataset of scene categories contains only 15 classes. In this paper we propose the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images. We use 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance. We measure human scene classification performance on the SUN database and compare this with computational methods. Additionally, we study a finer-grained scene representation to detect scenes embedded inside of larger …",
            "저자": "Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, Antonio Torralba",
            "전체 인용횟수": "3506회 인용20092010201120122013201420152016201720182019202020212022202391258100175225267330281268327284307362436",
            "컨퍼런스": "2010 IEEE computer society conference on computer vision and pattern recognition",
            "페이지": "3485-3492",
            "학술 문서": "Sun database: Large-scale scene recognition from abbey to zooJ Xiao, J Hays, KA Ehinger, A Oliva, A Torralba - 2010 IEEE computer society conference on computer …, 20103506회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Sun database: Large-scale scene recognition from abbey to zoo",
        "year": null
    },
    "Skip-thought vectors": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "권": "28",
            "설명": "We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.",
            "저널": "Advances in neural information processing systems",
            "저자": "Ryan Kiros, Yukun Zhu, Russ R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, Sanja Fidler",
            "전체 인용횟수": "3002회 인용20152016201720182019202020212022202336178309484569496394299209",
            "학술 문서": "Skip-thought vectorsR Kiros, Y Zhu, RR Salakhutdinov, R Zemel, R Urtasun… - Advances in neural information processing systems, 20152966회 인용 관련 학술자료 전체 19개의 버전 Skip-thought vectors*K Ryan, Z Yukun, S Ruslan, Z Richard, U Raquel… - Advances in neural information processing systems, 201549회 인용 관련 학술자료 Skip-thought vectors (2015)*R Kiros, Y Zhu, R Salakhutdinov, RS Zemel, A Torralba… - arXiv preprint arXiv:1506.06726, 202013회 인용 관련 학술자료 Skip-thought vectors*A Torralba, S Fidler - Advances in neural, 20157회 인용 관련 학술자료 Skip-thought vectors. CoRR abs/1506.06726 (2015)*R Kiros, Y Zhu, R Salakhutdinov, RS Zemel, A Torralba… - arXiv preprint arXiv:1506.06726, 20156회 인용 관련 학술자료 Skip-thought vectors. CoRR abs/1506.06726*R Kiros, Y Zhu, R Salakhutdinov, RS Zemel, A Torralba… - arXiv preprint arXiv:1506.06726, 20156회 인용 관련 학술자료 Skip-Thought Vectors. arXiv e-prints, page*R Kiros, Y Zhu, R Salakhutdinov, RS Zemel, A Torralba… - arXiv preprint arXiv:1506.06726, 20154회 인용 관련 학술자료 Skip-Thought Vectors NIPS. 3294--3302*R Kiros, Y Zhu, R Salakhutdinov, R Zemel, R Urtasun… - Google Scholar Google Scholar Digital Library Digital …, 20152회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Skip-thought vectors",
        "year": null
    },
    "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in the current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.",
            "저자": "Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler",
            "전체 인용횟수": "2705회 인용20152016201720182019202020212022202375568102254440526687548",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "19-27",
            "학술 문서": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading booksY Zhu, R Kiros, R Zemel, R Salakhutdinov, R Urtasun… - Proceedings of the IEEE international conference on …, 20152678회 인용 관련 학술자료 전체 21개의 버전 Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books. arXiv 2015*Y Zhu, R Kiros, R Zemel, R Salakhutdinov, R Urtasun… - arXiv preprint arXiv:1506.0672433회 인용 관련 학술자료 Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books. June 2015*Y Zhu, R Kiros, R Zemel, R Salakhutdinov, R Urtasun… - arXiv preprint arXiv:1506.06724, 20156회 인용 관련 학술자료 Aligning books and movies: towards story-like visual explanations by watching movies and reading books. arXiv e-prints, page*Y Zhu, R Kiros, R Zemel, R Salakhutdinov, R Urtasun… - arXiv preprint arXiv:1506.06724, 20154회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
        "year": null
    },
    "Scene parsing through ade20k dataset": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "Scene parsing, or recognizing and segmenting objects and stuff in an image, is one of the key problems in computer vision. Despite the community's efforts in data collection, there are still few image datasets covering a wide range of scenes and object categories with dense and detailed annotations for scene parsing. In this paper, we introduce and analyze the ADE20K dataset, spanning diverse annotations of scenes, objects, parts of objects, and in some cases even parts of parts. A scene parsing benchmark is built upon the ADE20K with 150 object and stuff classes included. Several segmentation baseline models are evaluated on the benchmark. A novel network design called Cascade Segmentation Module is proposed to parse a scene into stuff, objects, and object parts in a cascade and improve over the baselines. We further show that the trained scene parsing networks can lead to applications such as image content removal and scene synthesis (Dataset and pretrained models are available at http://groups. csail. mit. edu/vision/datasets/ADE20K/).",
            "저자": "Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, Antonio Torralba",
            "전체 인용횟수": "2546회 인용201720182019202020212022202322125267357458595709",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "633-641",
            "학술 문서": "Scene parsing through ade20k datasetB Zhou, H Zhao, X Puig, S Fidler, A Barriuso… - Proceedings of the IEEE conference on computer …, 20172538회 인용 관련 학술자료 전체 12개의 버전 Scene parsing through ade20k dataset*Z Bolei, Z Hang, P Xavier, F Sanja, B Adela, T Antonio - Proceedings of the IEEE conference on computer …, 201727회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Scene parsing through ade20k dataset",
        "year": null
    },
    "Learning to predict where humans look": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/9/29",
            "게시자": "IEEE",
            "설명": "For many applications in graphics, design, and human computer interaction, it is essential to understand where humans look in a scene. Where eye tracking devices are not a viable option, models of saliency can be used to predict fixation locations. Most saliency approaches are based on bottom-up computation that does not consider top-down image semantics and often does not match actual eye movements. To address this problem, we collected eye tracking data of 15 viewers on 1003 images and use this database as training and testing examples to learn a model of saliency based on low, middle and high-level image features. This large database of eye tracking data is publicly available with this paper.",
            "저자": "Tilke Judd, Krista Ehinger, Frédo Durand, Antonio Torralba",
            "전체 인용횟수": "2463회 인용201020112012201320142015201620172018201920202021202220232674115170231231250269237215193158149107",
            "컨퍼런스": "2009 IEEE 12th international conference on computer vision",
            "페이지": "2106-2113",
            "학술 문서": "Learning to predict where humans lookT Judd, K Ehinger, F Durand, A Torralba - 2009 IEEE 12th international conference on computer …, 20092462회 인용 관련 학술자료 전체 21개의 버전 Learning to predict where humans look, 2009*T Judd, K Ehinger, F Durand, A Torralba - Computer Vision, IEEE 12th international conference …, 20093회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning to predict where humans look",
        "year": null
    },
    "Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search.": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006/10",
            "게시자": "American Psychological Association",
            "권": "113",
            "설명": "Many experiments have shown that the human visual system makes extensive use of contextual information for facilitating object search in natural scenes. However, the question of how to formally model contextual influences is still open. On the basis of a Bayesian framework, the authors present an original approach of attentional guidance by global scene context. The model comprises 2 parallel pathways; one pathway computes local features (saliency) and the other computes global (scene-centered) features. The contextual guidance model of attention combines bottom-up saliency, scene context, and top-down mechanisms at an early stage of visual processing and predicts the image regions likely to be fixated by human observers performing natural search tasks in real-world scenes.(PsycINFO Database Record (c) 2016 APA, all rights reserved)",
            "저널": "Psychological review",
            "저자": "Antonio Torralba, Aude Oliva, Monica S Castelhano, John M Henderson",
            "전체 인용횟수": "2125회 인용200720082009201020112012201320142015201620172018201920202021202220233690123119141144180152139166183117148106987966",
            "페이지": "766",
            "학술 문서": "Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search.A Torralba, A Oliva, MS Castelhano, JM Henderson - Psychological review, 20062125회 인용 관련 학술자료 전체 25개의 버전 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search.",
        "year": null
    },
    "Sift flow: Dense correspondence across scenes and its applications": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/8/19",
            "게시자": "IEEE",
            "권": "33",
            "설명": "While image alignment has been studied in different areas of computer vision for decades, aligning images depicting different scenes remains a challenging problem. Analogous to optical flow, where an image is aligned to its temporally adjacent frame, we propose SIFT flow, a method to align an image to its nearest neighbors in a large image corpus containing a variety of scenes. The SIFT flow algorithm consists of matching densely sampled, pixelwise SIFT features between two images while preserving spatial discontinuities. The SIFT features allow robust matching across different scene/object appearances, whereas the discontinuity-preserving spatial model allows matching of objects located at different parts of the scene. Experiments show that the proposed approach robustly aligns complex scene pairs containing significant spatial differences. Based on SIFT flow, we propose an alignment-based large …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Ce Liu, Jenny Yuen, Antonio Torralba",
            "전체 인용횟수": "1968회 인용2009201020112012201320142015201620172018201920202021202220238225709713817224122021522515913713883",
            "페이지": "978-994",
            "학술 문서": "Sift flow: Dense correspondence across scenes and its applicationsC Liu, J Yuen, A Torralba - IEEE transactions on pattern analysis and machine …, 20101968회 인용 관련 학술자료 전체 19개의 버전 ",
            "호": "5"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Sift flow: Dense correspondence across scenes and its applications",
        "year": null
    },
    "Building the gist of a scene: The role of global image features in recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006/1/1",
            "게시자": "Elsevier",
            "권": "155",
            "설명": "Humans can recognize the gist of a novel image in a single glance, independent of its complexity. How is this remarkable feat accomplished? On the basis of behavioral and computational evidence, this paper describes a formal approach to the representation and the mechanism of scene gist understanding, based on scene-centered, rather than object-centered primitives. We show that the structure of a scene image can be estimated by the mean of global image features, providing a statistical summary of the spatial layout properties (Spatial Envelope representation) of the scene. Global features are based on configurations of spatial scales and are estimated without invoking segmentation or grouping operations. The scene-centered approach is not an alternative to local image analysis but would serve as a feed-forward and parallel pathway of visual processing, able to quickly constrain local feature analysis and …",
            "저자": "Aude Oliva, Antonio Torralba",
            "전체 인용횟수": "1881회 인용2006200720082009201020112012201320142015201620172018201920202021202220235324558909711511516518113117512011313210310573",
            "출처": "Progress in brain research",
            "페이지": "23-36",
            "학술 문서": "Building the gist of a scene: The role of global image features in recognitionA Oliva, A Torralba - Progress in brain research, 20061881회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Building the gist of a scene: The role of global image features in recognition",
        "year": null
    },
    "Recognizing indoor scenes": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/6/20",
            "게시자": "IEEE",
            "설명": "Indoor scene recognition is a challenging open problem in high level vision. Most scene recognition models that work well for outdoor scenes perform poorly in the indoor domain. The main difficulty is that while some indoor scenes (e.g. corridors) can be well characterized by global spatial properties, others (e.g, bookstores) are better characterized by the objects they contain. More generally, to address the indoor scenes recognition problem we need a model that can exploit local and global discriminative information. In this paper we propose a prototype based model that can successfully combine both sources of information. To test our approach we created a dataset of 67 indoor scenes categories (the largest available) covering a wide range of domains. The results show that our approach can significantly outperform a state of the art classifier for the task.",
            "저자": "Ariadna Quattoni, Antonio Torralba",
            "전체 인용횟수": "1825회 인용2009201020112012201320142015201620172018201920202021202220231035427786104155177156160147160168172140",
            "컨퍼런스": "2009 IEEE conference on computer vision and pattern recognition",
            "페이지": "413-420",
            "학술 문서": "Recognizing indoor scenesA Quattoni, A Torralba - 2009 IEEE conference on computer vision and pattern …, 20091825회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Recognizing indoor scenes",
        "year": null
    },
    "Sharing features: efficient boosting procedures for multiclass object detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004",
            "설명": "We consider the problem of detecting a large number of different object classes in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, which can be slow and require much training data. We present a multi-class boosting procedure (joint boosting) that reduces both the computational and sample complexity, by finding common features that can be shared across the classes. The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required is observed to scale approximately logarithmically with the number of classes. In addition, we find that the features selected by independently trained classifiers are often specific to the class, whereas the features selected by the jointly trained classifiers are more generic features, such as lines and edges.",
            "저널": "Proc. of IEEE Conf. on CVPR",
            "저자": "Antonio Torralba, K Murphy, W Freeman",
            "전체 인용횟수": "1766회 인용20032004200520062007200820092010201120122013201420152016201720182019202020212022202369599910512812917214315514812811010068504029213312",
            "학술 문서": "Sharing visual features for multiclass and multiview object detection*A Torralba, KP Murphy, WT Freeman - IEEE Transactions on Pattern Analysis and Machine …, 2007973회 인용 관련 학술자료 전체 33개의 버전 Sharing features: efficient boosting procedures for multiclass object detectionA Torralba, KP Murphy, WT Freeman - Proceedings of the 2004 IEEE Computer Society …, 2004852회 인용 관련 학술자료 전체 26개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Sharing features: efficient boosting procedures for multiclass object detection",
        "year": null
    },
    "Generating videos with scene dynamics": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "권": "29",
            "설명": "We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (eg action classification) and video generation tasks (eg future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation.",
            "저널": "Advances in neural information processing systems",
            "저자": "Carl Vondrick, Hamed Pirsiavash, Antonio Torralba",
            "전체 인용횟수": "1586회 인용2016201720182019202020212022202312103231263293239255162",
            "학술 문서": "Generating videos with scene dynamicsC Vondrick, H Pirsiavash, A Torralba - Advances in neural information processing systems, 20161580회 인용 관련 학술자료 전체 17개의 버전 Generating videos with scene dynamics (2016)*C Vondrick, H Pirsiavash, A Torralba - arXiv preprint arXiv:1609.02612, 20164회 인용 관련 학술자료 Generating videos with scene dynamics. CoRR abs/1609.02612 (2016)*C Vondrick, H Pirsiavash, A Torralba - arXiv preprint arXiv:1609.02612, 20164회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Generating videos with scene dynamics",
        "year": null
    },
    "Network dissection: Quantifying interpretability of deep visual representations": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a data set of concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are labeled across a broad range of visual concepts including objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability is an axis-independent property of the representation space, then we apply the method to compare the latent representations of various networks when trained to solve different classification problems. We further analyze the effect of training iterations, compare networks trained with different initializations, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power",
            "저자": "David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, Antonio Torralba",
            "전체 인용횟수": "1480회 인용201720182019202020212022202332159190254278304253",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "6541-6549",
            "학술 문서": "Network dissection: Quantifying interpretability of deep visual representationsD Bau, B Zhou, A Khosla, A Oliva, A Torralba - Proceedings of the IEEE conference on computer …, 20171472회 인용 관련 학술자료 전체 15개의 버전 Network Dissection: Quantifying Interpretability of Deep Visual Representations, 2017*D Bau, B Zhou, A Khosla, A Oliva, A Torralba - URL http://openaccess. thecvf. com/content_cvpr_2017 …, 20179회 인용 관련 학술자료 Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*D Bau, B Zhou, A Khosla, A Oliva, A Torralba - 20174회 인용 관련 학술자료 Network dissection: Quantifying interpretability of deep visual representations. Computer Vision and Pattern Recognition (CVPR)*D Bau, B Zhou, A Khosla, A Oliva, A Torralba - 2017 IEEE Conference on, 20173회 인용 관련 학술자료 Network dissection: quantifying interpretability of deep visual representations. arXiv e-prints*D Bau, B Zhou, A Khosla, A Oliva, A Torralba - arXiv preprint arXiv:1704.05796, 20172회 인용 관련 학술자료 Network Dissection: Quantifying Interpretability of Deep Visual Representation*B Zhou, A Oliva, A Torralba - Proceedings of the IEEE/CVF Conference on Computer …, 20171회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Network dissection: Quantifying interpretability of deep visual representations",
        "year": null
    },
    "The flipped voltage follower: A useful cell for low-voltage low-power circuit design": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2002/5/26",
            "게시자": "IEEE",
            "권": "3",
            "설명": "In this paper a new basic cell for low-power and/or low-voltage operation is identified. It is shown that different versions of this cell, called \"flipped voltage follower\", have been used in the past for different applications. New circuits using this cell are also proposed here.",
            "저자": "J Ramirez-Angulo, RG Carvajal, A Torralba, JAGJ Galan, AP Vega-Leal, JATJ Tombs",
            "전체 인용횟수": "190회 인용20022003200420052006200720082009201020112012201320142015201620172018201920202021202220235112429161814748633345447572",
            "컨퍼런스": "2002 IEEE International Symposium on Circuits and Systems. Proceedings (Cat. No. 02CH37353)",
            "페이지": "III-III",
            "학술 문서": "The flipped voltage follower: A useful cell for low-voltage low-power circuit designJ Ramirez-Angulo, RG Carvajal, A Torralba, J Galan… - 2002 IEEE International Symposium on Circuits and …, 2002190회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The flipped voltage follower: A useful cell for low-voltage low-power circuit design",
        "year": null
    },
    "Speed control of induction motors using a novel fuzzy sliding-mode structure": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2002/6",
            "게시자": "IEEE",
            "권": "10",
            "설명": "This paper presents a new approach to indirect vector control of induction motors. Two nonlinear controllers, one of sliding mode type and the other PI-fuzzy logic-based, define a new control structure. Both controllers are combined by means of an expert system based on Takagi-Sugeno fuzzy reasoning. The sliding-mode controller acts mainly in a transient state while the PI-like fuzzy controller acts in the steady state. The new structure embodies the advantages that both nonlinear controllers offer: sliding-mode controllers increasing system stability limits, and PI-like fuzzy logic based controllers reducing the chattering in permanent state. The scheme has been implemented and experimentally validated.",
            "저널": "IEEE Transactions on Fuzzy Systems",
            "저자": "F Barrero, A Gonzalez, A Torralba, Eduardo Galvan, Leopoldo García Franquelo",
            "전체 인용횟수": "220회 인용2002200320042005200620072008200920102011201220132014201520162017201820192020202120222023147118109810241117171014161394592",
            "페이지": "375-383",
            "학술 문서": "Speed control of induction motors using a novel fuzzy sliding-mode structureF Barrero, A Gonzalez, A Torralba, E Galvan… - IEEE Transactions on Fuzzy Systems, 2002220회 인용 관련 학술자료 전체 5개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Speed control of induction motors using a novel fuzzy sliding-mode structure",
        "year": null
    },
    "Low supply voltage high-performance CMOS current mirror with low input and output voltage requirements": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004/3/22",
            "게시자": "IEEE",
            "권": "51",
            "설명": "This paper presents a scheme for the efficient implementation of a low supply voltage continuous-time high-performance CMOS current mirror with low input and output voltage requirements. This circuit combines a shunt input feedback and a regulated cascode output stage to achieve low input resistance and very high output resistance. It can be used as a high-precision current mirror in analog and mixed signal circuits with a power supply close to a transistor's threshold voltage. The proposed current mirror has been simulated and a bandwidth of 40 MHz has been obtained. An experimental chip prototype has been sent for fabrication and has been experimentally verified, obtaining 0.15-V input-output voltage requirements, 100-/spl Omega/ input resistance, and more than 200-M/spl Omega/ (G/spl Omega/ ideally) output resistance with a 1.2-V supply in a standard CMOS technology.",
            "저널": "IEEE Transactions on Circuits and Systems II: Express Briefs",
            "저자": "Jaime Ramirez-Angulo, Ramon Gonzalez Carvajal, Antonio Torralba",
            "전체 인용횟수": "159회 인용2001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202312118675671312148131366531136",
            "페이지": "124-129",
            "학술 문서": "Low supply voltage high-performance CMOS current mirror with low input and output voltage requirementsJ Ramirez-Angulo, RG Carvajal, A Torralba - IEEE Transactions on Circuits and Systems II: Express …, 2004152회 인용 관련 학술자료 전체 6개의 버전 Low supply voltage high-performance CMOS current mirror with low input and output voltage requirements*J Ramirez-Angulo, RG Carvajal, A Torralba - Proceedings of the 43rd IEEE Midwest Symposium on …, 20009회 인용 관련 학술자료 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Low supply voltage high-performance CMOS current mirror with low input and output voltage requirements",
        "year": null
    },
    "A continuous-time/spl sigma//spl delta/adc with increased immunity to interferers": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004/11/30",
            "게시자": "IEEE",
            "권": "39",
            "설명": "Receivers are being digitized in a quest for flexibility. Analog filters and programmable gain stages are being exchanged for digital processing at the price of a very challenging ADC. This paper presents an alternative solution where the filter and programmable gain functionality is integrated into a /spl Sigma//spl Delta/ ADC. The novel filtering ADC is realized by adding a high-pass feedback path to a conventional /spl Sigma//spl Delta/ ADC while a compensating low-pass filter in the forward path maintains stability. As such, the ADC becomes highly immune to interferers even if they exceed the maximum allowable input level for the wanted channel. As a consequence, the ADC input range can be programmed dynamically to the level of the wanted signal only. This results in an input-referred dynamic range of 89 dB in 1-MHz bandwidth and an intentionally moderate output signal-to-noise-and-distortion ratio of 46-59 …",
            "저널": "IEEE Journal of Solid-State Circuits",
            "저자": "Kathleen Philips, Peter ACM Nuijten, Raf LJ Roovers, Arthur HM van Roermund, Fernando Muñoz Chavero, Macarena Tejero Pallarés, Antonio Torralba",
            "전체 인용횟수": "136회 인용2004200520062007200820092010201120122013201420152016201720182019202020212022202314497613881810111110711231",
            "페이지": "2170-2178",
            "학술 문서": "A continuous-time/spl sigma//spl delta/adc with increased immunity to interferersK Philips, PACM Nuijten, RLJ Roovers… - IEEE Journal of Solid-State Circuits, 2004136회 인용 관련 학술자료 전체 17개의 버전 ",
            "호": "12"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A continuous-time/spl sigma//spl delta/adc with increased immunity to interferers",
        "year": null
    },
    "0.7-V three-stage class-AB CMOS operational transconductance amplifier": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/8/30",
            "게시자": "IEEE",
            "권": "63",
            "설명": "A simple high-performance architecture for bulk-driven operational transconductance amplifiers (OTAs) is presented. The solution, suitable for operation under sub 1-V single supply, is made up of three gain stages and, as an additional feature, provides inherent class-AB behavior with accurate and robust standby current control. The OTA is fabricated in a 180-nm standard CMOS technology, occupies an area of  and is powered from 0.7 V with a standby current consumption of around 36 . DC gain and unity gain frequency are 57 dB and 3 MHz, respectively, under a capacitive load of 20 pF. Overall good large-signal and small-signal performances are achieved, making the solution extremely competitive in comparison to the state of the art.",
            "저널": "IEEE Transactions on Circuits and Systems I: Regular Papers",
            "저자": "Elena Cabrera-Bernal, Salvatore Pennisi, Alfio Dario Grasso, Antonio Torralba, Ramón Gonzalez Carvajal",
            "전체 인용횟수": "130회 인용20172018201920202021202220234201424371713",
            "페이지": "1807-1815",
            "학술 문서": "0.7-V three-stage class-AB CMOS operational transconductance amplifierE Cabrera-Bernal, S Pennisi, AD Grasso, A Torralba… - IEEE Transactions on Circuits and Systems I: Regular …, 2016130회 인용 관련 학술자료 ",
            "호": "11"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "0.7-V three-stage class-AB CMOS operational transconductance amplifier",
        "year": null
    },
    "A new family of very low-voltage analog circuits based on quasi-floating-gate transistors": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2003/5/13",
            "게시자": "IEEE",
            "권": "50",
            "설명": "A new family of very low-voltage analog circuits is introduced. These circuits do not show the GB degradation that characterizes other low-voltage approaches based on floating-gate transistors. The proposed approach is validated with experimental results of a CMOS mixer in 0.5-/spl mu/m CMOS technology with 0.7-V input signal swing that operates on a single 0.8-V supply with transistor threshold voltages of 0.67 V.",
            "저널": "IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing",
            "저자": "Jaime Ramirez-Angulo, Carlos A Urquidi, Ramon González-Carvajal, Antonio Torralba, Antonio López-Martín",
            "전체 인용횟수": "126회 인용200320042005200620072008200920102011201220132014201520162017201820192020202120222023210821687137236668266106",
            "페이지": "214-220",
            "학술 문서": "A new family of very low-voltage analog circuits based on quasi-floating-gate transistorsJ Ramirez-Angulo, CA Urquidi, R González-Carvajal… - IEEE Transactions on Circuits and Systems II: Analog …, 2003126회 인용 관련 학술자료 전체 2개의 버전 ",
            "호": "5"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A new family of very low-voltage analog circuits based on quasi-floating-gate transistors",
        "year": null
    },
    "FASY: A fuzzy-logic based tool for analog synthesis": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1996/7",
            "게시자": "IEEE",
            "권": "15",
            "설명": "A CAD tool for analog circuit synthesis is presented. This tool, called FASY, uses fuzzy-logic based reasoning to select one topology among a fixed set of alternatives. For the selected topology, a two-phase optimizer sizes all elements to satisfy the performance constraints minimizing a cost function. In FASY, the decision rules used in the topology selection process are introduced by an expert designer or automatically generated by means of a learning process that uses the optimizer mentioned above. The capability of learning topology selection rules by experience, is unique in FASY. Practical examples demonstrate the tool ability of this tool to learn topology selection rules and to synthesize analog cells with different circuit topologies.",
            "저널": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
            "저자": "Antonio Torralba, Jorge Chavez, Leopoldo García Franquelo",
            "전체 인용횟수": "112회 인용19961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220231855942543884353353483251",
            "페이지": "705-715",
            "학술 문서": "FASY: A fuzzy-logic based tool for analog synthesisA Torralba, J Chavez, LG Franquelo - IEEE Transactions on Computer-Aided Design of …, 1996112회 인용 관련 학술자료 전체 7개의 버전 ",
            "호": "7"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "FASY: A fuzzy-logic based tool for analog synthesis",
        "year": null
    },
    "A low-power low-voltage OTA-C sinusoidal oscillator with a large tuning range": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/2/14",
            "게시자": "IEEE",
            "권": "52",
            "설명": "A new operational transconductance amplifier and capacitor based sinusoidal voltage controlled oscillator is presented. The transconductor uses two cross-coupled class-AB pseudo-differential pairs biased by a flipped voltage follower, and it exhibits a wide transconductance range with low power consumption and high linearity. The oscillator has been fabricated in a standard 0.8-/spl mu/m CMOS process. Experimental results show a frequency tuning range from 1 to 25 MHz. The amplitude is controlled by the transconductor nonlinear characteristic. The circuit is operated at 2-V supply voltage with only 1.58 mW of maximum quiescent power consumption.",
            "저널": "IEEE Transactions on Circuits and Systems I: Regular Papers",
            "저자": "J Galan, Ramón González Carvajal, Antonio Torralba, Fernando Muñoz, Jaime Ramirez-Angulo",
            "전체 인용횟수": "102회 인용20062007200820092010201120122013201420152016201720182019202020212022202374511657105763434465",
            "페이지": "283-291",
            "학술 문서": "A low-power low-voltage OTA-C sinusoidal oscillator with a large tuning rangeJ Galan, RG Carvajal, A Torralba, F Muñoz… - IEEE Transactions on Circuits and Systems I: Regular …, 2005102회 인용 관련 학술자료 전체 2개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A low-power low-voltage OTA-C sinusoidal oscillator with a large tuning range",
        "year": null
    },
    "Low-voltage CMOS op-amp with rail-to-rail input and output signal swing for continuous-time signal processing using multiple-input floating-gate transistors": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2001/1",
            "게시자": "IEEE",
            "권": "48",
            "설명": "A scheme for low-voltage CMOS op-amp operation with rail-to-rail input and output signal swing and constant g/sub m/ is presented. Single-ended and fully differential versions are discussed. The scheme is based on the use of multiple-input floating-gate transistors and allows direct implementation of linear weighted addition of continuous-time signals. Simulations are presented that verify the scheme operating with a 1.2-V single supply, 1.2-V input and output solving, 5-MHz op-amp gain-bandwidth product, and a 192-/spl mu/W power dissipation with a 50-pF load and 300/spl times/300 /spl mu/m/sup 2/ silicon area. These results are obtained for 0.85-V transistor threshold voltages. Experimental results are shown that verify the correct functionality of the proposed approach.",
            "저널": "IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing",
            "저자": "J Ramirez-Angulo, RG Carvajal, J Tombs, A Torralba",
            "전체 인용횟수": "98회 인용20012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023176861296273173322212114",
            "페이지": "111-116",
            "학술 문서": "Low-voltage CMOS op-amp with rail-to-rail input and output signal swing for continuous-time signal processing using multiple-input floating-gate transistorsJ Ramirez-Angulo, RG Carvajal, J Tombs, A Torralba - IEEE Transactions on Circuits and Systems II: Analog …, 200198회 인용 관련 학술자료 전체 2개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Low-voltage CMOS op-amp with rail-to-rail input and output signal swing for continuous-time signal processing using multiple-input floating-gate transistors",
        "year": null
    },
    "Fuzzy hardware: architectures and applications": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1998",
            "게시자": "Springer Science & Business Media",
            "설명": "Fuzzy hardware developments have been a major force driving the applications of fuzzy set theory and fuzzy logic in both science and engineering. This volume provides the reader with a comprehensive up-to-date look at recent works describing new innovative developments of fuzzy hardware. An important research trend is the design of improved fuzzy hardware. There is an increasing interest in both analog and digital implementations of fuzzy controllers in particular and fuzzy systems in general. Specialized analog and digital VLSI implementations of fuzzy systems, in the form of dedicated architectures, aim at the highest implementation efficiency. This particular efficiency is asserted in terms of processing speed and silicon utilization. Processing speed in particular has caught the attention of developers of fuzzy hardware and researchers in the field. The volume includes detailed material on a variety of fuzzy hardware related topics such as: Historical review of fuzzy hardware research Fuzzy hardware based on encoded trapezoids Pulse stream techniques for fuzzy hardware Hardware realization of fuzzy neural networks Design of analog neuro-fuzzy systems in CMOS digital technologies Fuzzy controller synthesis method Automatic design of digital and analog neuro-fuzzy controllers Electronic implementation of complex controllers Silicon compilation of fuzzy hardware systems Digital fuzzy hardware processing Parallel processor architecture for real-time fuzzy applications Fuzzy cellular systems Fuzzy Hardware: Architectures and Applications is a technical reference book for researchers, engineers and scientists interested in fuzzy …",
            "저자": "Abraham Kandel, Gideon Langholz",
            "전체 인용횟수": "92회 인용199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202226344368444547533263311",
            "학술 문서": "Fuzzy hardware: architectures and applicationsA Kandel, G Langholz - 199892회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Fuzzy hardware: architectures and applications",
        "year": null
    },
    "Comparison of conventional and new flipped voltage structures with increased input/output signal swing and current sourcing/sinking capabilities": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/8/7",
            "게시자": "IEEE",
            "설명": "A systematic comparison of flipped voltage followers circuits is presented. Two new versions of the flipped voltage follower are introduced. They are characterized by very low output impedance, high bandwidth speed, wide signal swing. All structures can be easily modified for class AB operation. Simulation results show that the newly introduced structures have optimal characteristics",
            "저자": "Jaime Ramírez-Angulo, S Gupta, Ivan Padilla, RG Carvajal, A Torralba, M Jimenez, F Munoz",
            "전체 인용횟수": "77회 인용20082009201020112012201320142015201620172018201920202021202220235184676225655456",
            "컨퍼런스": "48th Midwest Symposium on Circuits and Systems, 2005.",
            "페이지": "1151-1154",
            "학술 문서": "Comparison of conventional and new flipped voltage structures with increased input/output signal swing and current sourcing/sinking capabilitiesJ Ramírez-Angulo, S Gupta, I Padilla, RG Carvajal… - 48th Midwest Symposium on Circuits and Systems …, 200577회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Comparison of conventional and new flipped voltage structures with increased input/output signal swing and current sourcing/sinking capabilities",
        "year": null
    },
    "A 4.7 mW 89.5 dB DR CT complex/spl Delta//spl Sigma/ADC with built-in LPF": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/2/10",
            "게시자": "IEEE",
            "설명": "A CT complex /spl Delta//spl Sigma/ ADC with built-in LPF is presented. A modified feedback topology is used to improve robustness to interferers near f/sub g//2 or f/sub s/. Adding programmable gain control, the 0.18 /spl mu/m CMOS ADC achieves 89.5dB DR in a 1MHz BW, consuming 4.7mW from a 1.8V supply.",
            "저자": "F Munoz, K Philips, A Torralba",
            "전체 인용횟수": "76회 인용2006200720082009201020112012201320142015201620172018201920202021202220235142109441444561111",
            "컨퍼런스": "ISSCC. 2005 IEEE International Digest of Technical Papers. Solid-State Circuits Conference, 2005.",
            "페이지": "500-613",
            "학술 문서": "A 4.7 mW 89.5 dB DR CT complex/spl Delta//spl Sigma/ADC with built-in LPFF Munoz, K Philips, A Torralba - ISSCC. 2005 IEEE International Digest of Technical …, 200576회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A 4.7 mW 89.5 dB DR CT complex/spl Delta//spl Sigma/ADC with built-in LPF",
        "year": null
    },
    "Floating-gate-based tunable CMOS low-voltage linear transconductor and its application to HF g/sub m/-C filter design": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2001/1",
            "게시자": "IEEE",
            "권": "48",
            "설명": "This paper presents a new CMOS low-voltage linear transconductor for very high frequency. It uses multiple-input floating-gate transistors in each inverter of the differential structure transconductor presented by Nauta [1992]. The proposed transconductor operates under a constant low-voltage supply as low as 1.2 V, and its transconductance and output resistance are independently tunable, as shown by experimental measurements. This architecture is suitable for use in high-frequency continuous-time filters with programmable center frequency and quality factor. Simulation results of a 10.7-MHz and Q=40 g/sub m/-C filter operating with a voltage supply of 1.4 V and with rail-to rail input swing are presented.",
            "저널": "IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing",
            "저자": "Fernando Muñoz, Antonio Torralba, Ramón G Carvajal, J Tombs, Jaime Ramirez-Angulo",
            "전체 인용횟수": "72회 인용2001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202322421591121365315334261",
            "페이지": "106-110",
            "학술 문서": "Floating-gate-based tunable CMOS low-voltage linear transconductor and its application to HF g/sub m/-C filter designF Muñoz, A Torralba, RG Carvajal, J Tombs… - IEEE Transactions on Circuits and Systems II: Analog …, 200172회 인용 관련 학술자료 전체 3개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Floating-gate-based tunable CMOS low-voltage linear transconductor and its application to HF g/sub m/-C filter design",
        "year": null
    },
    "Low-voltage CMOS operational amplifiers with wide input-output swing based on a novel scheme": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2000/5",
            "게시자": "IEEE",
            "권": "47",
            "설명": "A scheme to achieve low-voltage wide-bandwidth operation of CMOS op amps with rail-to-rail input and output swing and constant gm is presented. It is based on a novel concept that uses a floating voltage controlled voltage source in the feedback path of the op amp in order to keep its input terminals close to one of the supply rails. Postlayout simulations on a 1.2 V rail-to-rail op amp with 13 MHz GB are presented which verify the proposed scheme.",
            "저널": "IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications",
            "저자": "J Ramirez-Angulo, A Torralba, RG Carvajal, J Tombs",
            "전체 인용횟수": "67회 인용20002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202326445342523247342221",
            "페이지": "772-774",
            "학술 문서": "Low-voltage CMOS operational amplifiers with wide input-output swing based on a novel schemeJ Ramirez-Angulo, A Torralba, RG Carvajal, J Tombs - IEEE Transactions on Circuits and Systems I …, 200067회 인용 관련 학술자료 전체 4개의 버전 ",
            "호": "5"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Low-voltage CMOS operational amplifiers with wide input-output swing based on a novel scheme",
        "year": null
    },
    "ASIC implementation of a digital tachometer with high precision in a wide speed range": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1996/12",
            "게시자": "IEEE",
            "권": "43",
            "설명": "A common method in adjustable speed drives uses an incremental shaft encoder and an electronic circuit for velocity estimation. The usual method of counting pulses coming from the encoder in a fixed period of time produces a high-precision velocity estimate in the high-speed range. High precision in the low-speed range can be achieved measuring the elapsed time between two successive pulses coming from the encoder. In this paper, a mixed method that combines the best of the two previously mentioned approaches has been implemented using a simple electronic circuit based on one field-programmable gate array (FPGA) and one read-only memory (ROM).",
            "저널": "IEEE Transactions on Industrial Electronics",
            "저자": "Eduardo Galvan, Antonio Torralba, Leopoldo García Franquelo",
            "전체 인용횟수": "66회 인용19981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023153133141434363334321212",
            "페이지": "655-660",
            "학술 문서": "ASIC implementation of a digital tachometer with high precision in a wide speed rangeE Galvan, A Torralba, LG Franquelo - IEEE Transactions on Industrial Electronics, 199666회 인용 관련 학술자료 전체 7개의 버전 ",
            "호": "6"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "ASIC implementation of a digital tachometer with high precision in a wide speed range",
        "year": null
    },
    "Simple class-AB voltage follower with slew rate and bandwidth enhancement and no extra static power or supply requirements": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006/7/6",
            "게시자": "The Institution of Engineering & Technology",
            "권": "42",
            "설명": "A simple modification of the MOS voltage follower is introduced, that provides it with efficient class-AB operation. The modified circuit has dynamic output currents and bandwidth that are essentially larger than the conventional MOS voltage follower. This is achieved with the same static power dissipation, very small additional circuit complexity and lower distortion. Experimental verification of all these characteristics is provided.",
            "저널": "Electronics letters",
            "저자": "J Ramirez-Angulo, AJ Lopez-Martin, RG Carvajal, A Torralba, M Jimenez",
            "전체 인용횟수": "64회 인용2007200820092010201120122013201420152016201720182019202020212022202334542324624534552",
            "페이지": "1",
            "학술 문서": "Simple class-AB voltage follower with slew rate and bandwidth enhancement and no extra static power or supply requirementsJ Ramirez-Angulo, AJ Lopez-Martin, RG Carvajal… - Electronics letters, 200664회 인용 관련 학술자료 전체 6개의 버전 ",
            "호": "14"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Simple class-AB voltage follower with slew rate and bandwidth enhancement and no extra static power or supply requirements",
        "year": null
    },
    "A new class AB differential input stage for implementation of low-voltage high slew rate op amps and linear transconductors": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2001/5/6",
            "게시자": "IEEE",
            "권": "1",
            "설명": "A new class AB differential stage that operates with a single supply voltage of less than two transistor threshold voltages is introduced. This circuit has utilization in high slew rate one stage op amps, two stage op amps with class AB input and output stages and linear transconductors. The circuit was verified with simulations and experimentally. It it is shown to have lower voltage supply requirements than other commonly used structures reported in literature.",
            "저자": "Jaime Ramirez-Angulo, Ramon Gonzalez-Carvajal, Antonio Torralba, Carlos Nieva",
            "전체 인용횟수": "64회 인용20012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023141846111132243751621",
            "컨퍼런스": "ISCAS 2001. The 2001 IEEE International Symposium on Circuits and Systems (Cat. No. 01CH37196)",
            "페이지": "671-674",
            "학술 문서": "A new class AB differential input stage for implementation of low-voltage high slew rate op amps and linear transconductorsJ Ramirez-Angulo, R Gonzalez-Carvajal, A Torralba… - ISCAS 2001. The 2001 IEEE International Symposium …, 200164회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A new class AB differential input stage for implementation of low-voltage high slew rate op amps and linear transconductors",
        "year": null
    },
    "New output stage for low supply voltage, high-performance CMOS current mirrors": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2003/5/25",
            "게시자": "IEEE",
            "권": "1",
            "설명": "This paper presents a new simple output stage for CMOS current mirrors. The stage is a new low-voltage regulated cascode circuit, which achieves a very high output impedance and accurate current copy when combined with a suitable input stage. A 122 MHz bandwidth with 1 V supply voltage has been obtained using a standard 0.35 /spl mu/m CMOS technology.",
            "저자": "A Torralba, Ramón González Carvajal, Fernando Muñoz, Jaime Ramirez-Angulo",
            "전체 인용횟수": "58회 인용200320042005200620072008200920102011201220132014201520162017201820192020202120223232211513456311114",
            "컨퍼런스": "Proceedings of the 2003 International Symposium on Circuits and Systems, 2003. ISCAS'03.",
            "페이지": "I-I",
            "학술 문서": "New output stage for low supply voltage, high-performance CMOS current mirrorsA Torralba, RG Carvajal, F Muñoz, J Ramirez-Angulo - Proceedings of the 2003 International Symposium on …, 200358회 인용 관련 학술자료 전체 5개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "New output stage for low supply voltage, high-performance CMOS current mirrors",
        "year": null
    },
    "Tunable linear MOS resistors using quasi-floating-gate techniques": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/1/19",
            "게시자": "IEEE",
            "권": "56",
            "설명": "A family of tunable MOS resistors based on quasi-floating-gate (QFG) transistors biased in the triode region is analyzed in this paper. From the study results, a new device that outperforms previous implementations, is presented. By means of a capacitive divider, the ac component of the drain-to-source voltage scaled with a factor alpha les 1 is added to the gate-to-source voltage leading to a cancellation of the nonlinear terms. The effect of alpha on resistor linearity is analytically studied. Simulation results are also provided for different technologies. Finally, a complete transconductor has been built which preserves the linearity of the MOS resistor. Three versions of the transconductor have been fabricated for different values of alpha (alpha = 0, 0.5, and 1) in a 0.5 mum CMOS technology with plusmn1.65-V supply voltage. Experimental results show (for alpha = 1 ) a THD of - 57 dB (HD2=-70 dB) at 1 MHz for 2-V …",
            "저널": "IEEE Transactions on Circuits and Systems II: Express Briefs",
            "저자": "A Torralba, C Lujan-Martinez, Roman G Carvajal, J Galan, Melita Pennisi, Jaime Ramirez-Angulo, A Lopez-Martin",
            "전체 인용횟수": "53회 인용200920102011201220132014201520162017201820192020202120222023233954424344321",
            "페이지": "41-45",
            "학술 문서": "Tunable linear MOS resistors using quasi-floating-gate techniquesA Torralba, C Lujan-Martinez, RG Carvajal, J Galan… - IEEE Transactions on Circuits and Systems II: Express …, 200953회 인용 관련 학술자료 전체 3개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Tunable linear MOS resistors using quasi-floating-gate techniques",
        "year": null
    },
    "Online object tracking: A benchmark": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013",
            "설명": "Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.",
            "저자": "Yi Wu, Jongwoo Lim, Ming-Hsuan Yang",
            "전체 인용횟수": "6824회 인용2013201420152016201720182019202020212022202324196449669680897947855823694508",
            "컨퍼런스": "IEEE Conference on Computer Vision and Pattern Recognition",
            "페이지": "2411-2418",
            "학술 문서": "Online object tracking: A benchmarkY Wu, J Lim, MH Yang - Proceedings of the IEEE conference on computer …, 20136817회 인용 관련 학술자료 전체 37개의 버전 Online object tracking: A benchmark [c]. computer vision and pattern recogniton (cvpr)*Y Wu, J Lim, MH Yang - 2013 IEEE Conference on, IEEE, 20137회 인용 관련 학술자료 Online object tracking: A benchmark supplemental material*W Yi, J Lim, M Yang - IEEE, 20135회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Online object tracking: A benchmark",
        "year": null
    },
    "Detecting Faces in Images: A Survey": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2002/1",
            "권": "24",
            "설명": "Images containing faces are essential to intelligent vision-based human-computer interaction, and research efforts in face processing include face recognition, face tracking, pose estimation and expression recognition. However, many reported methods assume that the faces in an image or an image sequence have been identified and localized. To build fully automated systems that analyze the information contained in face images, robust and efficient face detection algorithms are required. Given a single image, the goal of face detection is to identify all image regions which contain a face, regardless of its 3D position, orientation and lighting conditions. Such a problem is challenging because faces are non-rigid and have a high degree of variability in size, shape, color and texture. Numerous techniques have been developed to detect faces in a single image, and the purpose of this paper is to categorize and …",
            "저널": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "저자": "Ming-Hsuan Yang, David J Kriegman, Narendra Ahuja",
            "전체 인용횟수": "6040회 인용2001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202318942313254133584064254284013733913833372672331731751541059710360",
            "페이지": "34 - 58",
            "학술 문서": "Detecting faces in images: A surveyMH Yang, DJ Kriegman, N Ahuja - IEEE Transactions on pattern analysis and machine …, 20026003회 인용 관련 학술자료 전체 14개의 버전 Member, Ieee, David J*MH Yang - Kriegman, Senior Member, IEEE, and Narendra Ahuja …, 200227회 인용 관련 학술자료 Kriegman, and Narendra Ahuja,\" Detecting faces in images: A survey,\"*MH Yang, J David - IEEE Trans. Pattern Anal. Mach. Inteli, 200218회 인용 관련 학술자료 J. & AHUJA*MH Yang, D Kriegman - NARENDRA:\" Detecting faces in images: a survey\" en …, 20029회 인용 관련 학술자료 Detecting Faces i n Images: A Survey*MH Yang, DJ Kriegman, N Ahuja - IEEE Trans. on Pattern Analysis andMachine …3회 인용 관련 학술자료 KD-J., AN, Detecting faces in images: a survey, Pattern Analysis and Machine Intelligence*MH Yang - IEEE Transactions on, 20022회 인용 관련 학술자료 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Detecting Faces in Images: A Survey",
        "year": null
    },
    "Incremental learning for robust visual tracking": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008/5/1",
            "게시자": "Springer US",
            "권": "77",
            "설명": "  Visual tracking, in essence, deals with non-stationary image streams that change over time. While most existing algorithms are able to track objects well in controlled environments, they usually fail in the presence of significant variation of the object’s appearance or surrounding illumination. One reason for such failures is that many algorithms employ fixed appearance models of the target. Such models are trained using only appearance data available before tracking begins, which in practice limits the range of appearances that are modeled, and ignores the large volume of information (such as shape changes or specific lighting conditions) that becomes available during tracking. In this paper, we present a tracking method that incrementally learns a low-dimensional subspace representation, efficiently adapting online to changes in the appearance of the target. The model update, based on incremental …",
            "저널": "International Journal of Computer Vision",
            "저자": "David A Ross, Jongwoo Lim, Ruei-Sung Lin, Ming-Hsuan Yang",
            "전체 인용횟수": "3982회 인용2008200920102011201220132014201520162017201820192020202120222023255711514921928840951051939836929820815512380",
            "페이지": "125-141",
            "학술 문서": "Incremental learning for robust visual trackingDA Ross, J Lim, RS Lin, MH Yang - International journal of computer vision, 20083982회 인용 관련 학술자료 전체 35개의 버전 ",
            "호": "1-3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Incremental learning for robust visual tracking",
        "year": null
    },
    "Saliency Detection via Graph-Based Manifold Ranking": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013",
            "설명": "Most existing bottom-up methods measure the foreground saliency of a pixel or region based on its contrast within a local context or the entire image, whereas a few methods focus on segmenting out background regions and thereby salient objects. Instead of considering the contrast between the salient objects and their surrounding regions, we consider both foreground and background cues in a different way. We rank the similarity of the image elements (pixels or regions) with foreground cues or background cues via graph-based manifold ranking. The saliency of the image elements is defined based on their relevances to the given seeds or queries. We represent the image as a close-loop graph with superpixels as nodes. These nodes are ranked based on the similarity to background and foreground queries, based on affinity matrices. Saliency detection is carried out in a two-stage scheme to extract background regions and foreground salient objects efficiently. Experimental results on two large benchmark databases demonstrate the proposed method performs well when against the state-of-the-art methods in terms of accuracy and speed. We also create a more difficult benchmark database containing 5,172 images to test the proposed saliency model and make this database publicly available with this paper for further studies in the saliency field.",
            "저자": "Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, Ming-Hsuan Yang",
            "전체 인용횟수": "2712회 인용20132014201520162017201820192020202120222023983198258317321337327313275238",
            "컨퍼런스": "IEEE Conference on Computer Vision and Pattern Recognition",
            "페이지": "3166-3173",
            "학술 문서": "Saliency detection via graph-based manifold rankingC Yang, L Zhang, H Lu, X Ruan, MH Yang - Proceedings of the IEEE conference on computer …, 20132712회 인용 관련 학술자료 전체 23개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Saliency Detection via Graph-Based Manifold Ranking",
        "year": null
    },
    "Robust Object Tracking with Online Multiple Instance Learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011",
            "권": "33",
            "설명": "In this paper, we address the problem of tracking an object in a video given its location in the first frame and no other information. Recently, a class of tracking techniques called “tracking by detection” has been shown to give promising results at real-time speeds. These methods train a discriminative classifier in an online manner to separate the object from the background. This classifier bootstraps itself by using the current tracker state to extract positive and negative examples from the current frame. Slight inaccuracies in the tracker can therefore lead to incorrectly labeled training examples, which degrade the classifier and can cause drift. In this paper, we show that using Multiple Instance Learning (MIL) instead of traditional supervised learning avoids these problems and can therefore lead to a more robust tracker with fewer parameter tweaks. We propose a novel online MIL algorithm for object tracking that …",
            "저널": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "저자": "Boris Babenko, Ming-Hsuan Yang, Serge Belongie",
            "전체 인용횟수": "2650회 인용201120122013201420152016201720182019202020212022202313541852903873523163162521721249149",
            "페이지": "1619-1632",
            "학술 문서": "Robust object tracking with online multiple instance learningB Babenko, MH Yang, S Belongie - IEEE transactions on pattern analysis and machine …, 20102650회 인용 관련 학술자료 전체 13개의 버전 ",
            "호": "8"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Robust Object Tracking with Online Multiple Instance Learning",
        "year": null
    },
    "Deep laplacian pyramid networks for fast and accurate super-resolution": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "Convolutional neural networks have recently demonstrated high-quality reconstruction for single-image super-resolution. In this paper, we propose the Laplacian Pyramid Super-Resolution Network (LapSRN) to progressively reconstruct the sub-band residuals of high-resolution images. At each pyramid level, our model takes coarse-resolution feature maps as input, predicts the high-frequency residuals, and uses transposed convolutions for upsampling to the finer level. Our method does not require the bicubic interpolation as the pre-processing step and thus dramatically reduces the computational complexity. We train the proposed LapSRN with deep supervision using a robust Charbonnier loss function and achieve high-quality reconstruction. Furthermore, our network generates multi-scale predictions in one feed-forward pass through the progressive reconstruction, thereby facilitates resource-aware applications. Extensive quantitative and qualitative evaluations on benchmark datasets show that the proposed algorithm performs favorably against the state-of-the-art methods in terms of speed and accuracy.",
            "저자": "Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, Ming-Hsuan Yang",
            "전체 인용횟수": "2635회 인용201720182019202020212022202318211355454540566473",
            "컨퍼런스": "IEEE Conference on Computer Vision and Pattern Recognition",
            "페이지": "624-632",
            "학술 문서": "Deep laplacian pyramid networks for fast and accurate super-resolutionWS Lai, JB Huang, N Ahuja, MH Yang - Proceedings of the IEEE conference on computer …, 20172635회 인용 관련 학술자료 전체 21개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep laplacian pyramid networks for fast and accurate super-resolution",
        "year": null
    },
    "Fast Compressive Tracking": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "게시자": "IEEE",
            "권": "36",
            "설명": "It is a challenging task to develop effective and efficient appearance models for robust object tracking due to factors such as pose variation, illumination change, occlusion, and motion blur. Existing online tracking algorithms often update models with samples from observations in recent frames. Despite much success has been demonstrated, numerous issues remain to be addressed. First, while these adaptive appearance models are data-dependent, there does not exist sufficient amount of data for online algorithms to learn at the outset. Second, online tracking algorithms often encounter the drift problems. As a result of self-taught learning, misaligned samples are likely to be added and degrade the appearance models. In this paper, we propose a simple yet effective and efficient tracking algorithm with an appearance model based on features extracted from a multiscale image feature space with dataindependent …",
            "저널": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "저자": "Kaihua Zhang, Lei Zhang, M.-H. Yang",
            "전체 인용횟수": "2471회 인용20132014201520162017201820192020202120222023104243411443352333219149805231",
            "페이지": "2002-2015",
            "학술 문서": "Fast compressive trackingK Zhang, L Zhang, MH Yang - IEEE transactions on pattern analysis and machine …, 20142445회 인용 관련 학술자료 전체 31개의 버전 Fast compressive tracking*Z Kaihua, Z Lei, Y Ming-Hsuan - IEEE transactions on pattern analysis and machine …, 201451회 인용 관련 학술자료 ",
            "호": "10"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Fast Compressive Tracking",
        "year": null
    },
    "Real-time compressive tracking": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/10/8",
            "저자": "Kaihua Zhang, Lei Zhang, Ming-Hsuan Yang",
            "전체 인용횟수": "2445회 인용20132014201520162017201820192020202120222023104242407436347330215148805131",
            "컨퍼런스": "European conference on Computer Vision",
            "페이지": "864-877",
            "학술 문서": "Fast compressive tracking*K Zhang, L Zhang, MH Yang - IEEE transactions on pattern analysis and machine …, 20142445회 인용 관련 학술자료 전체 31개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Real-time compressive tracking",
        "year": null
    },
    "Hierarchical convolutional features for visual tracking": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "Visual object tracking is challenging as target objects often undergo significant appearance changes caused by deformation, abrupt motion, background clutter and occlusion. In this paper, we exploit features extracted from deep convolutional neural networks trained on object recognition datasets to improve tracking accuracy and robustness. The outputs of the last convolutional layers encode the semantic information of targets and such representations are robust to significant appearance variations. However, their spatial resolution is too coarse to precisely localize targets. In contrast, earlier convolutional layers provide more precise localization but are less invariant to appearance changes. We interpret the hierarchies of convolutional layers as a nonlinear counterpart of an image pyramid representation and exploit these multiple levels of abstraction for visual tracking. Specifically, we adaptively learn correlation filters on each convolutional layer to encode the target appearance. We hierarchically infer the maximum response of each layer to locate targets. Extensive experimental results on a largescale benchmark dataset show that the proposed algorithm performs favorably against state-of-the-art methods.",
            "저자": "Chao Ma, Jia-Bin Huang, Xiaokang Yang, Ming-Hsuan Yang",
            "전체 인용횟수": "2099회 인용2016201720182019202020212022202363205389439342300202133",
            "컨퍼런스": "IEEE International Conference on Computer Vision",
            "페이지": "3074-3082",
            "학술 문서": "Hierarchical convolutional features for visual trackingC Ma, JB Huang, X Yang, MH Yang - Proceedings of the IEEE international conference on …, 20152099회 인용 관련 학술자료 전체 18개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Hierarchical convolutional features for visual tracking",
        "year": null
    },
    "Res2net: A new multi-scale backbone architecture": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/8/30",
            "게시자": "IEEE",
            "설명": "Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over …",
            "저널": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "저자": "Shanghua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang, Philip HS Torr",
            "전체 인용횟수": "2018회 인용2019202020212022202328164431670711",
            "학술 문서": "Res2net: A new multi-scale backbone architectureSH Gao, MM Cheng, K Zhao, XY Zhang, MH Yang… - IEEE transactions on pattern analysis and machine …, 20192018회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Res2net: A new multi-scale backbone architecture",
        "year": null
    },
    "Single image dehazing via multi-scale convolutional neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": " The performance of existing image dehazing methods is limited by hand-designed features, such as the dark channel, color disparity and maximum contrast, with complex fusion schemes. In this paper, we propose a multi-scale deep neural network for single-image dehazing by learning the mapping between hazy images and their corresponding transmission maps. The proposed algorithm consists of a coarse-scale net which predicts a holistic transmission map based on the entire image, and a fine-scale net which refines results locally. To train the multi-scale deep network, we synthesize a dataset comprised of hazy images and corresponding transmission maps based on the NYU Depth dataset. Extensive experiments demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods on both synthetic and real-world images in terms of quality and speed.",
            "저자": "Wenqi Ren, Si Liu, Hua Zhang, Jinshan Pan, Xiaochun Cao, Ming-Hsuan Yang",
            "전체 인용횟수": "1618회 인용201720182019202020212022202341162223269309326281",
            "컨퍼런스": "European conference on Computer Vision",
            "페이지": "154-169",
            "학술 문서": "Single image dehazing via multi-scale convolutional neural networksW Ren, S Liu, H Zhang, J Pan, X Cao, MH Yang - Computer Vision–ECCV 2016: 14th European …, 20161618회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Single image dehazing via multi-scale convolutional neural networks",
        "year": null
    },
    "Visual tracking via adaptive structural local sparse appearance model": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/6/16",
            "게시자": "IEEE",
            "설명": "Sparse representation has been applied to visual tracking by finding the best candidate with minimal reconstruction error using target templates. However most sparse representation based trackers only consider the holistic representation and do not make full use of the sparse coefficients to discriminate between the target and the background, and hence may fail with more possibility when there is similar object or occlusion in the scene. In this paper we develop a simple yet robust tracking method based on the structural local sparse appearance model. This representation exploits both partial information and spatial information of the target based on a novel alignment-pooling method. The similarity obtained by pooling across the local patches helps not only locate the target more accurately but also handle occlusion. In addition, we employ a template update strategy which combines incremental subspace learning …",
            "저자": "Xu Jia, Huchuan Lu, Ming-Hsuan Yang",
            "전체 인용횟수": "1599회 인용201220132014201520162017201820192020202120222023959133245281219209169106725222",
            "컨퍼런스": "IEEE Conference on Computer Vision and Pattern Recognition",
            "페이지": "1822-1829",
            "학술 문서": "Visual tracking via adaptive structural local sparse appearance modelX Jia, H Lu, MH Yang - 2012 IEEE Conference on computer vision and pattern …, 20121599회 인용 관련 학술자료 전체 19개의 버전 Visual tracking via adaptive structural local sparse appearance model*S He, QX Yang, R Lau, J Wang, MH Yang - 20135회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Visual tracking via adaptive structural local sparse appearance model",
        "year": null
    },
    "Learning to adapt structured output space for semantic segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/2/28",
            "설명": "Convolutional neural network-based approaches for semantic segmentation rely on supervision with pixel-level ground truth, but may not generalize well to unseen image domains. As the labeling process is tedious and labor intensive, developing algorithms that can adapt source ground truth labels to the target domain is of great interest. In this paper, we propose an adversarial learning method for domain adaptation in the context of semantic segmentation. Considering semantic segmentations as structured outputs that contain spatial similarities between the source and target domains, we adopt adversarial learning in the output space. To further enhance the adapted model, we construct a multi-level adversarial network to effectively perform output space domain adaptation at different feature levels. To further improve our method, we utilize multi-level output adaptation based on feature maps at different levels. Extensive experiments and ablation study are conducted under various domain adaptation settings, including synthetic-to-real and cross-city scenarios. We show that the proposed method performs favorably against the state-of-the-art methods in terms of accuracy and visual quality.",
            "저자": "Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, Manmohan Chandraker",
            "전체 인용횟수": "1448회 인용20182019202020212022202320125225328390355",
            "컨퍼런스": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "학술 문서": "Learning to adapt structured output space for semantic segmentationYH Tsai, WC Hung, S Schulter, K Sohn, MH Yang… - Proceedings of the IEEE conference on computer …, 20181447회 인용 관련 학술자료 전체 19개의 버전 1019 M. Chandraker,“Learning to adapt structured output space for semantic 1020 segmentation,”YH Tsai, WC Hung, S Schulter, K Sohn, MH Yang - Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit, 20186회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning to adapt structured output space for semantic segmentation",
        "year": null
    },
    "Robust object tracking via sparsity-based collaborative model": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/6/16",
            "게시자": "IEEE",
            "설명": "In this paper we propose a robust object tracking algorithm using a collaborative model. As the main challenge for object tracking is to account for drastic appearance change, we propose a robust appearance model that exploits both holistic templates and local representations. We develop a sparsity-based discriminative classifier (SD-C) and a sparsity-based generative model (SGM). In the S-DC module, we introduce an effective method to compute the confidence value that assigns more weights to the foreground than the background. In the SGM module, we propose a novel histogram-based method that takes the spatial information of each patch into consideration with an occlusion handing scheme. Furthermore, the update scheme considers both the latest observations and the original template, thereby enabling the tracker to deal with appearance change effectively and alleviate the drift problem. Numerous …",
            "저자": "Wei Zhong, Huchuan Lu, Ming-Hsuan Yang",
            "전체 인용횟수": "1307회 인용20122013201420152016201720182019202020212022202375211521125518617011680373713",
            "컨퍼런스": "IEEE Conference on Computer Vision and Pattern Recognition",
            "페이지": "1838-1845",
            "학술 문서": "Robust object tracking via sparsity-based collaborative modelW Zhong, H Lu, MH Yang - 2012 IEEE Conference on Computer vision and pattern …, 20121307회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Robust object tracking via sparsity-based collaborative model",
        "year": null
    },
    "Diverse image-to-image translation via disentangled representations": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/8/2",
            "권": "1",
            "설명": "Image-to-image translation aims to learn the mapping between two visual domains. There are two main challenges for many applications: 1) the lack of aligned training pairs and 2) multiple possible outputs from a single input image. In this work, we present an approach based on disentangled representation for producing diverse outputs without paired training images. To achieve diversity, we propose to embed images onto two spaces: a domain-invariant content space capturing shared information across domains and a domain-specific attribute space. Using the disentangled features as inputs greatly reduces mode collapse. To handle unpaired training data, we introduce a novel cross-cycle consistency loss. Qualitative results show that our model can generate diverse and realistic images on a wide range of tasks. We validate the effectiveness of our approach through extensive evaluation.",
            "저자": "Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Singh, Ming-Hsuan Yang",
            "전체 인용횟수": "1228회 인용20182019202020212022202313158257302285209",
            "컨퍼런스": "European Conference on Computer Vision",
            "페이지": "36-52",
            "학술 문서": "Diverse image-to-image translation via disentangled representationsHY Lee, HY Tseng, JB Huang, M Singh, MH Yang - Proceedings of the European conference on computer …, 20181228회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Diverse image-to-image translation via disentangled representations",
        "year": null
    },
    "Face recognition using kernel methods": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2002",
            "설명": "Principal Component Analysis and Fisher Linear Discriminant methods have demonstrated their success in face detection, recog (cid: 173) nition, and tracking. The representation in these subspace methods is based on second order statistics of the image set, and does not address higher order statistical dependencies such as the relation (cid: 173) ships among three or more pixels. Recently Higher Order Statistics and Independent Component Analysis (ICA) have been used as in (cid: 173) formative low dimensional representations for visual recognition. In this paper, we investigate the use of Kernel Principal Compo (cid: 173) nent Analysis and Kernel Fisher Linear Discriminant for learning low dimensional representations for face recognition, which we call Kernel Eigenface and Kernel Fisherface methods. While Eigenface and Fisherface methods aim to find projection directions based on the second order correlation of samples, Kernel Eigenface and Ker (cid: 173) nel Fisherface methods provide generalizations which take higher order correlations into account. We compare the performance of kernel methods with Eigenface, Fisherface and ICA-based meth (cid: 173) ods for face recognition with variation in pose, scale, lighting and expression. Experimental results show that kernel methods pro (cid: 173) vide better representations and achieve lower error rates for face recognition.",
            "저자": "Ming-Hsuan Yang",
            "전체 인용횟수": "1181회 인용2001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202349203974849691857678816563615638324431211011",
            "컨퍼런스": "Advances in Neural Informaiton Processing Systems",
            "페이지": "215-220",
            "학술 문서": "Face recognition using kernel methodsMH Yang - Advances in Neural Information Processing Systems, 20011134회 인용 관련 학술자료 전체 20개의 버전 Face recognition using kernel methodsMH Yang - Advances in neural information processing systems, 200266회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Face recognition using kernel methods",
        "year": null
    },
    "Long-term correlation tracking": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "In this paper, we address the problem of long-term visual tracking where the target objects undergo significant appearance variation due to deformation, abrupt motion, heavy occlusion and out-of-the-view. In this setting, we decompose the task of tracking into translation and scale estimation of objects. We show that the correlation between temporal context considerably improves the accuracy and reliability for translation estimation, and it is effective to learn the discriminative correlation filters from the most confident frames to estimate the scale change. In addition, we train an online random fern classifier to re-detect objects in case of tracking failure. Extensive experimental results on large-scale benchmark datasets show that the proposed algorithm performs favorably against the state-of-the-art methods in terms of efficiency, accuracy, and robustness.",
            "저자": "Chao Ma, Xiaokang Yang, Chongyang Zhang, Ming-Hsuan Yang",
            "전체 인용횟수": "1148회 인용2015201620172018201920202021202220239571272342531811309151",
            "컨퍼런스": "IEEE Conference on Computer Vision and Pattern Recognition",
            "페이지": "5388-5396",
            "학술 문서": "Long-term correlation trackingC Ma, X Yang, C Zhang, MH Yang - Proceedings of the IEEE conference on computer …, 20151148회 인용 관련 학술자료 전체 18개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Long-term correlation tracking",
        "year": null
    },
    "Clinically applicable deep learning for diagnosis and referral in retinal disease": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/9",
            "게시자": "Nature Publishing Group",
            "권": "24",
            "설명": "The volume and complexity of diagnostic imaging is increasing at a pace faster than the availability of human expertise to interpret it. Artificial intelligence has shown great promise in classifying two-dimensional photographs of some common diseases and typically relies on databases of millions of annotated images. Until now, the challenge of reaching the performance of expert clinicians in a real-world clinical pathway with three-dimensional diagnostic scans has remained unsolved. Here, we apply a novel deep learning architecture to a clinically heterogeneous set of three-dimensional optical coherence tomography scans from patients referred to a major eye hospital. We demonstrate performance in making a referral recommendation that reaches or exceeds that of experts on a range of sight-threatening retinal diseases after training on only 14,884 scans. Moreover, we demonstrate that the tissue …",
            "저널": "Nature medicine",
            "저자": "Jeffrey De Fauw, Joseph R Ledsam, Bernardino Romera-Paredes, Stanislav Nikolov, Nenad Tomasev, Sam Blackwell, Harry Askham, Xavier Glorot, Brendan O’Donoghue, Daniel Visentin, George Van Den Driessche, Balaji Lakshminarayanan, Clemens Meyer, Faith Mackinder, Simon Bouton, Kareem Ayoub, Reena Chopra, Dominic King, Alan Karthikesalingam, Cían O Hughes, Rosalind Raine, Julian Hughes, Dawn A Sim, Catherine Egan, Adnan Tufail, Hugh Montgomery, Demis Hassabis, Geraint Rees, Trevor Back, Peng T Khaw, Mustafa Suleyman, Julien Cornebise, Pearse A Keane, Olaf Ronneberger",
            "전체 인용횟수": "2074회 인용20182019202020212022202342301457465404363",
            "페이지": "1342-1350",
            "학술 문서": "Clinically applicable deep learning for diagnosis and referral in retinal diseaseJ De Fauw, JR Ledsam, B Romera-Paredes, S Nikolov… - Nature medicine, 20182074회 인용 관련 학술자료 전체 10개의 버전 ",
            "호": "9"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Clinically applicable deep learning for diagnosis and referral in retinal disease",
        "year": null
    },
    "Highly accurate protein structure prediction for the human proteome": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021/8/26",
            "게시자": "Nature Publishing Group UK",
            "권": "596",
            "설명": "Protein structures can provide invaluable information, both for reasoning about biological processes and for enabling interventions such as structure-based drug development or targeted mutagenesis. After decades of effort, 17% of the total residues in human protein sequences are covered by an experimentally determined structure. Here we markedly expand the structural coverage of the proteome by applying the state-of-the-art machine learning method, AlphaFold, at a scale that covers almost the entire human proteome (98.5% of human proteins). The resulting dataset covers 58% of residues with a confident prediction, of which a subset (36% of all residues) have very high confidence. We introduce several metrics developed by building on the AlphaFold model and use them to interpret the dataset, identifying strong multi-domain predictions as well as regions that are likely to be disordered. Finally, we provide …",
            "저널": "Nature",
            "저자": "Kathryn Tunyasuvunakool, Jonas Adler, Zachary Wu, Tim Green, Michal Zielinski, Augustin Žídek, Alex Bridgland, Andrew Cowie, Clemens Meyer, Agata Laydon, Sameer Velankar, Gerard J Kleywegt, Alex Bateman, Richard Evans, Alexander Pritzel, Michael Figurnov, Olaf Ronneberger, Russ Bates, Simon AA Kohl, Anna Potapenko, Andrew J Ballard, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Ellen Clancy, David Reiman, Stig Petersen, Andrew W Senior, Koray Kavukcuoglu, Ewan Birney, Pushmeet Kohli, John Jumper, Demis Hassabis",
            "전체 인용횟수": "1682회 인용202120222023159752757",
            "페이지": "590-596",
            "학술 문서": "Highly accurate protein structure prediction for the human proteomeK Tunyasuvunakool, J Adler, Z Wu, T Green, M Zielinski… - Nature, 20211682회 인용 관련 학술자료 전체 10개의 버전 ",
            "호": "7873"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Highly accurate protein structure prediction for the human proteome",
        "year": null
    },
    "Medical image computing and computer-assisted intervention–MICCAI 2015": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/10/5",
            "게시자": "Springer International Publishing",
            "권": "9351",
            "저널": "Lecture Notes in Computer Science",
            "저자": "Olaf Ronneberger, Philipp Fischer, Thomas Brox, Nassir Navab, Joachim Hornegger, William M Wells, Alejandro F Frangi",
            "전체 인용횟수": "1638회 인용20172018201920202021202220231454110238411466341",
            "페이지": "234-241",
            "학술 문서": "Medical image computing and computer-assisted intervention–MICCAI 2015O Ronneberger, P Fischer, T Brox, N Navab… - Lecture Notes in Computer Science, 20151638회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Medical image computing and computer-assisted intervention–MICCAI 2015",
        "year": null
    },
    "Protein complex prediction with AlphaFold-Multimer": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021/10/4",
            "게시자": "Cold Spring Harbor Laboratory",
            "설명": "While the vast majority of well-structured single protein chains can now be predicted to high accuracy due to the recent AlphaFold [1] model, the prediction of multi-chain protein complexes remains a challenge in many cases. In this work, we demonstrate that an AlphaFold model trained specifically for multimeric inputs of known stoichiometry, which we call AlphaFold-Multimer, significantly increases accuracy of predicted multimeric interfaces over input-adapted single-chain AlphaFold while maintaining high intra-chain accuracy. On a benchmark dataset of 17 heterodimer proteins without templates (introduced in [2]) we achieve at least medium accuracy (DockQ [3]≥0.49) on 13 targets and high accuracy (DockQ≥0.8) on 7 targets, compared to 9 targets of at least medium accuracy and 4 of high accuracy for the previous state of the art system (an AlphaFold-based system from [2]). We also predict structures for a large dataset of 4,446 recent protein complexes, from which we score all non-redundant interfaces with low template identity. For heteromeric interfaces we successfully predict the interface (DockQ≥0.23) in 70% of cases, and produce high accuracy predictions (DockQ≥0.8) in 26% of cases, an improvement of +27 and +14 percentage points over the flexible linker modification of AlphaFold [4] respectively. For homomeric interfaces we successfully predict the interface in 72% of cases, and produce high accuracy predictions in 36% of cases, an improvement of +8 and +7 percentage points respectively.",
            "저널": "biorxiv",
            "저자": "Richard Evans, Michael O’Neill, Alexander Pritzel, Natasha Antropova, Andrew Senior, Tim Green, Augustin Žídek, Russ Bates, Sam Blackwell, Jason Yim, Olaf Ronneberger, Sebastian Bodenstein, Michal Zielinski, Alex Bridgland, Anna Potapenko, Andrew Cowie, Kathryn Tunyasuvunakool, Rishub Jain, Ellen Clancy, Pushmeet Kohli, John Jumper, Demis Hassabis",
            "전체 인용횟수": "1167회 인용20212022202327434696",
            "페이지": "2021.10. 04.463034",
            "학술 문서": "Protein complex prediction with AlphaFold-MultimerR Evans, M O'Neill, A Pritzel, N Antropova, A Senior… - biorxiv, 20211167회 인용 관련 학술자료 전체 5개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Protein complex prediction with AlphaFold-Multimer",
        "year": null
    },
    "A large annotated medical image dataset for the development and evaluation of segmentation algorithms": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/2/25",
            "설명": "Semantic segmentation of medical images aims to associate a pixel with a label in a medical image without human initialization. The success of semantic segmentation algorithms is contingent on the availability of high-quality imaging data with corresponding labels provided by experts. We sought to create a large collection of annotated medical image datasets of various clinically relevant anatomies available under open source license to facilitate the development of semantic segmentation algorithms. Such a resource would allow: 1) objective assessment of general-purpose segmentation methods through comprehensive benchmarking and 2) open and free access to medical image data for any researcher interested in the problem domain. Through a multi-institutional effort, we generated a large, curated dataset representative of several highly variable segmentation tasks that was used in a crowd-sourced challenge - the Medical Segmentation Decathlon held during the 2018 Medical Image Computing and Computer Aided Interventions Conference in Granada, Spain. Here, we describe these ten labeled image datasets so that these data may be effectively reused by the research community.",
            "저널": "arXiv preprint arXiv:1902.09063",
            "저자": "Amber L Simpson, Michela Antonelli, Spyridon Bakas, Michel Bilello, Keyvan Farahani, Bram Van Ginneken, Annette Kopp-Schneider, Bennett A Landman, Geert Litjens, Bjoern Menze, Olaf Ronneberger, Ronald M Summers, Patrick Bilic, Patrick F Christ, Richard KG Do, Marc Gollub, Jennifer Golia-Pernicka, Stephan H Heckers, William R Jarnagin, Maureen K McHugo, Sandy Napel, Eugene Vorontsov, Lena Maier-Hein, M Jorge Cardoso",
            "전체 인용횟수": "795회 인용201920202021202220232490197238239",
            "학술 문서": "A large annotated medical image dataset for the development and evaluation of segmentation algorithmsAL Simpson, M Antonelli, S Bakas, M Bilello… - arXiv preprint arXiv:1902.09063, 2019795회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A large annotated medical image dataset for the development and evaluation of segmentation algorithms",
        "year": null
    },
    "Gland segmentation in colon histology images: The glas challenge contest": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/1/1",
            "게시자": "Elsevier",
            "권": "35",
            "설명": "Colorectal adenocarcinoma originating in intestinal glandular structures is the most common form of colon cancer. In clinical practice, the morphology of intestinal glands, including architectural appearance and glandular formation, is used by pathologists to inform prognosis and plan the treatment of individual patients. However, achieving good inter-observer as well as intra-observer reproducibility of cancer grading is still a major challenge in modern pathology. An automated approach which quantifies the morphology of glands is a solution to the problem.This paper provides an overview to the Gland Segmentation in Colon Histology Images Challenge Contest (GlaS) held at MICCAI’2015. Details of the challenge, including organization, dataset and evaluation criteria, are presented, along with the method descriptions and evaluation results from the top performing methods.",
            "저자": "Korsuk Sirinukunwattana, Josien PW Pluim, Hao Chen, Xiaojuan Qi, Pheng-Ann Heng, Yun Bo Guo, Li Yang Wang, Bogdan J Matuszewski, Elia Bruni, Urko Sanchez, Anton Böhm, Olaf Ronneberger, Bassem Ben Cheikh, Daniel Racoceanu, Philipp Kainz, Michael Pfeiffer, Martin Urschler, David RJ Snead, Nasir M Rajpoot",
            "전체 인용횟수": "655회 인용2016201720182019202020212022202393140959496125161",
            "출처": "Medical image analysis",
            "페이지": "489-502",
            "학술 문서": "Gland segmentation in colon histology images: The glas challenge contestK Sirinukunwattana, JPW Pluim, H Chen, X Qi… - Medical image analysis, 2017655회 인용 관련 학술자료 전체 30개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Gland segmentation in colon histology images: The glas challenge contest",
        "year": null
    },
    "An objective comparison of cell-tracking algorithms": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/12/1",
            "게시자": "Nature Publishing Group UK",
            "권": "14",
            "설명": "We present a combined report on the results of three editions of the Cell Tracking Challenge, an ongoing initiative aimed at promoting the development and objective evaluation of cell segmentation and tracking algorithms. With 21 participating algorithms and a data repository consisting of 13 data sets from various microscopy modalities, the challenge displays today's state-of-the-art methodology in the field. We analyzed the challenge results using performance measures for segmentation and tracking that rank all participating methods. We also analyzed the performance of all of the algorithms in terms of biological measures and practical usability. Although some methods scored high in all technical aspects, none obtained fully correct solutions. We found that methods that either take prior information into account using learning strategies or analyze cells in a global spatiotemporal video context performed better …",
            "저널": "Nature methods",
            "저자": "Vladimír Ulman, Martin Maška, Klas EG Magnusson, Olaf Ronneberger, Carsten Haubold, Nathalie Harder, Pavel Matula, Petr Matula, David Svoboda, Miroslav Radojevic, Ihor Smal, Karl Rohr, Joakim Jaldén, Helen M Blau, Oleh Dzyubachyk, Boudewijn Lelieveldt, Pengdong Xiao, Yuexiang Li, Siu-Yeung Cho, Alexandre C Dufour, Jean-Christophe Olivo-Marin, Constantino C Reyes-Aldasoro, Jose A Solis-Lemus, Robert Bensch, Thomas Brox, Johannes Stegmaier, Ralf Mikut, Steffen Wolf, Fred A Hamprecht, Tiago Esteves, Pedro Quelhas, Ömer Demirel, Lars Malmström, Florian Jug, Pavel Tomancak, Erik Meijering, Arrate Muñoz-Barrutia, Michal Kozubek, Carlos Ortiz-de-Solorzano",
            "전체 인용횟수": "519회 인용201720182019202020212022202355176889911281",
            "페이지": "1141-1152",
            "학술 문서": "An objective comparison of cell-tracking algorithmsV Ulman, M Maška, KEG Magnusson, O Ronneberger… - Nature methods, 2017519회 인용 관련 학술자료 전체 25개의 버전 ",
            "호": "12"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "An objective comparison of cell-tracking algorithms",
        "year": null
    },
    "A probabilistic u-net for segmentation of ambiguous images": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "권": "31",
            "설명": "Many real-world vision problems suffer from inherent ambiguities. In clinical applications for example, it might not be clear from a CT scan alone which particular region is cancer tissue. Therefore a group of graders typically produces a set of diverse but plausible segmentations. We consider the task of learning a distribution over segmentations given an input. To this end we propose a generative segmentation model based on a combination of a U-Net with a conditional variational autoencoder that is capable of efficiently producing an unlimited number of plausible hypotheses. We show on a lung abnormalities segmentation task and on a Cityscapes segmentation task that our model reproduces the possible segmentation variants as well as the frequencies with which they occur, doing so significantly better than published approaches. These models could have a high impact in real-world applications, such as being used as clinical decision-making algorithms accounting for multiple plausible semantic segmentation hypotheses to provide possible diagnoses and recommend further actions to resolve the present ambiguities.",
            "저널": "Advances in neural information processing systems",
            "저자": "Simon Kohl, Bernardino Romera-Paredes, Clemens Meyer, Jeffrey De Fauw, Joseph R Ledsam, Klaus Maier-Hein, SM Eslami, Danilo Jimenez Rezende, Olaf Ronneberger",
            "전체 인용횟수": "495회 인용20182019202020212022202324193108127120",
            "학술 문서": "A probabilistic u-net for segmentation of ambiguous imagesS Kohl, B Romera-Paredes, C Meyer, J De Fauw… - Advances in neural information processing systems, 2018495회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A probabilistic u-net for segmentation of ambiguous images",
        "year": null
    },
    "A new fate mapping system reveals context-dependent random or clonal expansion of microglia": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/6",
            "게시자": "Nature Publishing Group US",
            "권": "20",
            "설명": "Microglia constitute a highly specialized network of tissue-resident immune cells that is important for the control of tissue homeostasis and the resolution of diseases of the CNS. Little is known about how their spatial distribution is established and maintained in vivo. Here we establish a new multicolor fluorescence fate mapping system to monitor microglial dynamics during steady state and disease. Our findings suggest that microglia establish a dense network with regional differences, and the high regional turnover rates found challenge the universal concept of microglial longevity. Microglial self-renewal under steady state conditions constitutes a stochastic process. During pathology this randomness shifts to selected clonal microglial expansion. In the resolution phase, excess disease-associated microglia are removed by a dual mechanism of cell egress and apoptosis to re-establish the stable microglial network …",
            "저널": "Nature neuroscience",
            "저자": "Tuan Leng Tay, Dominic Mai, Jana Dautzenberg, Francisco Fernández-Klett, Gen Lin, null Sagar, Moumita Datta, Anne Drougard, Thomas Stempfl, Alberto Ardura-Fabregat, Ori Staszewski, Anca Margineanu, Anje Sporbert, Lars M Steinmetz, J Andrew Pospisilik, Steffen Jung, Josef Priller, Dominic Gruen, Olaf Ronneberger, Marco Prinz",
            "전체 인용횟수": "494회 인용201720182019202020212022202320789581937550",
            "페이지": "793-803",
            "학술 문서": "A new fate mapping system reveals context-dependent random or clonal expansion of microgliaTL Tay, D Mai, J Dautzenberg, F Fernández-Klett, G Lin… - Nature neuroscience, 2017494회 인용 관련 학술자료 전체 13개의 버전 ",
            "호": "6"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A new fate mapping system reveals context-dependent random or clonal expansion of microglia",
        "year": null
    },
    "U-Net: Convolutional networks for biomedical image segmentation. arXiv 2015": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "저널": "arXiv preprint arXiv:1505.04597",
            "저자": "Olaf Ronneberger, Philipp Fischer, Thomas Brox",
            "전체 인용횟수": "461회 인용2018201920202021202220235164984152155",
            "학술 문서": "U-Net: Convolutional networks for biomedical image segmentation. arXiv 2015O Ronneberger, P Fischer, T Brox - arXiv preprint arXiv:1505.04597, 2015461회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "U-Net: Convolutional networks for biomedical image segmentation. arXiv 2015",
        "year": null
    },
    "The medical segmentation decathlon": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2022/7/15",
            "게시자": "Nature Publishing Group UK",
            "권": "13",
            "설명": "International challenges have become the de facto standard for comparative assessment of image analysis algorithms. Although segmentation is the most widely investigated medical image processing task, the various challenges have been organized to focus only on specific clinical tasks. We organized the Medical Segmentation Decathlon (MSD)—a biomedical image analysis challenge, in which algorithms compete in a multitude of both tasks and modalities to investigate the hypothesis that a method capable of performing well on multiple tasks will generalize well to a previously unseen task and potentially outperform a custom-designed solution. MSD results confirmed this hypothesis, moreover, MSD winner continued generalizing well to a wide range of other clinical problems for the next two years. Three main conclusions can be drawn from this study: (1) state-of-the-art image segmentation algorithms …",
            "저널": "Nature communications",
            "저자": "Michela Antonelli, Annika Reinke, Spyridon Bakas, Keyvan Farahani, Annette Kopp-Schneider, Bennett A Landman, Geert Litjens, Bjoern Menze, Olaf Ronneberger, Ronald M Summers, Bram Van Ginneken, Michel Bilello, Patrick Bilic, Patrick F Christ, Richard KG Do, Marc J Gollub, Stephan H Heckers, Henkjan Huisman, William R Jarnagin, Maureen K McHugo, Sandy Napel, Jennifer S Golia Pernicka, Kawal Rhode, Catalina Tobon-Gomez, Eugene Vorontsov, James A Meakin, Sebastien Ourselin, Manuel Wiesenfarth, Pablo Arbeláez, Byeonguk Bae, Sihong Chen, Laura Daza, Jianjiang Feng, Baochun He, Fabian Isensee, Yuanfeng Ji, Fucang Jia, Ildoo Kim, Klaus Maier-Hein, Dorit Merhof, Akshay Pai, Beomhee Park, Mathias Perslev, Ramin Rezaiifar, Oliver Rippel, Ignacio Sarasua, Wei Shen, Jaemin Son, Christian Wachinger, Liansheng Wang, Yan Wang, Yingda Xia, Daguang Xu, Zhanwei Xu, Yefeng Zheng, Amber L Simpson, Lena Maier-Hein, M Jorge Cardoso",
            "전체 인용횟수": "459회 인용20212022202322128306",
            "페이지": "4128",
            "학술 문서": "The medical segmentation decathlonM Antonelli, A Reinke, S Bakas, K Farahani… - Nature communications, 2022459회 인용 관련 학술자료 전체 17개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The medical segmentation decathlon",
        "year": null
    },
    "Chemotaxonomic identification of single bacteria by micro-Raman spectroscopy: application to clean-room-relevant biological contaminations": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/3",
            "게시자": "American Society for Microbiology",
            "권": "71",
            "설명": " Microorganisms, such as bacteria, which might be present as contamination inside an industrial food or pharmaceutical clean room process need to be identified on short time scales in order to minimize possible health hazards as well as production downtimes causing financial deficits. Here we describe the first results of single-particle micro-Raman measurements in combination with a classification method, the so-called support vector machine technique, allowing for a fast, reliable, and nondestructive online identification method for single bacteria.",
            "저널": "Applied and environmental microbiology",
            "저자": "Petra Rösch, Michaela Harz, Michael Schmitt, Klaus-Dieter Peschke, Olaf Ronneberger, Hans Burkhardt, Hans-Walter Motzkus, Markus Lankers, Stefan Hofer, Hans Thiele, Jurgen Popp",
            "전체 인용횟수": "363회 인용200520062007200820092010201120122013201420152016201720182019202020212022202351721193239352326252113181631712106",
            "페이지": "1626-1637",
            "학술 문서": "Chemotaxonomic identification of single bacteria by micro-Raman spectroscopy: application to clean-room-relevant biological contaminationsP Rösch, M Harz, M Schmitt, KD Peschke… - Applied and environmental microbiology, 2005363회 인용 관련 학술자료 전체 25개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Chemotaxonomic identification of single bacteria by micro-Raman spectroscopy: application to clean-room-relevant biological contaminations",
        "year": null
    },
    "Standardized evaluation of algorithms for computer-aided diagnosis of dementia based on structural MRI: the CADDementia challenge": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/5/1",
            "게시자": "Academic Press",
            "권": "111",
            "설명": "Algorithms for computer-aided diagnosis of dementia based on structural MRI have demonstrated high performance in the literature, but are difficult to compare as different data sets and methodology were used for evaluation. In addition, it is unclear how the algorithms would perform on previously unseen data, and thus, how they would perform in clinical practice when there is no real opportunity to adapt the algorithm to the data at hand. To address these comparability, generalizability and clinical applicability issues, we organized a grand challenge that aimed to objectively compare algorithms based on a clinically representative multi-center data set. Using clinical practice as the starting point, the goal was to reproduce the clinical diagnosis. Therefore, we evaluated algorithms for multi-class classification of three diagnostic groups: patients with probable Alzheimer's disease, patients with mild cognitive impairment …",
            "저널": "NeuroImage",
            "저자": "Esther E Bron, Marion Smits, Wiesje M Van Der Flier, Hugo Vrenken, Frederik Barkhof, Philip Scheltens, Janne M Papma, Rebecca ME Steketee, Carolina Méndez Orellana, Rozanna Meijboom, Madalena Pinto, Joana R Meireles, Carolina Garrett, António J Bastos-Leite, Ahmed Abdulkadir, Olaf Ronneberger, Nicola Amoroso, Roberto Bellotti, David Cárdenas-Peña, Andrés M Álvarez-Meza, Chester V Dolph, Khan M Iftekharuddin, Simon F Eskildsen, Pierrick Coupé, Vladimir S Fonov, Katja Franke, Christian Gaser, Christian Ledig, Ricardo Guerrero, Tong Tong, Katherine R Gray, Elaheh Moradi, Jussi Tohka, Alexandre Routier, Stanley Durrleman, Alessia Sarica, Giuseppe Di Fatta, Francesco Sensi, Andrea Chincarini, Garry M Smith, Zhivko V Stoyanov, Lauge Sørensen, Mads Nielsen, Sabina Tangaro, Paolo Inglese, Christian Wachinger, Martin Reuter, John C van Swieten, Wiro J Niessen, Stefan Klein, Alzheimer's Disease Neuroimaging Initiative",
            "전체 인용횟수": "341회 인용20142015201620172018201920202021202220231133552444639413233",
            "페이지": "562-579",
            "학술 문서": "Standardized evaluation of algorithms for computer-aided diagnosis of dementia based on structural MRI: the CADDementia challengeEE Bron, M Smits, WM Van Der Flier, H Vrenken… - NeuroImage, 2015341회 인용 관련 학술자료 전체 40개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Standardized evaluation of algorithms for computer-aided diagnosis of dementia based on structural MRI: the CADDementia challenge",
        "year": null
    },
    "A benchmark for comparison of dental radiography analysis algorithms": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/7/1",
            "게시자": "Elsevier",
            "권": "31",
            "설명": "Dental radiography plays an important role in clinical diagnosis, treatment and surgery. In recent years, efforts have been made on developing computerized dental X-ray image analysis systems for clinical usages. A novel framework for objective evaluation of automatic dental radiography analysis algorithms has been established under the auspices of the IEEE International Symposium on Biomedical Imaging 2015 Bitewing Radiography Caries Detection Challenge and Cephalometric X-ray Image Analysis Challenge. In this article, we present the datasets, methods and results of the challenge and lay down the principles for future uses of this benchmark. The main contributions of the challenge include the creation of the dental anatomy data repository of bitewing radiographs, the creation of the anatomical abnormality classification data repository of cephalometric radiographs, and the definition of objective …",
            "저널": "Medical image analysis",
            "저자": "Ching-Wei Wang, Cheng-Ta Huang, Jia-Hong Lee, Chung-Hsing Li, Sheng-Wei Chang, Ming-Jhih Siao, Tat-Ming Lai, Bulat Ibragimov, Tomaž Vrtovec, Olaf Ronneberger, Philipp Fischer, Tim F Cootes, Claudia Lindner",
            "전체 인용횟수": "309회 인용2016201720182019202020212022202337253048577462",
            "페이지": "63-76",
            "학술 문서": "A benchmark for comparison of dental radiography analysis algorithmsCW Wang, CT Huang, JH Lee, CH Li, SW Chang… - Medical image analysis, 2016309회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A benchmark for comparison of dental radiography analysis algorithms",
        "year": null
    },
    "Deep learning to achieve clinically applicable segmentation of head and neck anatomy for radiotherapy": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/9/12",
            "설명": "Over half a million individuals are diagnosed with head and neck cancer each year worldwide. Radiotherapy is an important curative treatment for this disease, but it requires manual time consuming delineation of radio-sensitive organs at risk (OARs). This planning process can delay treatment, while also introducing inter-operator variability with resulting downstream radiation dose differences. While auto-segmentation algorithms offer a potentially time-saving solution, the challenges in defining, quantifying and achieving expert performance remain. Adopting a deep learning approach, we demonstrate a 3D U-Net architecture that achieves expert-level performance in delineating 21 distinct head and neck OARs commonly segmented in clinical practice. The model was trained on a dataset of 663 deidentified computed tomography (CT) scans acquired in routine clinical practice and with both segmentations taken from clinical practice and segmentations created by experienced radiographers as part of this research, all in accordance with consensus OAR definitions. We demonstrate the model's clinical applicability by assessing its performance on a test set of 21 CT scans from clinical practice, each with the 21 OARs segmented by two independent experts. We also introduce surface Dice similarity coefficient (surface DSC), a new metric for the comparison of organ delineation, to quantify deviation between OAR surface contours rather than volumes, better reflecting the clinical task of correcting errors in the automated organ segmentations. The model's generalisability is then demonstrated on two distinct open source datasets, reflecting different …",
            "저널": "arXiv preprint arXiv:1809.04430",
            "저자": "Stanislav Nikolov, Sam Blackwell, Alexei Zverovitch, Ruheena Mendes, Michelle Livne, Jeffrey De Fauw, Yojan Patel, Clemens Meyer, Harry Askham, Bernardino Romera-Paredes, Christopher Kelly, Alan Karthikesalingam, Carlton Chu, Dawn Carnell, Cheng Boon, Derek D'Souza, Syed Ali Moinuddin, Bethany Garie, Yasmin McQuinlan, Sarah Ireland, Kiarna Hampton, Krystle Fuller, Hugh Montgomery, Geraint Rees, Mustafa Suleyman, Trevor Back, Cían Hughes, Joseph R Ledsam, Olaf Ronneberger",
            "전체 인용횟수": "295회 인용20182019202020212022202332562777552",
            "학술 문서": "Deep learning to achieve clinically applicable segmentation of head and neck anatomy for radiotherapyS Nikolov, S Blackwell, A Zverovitch, R Mendes… - arXiv preprint arXiv:1809.04430, 2018295회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep learning to achieve clinically applicable segmentation of head and neck anatomy for radiotherapy",
        "year": null
    },
    "Comprehensive catecholaminergic projectome analysis reveals single-neuron integration of zebrafish ascending and descending dopaminergic systems": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/1/25",
            "게시자": "Nature Publishing Group UK",
            "권": "2",
            "설명": "Essential components of animal behaviour are modulated by dopaminergic (DA) and noradrenergic circuitry. In this study, we reveal at cellular resolution the complete set of projections ('projectome') of every single type of DA and noradrenergio neurons in the central nervous system of zebrafish larvae. The most extensive DA projections are established by posterior tubercular otp-dependent neurons, with individual somata integrating the ascending DA system, the descending diencephalospinal, as well as the endohypothalamic circuitry. These findings suggest a major role in the modulation of physiology and behaviour for otp-dependent DA neurons, which correlate with the mammalian A11 group. We further identified an endogenous subpallial DA system that not only provides most of the local DA projections, but also connects to the ventral diencephalon. The catecholaminergic projectome map provides a …",
            "저널": "Nature communications",
            "저자": "Tuan Leng Tay, Olaf Ronneberger, Soojin Ryu, Roland Nitschke, Wolfgang Driever",
            "전체 인용횟수": "293회 인용20112012201320142015201620172018201920202021202220235192716222227182731292821",
            "페이지": "171",
            "학술 문서": "Comprehensive catecholaminergic projectome analysis reveals single-neuron integration of zebrafish ascending and descending dopaminergic systemsTL Tay, O Ronneberger, S Ryu, R Nitschke, W Driever - Nature communications, 2011293회 인용 관련 학술자료 전체 12개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Comprehensive catecholaminergic projectome analysis reveals single-neuron integration of zebrafish ascending and descending dopaminergic systems",
        "year": null
    },
    "Combined Measurement of the Higgs Boson Mass in  Collisions at  and 8 TeV with the ATLAS and CMS Experiments": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/3/26",
            "설명": "A measurement of the Higgs boson mass is presented based on the combined data samples of the ATLAS and CMS experiments at the CERN LHC in the  and  decay channels. The results are obtained from a simultaneous fit to the reconstructed invariant mass peaks in the two channels and for the two experiments. The measured masses from the individual channels and the two experiments are found to be consistent among themselves. The combined measured mass of the Higgs boson is .",
            "저널": "arXiv preprint arXiv:1503.07589",
            "저자": "CMS Collaborations",
            "전체 인용횟수": "2572회 인용20152016201720182019202020212022202323047247246730321316712985",
            "학술 문서": "Combined Measurement of the Higgs Boson Mass in $ pp $ Collisions at $\\sqrt {s}= 7$ and 8 TeV with the ATLAS and CMS ExperimentsCMS Collaborations - arXiv preprint arXiv:1503.07589, 20151164회 인용 관련 학술자료 전체 50개의 버전 Combined Measurement of the Higgs Boson Mass in p p Collisions at s= 7 and 8 TeV with the ATLAS and CMS ExperimentsG Aad, B Abbott, J Abdallah, R Aben, M Abolins… - Physical review letters, 2015939회 인용 관련 학술자료 전체 157개의 버전 ATLAS and CMS collaborations*G Aad - Phys. Rev. Lett, 2015770회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Combined Measurement of the Higgs Boson Mass in  Collisions at  and 8 TeV with the ATLAS and CMS Experiments",
        "year": null
    },
    "Jet energy measurement and its systematic uncertainty in proton–proton collisions at  TeV with the ATLAS detector": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/1",
            "게시자": "Springer Berlin Heidelberg",
            "권": "75",
            "설명": " The jet energy scale (JES) and its systematic uncertainty are determined for jets measured with the ATLAS detector using proton–proton collision data with a centre-of-mass energy of  TeV corresponding to an integrated luminosity of  . Jets are reconstructed from energy deposits forming topological clusters of calorimeter cells using the anti- algorithm with distance parameters  or , and are calibrated using MC simulations. A residual JES correction is applied to account for differences between data and MC simulations. This correction and its systematic uncertainty are estimated using a combination of in situ techniques exploiting the transverse momentum balance between a jet and a reference object such as a photon or a  boson, for  and pseudorapidities . The effect of multiple proton–proton interactions is corrected for, and an uncertainty is evaluated using in situ techniques …",
            "저널": "The European Physical Journal C",
            "저자": "Atlas Collaboration atlas. publications@ cern. ch, Georges Aad, T Abajyan, B Abbott, J Abdallah, S Abdel Khalek, O Abdinov, R Aben, B Abi, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, TN Addy, J Adelman, S Adomeit, T Adye, S Aefsky, T Agatonovic-Jovin, JA Aguilar-Saavedra, M Agustoni, SP Ahlen, A Ahmad, F Ahmadov, G Aielli, TPA Åkesson, G Akimoto, AV Akimov, MA Alam, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, F Alessandria, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, M Aliev, G Alimonti, L Alio, J Alison, BMM Allbrooke, LJ Allison, PP Allport, SE Allwood-Spiers, J Almond, A Aloisio, R Alon, A Alonso, F Alonso, A Altheimer, B Alvarez Gonzalez, MG Alviggi, K Amako, Y Amaral Coutinho, C Amelung, VV Ammosov, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, S Angelidakis, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, ATH Arce, S Arfaoui, JF Arguin, S Argyropoulos, E Arik, M Arik, AJ Armbruster, O Arnaez, V Arnal, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, S Ask, B Åsman, L Asquith, K Assamagan, R Astalos, A Astbury, M Atkinson, NB Atlay, B Auerbach, E Auge, K Augsten, M Aurousseau, G Avolio, G Azuelos, Y Azuma, MA Baak, C Bacci, AM Bach, H Bachacou, K Bachas, M Backes, M Backhaus, J Backus Mayes, E Badescu, P Bagiacchi, P Bagnaia, Y Bai, DC Bailey, T Bain, JT Baines, OK Baker, S Baker, P Balek, F Balli, E Banas, Sw Banerjee, D Banfi, A Bangert, V Bansal",
            "전체 인용횟수": "2063회 인용20142015201620172018201920202021202220237650445945224713052603232",
            "페이지": "1-101",
            "학술 문서": "Jet energy measurement and its systematic uncertainty in proton–proton collisions at s= 7 s= 7 TeV with the ATLAS detectorAtlas Collaboration atlas. publications@ cern. ch… - The European Physical Journal C, 20152063회 인용 관련 학술자료 전체 118개의 버전 Jet energy measurement and its systematic uncertainty in proton–proton collisions at√ s= 7 TeV with the ATLAS detectorMJ Alconada Verzini, F Alonso, XS Anduaga, MT Dova… - The European Physical Journal C, 2015관련 학술자료 전체 2개의 버전 Jet energy measurement and its systematic uncertainty in proton-proton collisions at root s= 7 TeV with the ATLAS detector*MA Díaz - 2015관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Jet energy measurement and its systematic uncertainty in proton–proton collisions at  TeV with the ATLAS detector",
        "year": null
    },
    "Performance of b-jet identification in the ATLAS experiment": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/4/4",
            "게시자": "IOP Publishing",
            "권": "11",
            "설명": "The identification of jets containing b hadrons is important for the physics programme of the ATLAS experiment at the Large Hadron Collider. Several algorithms to identify jets containing b hadrons are described, ranging from those based on the reconstruction of an inclusive secondary vertex or the presence of tracks with large impact parameters to combined tagging algorithms making use of multi-variate discriminants. An independent b-tagging algorithm based on the reconstruction of muons inside jets as well as the b-tagging algorithm used in the online trigger are also presented. The b-jet tagging efficiency, the c-jet tagging efficiency and the mistag rate for light flavour jets in data have been measured with a number of complementary methods. The calibration results are presented as scale factors defined as the ratio of the efficiency (or mistag rate) in data to that in simulation. In the case of b jets, where more …",
            "저널": "Journal of instrumentation",
            "저자": "Atlas Collaboration",
            "전체 인용횟수": "1285회 인용20162017201820192020202120222023132261407209111635735",
            "페이지": "P04008",
            "학술 문서": "Performance of b-jet identification in the ATLAS experimentAtlas Collaboration - Journal of instrumentation, 20161285회 인용 관련 학술자료 전체 80개의 버전 ",
            "호": "04"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Performance of b-jet identification in the ATLAS experiment",
        "year": null
    },
    "Measurements of Higgs boson production and couplings in diboson final states with the ATLAS detector at the LHC": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/10/7",
            "게시자": "North-Holland",
            "권": "726",
            "설명": "Measurements are presented of production properties and couplings of the recently discovered Higgs boson using the decays into boson pairs, H→ γ γ, H→ Z Z⁎→ 4 ℓ and H→ W W⁎→ ℓ ν ℓ ν. The results are based on the complete pp collision data sample recorded by the ATLAS experiment at the CERN Large Hadron Collider at centre-of-mass energies of s= 7 TeV and s= 8 TeV, corresponding to an integrated luminosity of about 25 fb− 1. Evidence for Higgs boson production through vector-boson fusion is reported. Results of combined fits probing Higgs boson couplings to fermions and bosons, as well as anomalous contributions to loop-induced production and decay modes, are presented. All measurements are consistent with expectations for the Standard Model Higgs boson.",
            "저널": "Physics Letters B",
            "저자": "Georges Aad, Tatevik Abajyan, Brad Abbott, Jalal Abdallah, S Abdel Khalek, Rosemarie Aben, B Abi, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, TN Addy, J Adelman, S Adomeit, T Adye, S Aefsky, T Agatonovic-Jovin, JA Aguilar-Saavedra, M Agustoni, SP Ahlen, A Ahmad, M Ahsan, G Aielli, TPA Åkesson, G Akimoto, AV Akimov, MA Alam, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, F Alessandria, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, M Aliev, G Alimonti, L Alio, J Alison, BMM Allbrooke, LJ Allison, PP Allport, SE Allwood-Spiers, J Almond, A Aloisio, R Alon, A Alonso, F Alonso, A Altheimer, B Alvarez Gonzalez, MG Alviggi, K Amako, Y Amaral Coutinho, C Amelung, VV Ammosov, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, S Angelidakis, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, ATH Arce, S Arfaoui, JF Arguin, S Argyropoulos, E Arik, M Arik, AJ Armbruster, O Arnaez, V Arnal, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, S Ask, B Åsman, L Asquith, K Assamagan, R Astalos, A Astbury, M Atkinson, NB Atlay, B Auerbach, E Auge, K Augsten, M Aurousseau, G Avolio, D Axen, G Azuelos, Y Azuma, MA Baak, C Bacci, AM Bach, H Bachacou, K Bachas, M Backes, M Backhaus, J Backus Mayes, E Badescu, P Bagiacchi, P Bagnaia, Y Bai, DC Bailey, T Bain, JT Baines, OK Baker, S Baker, P Balek, F Balli, E Banas, Sw Banerjee, D Banfi, A Bangert, V Bansal, HS Bansil, L Barak",
            "전체 인용횟수": "1233회 인용201320142015201620172018201920202021202220239438334312998574228231511",
            "페이지": "88-119",
            "학술 문서": "Measurements of Higgs boson production and couplings in diboson final states with the ATLAS detector at the LHCG Aad, T Abajyan, B Abbott, J Abdallah, SA Khalek… - Physics Letters B, 20131233회 인용 관련 학술자료 전체 104개의 버전 ",
            "호": "1-3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Measurements of Higgs boson production and couplings in diboson final states with the ATLAS detector at the LHC",
        "year": null
    },
    "The roostats project": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/9/6",
            "설명": "RooStats is a project to create advanced statistical tools required for the analysis of LHC data, with emphasis on discoveries, confidence intervals, and combined measurements. The idea is to provide the major statistical techniques as a set of C++ classes with coherent interfaces, so that can be used on arbitrary model and datasets in a common way. The classes are built on top of the RooFit package, which provides functionality for easily creating probability models, for analysis combinations and for digital publications of the results. We will present in detail the design and the implementation of the different statistical methods of RooStats. We will describe the various classes for interval estimation and for hypothesis test depending on different statistical techniques such as those based on the likelihood function, or on frequentists or bayesian statistics. These methods can be applied in complex problems, including cases with multiple parameters of interest and various nuisance parameters.",
            "저널": "arXiv preprint arXiv:1009.1003",
            "저자": "Lorenzo Moneta, Kevin Belasco, Kyle Cranmer, Sven Kreiss, Alfio Lazzaro, Danilo Piparo, Gregory Schott, Wouter Verkerke, Matthias Wolf",
            "전체 인용횟수": "1150회 인용201020112012201320142015201620172018201920202021202220235336065728611112317411855936883",
            "학술 문서": "The roostats projectL Moneta, K Belasco, K Cranmer, S Kreiss, A Lazzaro… - arXiv preprint arXiv:1009.1003, 20101150회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The roostats project",
        "year": null
    },
    "Measurements of the Higgs boson production and decay rates and coupling strengths using pp collision data at  s = 7 and 8 TeV in the ATLAS experiment": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/1",
            "게시자": "Springer Berlin Heidelberg",
            "권": "76",
            "설명": "Combined analyses of the Higgs boson production and decay rates as well as its coupling strengths to vector bosons and fermions are presented. The combinations include the results of the analyses of the and decay modes, and the constraints on the associated production with a pair of top quarks and on the off-shell coupling strengths of the Higgs boson. The results are based on the LHC proton-proton collision datasets, with integrated luminosities of up to 4.7 at TeV and 20.3 at TeV, recorded by the ATLAS detector in 2011 and 2012. Combining all production modes and decay channels, the measured signal yield, normalised to the Standard Model expectation, is. The observed Higgs boson production and decay rates are interpreted in a leading-order coupling framework, exploring a wide range of benchmark coupling models both with and without …",
            "저널": "The European Physical Journal C",
            "저자": "Georges Aad, Brad Abbott, Jalal Abdallah, R Aben, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, J Adelman, S Adomeit, T Adye, AA Affolder, T Agatonovic-Jovin, JA Aguilar-Saavedra, SP Ahlen, F Ahmadov, G Aielli, H Akerstedt, TPA Åkesson, G Akimoto, AV Akimov, GL Alberghi, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, T Alexopoulos, M Alhroob, G Alimonti, L Alio, J Alison, SP Alkire, BMM Allbrooke, PP Allport, A Aloisio, A Alonso, F Alonso, C Alpigiani, A Altheimer, B Alvarez Gonzalez, D Álvarez Piqueras, MG Alviggi, BT Amadio, K Amako, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, JK Anders, KJ Anderson, A Andreazza, V Andrei, S Angelidakis, I Angelozzi, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, Nuno Anjos, A Annovi, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, G Arabidze, Y Arai, JP Araque, ATH Arce, FA Arduh, JF Arguin, S Argyropoulos, M Arik, AJ Armbruster, O Arnaez, V Arnal, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, A Ashkenazi, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, B Auerbach, K Augsten, M Aurousseau, G Avolio, B Axen, MK Ayoub, G Azuelos, MA Baak, AE Baas, C Bacci, H Bachacou, K Bachas, M Backes, M Backhaus, P Bagiacchi, P Bagnaia, Y Bai, T Bain, JT Baines, OK Baker, Petr Balek, T Balestri, F Balli, E Banas, Sw Banerjee, AAE Bannoura, HS Bansil, L Barak, EL Barberio, D Barberis, M Barbero, T Barillari, M Barisonzi, T Barklow, N Barlow, SL Barnes, BM Barnett, RM Barnett, Z Barnovska, A Baroncelli, G Barone",
            "전체 인용횟수": "932회 인용20152016201720182019202020212022202310421213122110046543123",
            "페이지": "1-51",
            "학술 문서": "Measurements of the Higgs boson production and decay rates and coupling strengths using pp collision data at $$\\sqrt {s}= 7$$ s= 7 and 8 TeV in the ATLAS experimentG Aad, B Abbott, J Abdallah, R Aben, M Abolins… - The European Physical Journal C, 2016928회 인용 관련 학술자료 전체 77개의 버전 Measurements of the Higgs boson production and decay rates and coupling strengths using pp collision data at $\\sqrt {s}= 7$ and 8 TeV in the ATLAS experimentG Aad, B Abbott, J Abdallah, O Abdinov, R Aben… - Eur. Phys. JC, 20165회 인용 관련 학술자료 전체 31개의 버전 Measurements of the Higgs boson production and decay rates and coupling strengths using pp collision data at√ s= 7 and 8*ATLAS collaboration, G Aad - TeV in the ATLAS experiment2회 인용 관련 학술자료 Measurements of the Higgs boson production and decay rates and coupling strengths using pp collision data at root s= 7 and 8 TeV in the ATLAS experimentATLAS Collaboration - 20161회 인용 관련 학술자료 Measurements of the Higgs boson production and decay rates and coupling strengths using pp collision data at [... formula...] and 8 TeV in the ATLAS experimentG Aad, B Abbott, J Abdallah, R Aben, M Abolins… - The European Physical Journal. C, Particles and Fields, 20161회 인용 관련 학술자료 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Measurements of the Higgs boson production and decay rates and coupling strengths using pp collision data at  s = 7 and 8 TeV in the ATLAS experiment",
        "year": null
    },
    "Observation of Associated Near-Side and Away-Side Long-Range Correlations in  Proton-Lead Collisions with the ATLAS Detector": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/5/1",
            "게시자": "American Physical Society",
            "권": "110",
            "설명": "Two-particle correlations in relative azimuthal angle (Δ ϕ) and pseudorapidity (Δ η) are measured in s NN= 5.02 TeV p+ Pb collisions using the ATLAS detector at the LHC. The measurements are performed using approximately 1 μ b− 1 of data as a function of transverse momentum (p T) and the transverse energy (Σ E T Pb) summed over 3.1< η< 4.9 in the direction of the Pb beam. The correlation function, constructed from charged particles, exhibits a long-range (2<| Δ η|< 5)“near-side”(Δ ϕ∼ 0) correlation that grows rapidly with increasing Σ E T Pb. A long-range “away-side”(Δ ϕ∼ π) correlation, obtained by subtracting the expected contributions from recoiling dijets and other sources estimated using events with small Σ E T Pb, is found to match the near-side correlation in magnitude, shape (in Δ η and Δ ϕ) and Σ E T Pb dependence. The resultant Δ ϕ correlation is approximately symmetric about π/2, and is …",
            "저널": "Physical review letters",
            "저자": "Georges Aad, Tatevik Abajyan, Brad Abbott, Jalal Abdallah, S Abdel Khalek, AA Abdelalim, R Aben, B Abi, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, BS Acharya, L Adamczyk, DL Adams, TN Addy, J Adelman, S Adomeit, P Adragna, T Adye, S Aefsky, JA Aguilar-Saavedra, M Agustoni, SP Ahlen, F Ahles, A Ahmad, M Ahsan, G Aielli, TPA Åkesson, G Akimoto, AV Akimov, MA Alam, J Albert, S Albrand, M Aleksa, IN Aleksandrov, F Alessandria, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, M Aliev, G Alimonti, J Alison, BMM Allbrooke, LJ Allison, PP Allport, SE Allwood-Spiers, J Almond, A Aloisio, R Alon, Alejandro Alonso, F Alonso, A Altheimer, B Alvarez Gonzalez, MG Alviggi, K Amako, C Amelung, VV Ammosov, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, S Angelidakis, P Anger, A Angerami, F Anghinolfi, A Anisenkov, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, ATH Arce, S Arfaoui, JF Arguin, S Argyropoulos, E Arik, M Arik, AJ Armbruster, O Arnaez, V Arnal, A Artamonov, G Artoni, D Arutinov, S Asai, S Ask, B Åsman, L Asquith, K Assamagan, R Astalos, A Astbury, M Atkinson, B Auerbach, E Auge, K Augsten, M Aurousseau, G Avolio, D Axen, G Azuelos, Y Azuma, MA Baak, G Baccaglioni, C Bacci, AM Bach, H Bachacou, K Bachas, M Backes, M Backhaus, J Backus Mayes, E Badescu, P Bagnaia, Y Bai, DC Bailey, T Bain, JT Baines, OK Baker, S Baker, P Balek, F Balli, E Banas, P Banerjee, Sw Banerjee, D Banfi, A Bangert, V Bansal, HS Bansil, L Barak, SP Baranov, T Barber, EL Barberio",
            "전체 인용횟수": "928회 인용2012201320142015201620172018201920202021202220233561351021151171027957524761",
            "페이지": "182302",
            "학술 문서": "Observation of associated near-side and away-side long-range correlations in s N N= 5.02 TeV proton-lead collisions with the ATLAS detectorG Aad, T Abajyan, B Abbott, J Abdallah, SA Khalek… - Physical review letters, 2013906회 인용 관련 학술자료 전체 72개의 버전 Observation of Associated Near-side and Away-side Long-range Correlations in√ sNN= 5.02 TeV Proton-lead Collisions with the ATLAS DetectorG Aad, T Abajyan, B Abbott, J Abdallah, SA Khalek… - 201218회 인용 관련 학술자료 전체 19개의 버전 Observation of Associated Near-Side and Away-Side Long-Range Correlations in*G Aad - 20134회 인용 관련 학술자료 전체 5개의 버전 Observation of Associated Near-Side and Away-Side Long-Range Correlations in√ sNN= 5.02 TeV Proton-Lead Collisions with the ATLAS DetectorC ATLAS, M Agustoni, L Ancu, A Battaglia, HP Beck… - Physical review letters, 20134회 인용 관련 학술자료 Observation of associated near-side and away-side long-range correlations in root S-NN= 5.02 TeV Proton-Lead collisions with the ATLAS DetectorATLAS Collaboration - 2013관련 학술자료 전체 3개의 버전 ",
            "호": "18"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Observation of Associated Near-Side and Away-Side Long-Range Correlations in  Proton-Lead Collisions with the ATLAS Detector",
        "year": null
    },
    "Measurement of the inclusive W±and Z/γ* cross sections in the e and μ decay channels in pp collisions at√ s= 7 TeV with the ATLAS detector": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012",
            "게시자": "American Physical Society",
            "권": "85",
            "설명": "The inclusive Drell-Yan [1] production cross sections of W and Z bosons have been an important testing ground for QCD. Theoretical calculations of this process extend to next-to-leading order (NLO)[2–4] and next-to-next-toleading order (NNLO)[5–9] perturbation theory. Crucial ingredients of the resulting QCD cross section calculations are the parametrizations of the momentum distribution functions of partons in the proton (PDFs). These have been determined recently in a variety of phenomenological analyses to NLO QCD by the CTEQ [10, 11] group and to NNLO by the MSTW [12], ABKM [13, 14], HERAPDF [15, 16], JR [17], and NNPDF [18, 19] groups. The present measurement determines the cross sections times leptonic branching ratios, WÆ Á BRðW!‘Þ and Z= Ã Á BRðZ= Ã!‘‘Þ, of inclusive W and Z production for electron and muon final states, where ‘¼ e,. Compared to the initial measurement by the ATLAS Collaboration [20], the data set is enlarged by 100 and the luminosity uncertainty significantly reduced [21] from 11% to 3.4%. The CMS Collaboration has updated their initial measurement of total W and Z cross sections [22] to include data corresponding to an integrated luminosity similar to that used here [23]. Similar measurements have been performed at the p\" p collider Tevatron by the CDF and D0 collaborations [24, 25]. The presented cross section values are integrated over the fiducial region of the analysis and also extrapolated to the full kinematic range. The data are also reported differentially, as functions of the lepton pseudorapidity, 3 l, for the Wæ cross sections, and of the boson rapidity, yZ, for the Z= Ã cross section. For the …",
            "저널": "Physical review. D-particles, fields, gravitation, and cosmology",
            "저자": "Lucian Ancu, Andreas Battaglia, Hans Peter Beck, Claudia Borer, Antonio Ereditato, Maria Fonseca, Valentina Gallo, Sigve Haug, Sonja Kabana, Tobias Kruker, Lukas Marti, Klaus Pretzl, Cyril Topfel, Nicola Venturi, Michael Weber, ATLAS Collaboration",
            "전체 인용횟수": "838회 인용2011201220132014201520162017201820192020202120222023181261561259012965471710211910",
            "페이지": "72004",
            "학술 문서": "Measurement of the inclusive W±and Z/γ* cross sections in the e and μ decay channels in pp collisions at√ s= 7 TeV with the ATLAS detectorL Ancu, A Battaglia, HP Beck, C Borer, A Ereditato… - Physical review. D-particles, fields, gravitation, and …, 2012546회 인용 관련 학술자료 전체 23개의 버전 Measurement of the isolated diphoton cross-section in pp collisions at sqrt (s)= 7 TeV with the ATLAS detector*Atlas Collaboration - arXiv preprint arXiv:1107.0581, 2011209회 인용 관련 학술자료 전체 6개의 버전 A search for t̄t resonances with the ATLAS detector in 2.05 fb− 1 of proton-proton collisions at s=7~TeV*ATLAS Collaboration atlas. publications@ cern. ch… - The European Physical Journal C, 2012101회 인용 관련 학술자료 전체 73개의 버전 A Search for tt Resonances with ATLAS in 2.05 fb− 1 of Proton-Proton Collisions at√ s= 7 TeV*ATLAS Collaboration - arXiv preprint arXiv:1205.53714회 인용 관련 학술자료 A search for $ t\\bar {t} $ resonances with the ATLAS detector in 2.05 fb-1 of proton-proton collisions at $\\sqrt {s}= 7~\\mathrm {TeV} $*G Aad, B Abbott, J Abdallah, S Abdel Khalek… - European Physical Journal. C, Particles and Fields, 20121회 인용 관련 학술자료 전체 11개의 버전 Kshort and Lambda production in pp interactions at sqrt (s)= 0.9 and 7 TeV measured with the ATLAS detector at the LHC*ATLAS Collaboration - arXiv preprint arXiv:1111.1297, 20111회 인용 관련 학술자료 Measurement of the inclusive W±and Z/γ* cross sections in the e and μ decay channels in pp collisions at√ s= 7 TeV with the ATLAS detectorG Aad, B Acharya - Phys. Rev., 2012A search for t (t) over-bar resonances with the ATLAS detector in 2.05 fb (-1) of proton-proton collisions at root s= 7 TeV*ATLAS Collaboration - 2012관련 학술자료 전체 2개의 버전 A search for ttbar resonances with the ATLAS detector in 2.05 fb^-1 of proton-proton collisions at sqrt (s)= 7 TeV*W Bhimji, R Harrington, V Martin, ATLAS Collaboration - European Physical Journal C, 2012전체 2개의 버전 ",
            "호": "7"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Measurement of the inclusive W±and Z/γ* cross sections in the e and μ decay channels in pp collisions at√ s= 7 TeV with the ATLAS detector",
        "year": null
    },
    "Study of the spin and parity of the Higgs boson in diboson decays with the ATLAS detector": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/10",
            "게시자": "Springer Berlin Heidelberg",
            "권": "75",
            "설명": " Studies of the spin, parity and tensor couplings of the Higgs boson in the ,  and  decay processes at the LHC are presented. The investigations are based on  of pp collision data collected by the ATLAS experiment at  TeV and  TeV. The Standard Model (SM) Higgs boson hypothesis, corresponding to the quantum numbers , is tested against several alternative spin scenarios, including non-SM spin-0 and spin-2 models with universal and non-universal couplings to fermions and vector bosons. All tested alternative models are excluded in favour of the SM Higgs boson hypothesis at more than 99.9 % confidence level. Using the  and  decays, the tensor structure of the interaction between the spin-0 boson and the SM vector bosons is also investigated. The observed distributions of variables sensitive to the non-SM …",
            "저널": "The European Physical Journal C",
            "저자": "Georges Aad, B Abbott, J Abdallah, O Abdinov, R Aben, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, J Adelman, S Adomeit, T Adye, AA Affolder, T Agatonovic-Jovin, J Agricola, JA Aguilar-Saavedra, SP Ahlen, F Ahmadov, G Aielli, H Akerstedt, TPA Åkesson, AV Akimov, GL Alberghi, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, T Alexopoulos, M Alhroob, G Alimonti, L Alio, J Alison, SP Alkire, BMM Allbrooke, PP Allport, A Aloisio, A Alonso, F Alonso, C Alpigiani, A Altheimer, B Alvarez Gonzalez, D Alvarez Piqueras, MG Alviggi, BT Amadio, K Amako, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, JK Anders, KJ Anderson, A Andreazza, V Andrei, S Angelidakis, I Angelozzi, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, G Arabidze, Y Arai, JP Araque, ATH Arce, FA Arduh, JF Arguin, S Argyropoulos, M Arik, AJ Armbruster, O Arnaez, V Arnal, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, A Ashkenazi, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, B Auerbach, K Augsten, M Aurousseau, G Avolio, B Axen, MK Ayoub, G Azuelos, MA Baak, AE Baas, C Bacci, H Bachacou, K Bachas, M Backes, M Backhaus, P Bagiacchi, P Bagnaia, Y Bai, T Bain, JT Baines, OK Baker, EM Baldin, P Balek, T Balestri, F Balli, E Banas, Sw Banerjee, AAE Bannoura, HS Bansil, L Barak, EL Barberio, D Barberis, M Barbero, T Barillari, M Barisonzi, T Barklow, N Barlow, SL Barnes, BM Barnett, RM Barnett, Z Barnovska",
            "전체 인용횟수": "767회 인용2015201620172018201920202021202220233012211014611697545437",
            "페이지": "1-36",
            "학술 문서": "Study of the spin and parity of the Higgs boson in diboson decays with the ATLAS detectorG Aad, B Abbott, J Abdallah, O Abdinov, R Aben… - The European Physical Journal C, 2015767회 인용 관련 학술자료 전체 147개의 버전 Study of the spin and parity of the Higgs boson in diboson decays with the ATLAS detector (vol 75, 476, 2015) i*G Aad, B Abbott, J Abdallah, R Aben, M Abolins… - … PHYSICAL JOURNAL. C, PARTICLES AND FIELDS, 2016전체 3개의 버전 Study of the spin and parity of the Higgs boson in diboson decays with the ATLAS detectorA Collaboration - Eur. Phys. J. C, 2015관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Study of the spin and parity of the Higgs boson in diboson decays with the ATLAS detector",
        "year": null
    },
    "Evidence for the Higgs-boson Yukawa coupling to tau leptons with the ATLAS detector": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/4",
            "게시자": "Springer Berlin Heidelberg",
            "권": "2015",
            "설명": "Results of a search for H→ ττ decays are presented, based on the full set of proton-proton collision data recorded by the ATLAS experiment at the LHC during 2011 and 2012. The data correspond to integrated luminosities of 4.5 fb− 1 and 20.3 fb− 1 at centre-of-mass energies of TeV and TeV respectively. All combinations of leptonic ( with ℓ= e, μ) and hadronic (τ→ hadrons ν) tau decays are considered. An excess of events over the expected background from other Standard Model processes is found with an observed (expected) significance of 4.5 (3.4) standard deviations. This excess provides evidence for the direct coupling of the recently discovered Higgs boson to fermions. The measured signal strength, normalised to the Standard Model expectation, of μ= 1. 43+ 0.43− 0.37 is consistent with the predicted Yukawa coupling strength in the Standard Model.",
            "저널": "Journal of High Energy Physics",
            "저자": "Georges Aad, Brad Abbott, Jalal Abdallah, Samah Abdel Khalek, R Aben, B Abi, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, J Adelman, S Adomeit, T Adye, T Agatonovic-Jovin, JA Aguilar-Saavedra, M Agustoni, SP Ahlen, F Ahmadov, G Aielli, H Akerstedt, TPA Åkesson, G Akimoto, AV Akimov, GL Alberghi, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, G Alexandre, T Alexopoulos, Muhammad Alhroob, G Alimonti, L Alio, J Alison, BMM Allbrooke, LJ Allison, PP Allport, A Aloisio, A Alonso, F Alonso, C Alpigiani, A Altheimer, B Alvarez Gonzalez, D Álvarez Piqueras, MG Alviggi, K Amako, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, S Angelidakis, I Angelozzi, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, G Arabidze, Y Arai, JP Araque, ATH Arce, FA Arduh, JF Arguin, S Argyropoulos, M Arik, AJ Armbruster, O Arnaez, V Arnal, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, A Ashkenazi, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, B Auerbach, K Augsten, M Aurousseau, G Avolio, B Axen, G Azuelos, Y Azuma, MA Baak, AE Baas, C Bacci, H Bachacou, K Bachas, M Backes, M Backhaus, E Badescu, P Bagiacchi, P Bagnaia, Y Bai, T Bain, JT Baines, OK Baker, P Balek, F Balli, E Banas, Sw Banerjee, AAE Bannoura, HS Bansil, L Barak, SP Baranov, EL Barberio, D Barberis, M Barbero, T Barillari, M Barisonzi, T Barklow, N Barlow, SL Barnes, BM Barnett, RM Barnett",
            "전체 인용횟수": "740회 인용201420152016201720182019202020212022202361351851011059141202426",
            "페이지": "1-74",
            "학술 문서": "Evidence for the Higgs-boson Yukawa coupling to tau leptons with the ATLAS detectorG Aad, B Abbott, J Abdallah, S Abdel Khalek, R Aben… - Journal of High Energy Physics, 2015740회 인용 관련 학술자료 전체 89개의 버전 Evidence for the Higgs-boson Yukawa coupling to tau leptons with the ATLAS detectorATLAS collaboration - 2015관련 학술자료 전체 3개의 버전 Evidence for the Higgs-boson Yukawa coupling to tau leptons with the ATLAS detectorM Beckingham, PF Harrison, M Janus, C Jeske… - Journal of High Energy Physics, 2015관련 학술자료 전체 3개의 버전 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Evidence for the Higgs-boson Yukawa coupling to tau leptons with the ATLAS detector",
        "year": null
    },
    "Pyramid scene parsing network": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes.",
            "저자": "Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia",
            "전체 인용횟수": "12328회 인용201720182019202020212022202312167412911734244229882992",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "2881-2890",
            "학술 문서": "Pyramid scene parsing networkH Zhao, J Shi, X Qi, X Wang, J Jia - Proceedings of the IEEE conference on computer …, 201712328회 인용 관련 학술자료 전체 24개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pyramid scene parsing network",
        "year": null
    },
    "Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing text-to-image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256x256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.",
            "저자": "Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris N Metaxas",
            "전체 인용횟수": "3119회 인용2017201820192020202120222023110331477552565545512",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "5907-5915",
            "학술 문서": "Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networksH Zhang, T Xu, H Li, S Zhang, X Wang, X Huang… - Proceedings of the IEEE international conference on …, 20173119회 인용 관련 학술자료 전체 30개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks",
        "year": null
    },
    "Deepreid: Deep filter pairing neural network for person re-identification": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "설명": "Person re-identification is to match pedestrian images from disjoint camera views detected by pedestrian detectors. Challenges are presented in the form of complex variations of lightings, poses, viewpoints, blurring effects, image resolutions, camera settings, occlusions and background clutter across camera views. In addition, misalignment introduced by the pedestrian detector will affect most existing person re-identification methods that use manually cropped pedestrian images and assume perfect detection. In this paper, we propose a novel filter pairing neural network (FPNN) to jointly handle misalignment, photometric and geometric transforms, occlusions and background clutter. All the key components are jointly optimized to maximize the strength of each component when cooperating with others. In contrast to existing works that use handcrafted features, our method automatically learns features optimal for the re-identification task from data. The learned filter pairs encode photometric transforms. Its deep architecture makes it possible to model a mixture of complex photometric and geometric transforms. We build the largest benchmark re-id dataset with 13,164 images of 1,360 pedestrians. Unlike existing datasets, which only provide manually cropped pedestrian images, our dataset provides automatically detected bounding boxes for evaluation close to practical applications. Our neural network significantly outperforms state-of-the-art methods on this dataset.",
            "저자": "Wei Li, Rui Zhao, Tong Xiao, Xiaogang Wang",
            "전체 인용횟수": "2848회 인용20152016201720182019202020212022202350121254348485437435385286",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "152-159",
            "학술 문서": "Deepreid: Deep filter pairing neural network for person re-identificationW Li, R Zhao, T Xiao, X Wang - Proceedings of the IEEE conference on computer …, 20142848회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deepreid: Deep filter pairing neural network for person re-identification",
        "year": null
    },
    "Deep learning for generic object detection: A survey": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020/2",
            "게시자": "Springer US",
            "권": "128",
            "설명": " Object detection, one of the most fundamental and challenging problems in computer vision, seeks to locate object instances from a large number of predefined categories in natural images. Deep learning techniques have emerged as a powerful strategy for learning feature representations directly from data and have led to remarkable breakthroughs in the field of generic object detection. Given this period of rapid evolution, the goal of this paper is to provide a comprehensive survey of the recent achievements in this field brought about by deep learning techniques. More than 300 research contributions are included in this survey, covering many aspects of generic object detection: detection frameworks, object feature representation, object proposal generation, context modeling, training strategies, and evaluation metrics. We finish the survey by identifying promising directions for future research. ",
            "저널": "International journal of computer vision",
            "저자": "Li Liu, Wanli Ouyang, Xiaogang Wang, Paul Fieguth, Jie Chen, Xinwang Liu, Matti Pietikäinen",
            "전체 인용횟수": "2594회 인용20192020202120222023122398726752571",
            "페이지": "261-318",
            "학술 문서": "Deep learning for generic object detection: A surveyL Liu, W Ouyang, X Wang, P Fieguth, J Chen, X Liu… - International journal of computer vision, 20202594회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep learning for generic object detection: A survey",
        "year": null
    },
    "Pointrcnn: 3d object proposal generation and detection from point cloud": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "In this paper, we propose PointRCNN for 3D object detection from raw point cloud. The whole framework is composed of two stages: stage-1 for the bottom-up 3D proposal generation and stage-2 for refining proposals in the canonical coordinates to obtain the final detection results. Instead of generating proposals from RGB image or projecting point cloud to bird's view or voxels as previous methods do, our stage-1 sub-network directly generates a small number of high-quality 3D proposals from point cloud in a bottom-up manner via segmenting the point cloud of the whole scene into foreground points and background. The stage-2 sub-network transforms the pooled points of each proposal to canonical coordinates to learn better local spatial features, which is combined with global semantic features of each point learned in stage-1 for accurate box refinement and confidence prediction. Extensive experiments on the 3D detection benchmark of KITTI dataset show that our proposed architecture outperforms state-of-the-art methods with remarkable margins by using only point cloud as input. The code is available at https://github. com/sshaoshuai/PointRCNN.",
            "저자": "Shaoshuai Shi, Xiaogang Wang, Hongsheng Li",
            "전체 인용횟수": "2038회 인용2019202020212022202369285434619620",
            "컨퍼런스": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "페이지": "770-779",
            "학술 문서": "Pointrcnn: 3d object proposal generation and detection from point cloudS Shi, X Wang, H Li - Proceedings of the IEEE/CVF conference on computer …, 20192038회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pointrcnn: 3d object proposal generation and detection from point cloud",
        "year": null
    },
    "Pv-rcnn: Point-voxel feature set abstraction for 3d object detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020",
            "설명": "We present a novel and high-performance 3D object detection framework, named PointVoxel-RCNN (PV-RCNN), for accurate 3D object detection from point clouds. Our proposed method deeply integrates both 3D voxel Convolutional Neural Network (CNN) and PointNet-based set abstraction to learn more discriminative point cloud features. It takes advantages of efficient learning and high-quality proposals of the 3D voxel CNN and the flexible receptive fields of the PointNet-based networks. Specifically, the proposed framework summarizes the 3D scene with a 3D voxel CNN into a small set of keypoints via a novel voxel set abstraction module to save follow-up computations and also to encode representative scene features. Given the high-quality 3D proposals generated by the voxel CNN, the RoI-grid pooling is proposed to abstract proposal-specific features from the keypoints to the RoI-grid points via keypoint set abstraction. Compared with conventional pooling operations, the RoI-grid feature points encode much richer context information for accurately estimating object confidences and locations. Extensive experiments on both the KITTI dataset and the Waymo Open dataset show that our proposed PV-RCNN surpasses state-of-the-art 3D detection methods with remarkable margins.",
            "저자": "Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, Hongsheng Li",
            "전체 인용횟수": "1396회 인용202020212022202357281475572",
            "컨퍼런스": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "페이지": "10529-10538",
            "학술 문서": "Pv-rcnn: Point-voxel feature set abstraction for 3d object detectionS Shi, C Guo, L Jiang, Z Wang, J Shi, X Wang, H Li - Proceedings of the IEEE/CVF conference on computer …, 20201396회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pv-rcnn: Point-voxel feature set abstraction for 3d object detection",
        "year": null
    },
    "Context encoding for semantic segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "Recent work has made significant progress in improving spatial resolution for pixelwise labeling with Fully Convolutional Network (FCN) framework by employing Dilated/Atrous convolution, utilizing multi-scale features and refining boundaries. In this paper, we explore the impact of global contextual information in semantic segmentation by introducing the Context Encoding Module, which captures the semantic context of scenes and selectively highlights class-dependent featuremaps. The proposed Context Encoding Module significantly improves semantic segmentation results with only marginal extra computation cost over FCN. Our approach has achieved new state-of-the-art results 51.7% mIoU on PASCAL-Context, 85.9% mIoU on PASCAL VOC 2012. Our single model achieves a final score of 0.5567 on ADE20K test set, which surpass the winning entry of COCO-Place Challenge in 2017. In addition, we also explore how the Context Encoding Module can improve the feature representation of relatively shallow networks for the image classification on CIFAR-10 dataset. Our 14 layer network has achieved an error rate of 3.45%, which is comparable with state-of-the-art approaches with over 10 times more layers. The source code for the complete system are publicly available.",
            "저자": "Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi, Amit Agrawal",
            "전체 인용횟수": "1341회 인용20182019202020212022202331163270304296268",
            "컨퍼런스": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition",
            "페이지": "7151-7160",
            "학술 문서": "Context encoding for semantic segmentationH Zhang, K Dana, J Shi, Z Zhang, X Wang, A Tyagi… - Proceedings of the IEEE conference on Computer …, 20181341회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Context encoding for semantic segmentation",
        "year": null
    },
    "Cross-scene crowd counting via deep convolutional neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "Cross-scene crowd counting is a challenging task where no laborious data annotation is required for counting people in new target surveillance crowd scenes unseen in the training set. The performance of most existing crowd counting methods drops significantly when they are applied to an unseen scene. To address this problem, we propose a deep convolutional neural network (CNN) for crowd counting, and it is trained alternatively with two related learning objectives, crowd density and crowd count. This proposed switchable learning approach is able to obtain better local optimum for both objectives. To handle an unseen target crowd scene, we present a data-driven method to fine-tune the trained CNN model for the target scene. A new dataset including 108 crowd scenes with nearly 200,000 head annotations is introduced to better evaluate the accuracy of cross-scene crowd counting methods. Extensive experiments on the proposed and another two existing datasets demonstrate the effectiveness and reliability of our approach.",
            "저자": "Cong Zhang, Hongsheng Li, Xiaogang Wang, Xiaokang Yang",
            "전체 인용횟수": "1341회 인용20152016201720182019202020212022202374490153234229232198141",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "833-841",
            "학술 문서": "Cross-scene crowd counting via deep convolutional neural networksC Zhang, H Li, X Wang, X Yang - Proceedings of the IEEE conference on computer …, 20151341회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Cross-scene crowd counting via deep convolutional neural networks",
        "year": null
    },
    "Unsupervised salience learning for person re-identification": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013",
            "설명": "Human eyes can recognize person identities based on some small salient regions. However, such valuable salient information is often hidden when computing similarities of images with existing approaches. Moreover, many existing approaches learn discriminative features and handle drastic viewpoint change in a supervised way and require labeling new training data for a different pair of camera views. In this paper, we propose a novel perspective for person re-identification based on unsupervised salience learning. Distinctive features are extracted without requiring identity labels in the training procedure. First, we apply adjacency constrained patch matching to build dense correspondence between image pairs, which shows effectiveness in handling misalignment caused by large viewpoint and pose variations. Second, we learn human salience in an unsupervised manner. To improve the performance of person re-identification, human salience is incorporated in patch matching to find reliable and discriminative matched patches. The effectiveness of our approach is validated on the widely used VIPeR dataset and ETHZ dataset.",
            "저자": "Rui Zhao, Wanli Ouyang, Xiaogang Wang",
            "전체 인용횟수": "1319회 인용201320142015201620172018201920202021202220231067134152199205193139915845",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "3586-3593",
            "학술 문서": "Unsupervised salience learning for person re-identificationR Zhao, W Ouyang, X Wang - Proceedings of the IEEE conference on computer …, 20131319회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Unsupervised salience learning for person re-identification",
        "year": null
    },
    "Stackgan++: Realistic image synthesis with stacked generative adversarial networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/7/16",
            "게시자": "IEEE",
            "권": "41",
            "설명": "Although Generative Adversarial Networks (GANs) have shown remarkable success in various tasks, they still face challenges in generating high quality images. In this paper, we propose Stacked Generative Adversarial Networks (StackGANs) aimed at generating high-resolution photo-realistic images. First, we propose a two-stage generative adversarial network architecture, StackGAN-v1, for text-to-image synthesis. The Stage-I GAN sketches the primitive shape and colors of a scene based on a given text description, yielding low-resolution images. The Stage-II GAN takes Stage-I results and the text description as inputs, and generates high-resolution images with photo-realistic details. Second, an advanced multi-stage generative adversarial network architecture, StackGAN-v2, is proposed for both conditional and unconditional generative tasks. Our StackGAN-v2 consists of multiple generators and multiple …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris N Metaxas",
            "전체 인용횟수": "1304회 인용20182019202020212022202337120241287336275",
            "페이지": "1947-1962",
            "학술 문서": "Stackgan++: Realistic image synthesis with stacked generative adversarial networksH Zhang, T Xu, H Li, S Zhang, X Wang, X Huang… - IEEE transactions on pattern analysis and machine …, 20181304회 인용 관련 학술자료 전체 8개의 버전 ",
            "호": "8"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Stackgan++: Realistic image synthesis with stacked generative adversarial networks",
        "year": null
    },
    "Visual tracking with fully convolutional networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "We propose a new approach for general object tracking with fully convolutional neural network. Instead of treating convolutional neural network (CNN) as a black-box feature extractor, we conduct in-depth study on the properties of CNN features offline pre-trained on massive image data and classification task on ImageNet. The discoveries motivate the design of our tracking system. It is found that convolutional layers in different levels characterize the target from different perspectives. A top layer encodes more semantic features and serves as a category detector, while a lower layer carries more discriminative information and can better separate the target from distracters with similar appearance. Both layers are jointly used with a switch mechanism during tracking. It is also found that for a tracking target, only a subset of neurons are relevant. A feature map selection method is developed to remove noisy and irrelevant feature maps, which can reduce computation redundancy and improve tracking accuracy. Extensive evaluation on the widely used tracking benchmark shows that the proposed tacker outperforms the state-of-the-art significantly.",
            "저자": "Lijun Wang, Wanli Ouyang, Xiaogang Wang, Huchuan Lu",
            "전체 인용횟수": "1212회 인용2015201620172018201920202021202220233621662652381871457946",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "3119-3127",
            "학술 문서": "Visual tracking with fully convolutional networksL Wang, W Ouyang, X Wang, H Lu - Proceedings of the IEEE international conference on …, 20151212회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Visual tracking with fully convolutional networks",
        "year": null
    },
    "Learning from massive noisy labeled data for image classification": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "Large-scale supervised datasets are crucial to train convolutional neural networks (CNNs) for various computer vision problems. However, obtaining a massive amount of well-labeled data is usually very expensive and time consuming. In this paper, we introduce a general framework to train CNNs with only a limited number of clean labels and millions of easily obtained noisy labels. We model the relationships between images, class labels and label noises with a probabilistic graphical model and further integrate it into an end-to-end deep learning system. To demonstrate the effectiveness of our approach, we collect a large-scale real-world clothing classification dataset with both noisy and clean labels. Experiments on this dataset indicate that our approach can better correct the noisy labels and improves the performance of trained CNNs.",
            "저자": "Tong Xiao, Tian Xia, Yi Yang, Chang Huang, Xiaogang Wang",
            "전체 인용횟수": "1135회 인용2015201620172018201920202021202220235195983115173215237215",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "2691-2699",
            "학술 문서": "Learning from massive noisy labeled data for image classificationT Xiao, T Xia, Y Yang, C Huang, X Wang - Proceedings of the IEEE conference on computer …, 20151135회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning from massive noisy labeled data for image classification",
        "year": null
    },
    "Probabilistic robotics": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005",
            "게시자": "MIT Press",
            "설명": "Planning and navigation algorithms exploit statistics gleaned from uncertain, imperfect real-world environments to guide robots toward their goals and around obstacles.",
            "저자": "Sebastian Thrun, Wolfram Burgard, Dieter Fox",
            "전체 인용횟수": "13613회 인용200620072008200920102011201220132014201520162017201820192020202120222023179397496661796821829857895895868854915978874836741572",
            "학술 문서": "Probabilistic roboticsS Thrun - Communications of the ACM, 200213460회 인용 관련 학술자료 전체 15개의 버전 Probalistic robotics*S Thrun, W Burgard, D Fox - Kybernetes, 200674회 인용 관련 학술자료 Probabilistic robotics*W Burgard, D Fox, S Thrun - The MIT Press, 200564회 인용 관련 학술자료 Probabilistic robotics cambridgeS Thrun, W Burgard, D Fox - 200527회 인용 관련 학술자료 Probability robotics*S Thrun, W Burgard, D Fox - MIT press, 200517회 인용 관련 학술자료 Probabilistic Robotics [hardcover]S Thrun, W Burgard, D Fox - 20052회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Probabilistic robotics",
        "year": null
    },
    "Principles of robot motion: theory, algorithms, and implementation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/6",
            "게시자": "The MIT Press",
            "설명": "A text that makes the mathematical underpinnings of robot motion accessible and relates low-level details of implementation to high-level algorithmic concepts. Robot motion planning has become a major focus of robotics. Research findings can be applied not only to robotics but to planning routes on circuit boards, directing digital actors in computer graphics, robot-assisted surgery and medicine, and in novel areas such as drug design and protein folding. This text reflects the great advances that have taken place in the last ten years, including sensor-based planning, probabalistic planning, localization and mapping, and motion planning for dynamic and nonholonomic systems. Its presentation makes the mathematical underpinnings of robot motion accessible to students of computer science and engineering, rleating low-level implementation details to high-level algorithmic concepts.",
            "저자": "Howie Choset, Kevin M. Lynch, Seth Hutchinson, George Kantor, Wolfram Burgard, Lydia E. Kavraki, Sebastian Thrun",
            "전체 인용횟수": "4131회 인용20052006200720082009201020112012201320142015201620172018201920202021202220231780158193218248278282287297280252232257245218226203124",
            "페이지": "625",
            "학술 문서": "Principles of robot motion: theory, algorithms, and implementationsH Choset, KM Lynch, S Hutchinson, GA Kantor… - 20054119회 인용 관련 학술자료 전체 6개의 버전 Principles of robot motion: theory, algorithms, and implementations, ser. Intelligent Robotics and Autonomous Agents*H Choset, KM Lynch, S Hutchinson, GA Kantor… - Massachusetts Institute of Technology, Cambridge, 200512회 인용 관련 학술자료 Principles of Robot Motion: Theory, Algorithms, and Implementation ERRATA!!!!H Choset, K Lynch, S Hutchinson, G Kantor, W Burgard… - 20074회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Principles of robot motion: theory, algorithms, and implementation",
        "year": null
    },
    "The dynamic window approach to collision avoidance": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1997",
            "권": "4",
            "설명": "This approach, designed for mobile robots equipped with synchro-drives, is derived directly from the motion dynamics of the robot. In experiments, the dynamic window approach safely controlled the mobile robot RHINO at speeds of up to 95 cm/sec, in populated and dynamic environments.",
            "저널": "IEEE Robotics and Automation Magazine",
            "저자": "D. Fox, W Burgard, S. Thrun",
            "전체 인용횟수": "3952회 인용19971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202310262120163952426465111101131142132153162167166173196241268310348403356",
            "페이지": "1",
            "학술 문서": "The dynamic window approach to collision avoidanceD Fox, W Burgard, S Thrun - IEEE Robotics & Automation Magazine, 19973952회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The dynamic window approach to collision avoidance",
        "year": null
    },
    "A benchmark for the evaluation of RGB-D SLAM systems": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/10/7",
            "게시자": "IEEE",
            "설명": "In this paper, we present a novel benchmark for the evaluation of RGB-D SLAM systems. We recorded a large set of image sequences from a Microsoft Kinect with highly accurate and time-synchronized ground truth camera poses from a motion capture system. The sequences contain both the color and depth images in full sensor resolution (640 × 480) at video frame rate (30 Hz). The ground-truth trajectory was obtained from a motion-capture system with eight high-speed tracking cameras (100 Hz). The dataset consists of 39 sequences that were recorded in an office environment and an industrial hall. The dataset covers a large variety of scenes and camera motions. We provide sequences for debugging with slow motions as well as longer trajectories with and without loop closures. Most sequences were recorded from a handheld Kinect with unconstrained 6-DOF motions but we also provide sequences from a …",
            "저자": "Jürgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, Daniel Cremers",
            "전체 인용횟수": "3546회 인용2013201420152016201720182019202020212022202383116151217258346445435507462480",
            "컨퍼런스": "2012 IEEE/RSJ international conference on intelligent robots and systems",
            "페이지": "573-580",
            "학술 문서": "A benchmark for the evaluation of RGB-D SLAM systemsJ Sturm, N Engelhard, F Endres, W Burgard, D Cremers - 2012 IEEE/RSJ international conference on intelligent …, 20123546회 인용 관련 학술자료 전체 19개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A benchmark for the evaluation of RGB-D SLAM systems",
        "year": null
    },
    "OctoMap: An efficient probabilistic 3D mapping framework based on octrees": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/4",
            "게시자": "Springer US",
            "권": "34",
            "설명": " Three-dimensional models provide a volumetric representation of space which is important for a variety of robotic applications including flying robots and robots that are equipped with manipulators. In this paper, we present an open-source framework to generate volumetric 3D environment models. Our mapping approach is based on octrees and uses probabilistic occupancy estimation. It explicitly represents not only occupied space, but also free and unknown areas. Furthermore, we propose an octree map compression method that keeps the 3D models compact. Our framework is available as an open-source C++ library and has already been successfully applied in several robotics projects. We present a series of experimental results carried out with real robots and on publicly available real-world datasets. The results demonstrate that our approach is able to update the representation efficiently and …",
            "저널": "Autonomous robots",
            "저자": "Armin Hornung, Kai M Wurm, Maren Bennewitz, Cyrill Stachniss, Wolfram Burgard",
            "전체 인용횟수": "2978회 인용2013201420152016201720182019202020212022202356114140223268334315373391404342",
            "페이지": "189-206",
            "학술 문서": "OctoMap: An efficient probabilistic 3D mapping framework based on octreesA Hornung, KM Wurm, M Bennewitz, C Stachniss… - Autonomous robots, 20132978회 인용 관련 학술자료 전체 20개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "OctoMap: An efficient probabilistic 3D mapping framework based on octrees",
        "year": null
    },
    "Improved techniques for grid mapping with rao-blackwellized particle filters": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2007/2/5",
            "게시자": "IEEE",
            "권": "23",
            "설명": "Recently, Rao-Blackwellized particle filters (RBPF) have been introduced as an effective means to solve the simultaneous localization and mapping problem. This approach uses a particle filter in which each particle carries an individual map of the environment. Accordingly, a key question is how to reduce the number of particles. In this paper, we present adaptive techniques for reducing this number in a RBPF for learning grid maps. We propose an approach to compute an accurate proposal distribution, taking into account not only the movement of the robot, but also the most recent observation. This drastically decreases the uncertainty about the robot's pose in the prediction step of the filter. Furthermore, we present an approach to selectively carry out resampling operations, which seriously reduces the problem of particle depletion. Experimental results carried out with real mobile robots in large-scale indoor, as …",
            "저널": "IEEE transactions on Robotics",
            "저자": "Giorgio Grisetti, Cyrill Stachniss, Wolfram Burgard",
            "전체 인용횟수": "2941회 인용200720082009201020112012201320142015201620172018201920202021202220232242737284106156160178210223257291287275298193",
            "페이지": "34-46",
            "학술 문서": "Improved techniques for grid mapping with rao-blackwellized particle filtersG Grisetti, C Stachniss, W Burgard - IEEE transactions on Robotics, 20072941회 인용 관련 학술자료 전체 25개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Improved techniques for grid mapping with rao-blackwellized particle filters",
        "year": null
    },
    "g2o: A general framework for graph optimization": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/5/9",
            "게시자": "IEEE",
            "설명": "Many popular problems in robotics and computer vision including various types of simultaneous localization and mapping (SLAM) or bundle adjustment (BA) can be phrased as least squares optimization of an error function that can be represented by a graph. This paper describes the general structure of such problems and presents g 2 o, an open-source C++ framework for optimizing graph-based nonlinear error functions. Our system has been designed to be easily extensible to a wide range of problems and a new problem typically can be specified in a few lines of code. The current implementation provides solutions to several variants of SLAM and BA. We provide evaluations on a wide range of real-world and simulated datasets. The results demonstrate that while being general g 2 o offers a performance comparable to implementations of state of-the-art approaches for the specific problems.",
            "저자": "Rainer Kuemmerle, Giorgio Grisetti, Hauke Strasdat, Kurt Konolige, Wolfram Burgard",
            "전체 인용횟수": "2752회 인용20112012201320142015201620172018201920202021202220231973141154210241253309338261283257195",
            "컨퍼런스": "Robotics and Automation (ICRA), 2011 IEEE International Conference on",
            "페이지": "3607-3613",
            "학술 문서": "g 2 o: A general framework for graph optimizationR Kümmerle, G Grisetti, H Strasdat, K Konolige… - 2011 IEEE International Conference on Robotics and …, 20112748회 인용 관련 학술자료 전체 13개의 버전 G¡ sup¿ 2¡/sup¿ o: A general framework for graph optimization*R Kummerle, G Grisetti, H Strasdat, K Konolige… - International Conference on Robotics and Automation2회 인용 관련 학술자료 g2o: A general framework for graph optimization*R Kummer, G Grisetti, H Strasdat, K Konolige… - IEEE International Conference on Robotics and …2회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "g2o: A general framework for graph optimization",
        "year": null
    },
    "Robust Monte Carlo localization for mobile robots": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2001/5/1",
            "게시자": "Elsevier",
            "권": "128",
            "설명": "Mobile robot localization is the problem of determining a robot's pose from sensor data. This article presents a family of probabilistic localization algorithms known as Monte Carlo Localization (MCL). MCL algorithms represent a robot's belief by a set of weighted hypotheses (samples), which approximate the posterior under a common Bayesian formulation of the localization problem. Building on the basic MCL algorithm, this article develops a more robust algorithm called Mixture-MCL, which integrates two complimentary ways of generating samples in the estimation. To apply this algorithm to mobile robots equipped with range finders, a kernel density tree is learned that permits fast sampling. Systematic empirical results illustrate the robustness and computational efficiency of the approach.",
            "저널": "Artificial intelligence",
            "저자": "Sebastian Thrun, Dieter Fox, Wolfram Burgard, Frank Dellaert",
            "전체 인용횟수": "2687회 인용20012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023185380107139146148147152179148142150142125134114101109110767560",
            "페이지": "99-141",
            "학술 문서": "Robust Monte Carlo localization for mobile robotsS Thrun, D Fox, W Burgard, F Dellaert - Artificial intelligence, 20012685회 인용 관련 학술자료 전체 38개의 버전 WB, and Dellaert, F. 2001. Robust Monte Carlo Localization for Mobile Robots*S Thrun, D Fox - Artificial Intelligence4회 인용 관련 학술자료 ",
            "호": "1-2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Robust Monte Carlo localization for mobile robots",
        "year": null
    },
    "Monte carlo localization for mobile robots": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1999/5/10",
            "게시자": "IEEE",
            "권": "2",
            "설명": "To navigate reliably in indoor environments, a mobile robot must know where it is. Thus, reliable position estimation is a key problem in mobile robotics. We believe that probabilistic approaches are among the most promising candidates to providing a comprehensive and real-time solution to the robot localization problem. However, current methods still face considerable hurdles. In particular the problems encountered are closely related to the type of representation used to represent probability densities over the robot's state space. Earlier work on Bayesian filtering with particle-based density representations opened up a new approach for mobile robot localization based on these principles. We introduce the Monte Carlo localization method, where we represent the probability density involved by maintaining a set of samples that are randomly drawn from it. By using a sampling-based representation we obtain a …",
            "저자": "Frank Dellaert, Dieter Fox, Wolfram Burgard, Sebastian Thrun",
            "전체 인용횟수": "2335회 인용19992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220231528234262628410910312313512611511610812210511194113111971178494",
            "컨퍼런스": "Proceedings 1999 IEEE international conference on robotics and automation (Cat. No. 99CH36288C)",
            "페이지": "1322-1328",
            "학술 문서": "Monte carlo localization for mobile robotsF Dellaert, D Fox, W Burgard, S Thrun - Proceedings 1999 IEEE international conference on …, 19992335회 인용 관련 학술자료 전체 37개의 버전 Monte carlo localization for mobile robots*W Burgard, S Thrun, F Dellaert, D Fox - Proc. of IEEE/RSJ International Conference on …, 19993회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Monte carlo localization for mobile robots",
        "year": null
    },
    "Deep learning with convolutional neural networks for EEG decoding and visualization": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/11",
            "권": "38",
            "설명": " Deep learning with convolutional neural networks (deep ConvNets) has revolutionized computer vision through end‐to‐end learning, that is, learning from the raw data. There is increasing interest in using deep ConvNets for end‐to‐end EEG analysis, but a better understanding of how to design and train ConvNets for end‐to‐end EEG decoding and how to visualize the informative EEG features the ConvNets learn is still needed. Here, we studied deep ConvNets with a range of different architectures, designed for decoding imagined or executed tasks from raw EEG. Our results show that recent advances from the machine learning field, including batch normalization and exponential linear units, together with a cropped training strategy, boosted the deep ConvNets decoding performance, reaching at least as good performance as the widely used filter bank common spatial patterns (FBCSP) algorithm (mean …",
            "저널": "Human brain mapping",
            "저자": "Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, Tonio Ball",
            "전체 인용횟수": "2147회 인용201720182019202020212022202317157233300420533469",
            "페이지": "5391-5420",
            "학술 문서": "Deep learning with convolutional neural networks for EEG decoding and visualizationRT Schirrmeister, JT Springenberg, LDJ Fiederer… - Human brain mapping, 20172147회 인용 관련 학술자료 전체 13개의 버전 ",
            "호": "11"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep learning with convolutional neural networks for EEG decoding and visualization",
        "year": null
    },
    "Experiences with an interactive museum tour-guide robot": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1999/10/1",
            "게시자": "Elsevier",
            "권": "114",
            "설명": "This article describes the software architecture of an autonomous, interactive tour-guide robot. It presents a modular and distributed software architecture, which integrates localization, mapping, collision avoidance, planning, and various modules concerned with user interaction and Web-based telepresence. At its heart, the software approach relies on probabilistic computation, on-line learning, and any-time algorithms. It enables robots to operate safely, reliably, and at high speeds in highly dynamic environments, and does not require any modifications of the environment to aid the robot's operation. Special emphasis is placed on the design of interactive capabilities that appeal to people's intuition. The interface provides new means for human-robot interaction with crowds of people in public places, and it also provides people all around the world with the ability to establish a “virtual telepresence” using the Web …",
            "저널": "Artificial intelligence",
            "저자": "Wolfram Burgard, Armin B Cremers, Dieter Fox, Dirk Hähnel, Gerhard Lakemeyer, Dirk Schulz, Walter Steiner, Sebastian Thrun",
            "전체 인용횟수": "1960회 인용19981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023255756698610798106889483779069819472786779626681624545",
            "페이지": "3-55",
            "학술 문서": "Experiences with an interactive museum tour-guide robotW Burgard, AB Cremers, D Fox, D Hähnel… - Artificial intelligence, 19991955회 인용 관련 학술자료 전체 48개의 버전 gThe Interactive Museum Tour-Guide Robot,• hProc*W Burgard, AB Cremers, D Fox, D Hahnel… - of National Conference on Artificial Intelligence(AAAI' …, 199810회 인용 관련 학술자료 Lakemeyer*W Burgard, BA Cremers, D Fox, D Hahnel - R., Schulz, D., Steiner, W., Thrun, S., ilixpericnces with …, 19993회 인용 관련 학술자료 ",
            "호": "1-2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Experiences with an interactive museum tour-guide robot",
        "year": null
    },
    "Monte carlo localization: Efficient position estimation for mobile robots": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1999/7/18",
            "권": "1999",
            "설명": "This paper presents a new algorithm for mobile robot localization, called Monte Carlo Localization (MCL). MCL is a version of Markov localization, a family of probabilistic approaches that have recently been applied with great practical success. However, previous approaches were either computationally cumbersome (such as grid-based approaches that represent the state space by high-resolution 3D grids), or had to resort to extremely coarse-grained resolutions. Our approach is computationally efficient while retaining the ability to represent (almost) arbitrary distributions. MCL applies sampling-based methods for approximating probability distributions, in a way that places computation “where needed.” The number of samples is adapted on-line, thereby invoking large sample sets only when necessary. Empirical results illustrate that MCL yields improved accuracy while requiring an order of magnitude less computation when compared to previous approaches. It is also much easier to implement.",
            "저널": "Aaai/iaai",
            "저자": "Dieter Fox, Wolfram Burgard, Frank Dellaert, Sebastian Thrun",
            "전체 인용횟수": "1705회 인용1999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023142739656771908911479956867717965566163808376636144",
            "페이지": "2-2",
            "학술 문서": "Monte carlo localization: Efficient position estimation for mobile robotsD Fox, W Burgard, F Dellaert, S Thrun - Aaai/iaai, 19991705회 인용 관련 학술자료 전체 44개의 버전 Monte Carlo localization: Efficient position estimation for mobile robots, Proceeding of the National Conference on Artificial Intelligence, 343–349*D Fox, W Burgard, F Dellaert, S Thrun - 19992회 인용 관련 학술자료 ",
            "호": "343-349"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Monte carlo localization: Efficient position estimation for mobile robots",
        "year": null
    },
    "A tutorial on graph-based SLAM": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010",
            "게시자": "IEEE",
            "권": "2",
            "설명": "Being able to build a map of the environment and to simultaneously localize within this map is an essential skill for mobile robots navigating in unknown environments in absence of external referencing systems such as GPS. This so-called simultaneous localization and mapping (SLAM) problem has been one of the most popular research topics in mobile robotics for the last two decades and efficient approaches for solving this task have been proposed. One intuitive way of formulating SLAM is to use a graph whose nodes correspond to the poses of the robot at different points in time and whose edges represent constraints between the poses. The latter are obtained from observations of the environment or from movement actions carried out by the robot. Once such a graph is constructed, the map can be computed by finding the spatial configuration of the nodes that is mostly consistent with the measurements …",
            "저자": "Giorgio Grisetti, Rainer Kümmerle, Cyrill Stachniss, Wolfram Burgard",
            "전체 인용횟수": "1507회 인용2011201220132014201520162017201820192020202120222023720465797101145164173177170193144",
            "출처": "IEEE Intelligent Transportation Systems Magazine",
            "페이지": "31-43",
            "학술 문서": "A tutorial on graph-based SLAMG Grisetti, R Kümmerle, C Stachniss, W Burgard - IEEE Intelligent Transportation Systems Magazine, 20101507회 인용 관련 학술자료 전체 19개의 버전 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A tutorial on graph-based SLAM",
        "year": null
    },
    "Coordinated multi-robot exploration": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/5/31",
            "게시자": "IEEE",
            "권": "21",
            "설명": "In this paper, we consider the problem of exploring an unknown environment with a team of robots. As in single-robot exploration the goal is to minimize the overall exploration time. The key problem to be solved in the context of multiple robots is to choose appropriate target points for the individual robots so that they simultaneously explore different regions of the environment. We present an approach for the coordination of multiple robots, which simultaneously takes into account the cost of reaching a target point and its utility. Whenever a target point is assigned to a specific robot, the utility of the unexplored area visible from this target position is reduced. In this way, different target locations are assigned to the individual robots. We furthermore describe how our algorithm can be extended to situations in which the communication range of the robots is limited. Our technique has been implemented and tested …",
            "저널": "IEEE Transactions on robotics",
            "저자": "Wolfram Burgard, Mark Moors, Cyrill Stachniss, Frank E Schneider",
            "전체 인용횟수": "1436회 인용200520062007200820092010201120122013201420152016201720182019202020212022202364255768290819810380858372799487757757",
            "페이지": "376-386",
            "학술 문서": "Coordinated multi-robot explorationW Burgard, M Moors, C Stachniss, FE Schneider - IEEE Transactions on robotics, 20051436회 인용 관련 학술자료 전체 19개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Coordinated multi-robot exploration",
        "year": null
    },
    "Markov localization for mobile robots in dynamic environments": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1999",
            "권": "11",
            "설명": "Localization, that is the estimation of a robot's location from sensor data, is a fundamental problem in mobile robotics. This papers presents a version of Markov localization which provides accurate position estimates and which is tailored towards dynamic environments. The key idea of Markov localization is to maintain a probability density over the space of all locations of a robot in its environment. Our approach represents this space metrically, using a fine-grained grid to approximate densities. It is able to globally localize the robot from scratch and to recover from localization failures. It is robust to approximate models of the environment (such as occupancy grid maps) and noisy sensors (such as ultrasound sensors). Our approach also includes a filtering technique which allows a mobile robot to reliably estimate its position even in densely populated environments in which crowds of people block the robot's sensors for extended periods of time. The method described here has been implemented and tested in several real-world applications of mobile robots, including the deployments of two mobile robots as interactive museum tour-guides.",
            "저널": "Journal of Artificial Intelligence Research (JAIR)",
            "저자": "Dieter Fox, Sebastian Burgard, Wolfram, Thrun",
            "전체 인용횟수": "1415회 인용20002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202316386078748110411691827461884951624432413937232423",
            "학술 문서": "Markov localization for mobile robots in dynamic environmentsD Fox, W Burgard, S Thrun - Journal of artificial intelligence research, 19991415회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Markov localization for mobile robots in dynamic environments",
        "year": null
    },
    "A probabilistic approach to concurrent mapping and localization for mobile robots": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1998/7",
            "게시자": "Kluwer Academic Publishers",
            "권": "5",
            "설명": " This paper addresses the problem of building large-scale geometric maps of indoor environments with mobile robots. It poses the map building problem as a constrained, probabilistic maximum-likelihood estimation problem. It then devises a practical algorithm for generating the most likely map from data, along with the most likely path taken by the robot. Experimental results in cyclic environments of size up to 80×25 m illustrate the appropriateness of the approach.",
            "저널": "Autonomous Robots",
            "저자": "Sebastian Thrun, Wolfram Burgard, Dieter Fox",
            "전체 인용횟수": "1340회 인용1998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202318343558858110989849184695159415640313131243433172213",
            "페이지": "253-271",
            "학술 문서": "A probabilistic approach to concurrent mapping and localization for mobile robotsS Thrun, W Burgard, D Fox - Autonomous Robots, 19981340회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A probabilistic approach to concurrent mapping and localization for mobile robots",
        "year": null
    },
    "MINERVA: A second-generation museum tour-guide robot": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1999/5/10",
            "게시자": "IEEE",
            "권": "3",
            "설명": "This paper describes an interactive tour-guide robot, which was successfully exhibited in a Smithsonian museum. During its two weeks of operation, the robot interacted with thousands of people, traversing more than 44 km at speeds of up to 163 cm/sec. Our approach specifically addresses issues such as safe navigation in unmodified and dynamic environments, and short-term human-robot interaction. It uses learning pervasively at all levels of the software architecture.",
            "저자": "Sebastian Thrun, Maren Bennewitz, Wolfram Burgard, Armin B Cremers, Frank Dellaert, Dieter Fox, Dirk Hahnel, Charles Rosenberg, Nicholas Roy, Jamieson Schulte, Dirk Schulz",
            "전체 인용횟수": "1127회 인용199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202318363345454848445347464154585752555152594045313620",
            "컨퍼런스": "Proceedings 1999 IEEE International Conference on Robotics and Automation (Cat. No. 99CH36288C)",
            "학술 문서": "MINERVA: A second-generation museum tour-guide robotS Thrun, M Bennewitz, W Burgard, AB Cremers… - Proceedings 1999 IEEE International Conference on …, 19991127회 인용 관련 학술자료 전체 32개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "MINERVA: A second-generation museum tour-guide robot",
        "year": null
    },
    "Improving grid-based slam with rao-blackwellized particle filters by adaptive proposals and selective resampling": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/4/18",
            "게시자": "IEEE",
            "설명": "Recently Rao-Blackwellized particle filters have been introduced as effective means to solve the simultaneous localization and mapping (SLAM) problem. This approach uses a particle filter in which each particle carries an individual map of the environment. Accordingly, a key question is how to reduce the number of particles. In this paper we present adaptive techniques to reduce the number of particles in a Rao-Blackwellized particle filter for learning grid maps. We propose an approach to compute an accurate proposal distribution taking into account not only the movement of the robot but also the most recent observation. This drastically decrease the uncertainty about the robot's pose in the prediction step of the filter. Furthermore, we present an approach to selectively carry out re-sampling operations which seriously reduces the problem of particle depletion. Experimental results carried out with mobile robots in …",
            "저자": "Giorgio Grisetti, Cyrill Stachniss, Wolfram Burgard",
            "전체 인용횟수": "1111회 인용200520062007200820092010201120122013201420152016201720182019202020212022202320425047525142365557636186868091836736",
            "컨퍼런스": "Robotics and Automation, 2005. ICRA 2005. Proceedings of the 2005 IEEE International Conference on",
            "페이지": "2432-2437",
            "학술 문서": "Improving grid-based slam with rao-blackwellized particle filters by adaptive proposals and selective resamplingG Grisetti, C Stachniss, W Burgard - Proceedings of the 2005 IEEE international conference …, 20051111회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Improving grid-based slam with rao-blackwellized particle filters by adaptive proposals and selective resampling",
        "year": null
    },
    "Collaborative multi-robot exploration": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2000/4/24",
            "게시자": "IEEE",
            "권": "1",
            "설명": "In this paper we consider the problem of exploring an unknown environment by a team of robots. As in single-robot exploration the goal is to minimize the overall exploration time. The key problem to be solved therefore is to choose appropriate target points for the individual robots so that they simultaneously explore different regions of their environment. We present a probabilistic approach for the coordination of multiple robots which, in contrast to previous approaches, simultaneously takes into account the costs of reaching a target point and the utility of target points. The utility of target points is given by the size of the unexplored area that a robot can cover with its sensors upon reaching a target position. Whenever a target point is assigned to a specific robot, the utility of the unexplored area visible from this target position is reduced for the other robots. This way, a team of multiple robots assigns different target …",
            "저자": "Wolfram Burgard, Mark Moors, Dieter Fox, Reid Simmons, Sebastian Thrun",
            "전체 인용횟수": "1102회 인용20002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202371547596053464347525055747061455635484230452426",
            "컨퍼런스": "Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No. 00CH37065)",
            "페이지": "476-481",
            "학술 문서": "Collaborative multi-robot explorationW Burgard, M Moors, D Fox, R Simmons, S Thrun - Proceedings 2000 ICRA. Millennium Conference. IEEE …, 20001102회 인용 관련 학술자료 전체 27개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Collaborative multi-robot exploration",
        "year": null
    },
    "Mapping and localization with RFID technology": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004/4/26",
            "게시자": "IEEE",
            "권": "1",
            "설명": "We analyze whether radio frequency identification (RFID) technology can be used to improve the localization of mobile robots and persons in their environment. In particular we study the problem of localizing RFID tags with a mobile platform that is equipped with a pair of RFID antennas. We present a probabilistic measurement model for RFID readers that allow us to accurately localize RFID tags in the environment. We also demonstrate how such maps can be used to localize a robot and persons in their environment. Finally, we present experiments illustrating that the computational requirements for global robot localization can be reduced strongly by fusing RFID information with laser data.",
            "저자": "Dirk Hahnel, Wolfram Burgard, Dieter Fox, Ken Fishkin, Matthai Philipose",
            "전체 인용횟수": "1097회 인용2004200520062007200820092010201120122013201420152016201720182019202020212022202310306870991199994726760524235422331202818",
            "컨퍼런스": "IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA'04. 2004",
            "페이지": "1015-1020",
            "학술 문서": "Mapping and localization with RFID technologyD Hahnel, W Burgard, D Fox, K Fishkin, M Philipose - IEEE International Conference on Robotics and …, 20041095회 인용 관련 학술자료 전체 12개의 버전 Mapping and Localization with RFID Tags*D Hähnel, W Burgard, D Fox, K Fishkin, M Philipose - ICRA, 20043회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Mapping and localization with RFID technology",
        "year": null
    },
    "Dehazenet: An end-to-end system for single image haze removal": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/8/10",
            "게시자": "IEEE",
            "권": "25",
            "설명": "Single image haze removal is a challenging ill-posed problem. Existing methods use various constraints/priors to get plausible dehazing solutions. The key to achieve haze removal is to estimate a medium transmission map for an input hazy image. In this paper, we propose a trainable end-to-end system called DehazeNet, for medium transmission estimation. DehazeNet takes a hazy image as input, and outputs its medium transmission map that is subsequently used to recover a haze-free image via atmospheric scattering model. DehazeNet adopts convolutional neural network-based deep architecture, whose layers are specially designed to embody the established assumptions/priors in image dehazing. Specifically, the layers of Maxout units are used for feature extraction, which can generate almost all haze-relevant features. We also propose a novel nonlinear activation function in DehazeNet, called bilateral …",
            "저널": "IEEE transactions on image processing",
            "저자": "Bolun Cai, Xiangmin Xu, Kui Jia, Chunmei Qing, Dacheng Tao",
            "전체 인용횟수": "2486회 인용201620172018201920202021202220231077231299373461517507",
            "페이지": "5187-5198",
            "학술 문서": "Dehazenet: An end-to-end system for single image haze removalB Cai, X Xu, K Jia, C Qing, D Tao - IEEE transactions on image processing, 20162486회 인용 관련 학술자료 전체 12개의 버전 ",
            "호": "11"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Dehazenet: An end-to-end system for single image haze removal",
        "year": null
    },
    "Deep ordinal regression network for monocular depth estimation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "Monocular depth estimation, which plays a crucial role in understanding 3D scene geometry, is an ill-posed prob-lem. Recent methods have gained significant improvement by exploring image-level information and hierarchical features from deep convolutional neural networks (DCNNs). These methods model depth estimation as a regression problem and train the regression networks by minimizing mean squared error, which suffers from slow convergence and unsatisfactory local solutions. Besides, existing depth estimation networks employ repeated spatial pooling operations, resulting in undesirable low-resolution feature maps. To obtain high-resolution depth maps, skip-connections or multi-layer deconvolution networks are required, which complicates network training and consumes much more computations. To eliminate or at least largely reduce these problems, we introduce a spacing-increasing discretization (SID) strategy to discretize depth and recast depth network learning as an ordinal regression problem. By training the network using an ordinary regression loss, our method achieves much higher accuracy and faster convergence in synch. Furthermore, we adopt a multi-scale network structure which avoids unnecessary spatial pooling and captures multi-scale information in parallel. The proposed deep ordinal regression network (DORN) achieves state-of-the-art results on three challenging benchmarks, ie, KITTI [16], Make3D [49], and NYU Depth v2 [41], and outperforms existing methods by a large margin.",
            "저자": "Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, Dacheng Tao",
            "전체 인용횟수": "1623회 인용20182019202020212022202316162292385418337",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "2002-2011",
            "학술 문서": "Deep ordinal regression network for monocular depth estimationH Fu, M Gong, C Wang, K Batmanghelich, D Tao - Proceedings of the IEEE conference on computer …, 20181623회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep ordinal regression network for monocular depth estimation",
        "year": null
    },
    "Knowledge distillation: A survey": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021/6",
            "게시자": "Springer US",
            "권": "129",
            "설명": " In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the …",
            "저널": "International Journal of Computer Vision",
            "저자": "Jianping Gou, Baosheng Yu, Stephen J Maybank, Dacheng Tao",
            "전체 인용횟수": "1612회 인용202020212022202312185559847",
            "페이지": "1789-1819",
            "학술 문서": "Knowledge distillation: A surveyJ Gou, B Yu, SJ Maybank, D Tao - International Journal of Computer Vision, 20211612회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Knowledge distillation: A survey",
        "year": null
    },
    "Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the BRATS challenge": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/11/5",
            "설명": "Gliomas are the most common primary brain malignancies, with different degrees of aggressiveness, variable prognosis and various heterogeneous histologic sub-regions, i.e., peritumoral edematous/invaded tissue, necrotic core, active and non-enhancing core. This intrinsic heterogeneity is also portrayed in their radio-phenotype, as their sub-regions are depicted by varying intensity profiles disseminated across multi-parametric magnetic resonance imaging (mpMRI) scans, reflecting varying biological properties. Their heterogeneous shape, extent, and location are some of the factors that make these tumors difficult to resect, and in some cases inoperable. The amount of resected tumor is a factor also considered in longitudinal scans, when evaluating the apparent tumor for potential diagnosis of progression. Furthermore, there is mounting evidence that accurate segmentation of the various tumor sub-regions can offer the basis for quantitative image analysis towards prediction of patient overall survival. This study assesses the state-of-the-art machine learning (ML) methods used for brain tumor image analysis in mpMRI scans, during the last seven instances of the International Brain Tumor Segmentation (BraTS) challenge, i.e., 2012-2018. Specifically, we focus on i) evaluating segmentations of the various glioma sub-regions in pre-operative mpMRI scans, ii) assessing potential tumor progression by virtue of longitudinal growth of tumor sub-regions, beyond use of the RECIST/RANO criteria, and iii) predicting the overall survival from pre-operative mpMRI scans of patients that underwent gross total resection. Finally, we investigate the …",
            "저널": "arXiv preprint arXiv:1811.02629",
            "저자": "Spyridon Bakas, Mauricio Reyes, Andras Jakab, Stefan Bauer, Markus Rempfler, Alessandro Crimi, Russell Takeshi Shinohara, Christoph Berger, Sung Min Ha, Martin Rozycki, Marcel Prastawa, Esther Alberts, Jana Lipkova, John Freymann, Justin Kirby, Michel Bilello, Hassan Fathallah-Shaykh, Roland Wiest, Jan Kirschke, Benedikt Wiestler, Rivka Colen, Aikaterini Kotrotsou, Pamela Lamontagne, Daniel Marcus, Mikhail Milchenko, Arash Nazeri, Marc-Andre Weber, Abhishek Mahajan, Ujjwal Baid, Elizabeth Gerstner, Dongjin Kwon, Gagan Acharya, Manu Agarwal, Mahbubul Alam, Alberto Albiol, Antonio Albiol, Francisco J Albiol, Varghese Alex, Nigel Allinson, Pedro HA Amorim, Abhijit Amrutkar, Ganesh Anand, Simon Andermatt, Tal Arbel, Pablo Arbelaez, Aaron Avery, Muneeza Azmat, W Bai, Subhashis Banerjee, Bill Barth, Thomas Batchelder, Kayhan Batmanghelich, Enzo Battistella, Andrew Beers, Mikhail Belyaev, Martin Bendszus, Eze Benson, Jose Bernal, Halandur Nagaraja Bharath, George Biros, Sotirios Bisdas, James Brown, Mariano Cabezas, Shilei Cao, Jorge M Cardoso, Eric N Carver, Adrià Casamitjana, Laura Silvana Castillo, Marcel Catà, Philippe Cattin, Albert Cerigues, Vinicius S Chagas, Siddhartha Chandra, Yi-Ju Chang, Shiyu Chang, Ken Chang, Joseph Chazalon, Shengcong Chen, Wei Chen, Jefferson W Chen, Zhaolin Chen, Kun Cheng, Ahana Roy Choudhury, Roger Chylla, Albert Clérigues, Steven Colleman, Ramiro German Rodriguez Colmeiro, Marc Combalia, Anthony Costa, Xiaomeng Cui, Zhenzhen Dai, Lutao Dai, Laura Alexandra Daza, Eric Deutsch, Changxing Ding, Chao Dong, Shidu Dong, Wojciech Dudzik, Zach Eaton-Rosen, Gary Egan, Guilherme Escudero, Théo Estienne, Richard Everson, Jonathan Fabrizio, Yong Fan, Longwei Fang, Xue Feng, Enzo Ferrante, Lucas Fidon, Martin Fischer, Andrew P French, Naomi Fridman, Huan Fu, David Fuentes, Yaozong Gao, Evan Gates, David Gering, Amir Gholami, Willi Gierke, Ben Glocker, Mingming Gong, Sandra González-Villá, T Grosges, Yuanfang Guan, Sheng Guo, Sudeep Gupta, Woo-Sup Han, Il Song Han, Konstantin Harmuth, Huiguang He, Aura Hernández-Sabaté, Evelyn Herrmann, Naveen Himthani, Winston Hsu, Cheyu Hsu, Xiaojun Hu, Xiaobin Hu, Yan Hu, Yifan Hu, Rui Hua, Teng-Yi Huang, Weilin Huang, Sabine Van Huffel, Quan Huo, Vivek HV, Khan M Iftekharuddin, Fabian Isensee, Mobarakol Islam, Aaron S Jackson, Sachin R Jambawalikar",
            "전체 인용횟수": "1610회 인용2018201920202021202220236105285407391401",
            "학술 문서": "Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the BRATS challengeS Bakas, M Reyes, A Jakab, S Bauer, M Rempfler… - arXiv preprint arXiv:1811.02629, 20181610회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the BRATS challenge",
        "year": null
    },
    "General tensor discriminant analysis and gabor features for gait recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2007/8/27",
            "게시자": "IEEE",
            "권": "29",
            "설명": "The traditional image representations are not suited to conventional classification methods, such as the linear discriminant analysis (LDA), because of the under sample problem (USP): the dimensionality of the feature space is much higher than the number of training samples. Motivated by the successes of the two dimensional LDA (2DLDA) for face recognition, we develop a general tensor discriminant analysis (GTDA) as a preprocessing step for LDA. The benefits of GTDA compared with existing preprocessing methods, e.g., principal component analysis (PCA) and 2DLDA, include 1) the USP is reduced in subsequent classification by, for example, LDA; 2) the discriminative information in the training tensors is preserved; and 3) GTDA provides stable recognition rates because the alternating projection optimization algorithm to obtain a solution of GTDA converges, while that of 2DLDA does not.We use human …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Dacheng Tao, Xuelong Li, Xindong Wu, Stephen J Maybank",
            "전체 인용횟수": "1344회 인용200720082009201020112012201320142015201620172018201920202021202220238788991798911910713012597916357483329",
            "페이지": "1700-1715",
            "학술 문서": "General tensor discriminant analysis and gabor features for gait recognitionD Tao, X Li, X Wu, SJ Maybank - IEEE transactions on pattern analysis and machine …, 20071344회 인용 관련 학술자료 전체 18개의 버전 ",
            "호": "10"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "General tensor discriminant analysis and gabor features for gait recognition",
        "year": null
    },
    "A survey on multi-view learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/4/20",
            "설명": "In recent years, a great many methods of learning from multi-view data by considering the diversity of different views have been proposed. These views may be obtained from multiple sources or different feature subsets. In trying to organize and highlight similarities and differences between the variety of multi-view learning approaches, we review a number of representative multi-view learning algorithms in different areas and classify them into three groups: 1) co-training, 2) multiple kernel learning, and 3) subspace learning. Notably, co-training style algorithms train alternately to maximize the mutual agreement on two distinct views of the data; multiple kernel learning algorithms exploit kernels that naturally correspond to different views and combine kernels either linearly or non-linearly to improve learning performance; and subspace learning algorithms aim to obtain a latent subspace shared by multiple views by assuming that the input views are generated from this latent subspace. Though there is significant variance in the approaches to integrating multiple views to improve learning performance, they mainly exploit either the consensus principle or the complementary principle to ensure the success of multi-view learning. Since accessing multiple views is the fundament of multi-view learning, with the exception of study on learning a model from multiple views, it is also valuable to study how to construct multiple views and how to evaluate these views. Overall, by exploring the consistency and complementary properties of different views, multi-view learning is rendered more effective, more promising, and has better generalization ability than …",
            "저널": "arXiv preprint arXiv:1304.5634",
            "저자": "Chang Xu, Dacheng Tao, Chao Xu",
            "전체 인용횟수": "1289회 인용2013201420152016201720182019202020212022202343062119148151178164166133119",
            "학술 문서": "A survey on multi-view learningC Xu, D Tao, C Xu - arXiv preprint arXiv:1304.5634, 20131262회 인용 관련 학술자료 전체 7개의 버전 Chao Xu*C Xu, D Tao - A survey on multi-view learning. CoRR, abs/1304.5634, 201344회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A survey on multi-view learning",
        "year": null
    },
    "Benchmarking single-image dehazing and beyond": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/8/30",
            "게시자": "IEEE",
            "권": "28",
            "설명": "We present a comprehensive study and evaluation of existing single-image dehazing algorithms, using a new large-scale benchmark consisting of both synthetic and real-world hazy images, called REalistic Single-Image DEhazing (RESIDE). RESIDE highlights diverse data sources and image contents, and is divided into five subsets, each serving different training or evaluation purposes. We further provide a rich variety of criteria for dehazing algorithm evaluation, ranging from full-reference metrics to no-reference metrics and to subjective evaluation, and the novel task-driven evaluation. Experiments on RESIDE shed light on the comparisons and limitations of the state-of-the-art dehazing algorithms, and suggest promising future directions.",
            "저널": "IEEE Transactions on Image Processing",
            "저자": "Boyi Li, Wenqi Ren, Dengpan Fu, Dacheng Tao, Dan Feng, Wenjun Zeng, Zhangyang Wang",
            "전체 인용횟수": "1254회 인용201820192020202120222023980178262350370",
            "페이지": "492-505",
            "학술 문서": "Benchmarking single-image dehazing and beyondB Li, W Ren, D Fu, D Tao, D Feng, W Zeng, Z Wang - IEEE Transactions on Image Processing, 20181254회 인용 관련 학술자료 전체 11개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Benchmarking single-image dehazing and beyond",
        "year": null
    },
    "Asymmetric bagging and random subspace for support vector machines-based relevance feedback in image retrieval": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006/6/5",
            "게시자": "IEEE",
            "권": "28",
            "설명": "Relevance feedback schemes based on support vector machines (SVM) have been widely used in content-based image retrieval (CBIR). However, the performance of SVM-based relevance feedback is often poor when the number of labeled positive feedback samples is small. This is mainly due to three reasons: 1) an SVM classifier is unstable on a small-sized training set, 2) SVM's optimal hyperplane may be biased when the positive feedback samples are much less than the negative feedback samples, and 3) overfitting happens because the number of feature dimensions is much higher than the size of the training set. In this paper, we develop a mechanism to overcome these problems. To address the first two problems, we propose an asymmetric bagging-based SVM (AB-SVM). For the third problem, we combine the random subspace method and SVM for relevance feedback, which is named random …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Dacheng Tao, Xiaoou Tang, Xuelong Li, Xindong Wu",
            "전체 인용횟수": "1075회 인용2006200720082009201020112012201320142015201620172018201920202021202220235275466566357108831089583674347423625",
            "페이지": "1088-1099",
            "학술 문서": "Asymmetric bagging and random subspace for support vector machines-based relevance feedback in image retrievalD Tao, X Tang, X Li, X Wu - IEEE transactions on pattern analysis and machine …, 20061075회 인용 관련 학술자료 전체 14개의 버전 ",
            "호": "7"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Asymmetric bagging and random subspace for support vector machines-based relevance feedback in image retrieval",
        "year": null
    },
    "An underwater image enhancement benchmark dataset and beyond": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/11/28",
            "게시자": "IEEE",
            "권": "29",
            "설명": "Underwater image enhancement has been attracting much attention due to its significance in marine engineering and aquatic robotics. Numerous underwater image enhancement algorithms have been proposed in the last few years. However, these algorithms are mainly evaluated using either synthetic datasets or few selected real-world images. It is thus unclear how these algorithms would perform on images acquired in the wild and how we could gauge the progress in the field. To bridge this gap, we present the first comprehensive perceptual study and analysis of underwater image enhancement using large-scale real-world images. In this paper, we construct an Underwater Image Enhancement Benchmark (UIEB) including 950 real-world underwater images, 890 of which have the corresponding reference images. We treat the rest 60 underwater images which cannot obtain satisfactory reference images as …",
            "저널": "IEEE Transactions on Image Processing",
            "저자": "Chongyi Li, Chunle Guo, Wenqi Ren, Runmin Cong, Junhui Hou, Sam Kwong, Dacheng Tao",
            "전체 인용횟수": "928회 인용201920202021202220231479164312355",
            "페이지": "4376-4389",
            "학술 문서": "An underwater image enhancement benchmark dataset and beyondC Li, C Guo, W Ren, R Cong, J Hou, S Kwong, D Tao - IEEE Transactions on Image Processing, 2019928회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "An underwater image enhancement benchmark dataset and beyond",
        "year": null
    },
    "A survey of graph edit distance": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/2",
            "게시자": "Springer-Verlag",
            "권": "13",
            "설명": " Inexact graph matching has been one of the significant research foci in the area of pattern analysis. As an important way to measure the similarity between pairwise graphs error-tolerantly, graph edit distance (GED) is the base of inexact graph matching. The research advance of GED is surveyed in order to provide a review of the existing literatures and offer some insights into the studies of GED. Since graphs may be attributed or non-attributed and the definition of costs for edit operations is various, the existing GED algorithms are categorized according to these two factors and described in detail. After these algorithms are analyzed and their limitations are identified, several promising directions for further research are proposed.",
            "저널": "Pattern Analysis and applications",
            "저자": "Xinbo Gao, Bing Xiao, Dacheng Tao, Xuelong Li",
            "전체 인용횟수": "881회 인용201020112012201320142015201620172018201920202021202220231027316161787275937465828164",
            "페이지": "113-129",
            "학술 문서": "A survey of graph edit distanceX Gao, B Xiao, D Tao, X Li - Pattern Analysis and applications, 2010881회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A survey of graph edit distance",
        "year": null
    },
    "A survey on vision transformer": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2022/2/18",
            "게시자": "IEEE",
            "권": "45",
            "설명": "Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang, Dacheng Tao",
            "전체 인용횟수": "846회 인용20212022202312215604",
            "페이지": "87-110",
            "학술 문서": "A survey on vision transformerK Han, Y Wang, H Chen, X Chen, J Guo, Z Liu, Y Tang… - IEEE transactions on pattern analysis and machine …, 2022846회 인용 관련 학술자료 전체 7개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A survey on vision transformer",
        "year": null
    },
    "Classification with noisy labels by importance reweighting": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/7/15",
            "게시자": "IEEE",
            "권": "38",
            "설명": "In this paper, we study a classification problem in which sample labels are randomly corrupted. In this scenario, there is an unobservable sample with noise-free labels. However, before being observed, the true labels are independently flipped with a probability   , and the random label noise can be class-conditional. Here, we address two fundamental problems raised by this scenario. The first is how to best use the abundant surrogate loss functions designed for the traditional classification problem when there is label noise. We prove that any surrogate loss function can be used for classification with noisy labels by using importance reweighting, with consistency assurance that the label noise does not ultimately hinder the search for the optimal classifier of the noise-free sample. The other is the open problem of how to obtain the noise rate   . We show that the rate is upper bounded by the conditional …",
            "저널": "IEEE Transactions on pattern analysis and machine intelligence",
            "저자": "Tongliang Liu, Dacheng Tao",
            "전체 인용횟수": "836회 인용2015201620172018201920202021202220231277596785106156138134",
            "페이지": "447-461",
            "학술 문서": "Classification with noisy labels by importance reweightingT Liu, D Tao - IEEE Transactions on pattern analysis and machine …, 2015836회 인용 관련 학술자료 전체 15개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Classification with noisy labels by importance reweighting",
        "year": null
    },
    "Godec: Randomized low-rank & sparse matrix decomposition in noisy case": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/10/7",
            "설명": "Low-rank and sparse structures have been profoundly studied in matrix completion and compressed sensing. In this paper, we develop \"Go Decomposition\" (GoDec) to efficiently and robustly estimate the low-rank part L and the sparse part 5 of a matrix X = L + S + G with noise G. GoDec alternatively assigns the low-rank approximation of X - S to L and the sparse approximation of X - L to S. The algorithm can be significantly accelerated by bilateral random projections (BRP). We also propose GoDec for matrix completion as an important variant. We prove that the objective value ∥X - L - S∥F2 converges to a local minimum, while L and S linearly converge to local optimums. Theoretically, we analyze the influence of L, S and G to the asymptotic/convergence speeds in order to discover the robustness of GoDec. Empirical studies suggest the efficiency, robustness and effectiveness of GoDec comparing with representative matrix decomposition and completion tools, e.g., Robust PCA and OptSpace. Copyright 2011 by the author(s)/owner(s).",
            "저널": "Proceedings of the 28th International Conference on Machine Learning, ICML 2011",
            "저자": "Tianyi Zhou, Dacheng Tao",
            "전체 인용횟수": "799회 인용20112012201320142015201620172018201920202021202220232232650697489968275777454",
            "학술 문서": "Godec: Randomized low-rank & sparse matrix decomposition in noisy caseT Zhou, D Tao - Proceedings of the 28th International Conference on …, 2011799회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Godec: Randomized low-rank & sparse matrix decomposition in noisy case",
        "year": null
    },
    "Multi-store tracker (muster): A cognitive psychology inspired approach to object tracking": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "Variations in the appearance of a tracked object, such as changes in geometry/photometry, camera viewpoint, illumination, or partial occlusion, pose a major challenge to object tracking. Here, we adopt cognitive psychology principles to design a flexible representation that can adapt to changes in object appearance during tracking. Inspired by the well-known Atkinson-Shiffrin Memory Model, we propose MUlti-Store Tracker (MUSTer), a dual-component approach consisting of short-and long-term memory stores to process target appearance memories. A powerful and efficient Integrated Correlation Filter (ICF) is employed in the short-term store for short-term tracking. The integrated long-term component, which is based on keypoint matching-tracking and RANSAC estimation, can interact with the long-term memory and provide additional information for output control. MUSTer was extensively evaluated on the CVPR2013 Online Object Tracking Benchmark (OOTB) and ALOV++ datasets. The experimental results demonstrated the superior performance of MUSTer in comparison with other state-of-art trackers.",
            "저자": "Zhibin Hong, Zhe Chen, Chaohui Wang, Xue Mei, Danil Prokhorov, Dacheng Tao",
            "전체 인용횟수": "751회 인용201520162017201820192020202120222023117395162131104804828",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "749-758",
            "학술 문서": "Multi-store tracker (muster): A cognitive psychology inspired approach to object trackingZ Hong, Z Chen, C Wang, X Mei, D Prokhorov, D Tao - Proceedings of the IEEE conference on computer …, 2015751회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Multi-store tracker (muster): A cognitive psychology inspired approach to object tracking",
        "year": null
    },
    "Deep modular co-attention networks for visual question answering": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "Visual Question Answering (VQA) requires a fine-grained and simultaneous understanding of both the visual content of images and the textual content of questions. Therefore, designing an effectiveco-attention'model to associate key words in questions with key objects in images is central to VQA performance. So far, most successful attempts at co-attention learning have been achieved by using shallow models, and deep co-attention models show little improvement over their shallow counterparts. In this paper, we propose a deep Modular Co-Attention Network (MCAN) that consists of Modular Co-Attention (MCA) layers cascaded in depth. Each MCA layer models the self-attention of questions and images, as well as the question-guided-attention of images jointly using a modular composition of two basic attention units. We quantitatively and qualitatively evaluate MCAN on the benchmark VQA-v2 dataset and conduct extensive ablation studies to explore the reasons behind MCAN's effectiveness. Experimental results demonstrate that MCAN significantly outperforms the previous state-of-the-art. Our best single model delivers 70.63% overall accuracy on the test-dev set.",
            "저자": "Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, Qi Tian",
            "전체 인용횟수": "750회 인용2019202020212022202312113182196246",
            "컨퍼런스": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "페이지": "6281-6290",
            "학술 문서": "Deep modular co-attention networks for visual question answeringZ Yu, J Yu, Y Cui, D Tao, Q Tian - Proceedings of the IEEE/CVF conference on computer …, 2019750회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep modular co-attention networks for visual question answering",
        "year": null
    },
    "Multi-modal factorized bilinear pooling with co-attention learning for visual question answering": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "Visual question answering (VQA) is challenging because it requires a simultaneous understanding of both the visual content of images and the textual content of questions. The approaches used to represent the images and questions in a fine-grained manner and questions and to fuse these multi-modal features play key roles in performance. Bilinear pooling based models have been shown to outperform traditional linear models for VQA, but their high-dimensional representations and high computational complexity may seriously limit their applicability in practice. For multi-modal feature fusion, here we develop a Multi-modal Factorized Bilinear (MFB) pooling approach to efficiently and effectively combine multi-modal features, which results in superior performance for VQA compared with other bilinear pooling approaches. For fine-grained image and question representation, we develop a co-attention mechanism using an end-to-end deep network architecture to jointly learn both the image and question attentions. Combining the proposed MFB approach with co-attention learning in a new network architecture provides a unified model for VQA. Our experimental results demonstrate that the single MFB with co-attention model achieves new state-of-the-art performance on the real-world VQA dataset. Code available at https://github. com/yuzcccc/mfb",
            "저자": "Zhou Yu, Jun Yu, Jianping Fan, Dacheng Tao",
            "전체 인용횟수": "683회 인용2017201820192020202120222023542101114139153125",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "1821-1830",
            "학술 문서": "Multi-modal factorized bilinear pooling with co-attention learning for visual question answeringZ Yu, J Yu, J Fan, D Tao - Proceedings of the IEEE international conference on …, 2017683회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Multi-modal factorized bilinear pooling with co-attention learning for visual question answering",
        "year": null
    },
    "Superpixel classification based optic disc and optic cup segmentation for glaucoma screening": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/2/18",
            "게시자": "IEEE",
            "권": "32",
            "설명": "Glaucoma is a chronic eye disease that leads to vision loss. As it cannot be cured, detecting the disease in time is important. Current tests using intraocular pressure (IOP) are not sensitive enough for population based glaucoma screening. Optic nerve head assessment in retinal fundus images is both more promising and superior. This paper proposes optic disc and optic cup segmentation using superpixel classification for glaucoma screening. In optic disc segmentation, histograms, and center surround statistics are used to classify each superpixel as disc or non-disc. A self-assessment reliability score is computed to evaluate the quality of the automated optic disc segmentation. For optic cup segmentation, in addition to the histograms and center surround statistics, the location information is also included into the feature space to boost the performance. The proposed segmentation methods have been evaluated in …",
            "저널": "IEEE transactions on medical imaging",
            "저자": "Jun Cheng, Jiang Liu, Yanwu Xu, Fengshou Yin, Damon Wing Kee Wong, Ngan-Meng Tan, Dacheng Tao, Ching-Yu Cheng, Tin Aung, Tien Yin Wong",
            "전체 인용횟수": "678회 인용20132014201520162017201820192020202120222023424644660839781896062",
            "페이지": "1019-1032",
            "학술 문서": "Superpixel classification based optic disc and optic cup segmentation for glaucoma screeningJ Cheng, J Liu, Y Xu, F Yin, DWK Wong, NM Tan… - IEEE transactions on medical imaging, 2013678회 인용 관련 학술자료 전체 8개의 버전 ",
            "호": "6"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Superpixel classification based optic disc and optic cup segmentation for glaucoma screening",
        "year": null
    },
    "A review on generative adversarial networks: Algorithms, theory, and applications": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021/11/23",
            "게시자": "IEEE",
            "권": "35",
            "설명": "Generative adversarial networks (GANs) have recently become a hot research topic; however, they have been studied since 2014, and a large number of algorithms have been proposed. Nevertheless, few comprehensive studies explain the connections among different GAN variants and how they have evolved. In this paper, we attempt to provide a review of the various GAN methods from the perspectives of algorithms, theory, and applications. First, the motivations, mathematical representations, and structures of most GAN algorithms are introduced in detail, and we compare their commonalities and differences. Second, theoretical issues related to GANs are investigated. Finally, typical applications of GANs in image processing and computer vision, natural language processing, music, speech and audio, the medical field, and data science are discussed.",
            "저자": "Jie Gui, Zhenan Sun, Yonggang Wen, Dacheng Tao, Jieping Ye",
            "전체 인용횟수": "673회 인용202020212022202342134219276",
            "출처": "IEEE transactions on knowledge and data engineering",
            "페이지": "3313-3332",
            "학술 문서": "A review on generative adversarial networks: Algorithms, theory, and applicationsJ Gui, Z Sun, Y Wen, D Tao, J Ye - IEEE transactions on knowledge and data engineering, 2021673회 인용 관련 학술자료 전체 12개의 버전 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A review on generative adversarial networks: Algorithms, theory, and applications",
        "year": null
    },
    "Probabilistic graphical models: principles and techniques": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/7/31",
            "게시자": "MIT press",
            "설명": "A general framework for constructing and using probabilistic models of complex systems that would enable a computer to use available information for making decisions. Most tasks require a person or an automated system to reason—to reach conclusions based on available information. The framework of probabilistic graphical models, presented in this book, provides a general approach for this task. The approach is model-based, allowing interpretable models to be constructed and then manipulated by reasoning algorithms. These models can also be learned automatically from data, allowing the approach to be used in cases where manually constructing a model is difficult or even impossible. Because uncertainty is an inescapable aspect of most real-world applications, the book focuses on probabilistic models, which make the uncertainty explicit and provide models that are more faithful to reality. Probabilistic Graphical Models discusses a variety of models, spanning Bayesian networks, undirected Markov networks, discrete and continuous models, and extensions to deal with dynamical systems and relational data. For each class of models, the text describes the three fundamental cornerstones: representation, inference, and learning, presenting both basic concepts and advanced techniques. Finally, the book considers the use of the proposed framework for causal reasoning and decision making under uncertainty. The main text in each chapter provides the detailed technical development of the key ideas. Most chapters also include boxes with additional material: skill boxes, which describe techniques; case study boxes, which discuss …",
            "저자": "Daphne Koller, Nir Friedman",
            "전체 인용횟수": "10436회 인용20102011201220132014201520162017201820192020202120222023159344503669778799863829905885959922906774",
            "학술 문서": "Probabilistic graphical models: principles and techniquesD Koller, N Friedman - 200910436회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Probabilistic graphical models: principles and techniques",
        "year": null
    },
    "The genotype-tissue expression (GTEx) project": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/6",
            "게시자": "Nature Publishing Group",
            "권": "45",
            "설명": "Genome-wide association studies have identified thousands of loci for common diseases, but, for the majority of these, the mechanisms underlying disease susceptibility remain unknown. Most associated variants are not correlated with protein-coding changes, suggesting that polymorphisms in regulatory regions probably contribute to many disease phenotypes. Here we describe the Genotype-Tissue Expression (GTEx) project, which will establish a resource database and associated tissue bank for the scientific community to study the relationship between genetic variation and gene expression in human tissues.",
            "저널": "Nature genetics",
            "저자": "John Lonsdale, Jeffrey Thomas, Mike Salvatore, Rebecca Phillips, Edmund Lo, Saboor Shad, Richard Hasz, Gary Walters, Fernando Garcia, Nancy Young, Barbara Foster, Mike Moser, Ellen Karasik, Bryan Gillard, Kimberley Ramsey, Susan Sullivan, Jason Bridge, Harold Magazine, John Syron, Johnelle Fleming, Laura Siminoff, Heather Traino, Maghboeba Mosavel, Laura Barker, Scott Jewell, Dan Rohrer, Dan Maxim, Dana Filkins, Philip Harbach, Eddie Cortadillo, Bree Berghuis, Lisa Turner, Eric Hudson, Kristin Feenstra, Leslie Sobin, James Robb, Phillip Branton, Greg Korzeniewski, Charles Shive, David Tabor, Liqun Qi, Kevin Groch, Sreenath Nampally, Steve Buia, Angela Zimmerman, Anna Smith, Robin Burges, Karna Robinson, Kim Valentino, Deborah Bradbury, Mark Cosentino, Norma Diaz-Mayoral, Mary Kennedy, Theresa Engel, Penelope Williams, Kenyon Erickson, Kristin Ardlie, Wendy Winckler, Gad Getz, David DeLuca, Daniel MacArthur, Manolis Kellis, Alexander Thomson, Taylor Young, Ellen Gelfand, Molly Donovan, Yan Meng, George Grant, Deborah Mash, Yvonne Marcus, Margaret Basile, Jun Liu, Jun Zhu, Zhidong Tu, Nancy J Cox, Dan L Nicolae, Eric R Gamazon, Hae Kyung Im, Anuar Konkashbaev, Jonathan Pritchard, Matthew Stevens, Timothèe Flutre, Xiaoquan Wen, Emmanouil T Dermitzakis, Tuuli Lappalainen, Roderic Guigo, Jean Monlong, Michael Sammeth, Daphne Koller, Alexis Battle, Sara Mostafavi, Mark McCarthy, Manual Rivas, Julian Maller, Ivan Rusyn, Andrew Nobel, Fred Wright, Andrey Shabalin, Mike Feolo, Nataliya Sharopova, Anne Sturcke, Justin Paschal, James M Anderson, Elizabeth L Wilder, Leslie K Derr, Eric D Green, Jeffery P Struewing, Gary Temple, Simona Volpi, Joy T Boyer, Elizabeth J Thomson, Mark S Guyer, Cathy Ng, Assya Abdallah, Deborah Colantuoni, Thomas R Insel, Susan E Koester, A Roger Little, Patrick K Bender, Thomas Lehner, Yin Yao, Carolyn C Compton, Jimmie B Vaught, Sherilyn Sawyer, Nicole C Lockhart, Joanne Demchok, Helen F Moore",
            "전체 인용횟수": "7037회 인용201420152016201720182019202020212022202311621632247871888599911981121945",
            "페이지": "580-585",
            "학술 문서": "The genotype-tissue expression (GTEx) projectJ Lonsdale, J Thomas, M Salvatore, R Phillips, E Lo… - Nature genetics, 20137037회 인용 관련 학술자료 전체 21개의 버전 ",
            "호": "6"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The genotype-tissue expression (GTEx) project",
        "year": null
    },
    "The Genotype-Tissue Expression (GTEx) pilot analysis: multitissue gene regulation in humans": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/5/8",
            "게시자": "American Association for the Advancement of Science",
            "권": "348",
            "설명": "Understanding the functional consequences of genetic variation, and how it affects complex human disease and quantitative traits, remains a critical challenge for biomedicine. We present an analysis of RNA sequencing data from 1641 samples across 43 tissues from 175 individuals, generated as part of the pilot phase of the Genotype-Tissue Expression (GTEx) project. We describe the landscape of gene expression across tissues, catalog thousands of tissue-specific and shared regulatory expression quantitative trait loci (eQTL) variants, describe complex network relationships, and identify signals from genome-wide association studies explained by eQTLs. These findings provide a systematic understanding of the cellular and biological consequences of human genetic variation and of the heterogeneity of such effects among a diverse set of human tissues.",
            "저널": "Science",
            "저자": "GTEx Consortium, Kristin G Ardlie, David S Deluca, Ayellet V Segrè, Timothy J Sullivan, Taylor R Young, Ellen T Gelfand, Casandra A Trowbridge, Julian B Maller, Taru Tukiainen, Monkol Lek, Lucas D Ward, Pouya Kheradpour, Benjamin Iriarte, Yan Meng, Cameron D Palmer, Tõnu Esko, Wendy Winckler, Joel N Hirschhorn, Manolis Kellis, Daniel G MacArthur, Gad Getz, Andrey A Shabalin, Gen Li, Yi-Hui Zhou, Andrew B Nobel, Ivan Rusyn, Fred A Wright, Tuuli Lappalainen, Pedro G Ferreira, Halit Ongen, Manuel A Rivas, Alexis Battle, Sara Mostafavi, Jean Monlong, Michael Sammeth, Marta Mele, Ferran Reverter, Jakob M Goldmann, Daphne Koller, Roderic Guigó, Mark I McCarthy, Emmanouil T Dermitzakis, Eric R Gamazon, Hae Kyung Im, Anuar Konkashbaev, Dan L Nicolae, Nancy J Cox, Timothée Flutre, Xiaoquan Wen, Matthew Stephens, Jonathan K Pritchard, Zhidong Tu, Bin Zhang, Tao Huang, Quan Long, Luan Lin, Jialiang Yang, Jun Zhu, Jun Liu, Amanda Brown, Bernadette Mestichelli, Denee Tidwell, Edmund Lo, Mike Salvatore, Saboor Shad, Jeffrey A Thomas, John T Lonsdale, Michael T Moser, Bryan M Gillard, Ellen Karasik, Kimberly Ramsey, Christopher Choi, Barbara A Foster, John Syron, Johnell Fleming, Harold Magazine, Rick Hasz, Gary D Walters, Jason P Bridge, Mark Miklos, Susan Sullivan, Laura K Barker, Heather M Traino, Maghboeba Mosavel, Laura A Siminoff, Dana R Valley, Daniel C Rohrer, Scott D Jewell, Philip A Branton, Leslie H Sobin, Mary Barcus, Liqun Qi, Jeffrey McLean, Pushpa Hariharan, Ki Sung Um, Shenpei Wu, David Tabor, Charles Shive, Anna M Smith, Stephen A Buia, Anita H Undale, Karna L Robinson, Nancy Roche, Kimberly M Valentino, Angela Britton, Robin Burges, Debra Bradbury, Kenneth W Hambright, John Seleski, Greg E Korzeniewski, Kenyon Erickson, Yvonne Marcus, Jorge Tejada, Mehran Taherian, Chunrong Lu, Margaret Basile, Deborah C Mash, Simona Volpi, Jeffery P Struewing, Gary F Temple, Joy Boyer, Deborah Colantuoni, Roger Little, Susan Koester, Latarsha J Carithers, Helen M Moore, Ping Guan, Carolyn Compton, Sherilyn J Sawyer, Joanne P Demchok, Jimmie B Vaught, Chana A Rabiner, Nicole C Lockhart, Kristin G Ardlie, Gad Getz, Fred A Wright, Manolis Kellis, Simona Volpi, Emmanouil T Dermitzakis",
            "전체 인용횟수": "4352회 인용20152016201720182019202020212022202395386565681563591637450362",
            "페이지": "648-660",
            "학술 문서": "The Genotype-Tissue Expression (GTEx) pilot analysis: multitissue gene regulation in humansGTEx Consortium, KG Ardlie, DS Deluca, AV Segrè… - Science, 20154341회 인용 관련 학술자료 전체 34개의 버전 Ardlie, KG, Deluca, DS, Segre, AV, Sullivan, TJ, Young*GTEx Consortium - 201542회 인용 관련 학술자료 ",
            "호": "6235"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The Genotype-Tissue Expression (GTEx) pilot analysis: multitissue gene regulation in humans",
        "year": null
    },
    "FastSLAM: A factored solution to the simultaneous localization and mapping problem": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2002/7/28",
            "권": "593598",
            "설명": "The ability to simultaneously localize a robot and accurately map its surroundings is considered by many to be a key prerequisite of truly autonomous robots. However, few approaches to this problem scale up to handle the very large number of landmarks present in real environments. Kalman filter-based algorithms, for example, require time quadratic in the number of landmarks to incorporate each sensor observation. This paper presents FastSLAM, an algorithm that recursively estimates the full posterior distribution over robot pose and landmark locations, yet scales logarithmically with the number of landmarks in the map. This algorithm is based on an exact factorization of the posterior into a product of conditional landmark distributions and a distribution over robot paths. The algorithm has been run successfully on as many as 50,000 landmarks, environments far beyond the reach of previous approaches. Experimental results demonstrate the advantages and limitations of the FastSLAM algorithm on both simulated and realworld data.",
            "저널": "Aaai/iaai",
            "저자": "Michael Montemerlo, Sebastian Thrun, Daphne Koller, Ben Wegbreit",
            "전체 인용횟수": "3745회 인용2002200320042005200620072008200920102011201220132014201520162017201820192020202120222023164980110122170201200234205205201193211246223208204207165156109",
            "학술 문서": "FastSLAM: A factored solution to the simultaneous localization and mapping problemM Montemerlo, S Thrun, D Koller, B Wegbreit - Aaai/iaai, 20023745회 인용 관련 학술자료 전체 42개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "FastSLAM: A factored solution to the simultaneous localization and mapping problem",
        "year": null
    },
    "Support vector machine active learning with applications to text classification": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2001",
            "권": "2",
            "설명": "Support vector machines have met with significant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. In many settings, we also have the option of using pool-based active learning. Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing active learning with support vector machines, ie, an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm using the notion of a version space. We present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.",
            "저널": "Journal of machine learning research",
            "저자": "Simon Tong, Daphne Koller",
            "전체 인용횟수": "3696회 인용200320042005200620072008200920102011201220132014201520162017201820192020202120222023154357756699116131133191195244199247305305306268272219164",
            "페이지": "45-66",
            "학술 문서": "Support vector machine active learning with applications to text classificationS Tong, D Koller - Journal of machine learning research, 20013696회 인용 관련 학술자료 전체 25개의 버전 ",
            "호": "Nov"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Support vector machine active learning with applications to text classification",
        "year": null
    },
    "A gene-coexpression network for global discovery of conserved genetic modules": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2003/10/10",
            "게시자": "American Association for the Advancement of Science",
            "권": "302",
            "설명": "To elucidate gene function on a global scale, we identified pairs of genes that are coexpressed over 3182 DNA microarrays from humans, flies, worms, and yeast. We found 22,163 such coexpression relationships, each of which has been conserved across evolution. This conservation implies that the coexpression of these gene pairs confers a selective advantage and therefore that these genes are functionally related. Manyof these relationships provide strong evidence for the involvement of new genes in core biological functions such as the cell cycle, secretion, and protein expression. We experimentallyconfirmed the predictions implied bysome of these links and identified cell proliferation functions for several genes. By assembling these links into a gene-coexpression network, we found several components that were animal-specific as well as interrelationships between newly evolved and ancient modules.",
            "저널": "science",
            "저자": "Joshua M Stuart, Eran Segal, Daphne Koller, Stuart K Kim",
            "전체 인용횟수": "2520회 인용200320042005200620072008200920102011201220132014201520162017201820192020202120222023108812313413412713610710911512311712914616414813712311912590",
            "페이지": "249-255",
            "학술 문서": "A gene-coexpression network for global discovery of conserved genetic modulesJM Stuart, E Segal, D Koller, SK Kim - science, 20032520회 인용 관련 학술자료 전체 35개의 버전 ",
            "호": "5643"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A gene-coexpression network for global discovery of conserved genetic modules",
        "year": null
    },
    "Toward optimal feature selection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1996/7/3",
            "권": "96",
            "설명": "Our motivation: reducing the dimensionality of our feature space (that is, reducing the number of features used) Why should we strive to use less features? Very often time requirements for induction algorithms grow very fast as the number of features does (even exponentially!) Getting a model as simple as possible for a given training set can prevent overfitting (Occam’s razor principle)",
            "저널": "ICML",
            "저자": "Daphne Koller, Mehran Sahami",
            "전체 인용횟수": "2423회 인용199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023183734374361886699108114118124112109110104103116127971091131151097548",
            "페이지": "292",
            "학술 문서": "Toward optimal feature selectionD Koller, M Sahami - ICML, 19962423회 인용 관련 학술자료 전체 17개의 버전 ",
            "호": "28"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Toward optimal feature selection",
        "year": null
    },
    "Module networks: identifying regulatory modules and their condition-specific regulators from gene expression data": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2003/6/1",
            "게시자": "Nature Publishing Group US",
            "권": "34",
            "설명": "Much of a cell's activity is organized as a network of interacting modules: sets of genes coregulated to respond to different conditions. We present a probabilistic method for identifying regulatory modules from gene expression data. Our procedure identifies modules of coregulated genes, their regulators and the conditions under which regulation occurs, generating testable hypotheses in the form 'regulator X regulates module Y under conditions W'. We applied the method to a Saccharomyces cerevisiae expression data set, showing its ability to identify functionally coherent modules and their correct regulators. We present microarray experiments supporting three novel predictions, suggesting regulatory roles for previously uncharacterized proteins.",
            "저널": "Nature genetics",
            "저자": "Eran Segal, Michael Shapira, Aviv Regev, Dana Pe'er, David Botstein, Daphne Koller, Nir Friedman",
            "전체 인용횟수": "2157회 인용200320042005200620072008200920102011201220132014201520162017201820192020202120222023211311481461631621721621231091211041069081596652414240",
            "페이지": "166-176",
            "학술 문서": "Module networks: identifying regulatory modules and their condition-specific regulators from gene expression dataE Segal, M Shapira, A Regev, D Pe'er, D Botstein… - Nature genetics, 20032157회 인용 관련 학술자료 전체 55개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Module networks: identifying regulatory modules and their condition-specific regulators from gene expression data",
        "year": null
    },
    "Scape: shape completion and animation of people": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/7/1",
            "도서": "ACM SIGGRAPH 2005 Papers",
            "설명": "We introduce the SCAPE method (Shape Completion and Animation for PEople)---a data-driven method for building a human shape model that spans variation in both subject shape and pose. The method is based on a representation that incorporates both articulated and non-rigid deformations. We learn a pose deformation model that derives the non-rigid surface deformation as a function of the pose of the articulated skeleton. We also learn a separate model of variation based on body shape. Our two models can be combined to produce 3D surface models with realistic muscle deformation for different people in different poses, when neither appear in the training set. We show how the model can be used for shape completion --- generating a complete surface mesh given a limited set of markers specifying the target shape. We present applications of shape completion to partial view completion and motion capture …",
            "저자": "Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Sebastian Thrun, Jim Rodgers, James Davis",
            "전체 인용횟수": "2064회 인용2006200720082009201020112012201320142015201620172018201920202021202220232354365864838599101105109119147159206190201171",
            "페이지": "408-416",
            "학술 문서": "Scape: shape completion and animation of peopleD Anguelov, P Srinivasan, D Koller, S Thrun, J Rodgers… - ACM SIGGRAPH 2005 Papers, 20052064회 인용 관련 학술자료 전체 32개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Scape: shape completion and animation of people",
        "year": null
    },
    "Gene-expression profiles and transcriptional regulatory pathways that underlie the identity and diversity of mouse tissue macrophages": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/11",
            "게시자": "Nature Publishing Group US",
            "권": "13",
            "설명": "We assessed gene expression in tissue macrophages from various mouse organs. The diversity in gene expression among different populations of macrophages was considerable. Only a few hundred mRNA transcripts were selectively expressed by macrophages rather than dendritic cells, and many of these were not present in all macrophages. Nonetheless, well-characterized surface markers, including MerTK and FcγR1 (CD64), along with a cluster of previously unidentified transcripts, were distinctly and universally associated with mature tissue macrophages. TCEF3, C/EBP-α, Bach1 and CREG-1 were among the transcriptional regulators predicted to regulate these core macrophage-associated genes. The mRNA encoding other transcription factors, such as Gata6, was associated with single macrophage populations. We further identified how these transcripts and the proteins they encode facilitated …",
            "저널": "Nature immunology",
            "저자": "Emmanuel L Gautier, Tal Shay, Jennifer Miller, Melanie Greter, Claudia Jakubzick, Stoyan Ivanov, Julie Helft, Andrew Chow, Kutlu G Elpek, Simon Gordonov, Amin R Mazloom, Avi Ma'Ayan, Wei-Jen Chua, Ted H Hansen, Shannon J Turley, Miriam Merad, Gwendalyn J Randolph",
            "전체 인용횟수": "2011회 인용2013201420152016201720182019202020212022202393142189184239214208209200195124",
            "페이지": "1118-1128",
            "학술 문서": "Gene-expression profiles and transcriptional regulatory pathways that underlie the identity and diversity of mouse tissue macrophagesEL Gautier, T Shay, J Miller, M Greter, C Jakubzick… - Nature immunology, 20122011회 인용 관련 학술자료 전체 19개의 버전 ",
            "호": "11"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Gene-expression profiles and transcriptional regulatory pathways that underlie the identity and diversity of mouse tissue macrophages",
        "year": null
    },
    "Max-margin Markov networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2003",
            "권": "16",
            "설명": "In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches.",
            "저널": "Advances in neural information processing systems",
            "저자": "Ben Taskar, Carlos Guestrin, Daphne Koller",
            "전체 인용횟수": "1790회 인용2004200520062007200820092010201120122013201420152016201720182019202020212022202330737394103127155120127117142129101100835847383217",
            "학술 문서": "Max-margin Markov networksB Taskar, C Guestrin, D Koller - Advances in neural information processing systems, 20031790회 인용 관련 학술자료 전체 29개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Max-margin Markov networks",
        "year": null
    },
    "The Immunological Genome Project: networks of gene expression in immune cells": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008/10",
            "게시자": "Nature Publishing Group US",
            "권": "9",
            "설명": "The Immunological Genome Project combines immunology and computational biology laboratories in an effort to establish a complete 'road map' of gene-expression and regulatory networks in all immune cells",
            "저널": "Nature immunology",
            "저자": "Tracy SP Heng, Michio W Painter, the Immunological Genome Project Consortium, Kutlu Elpek, Veronika Lukacs-Kornek, Nora Mauermann, Shannon J Turley, Daphne Koller, Francis S Kim, Amy J Wagers, Natasha Asinovski, Scott Davis, Marlys Fassett, Markus Feuerer, Daniel HD Gray, Sokol Haxhinasto, Jonathan A Hill, Gordon Hyatt, Catherine Laplace, Kristen Leatherbee, Diane Mathis, Christophe Benoist, Radu Jianu, David H Laidlaw, J Adam Best, Jamie Knell, Ananda W Goldrath, Jessica Jarjoura, Joseph C Sun, Yanan Zhu, Lewis L Lanier, Ayla Ergun, Zheng Li, James J Collins, Susan A Shinton, Richard R Hardy, Randall Friedline, Katelyn Sylvia, Joonsoo Kang",
            "전체 인용횟수": "1731회 인용2009201020112012201320142015201620172018201920202021202220236182861907388107149169190183210183168",
            "페이지": "1091-1094",
            "학술 문서": "The Immunological Genome Project: networks of gene expression in immune cellsTSP Heng, MW Painter… - Nature immunology, 20081731회 인용 관련 학술자료 전체 16개의 버전 ",
            "호": "10"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The Immunological Genome Project: networks of gene expression in immune cells",
        "year": null
    },
    "FastSLAM 2.0: An improved particle filtering algorithm for simultaneous localization and mapping that provably converges": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2003/8/9",
            "권": "3",
            "설명": "In [15], Montemerlo et al. proposed an algorithm called FastSLAM as an efficient and robust solution to the simultaneous localization and mapping problem. This paper describes a modified version of FastSLAM which overcomes important deficiencies of the original algorithm. We prove convergence of this new algorithm for linear SLAM problems and provide real-world experimental results that illustrate an order of magnitude improvement in accuracy over the original FastSLAM algorithm.",
            "저널": "IJCAI",
            "저자": "Michael Montemerlo, Sebastian Thrun, Daphne Koller, Ben Wegbreit",
            "전체 인용횟수": "1590회 인용2003200420052006200720082009201020112012201320142015201620172018201920202021202220237262853919411512111910110791779182646259706251",
            "페이지": "1151-1156",
            "학술 문서": "FastSLAM 2.0: An improved particle filtering algorithm for simultaneous localization and mapping that provably convergesM Montemerlo, S Thrun, D Koller, B Wegbreit - IJCAI, 20031590회 인용 관련 학술자료 전체 29개의 버전 ",
            "호": "2003"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "FastSLAM 2.0: An improved particle filtering algorithm for simultaneous localization and mapping that provably converges",
        "year": null
    },
    "Self-paced learning for latent variable models": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010",
            "권": "23",
            "설명": "Latent variable models are a powerful tool for addressing several tasks in machine learning. However, the algorithms for learning the parameters of latent variable models are prone to getting stuck in a bad local optimum. To alleviate this problem, we build on the intuition that, rather than considering all samples simultaneously, the algorithm should be presented with the training data in a meaningful order that facilitates learning. The order of the samples is determined by how easy they are. The main challenge is that often we are not provided with a readily computable measure of the easiness of samples. We address this issue by proposing a novel, iterative self-paced learning algorithm where each iteration simultaneously selects easy samples and learns a new parameter vector. The number of samples selected is governed by a weight that is annealed until the entire training data has been considered. We empirically demonstrate that the self-paced learning algorithm outperforms the state of the art method for learning a latent structural SVM on four applications: object localization, noun phrase coreference, motif finding and handwritten digit recognition.",
            "저널": "Advances in neural information processing systems",
            "저자": "M Kumar, Benjamin Packer, Daphne Koller",
            "전체 인용횟수": "1484회 인용201120122013201420152016201720182019202020212022202311232426317895128158227221234211",
            "학술 문서": "Self-paced learning for latent variable modelsM Kumar, B Packer, D Koller - Advances in neural information processing systems, 20101484회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Self-paced learning for latent variable models",
        "year": null
    },
    "Hierarchically classifying documents using very few words": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1997/7/8",
            "권": "97",
            "설명": "The proliferation of topic hierarchies for text documents has resulted in a need for tools that automatically classify new documents within such hierarchies. Existing classification schemes which ignore the hierarchical structure and treat the topics as separate classes are often inade-¹uate in text classification where the there is a large number of classes and a huge number of relevant features needed to distinguish between them. We propose an approach that utilizes the hierarchical topic structure to decompose the classification task into a set of simpler problems, one at each node in the classification tree. As we show, each of these smaller problems can be solved accurately by focusing only on a very small set of features, those relevant to the task at hand. This set of relevant features varies widely throughout the hierarchy, so that, while the overall relevant feature set may be large, each classifier only examines a small subset. The use of reduced feature sets allows us to utilize more complex (probabilistic) models, without encountering many of the standard computational and robustness difficulties.",
            "저널": "ICML",
            "저자": "Daphne Koller, Mehran Sahami",
            "전체 인용횟수": "1436회 인용199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023534334654579177821058168866267605158514541333517292323",
            "페이지": "170-178",
            "학술 문서": "Hierarchically classifying documents using very few wordsD Koller, M Sahami - ICML, 19971436회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Hierarchically classifying documents using very few words",
        "year": null
    },
    "Population genomics of human gene expression": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2007/10",
            "게시자": "Nature Publishing Group US",
            "권": "39",
            "설명": "Genetic variation influences gene expression, and this variation in gene expression can be efficiently mapped to specific genomic regions and variants. Here we have used gene expression profiling of Epstein-Barr virus–transformed lymphoblastoid cell lines of all 270 individuals genotyped in the HapMap Consortium to elucidate the detailed features of genetic variation underlying gene expression variation. We find that gene expression is heritable and that differentiation between populations is in agreement with earlier small-scale studies. A detailed association analysis of over 2.2 million common SNPs per population (5% frequency in HapMap) with gene expression identified at least 1,348 genes with association signals in cis and at least 180 in trans. Replication in at least one independent population was achieved for 37% of cis signals and 15% of trans signals, respectively. Our results strongly support an …",
            "저널": "Nature genetics",
            "저자": "Barbara E Stranger, Alexandra C Nica, Matthew S Forrest, Antigone Dimas, Christine P Bird, Claude Beazley, Catherine E Ingle, Mark Dunning, Paul Flicek, Daphne Koller, Stephen Montgomery, Simon Tavaré, Panos Deloukas, Emmanouil T Dermitzakis",
            "전체 인용횟수": "1358회 인용200720082009201020112012201320142015201620172018201920202021202220234751171121101591241231208166606135373727",
            "페이지": "1217-1224",
            "학술 문서": "Population genomics of human gene expressionBE Stranger, AC Nica, MS Forrest, A Dimas, CP Bird… - Nature genetics, 20071358회 인용 관련 학술자료 전체 23개의 버전 ",
            "호": "10"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Population genomics of human gene expression",
        "year": null
    },
    "Learning probabilistic relational models": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1999/8",
            "권": "99",
            "설명": "A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with “flat” data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning—the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases.",
            "저널": "IJCAI",
            "저자": "Nir Friedman, Lise Getoor, Daphne Koller, Avi Pfeffer",
            "전체 인용횟수": "1179회 인용19992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220235122235543866688281567069815351664135372642342514",
            "페이지": "1300-1309",
            "학술 문서": "Learning probabilistic relational modelsN Friedman, L Getoor, D Koller, A Pfeffer - IJCAI, 19991179회 인용 관련 학술자료 전체 37개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning probabilistic relational models",
        "year": null
    },
    "The chemical genomic portrait of yeast: uncovering a phenotype for all genes": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008/4/18",
            "게시자": "American Association for the Advancement of Science",
            "권": "320",
            "설명": "Genetics aims to understand the relation between genotype and phenotype. However, because complete deletion of most yeast genes (∼80%) has no obvious phenotypic consequence in rich medium, it is difficult to study their functions. To uncover phenotypes for this nonessential fraction of the genome, we performed 1144 chemical genomic assays on the yeast whole-genome heterozygous and homozygous deletion collections and quantified the growth fitness of each deletion strain in the presence of chemical or environmental stress conditions. We found that 97% of gene deletions exhibited a measurable growth phenotype, suggesting that nearly all genes are essential for optimal growth in at least one condition.",
            "저널": "Science",
            "저자": "Maureen E Hillenmeyer, Eula Fung, Jan Wildenhain, Sarah E Pierce, Shawn Hoon, William Lee, Michael Proctor, Robert P St. Onge, Mike Tyers, Daphne Koller, Russ B Altman, Ronald W Davis, Corey Nislow, Guri Giaever",
            "전체 인용횟수": "1112회 인용200820092010201120122013201420152016201720182019202020212022202334829611211210484678267535334405040",
            "페이지": "362-365",
            "학술 문서": "The chemical genomic portrait of yeast: uncovering a phenotype for all genesME Hillenmeyer, E Fung, J Wildenhain, SE Pierce… - Science, 20081112회 인용 관련 학술자료 전체 23개의 버전 ",
            "호": "5874"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The chemical genomic portrait of yeast: uncovering a phenotype for all genes",
        "year": null
    },
    "Being Bayesian about network structure. A Bayesian approach to structure discovery in Bayesian networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2003/1",
            "게시자": "Kluwer Academic Publishers",
            "권": "50",
            "설명": " In many multivariate domains, we are interested in analyzing the dependency structure of the underlying distribution, e.g., whether two variables are in direct interaction. We can represent dependency structures using Bayesian network models. To analyze a given data set, Bayesian model selection attempts to find the most likely (MAP) model, and uses its structure to answer these questions. However, when the amount of available data is modest, there might be many models that have non-negligible posterior. Thus, we want compute the Bayesian posterior of a feature, i.e., the total posterior probability of all models that contain it. In this paper, we propose a new approach for this task. We first show how to efficiently compute a sum over the exponential number of networks that are consistent with a fixed order over network variables. This allows us to compute, for a given order, both the marginal probability of …",
            "저널": "Machine learning",
            "저자": "Nir Friedman, Daphne Koller",
            "전체 인용횟수": "1093회 인용20022003200420052006200720082009201020112012201320142015201620172018201920202021202220235152525355563606074696367575455494953543955",
            "페이지": "95-125",
            "학술 문서": "Being Bayesian about network structure. A Bayesian approach to structure discovery in Bayesian networksN Friedman, D Koller - Machine learning, 20031093회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Being Bayesian about network structure. A Bayesian approach to structure discovery in Bayesian networks",
        "year": null
    },
    "Simultaneous localization and mapping with sparse extended information filters": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004/8",
            "게시자": "SAGE Publications",
            "권": "23",
            "설명": "In this paper we describe a scalable algorithm for the simultaneous mapping and                     localization (SLAM) problem. SLAM is the problem of acquiring a map of a static                     environment with a mobile robot. The vast majority of SLAM algorithms are based                     on the extended Kalman filter (EKF). In this paper we advocate an algorithm that                     relies on the dual of the EKF, the extended information filter (EIF). We show                     that when represented in the information form, map posteriors are dominated by a                     small number of links that tie together nearby features in the map. This insight                     is developed into a sparse variant of the EIF, called the sparse extended                     information filter (SEIF). SEIFs represent maps by graphical networks of                     features that are locally interconnected, where links represent relative                     information between pairs of nearby …",
            "저널": "The international journal of robotics research",
            "저자": "Sebastian Thrun, Yufeng Liu, Daphne Koller, Andrew Y Ng, Zoubin Ghahramani, Hugh Durrant-Whyte",
            "전체 인용횟수": "1062회 인용20032004200520062007200820092010201120122013201420152016201720182019202020212022202382245818392787569585867574647393231311414",
            "페이지": "693-716",
            "학술 문서": "Simultaneous localization and mapping with sparse extended information filtersS Thrun, Y Liu, D Koller, AY Ng, Z Ghahramani… - The international journal of robotics research, 20041056회 인용 관련 학술자료 전체 10개의 버전 Simultaneous mapping and localization with sparse extended information fi Iters*S Thrun, D Koller, Z Ghahramani, H Durrant-Whyte… - Pmc. WAFR, 200213회 인용 관련 학술자료 Simultaneous localization and mapping with sparse extended information filtersT Sebastian, L Yufeng, K Daphne, G Zoubin, DW Hugh - The International Journal of Robotics Research, 20044회 인용 관련 학술자료 ",
            "호": "7-8"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Simultaneous localization and mapping with sparse extended information filters",
        "year": null
    },
    "Batch normalization: Accelerating deep network training by reducing internal covariate shift": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/6/1",
            "게시자": "pmlr",
            "설명": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.",
            "저자": "Sergey Ioffe, Christian Szegedy",
            "전체 인용횟수": "51175회 인용2015201620172018201920202021202220231789332656518774338379916487816821",
            "컨퍼런스": "International conference on machine learning",
            "페이지": "448-456",
            "학술 문서": "Batch normalization: Accelerating deep network training by reducing internal covariate shiftS Ioffe, C Szegedy - International conference on machine learning, 201551173회 인용 관련 학술자료 전체 34개의 버전 Normalization: accelerating deep network training by reducing internal covariate shift*S Ioffe, C Szegedy - arXiv preprint arXiv:1502.031675회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
        "year": null
    },
    "Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv 2015": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "저널": "arXiv preprint arXiv:1502.03167",
            "저자": "Sergey Ioffe, Christian Szegedy",
            "전체 인용횟수": "1102회 인용2015201620172018201920202021202220233132669170289249162117",
            "학술 문서": "Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv 2015S Ioffe, C Szegedy - arXiv preprint arXiv:1502.03167, 20151102회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv 2015",
        "year": null
    },
    "No fuss distance metric learning using proxies": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "We address the problem of distance metric learning (DML), defined as learning a distance consistent with a notion of semantic similarity. Traditionally, for this problem supervision is expressed in the form of sets of points that follow an ordinal relationship--an anchor point x is similar to a set of positive points Y, and dissimilar to a set of negative points Z, and a loss defined over these distances is minimized. While the specifics of the optimization differ, in this work we collectively call this type of supervision Triplets and all methods that follow this pattern Triplet-Based methods. These methods are challenging to optimize. A main issue is the need for finding informative triplets, which is usually achieved by a variety of tricks such as increasing the batch size, hard or semi-hard triplet mining, etc. Even with these tricks, the convergence rate of such methods is slow. In this paper we propose to optimize the triplet loss on a different space of triplets, consisting of an anchor data point and similar and dissimilar proxy points which are learned as well. These proxies approximate the original data points, so that a triplet loss over the proxies is a tight upper bound of the original loss. This proxy-based loss is empirically better behaved. As a result, the proxy-loss improves on state-of-art results for three standard zero-shot learning datasets, by up to 15% points, while converging three times as fast as other triplet-based losses.",
            "저자": "Yair Movshovitz-Attias, Alexander Toshev, Thomas K Leung, Sergey Ioffe, Saurabh Singh",
            "전체 인용횟수": "653회 인용201720182019202020212022202332866106145168131",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "360-368",
            "학술 문서": "No fuss distance metric learning using proxiesY Movshovitz-Attias, A Toshev, TK Leung, S Ioffe… - Proceedings of the IEEE international conference on …, 2017653회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "No fuss distance metric learning using proxies",
        "year": null
    },
    "Probabilistic linear discriminant analysis": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006",
            "게시자": "Springer Berlin Heidelberg",
            "설명": " Linear dimensionality reduction methods, such as LDA, are often used in object recognition for feature extraction, but do not address the problem of how to use these features for recognition. In this paper, we propose Probabilistic LDA, a generative probability model with which we can both extract the features and combine them for recognition. The latent variables of PLDA represent both the class of the object and the view of the object within a class. By making examples of the same class share the class variable, we show how to train PLDA and use it for recognition on previously unseen classes. The usual LDA features are derived as a result of training PLDA, but in addition have a probability model attached to them, which automatically gives more weight to the more discriminative features. With PLDA, we can build a model of a previously unseen class from a single example, and can combine multiple …",
            "저자": "Sergey Ioffe",
            "전체 인용횟수": "616회 인용200920102011201220132014201520162017201820192020202120222023769591219233649621089510067",
            "컨퍼런스": "Computer Vision–ECCV 2006: 9th European Conference on Computer Vision, Graz, Austria, May 7-13, 2006, Proceedings, Part IV 9",
            "페이지": "531-542",
            "학술 문서": "Probabilistic linear discriminant analysisS Ioffe - Computer Vision–ECCV 2006: 9th European …, 2006616회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Probabilistic linear discriminant analysis",
        "year": null
    },
    "Batch renormalization: Towards reducing minibatch dependence in batch-normalized models": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "권": "30",
            "설명": "Batch Normalization is quite effective at accelerating and improving the training of deep models. However, its effectiveness diminishes when the training minibatches are small, or do not consist of independent samples. We hypothesize that this is due to the dependence of model layer inputs on all the examples in the minibatch, and different activations being produced between training and inference. We propose Batch Renormalization, a simple and effective extension to ensure that the training and inference models generate the same outputs that depend on individual examples rather than the entire minibatch. Models trained with Batch Renormalization perform substantially better than batchnorm when training with small or non-iid minibatches. At the same time, Batch Renormalization retains the benefits of batchnorm such as insensitivity to initialization and training efficiency.",
            "저널": "Advances in neural information processing systems",
            "저자": "Sergey Ioffe",
            "전체 인용횟수": "566회 인용20172018201920202021202220231061821089212583",
            "학술 문서": "Batch renormalization: Towards reducing minibatch dependence in batch-normalized modelsS Ioffe - Advances in neural information processing systems, 2017566회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Batch renormalization: Towards reducing minibatch dependence in batch-normalized models",
        "year": null
    },
    "Scalable, high-quality object detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/12/3",
            "설명": "Current high-quality object detection approaches use the scheme of salience-based object proposal methods followed by post-classification using deep convolutional features. This spurred recent research in improving object proposal methods. However, domain agnostic proposal generation has the principal drawback that the proposals come unranked or with very weak ranking, making it hard to trade-off quality for running time. This raises the more fundamental question of whether high-quality proposal generation requires careful engineering or can be derived just from data alone. We demonstrate that learning-based proposal methods can effectively match the performance of hand-engineered methods while allowing for very efficient runtime-quality trade-offs. Using the multi-scale convolutional MultiBox (MSC-MultiBox) approach, we substantially advance the state-of-the-art on the ILSVRC 2014 detection challenge data set, with  mAP for a single model and  mAP for an ensemble of two models. MSC-Multibox significantly improves the proposal quality over its predecessor MultiBox~method: AP increases from  to  for the ILSVRC detection challenge. Finally, we demonstrate improved bounding-box recall compared to Multiscale Combinatorial Grouping with less proposals on the Microsoft-COCO data set.",
            "저널": "arXiv preprint arXiv:1412.1441",
            "저자": "Christian Szegedy, Scott Reed, Dumitru Erhan, Dragomir Anguelov, Sergey Ioffe",
            "전체 인용횟수": "530회 인용20142015201620172018201920202021202220232345164919348494729",
            "학술 문서": "Scalable, high-quality object detectionC Szegedy, S Reed, D Erhan, D Anguelov, S Ioffe - arXiv preprint arXiv:1412.1441, 2014530회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Scalable, high-quality object detection",
        "year": null
    },
    "Deep convolutional ranking for multilabel image annotation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/12/17",
            "설명": "Multilabel image annotation is one of the most important challenges in computer vision with many real-world applications. While existing work usually use conventional visual features for multilabel annotation, features based on Deep Neural Networks have shown potential to significantly boost performance. In this work, we propose to leverage the advantage of such features and analyze key components that lead to better performances. Specifically, we show that a significant performance gain could be obtained by combining convolutional architectures with approximate top- ranking objectives, as thye naturally fit the multilabel tagging problem. Our experiments on the NUS-WIDE dataset outperforms the conventional visual features by about 10%, obtaining the best reported performance in the literature.",
            "저널": "arXiv preprint arXiv:1312.4894",
            "저자": "Yunchao Gong, Yangqing Jia, Thomas Leung, Alexander Toshev, Sergey Ioffe",
            "전체 인용횟수": "529회 인용20142015201620172018201920202021202220237264663848962664031",
            "학술 문서": "Deep convolutional ranking for multilabel image annotationY Gong, Y Jia, T Leung, A Toshev, S Ioffe - arXiv preprint arXiv:1312.4894, 2013529회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep convolutional ranking for multilabel image annotation",
        "year": null
    },
    "International conference on machine learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "저널": "International conference on machine learning",
            "저자": "Sergey Ioffe, Christian Szegedy",
            "전체 인용횟수": "423회 인용201720182019202020212022202332640498012796",
            "페이지": "448-456",
            "학술 문서": "International conference on machine learningS Ioffe, C Szegedy - International conference on machine learning, 2015405회 인용 관련 학술자료 Proc. 32nd Int. Conf. Machine Learning*S Ioffe, C Szegedy, F Bach, D Blei - 201522회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "International conference on machine learning",
        "year": null
    },
    "Proceedings of the IEEE conference on computer vision and pattern recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "저널": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "저자": "Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi",
            "전체 인용횟수": "269회 인용20172018201920202021202220231111337508572",
            "페이지": "779-788",
            "학술 문서": "Proceedings of the IEEE conference on computer vision and pattern recognitionJ Redmon, S Divvala, R Girshick, A Farhadi - Proceedings of the IEEE conference on computer …, 2016269회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Proceedings of the IEEE conference on computer vision and pattern recognition",
        "year": null
    },
    "Automatic face detection and identity masking in images, and applications thereof": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/1/17",
            "발명자": "Sergey Ioffe, Lance Williams, Dennis Strelow, Andrea Frome, Luc Vincent",
            "설명": "A method and system of identity masking to obscure identities corresponding to face regions in an image is disclosed. A face detector is applied to detect a set of possible face regions in the image. Then an identity masker is used to process the detected face regions by identity masking techniques in order to obscure identities corresponding to the regions. For example, a detected face region can be blurred as if it is in motion by a motion blur algorithm, such that the blurred region can not be recognized as the original identity. Or the detected face region can be replaced by a substitute facial image by a face replacement algorithm to obscure the corresponding identity.",
            "전체 인용횟수": "336회 인용201120122013201420152016201720182019202020212022202321739923204447637235",
            "출원번호": "12078464",
            "특허 번호": "8098904",
            "특허청": "US",
            "학술 문서": "Automatic face detection and identity masking in images, and applications thereofS Ioffe, L Williams, D Strelow, A Frome, L Vincent - US Patent 8,098,904, 2012336회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Automatic face detection and identity masking in images, and applications thereof",
        "year": null
    },
    "Probabilistic methods for finding people": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2001/6",
            "게시자": "Kluwer Academic Publishers",
            "권": "43",
            "설명": " Finding people in pictures presents a particularly difficult object recognition problem. We show how to find people by finding candidate body segments, and then constructing assemblies of segments that are consistent with the constraints on the appearance of a person that result from kinematic properties. Since a reasonable model of a person requires at least nine segments, it is not possible to inspect every group, due to the huge combinatorial complexity. We propose two approaches to this problem. In one, the search can be pruned by using projected versions of a classifier that accepts groups corresponding to people. We describe an efficient projection algorithm for one popular classifier, and demonstrate that our approach can be used to determine whether images of real scenes contain people. The second approach employs a probabilistic framework, so that we can draw samples of …",
            "저널": "International Journal of Computer Vision",
            "저자": "Sergey Ioffe, David A.  Forsyth",
            "전체 인용횟수": "325회 인용2001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202311713282922313324192126913910454421",
            "페이지": "45-68",
            "학술 문서": "Probabilistic methods for finding peopleS Ioffe, DA Forsyth - International Journal of Computer Vision, 2001325회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Probabilistic methods for finding people",
        "year": null
    },
    "Improved consistent sampling, weighted minhash and l1 sketching": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/12/13",
            "게시자": "IEEE",
            "설명": "We propose a new Consistent Weighted Sampling method, where the probability of drawing identical samples for a pair of inputs is equal to their Jaccard similarity. Our method takes deterministic constant time per non-zero weight, improving on the best previous approach which takes expected constant time. The samples can be used as Weighted Minhash for efficient retrieval and compression (sketching) under Jaccard or L1 metric. A method is presented for using simple data statistics to reduce the running time of hash computation by two orders of magnitude. We compare our method with the random projection method and show that it has better characteristics for retrieval under L1. We present a novel method of mapping hashes to short bit-strings, apply it to Weighted Minhash, and achieve more accurate distance estimates from sketches than existing methods, as long as the inputs are sufficiently distinct. We …",
            "저자": "Sergey Ioffe",
            "전체 인용횟수": "248회 인용20102011201220132014201520162017201820192020202120222023111410102226253025282631",
            "컨퍼런스": "2010 IEEE international conference on data mining",
            "페이지": "246-255",
            "학술 문서": "Improved consistent sampling, weighted minhash and l1 sketchingS Ioffe - 2010 IEEE international conference on data mining, 2010248회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Improved consistent sampling, weighted minhash and l1 sketching",
        "year": null
    },
    "Rethinking the inception architecture for computer vision. 2015": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "게시자": "CoRR",
            "저널": "arXiv preprint arXiv:1512.00567",
            "저자": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna",
            "전체 인용횟수": "205회 인용2016201720182019202020212022202346262729423829",
            "학술 문서": "Rethinking the inception architecture for computer vision. 2015C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna - arXiv preprint arXiv:1512.00567, 2015205회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Rethinking the inception architecture for computer vision. 2015",
        "year": null
    },
    "Rethinking the inception architecture for computer vision. arXiv 2015": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "권": "1512",
            "저널": "arXiv preprint arXiv:1512.00567",
            "저자": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna",
            "전체 인용횟수": "170회 인용20172018201920202021202220234121916384337",
            "학술 문서": "Rethinking the inception architecture for computer vision. arXiv 2015C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna - arXiv preprint arXiv:1512.00567, 2016170회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Rethinking the inception architecture for computer vision. arXiv 2015",
        "year": null
    },
    "Temporal differences-based policy iteration and applications in neuro-dynamic programming": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1996",
            "권": "14",
            "설명": "We introduce a new policy iteration method for dynamic programming problems with discounted and undiscounted cost. The method is based on the notion of temporal differences, and is primarily geared to the case of large and complex problems where the use of approximations is essential. We develop the theory of the method without approximation, we describe how to embed it within a neuro-dynamic programming/reinforcement learning context where feature-based approximation architectures are used, we relate it to TD (λ) methods, and we illustrate its use in the training of a tetris playing program.",
            "저널": "Lab. for Info. and Decision Systems Report LIDS-P-2349, MIT, Cambridge, MA",
            "저자": "Dimitri P Bertsekas, Sergey Ioffe",
            "전체 인용횟수": "155회 인용199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023211111259799109108567141363555",
            "학술 문서": "Temporal differences-based policy iteration and applications in neuro-dynamic programmingDP Bertsekas, S Ioffe - Lab. for Info. and Decision Systems Report LIDS-P …, 1996155회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Temporal differences-based policy iteration and applications in neuro-dynamic programming",
        "year": null
    },
    "Human tracking with mixtures of trees": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2001/7/7",
            "게시자": "IEEE",
            "권": "1",
            "설명": "Tree-structured probabilistic models admit simple, fast inference. However they are not well suited to phenonena such as occlusion, where multiple components of an object may disappear simultaneously. We address this problem with mixtures of trees, and demonstrate an efficient and compact representation of this mixture, which admits simple learning and inference algorithms. We use this method to build an automated tracker for Muybridge sequences of a variety of human activities. Tracking is difficult, because the temporal dependencies rule out simple inference methods. We show how to use our model for efficient inference, using a method that employs alternate spatial and temporal inference. The result is a cracker that (a) uses a very loose motion model, and so can track many different activities at a variable frame rate and (b) is entirely, automatic.",
            "저자": "Sergey Ioffe, David Forsyth",
            "전체 인용횟수": "153회 인용20012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023310159121213138859554637111",
            "컨퍼런스": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001",
            "페이지": "690-695",
            "학술 문서": "Human tracking with mixtures of treesS Ioffe, D Forsyth - Proceedings Eighth IEEE International Conference on …, 2001153회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Human tracking with mixtures of trees",
        "year": null
    },
    "Endpoint based video fingerprinting": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/12/17",
            "발명자": "Jay Yagnik, Henry A Rowley, Sergey Ioffe",
            "설명": "A method and system generates and compares fingerprints for videos in a video library. The video fingerprints provide a compact representation of the temporal locations of discontinuities in the video that can be used to quickly and efficiently identify video content. Discontinuities can be, for example, shot boundaries in the video frame sequence or silent points in the audio stream. Because the fingerprints are based on structural discontinuity characteristics rather than exact bit sequences, visual content of videos can be effectively compared even when there are small differences between the videos in compression factors, source resolutions, start and stop times, frame rates, and so on. Comparison of video fingerprints can be used, for example, to search for and remove copyright protected videos from a video library. Furthermore, duplicate videos can be detected and discarded in order to preserve storage space.",
            "전체 인용횟수": "141회 인용201520162017201820192020202120222023231622323216152",
            "출원번호": "11765292",
            "특허 번호": "8611422",
            "특허청": "US",
            "학술 문서": "Endpoint based video fingerprintingJ Yagnik, HA Rowley, S Ioffe - US Patent 8,611,422, 2013141회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Endpoint based video fingerprinting",
        "year": null
    },
    "Dssd: Deconvolutional single shot detector": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/1/23",
            "설명": "The main contribution of this paper is an approach for introducing additional context into state-of-the-art general object detection. To achieve this we first combine a state-of-the-art classifier (Residual-101[14]) with a fast detection framework (SSD[18]). We then augment SSD+Residual-101 with deconvolution layers to introduce additional large-scale context in object detection and improve accuracy, especially for small objects, calling our resulting system DSSD for deconvolutional single shot detector. While these two contributions are easily described at a high-level, a naive implementation does not succeed. Instead we show that carefully adding additional stages of learned transformations, specifically a module for feed-forward connections in deconvolution and a new output module, enables this new approach and forms a potential way forward for further detection research. Results are shown on both PASCAL VOC and COCO detection. Our DSSD with  input achieves 81.5% mAP on VOC2007 test, 80.0% mAP on VOC2012 test, and 33.2% mAP on COCO, outperforming a state-of-the-art method R-FCN[3] on each dataset.",
            "저널": "arXiv preprint arXiv:1701.06659",
            "저자": "Cheng-Yang Fu, Wei Liu, Ananth Ranga, Ambrish Tyagi, Alexander C Berg",
            "전체 인용횟수": "2439회 인용201720182019202020212022202328160405484511468362",
            "학술 문서": "Dssd: Deconvolutional single shot detectorCY Fu, W Liu, A Ranga, A Tyagi, AC Berg - arXiv preprint arXiv:1701.06659, 20172428회 인용 관련 학술자료 전체 5개의 버전 Dssd: Deconvolutional single shot detector. arXiv 2017CY Fu, W Liu, A Ranga, A Tyagi, AC Berg - arXiv preprint arXiv:1701.06659, 2017209회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Dssd: Deconvolutional single shot detector",
        "year": null
    },
    "Attribute and simile classifiers for face verification": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/9/29",
            "게시자": "IEEE",
            "설명": "We present two novel methods for face verification. Our first method - “attribute” classifiers - uses binary classifiers trained to recognize the presence or absence of describable aspects of visual appearance (e.g., gender, race, and age). Our second method - “simile” classifiers - removes the manual labeling required for attribute classification and instead learns the similarity of faces, or regions of faces, to specific reference people. Neither method requires costly, often brittle, alignment between image pairs; yet, both methods produce compact visual descriptions, and work on real-world images. Furthermore, both the attribute and simile classifiers improve on the current state-of-the-art for the LFW data set, reducing the error rates compared to the current best by 23.92% and 26.34%, respectively, and 31.68% when combined. For further testing across pose, illumination, and expression, we introduce a new data set …",
            "저자": "Neeraj Kumar, Alexander C Berg, Peter N Belhumeur, Shree K Nayar",
            "전체 인용횟수": "1908회 인용20102011201220132014201520162017201820192020202120222023428598143196195215201174122991009587",
            "컨퍼런스": "2009 IEEE 12th international conference on computer vision",
            "페이지": "365-372",
            "학술 문서": "Attribute and simile classifiers for face verificationN Kumar, AC Berg, PN Belhumeur, SK Nayar - 2009 IEEE 12th international conference on computer …, 20091908회 인용 관련 학술자료 전체 21개의 버전 Attribute and Simile Classifiers Attribute and Simile Classifiers for Face Verification*N Kumar, AC Berg, PN Belhumeur, SK Nayar - 2009"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Attribute and simile classifiers for face verification",
        "year": null
    },
    "SVM-KNN: Discriminative nearest neighbor classification for visual category recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006/6/17",
            "게시자": "IEEE",
            "권": "2",
            "설명": "We consider visual category recognition in the framework of measuring similarities, or equivalently perceptual distances, to prototype examples of categories. This approach is quite flexible, and permits recognition based on color, texture, and particularly shape, in a homogeneous framework. While nearest neighbor classifiers are natural in this setting, they suffer from the problem of high variance (in bias-variance decomposition) in the case of limited sampling. Alternatively, one could use support vector machines but they involve time-consuming optimization and computation of pairwise distances. We propose a hybrid of these two methods which deals naturally with the multiclass setting, has reasonable computational complexity both in training and at run time, and yields excellent results in practice. The basic idea is to find close neighbors to a query sample and train a local support vector machine that preserves …",
            "저자": "Hao Zhang, Alexander C Berg, Michael Maire, Jitendra Malik",
            "전체 인용횟수": "1673회 인용20062007200820092010201120122013201420152016201720182019202020212022202315547892120124127134129143122108957576486344",
            "컨퍼런스": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)",
            "페이지": "2126-2136",
            "학술 문서": "SVM-KNN: Discriminative nearest neighbor classification for visual category recognitionH Zhang, AC Berg, M Maire, J Malik - 2006 IEEE Computer Society Conference on Computer …, 20061673회 인용 관련 학술자료 전체 18개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "SVM-KNN: Discriminative nearest neighbor classification for visual category recognition",
        "year": null
    },
    "Babytalk: Understanding and generating simple image descriptions": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/5/31",
            "게시자": "IEEE",
            "권": "35",
            "설명": "We present a system to automatically generate natural language descriptions from images. This system consists of two parts. The first part, content planning, smooths the output of computer vision-based detection and recognition algorithms with statistics mined from large pools of visually descriptive text to determine the best content words to use to describe an image. The second step, surface realization, chooses words to construct natural language sentences based on the predicted content and general statistics from natural language. We present multiple approaches for the surface realization step and evaluate each using automatic measures of similarity to human generated reference descriptions. We also collect forced choice human evaluations between descriptions from the proposed generation system and descriptions from competing approaches. The proposed system is very effective at producing relevant …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Girish Kulkarni, Visruth Premraj, Vicente Ordonez, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C Berg, Tamara L Berg",
            "전체 인용횟수": "1533회 인용201220132014201520162017201820192020202120222023324883117136167152186137153157120",
            "페이지": "2891-2903",
            "학술 문서": "Babytalk: Understanding and generating simple image descriptionsG Kulkarni, V Premraj, V Ordonez, S Dhar, S Li, Y Choi… - IEEE transactions on pattern analysis and machine …, 20131533회 인용 관련 학술자료 전체 24개의 버전 ",
            "호": "12"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Babytalk: Understanding and generating simple image descriptions",
        "year": null
    },
    "Parsenet: Looking wider to see better": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/6/15",
            "설명": "We present a technique for adding global context to deep convolutional networks for semantic segmentation. The approach is simple, using the average feature for a layer to augment the features at each location. In addition, we study several idiosyncrasies of training, significantly increasing the performance of baseline networks (e.g. from FCN). When we add our proposed global feature, and a technique for learning normalization parameters, accuracy increases consistently even over our improved versions of the baselines. Our proposed approach, ParseNet, achieves state-of-the-art performance on SiftFlow and PASCAL-Context with small additional computational cost over baselines, and near current state-of-the-art performance on PASCAL VOC 2012 semantic segmentation with a simple approach. Code is available at https://github.com/weiliu89/caffe/tree/fcn .",
            "저널": "arXiv preprint arXiv:1506.04579",
            "저자": "Wei Liu, Andrew Rabinovich, Alexander C Berg",
            "전체 인용횟수": "1401회 인용201520162017201820192020202120222023440112153208241266211148",
            "학술 문서": "Parsenet: Looking wider to see betterW Liu, A Rabinovich, AC Berg - arXiv preprint arXiv:1506.04579, 20151401회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Parsenet: Looking wider to see better",
        "year": null
    },
    "Classification using intersection kernel support vector machines is efficient": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008/6/23",
            "게시자": "IEEE",
            "설명": "Straightforward classification using kernelized SVMs requires evaluating the kernel for a test vector and each of the support vectors. For a class of kernels we show that one can do this much more efficiently. In particular we show that one can build histogram intersection kernel SVMs (IKSVMs) with runtime complexity of the classifier logarithmic in the number of support vectors as opposed to linear for the standard approach. We further show that by precomputing auxiliary tables we can construct an approximate classifier with constant runtime and space requirements, independent of the number of support vectors, with negligible loss in classification accuracy on various tasks. This approximation also applies to 1 - chi 2  and other kernels of similar form. We also introduce novel features based on a multi-level histograms of oriented edge energy and present experiments on various detection datasets. On the INRIA …",
            "저자": "Subhransu Maji, Alexander C Berg, Jitendra Malik",
            "전체 인용횟수": "1359회 인용2008200920102011201220132014201520162017201820192020202120222023740869614715818114011888786237412914",
            "컨퍼런스": "2008 IEEE conference on computer vision and pattern recognition",
            "페이지": "1-8",
            "학술 문서": "Classification using intersection kernel support vector machines is efficientS Maji, AC Berg, J Malik - 2008 IEEE conference on computer vision and pattern …, 20081359회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Classification using intersection kernel support vector machines is efficient",
        "year": null
    },
    "Shape matching and object recognition using low distortion correspondences": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/6/20",
            "게시자": "IEEE",
            "권": "1",
            "설명": "We approach recognition in the framework of deformable shape matching, relying on a new algorithm for finding correspondences between feature points. This algorithm sets up correspondence as an integer quadratic programming problem, where the cost function has terms based on similarity of corresponding geometric blur point descriptors as well as the geometric distortion between pairs of corresponding feature points. The algorithm handles outliers, and thus enables matching of exemplars to query images in the presence of occlusion and clutter. Given the correspondences, we estimate an aligning transform, typically a regularized thin plate spline, resulting in a dense correspondence between the two shapes. Object recognition is then handled in a nearest neighbor framework where the distance between exemplar and query is the matching cost between corresponding points. We show results on two …",
            "저자": "Alexander C Berg, Tamara L Berg, Jitendra Malik",
            "전체 인용횟수": "1198회 인용2004200520062007200820092010201120122013201420152016201720182019202020212022202332462718379106767910071846047434232453632",
            "컨퍼런스": "2005 IEEE computer society conference on computer vision and pattern recognition (CVPR'05)",
            "페이지": "26-33",
            "학술 문서": "Shape matching and object recognition using low distortion correspondencesAC Berg, TL Berg, J Malik - 2005 IEEE computer society conference on computer …, 20051198회 인용 관련 학술자료 전체 34개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Shape matching and object recognition using low distortion correspondences",
        "year": null
    },
    "Matchnet: Unifying feature and metric learning for patch-based matching": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "Motivated by recent successes on learning feature representations and on learning feature comparison functions, we propose a unified approach to combining both for training a patch matching system. Our system, dubbed MatchNet, consists of a deep convolutional network that extracts features from patches and a network of three fully connected layers that computes a similarity between the extracted features. To ensure experimental repeatability, we train MatchNet on standard datasets and employ an input sampler to augment the training set with synthetic exemplar pairs that reduce overfitting. Once trained, we achieve better computational efficiency during matching by disassembling MatchNet and separately applying the feature computation and similarity networks in two sequential stages. We perform a comprehensive set of experiments on standard datasets to carefully study the contributions of each aspect of MatchNet, with direct comparisons to established methods. Our results confirm that our unified approach improves accuracy over previous state-of-the-art results on patch matching datasets, while reducing the storage requirement for descriptors. We make pre-trained MatchNet publicly available.",
            "저자": "Xufeng Han, Thomas Leung, Yangqing Jia, Rahul Sukthankar, Alexander C Berg",
            "전체 인용횟수": "1005회 인용201520162017201820192020202120222023137111514116812816310285",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "3279-3286",
            "학술 문서": "Matchnet: Unifying feature and metric learning for patch-based matchingX Han, T Leung, Y Jia, R Sukthankar, AC Berg - Proceedings of the IEEE conference on computer …, 20151005회 인용 관련 학술자료 전체 22개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Matchnet: Unifying feature and metric learning for patch-based matching",
        "year": null
    },
    "Modeling context in referring expressions": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "게시자": "Springer International Publishing",
            "설명": " Humans refer to objects in their environments all the time, especially in dialogue with other people. We explore generating and comprehending natural language referring expressions for objects in images. In particular, we focus on incorporating better measures of visual context into referring expression models and find that visual comparison to other objects within an image helps improve performance significantly. We also develop methods to tie the language generation process together, so that we generate expressions for all objects of a particular category jointly. Evaluation on three recent datasets - RefCOCO, RefCOCO+, and RefCOCOg (Datasets and toolbox can be downloaded from                      https://github.com/lichengunc/refer                                        ), shows the advantages of our methods for both referring expression generation and comprehension.",
            "저자": "Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, Tamara L Berg",
            "전체 인용횟수": "788회 인용201520162017201820192020202120222023232952678782183275",
            "컨퍼런스": "Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14",
            "페이지": "69-85",
            "학술 문서": "Modeling context in referring expressionsL Yu, P Poirson, S Yang, AC Berg, TL Berg - Computer Vision–ECCV 2016: 14th European …, 2016788회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Modeling context in referring expressions",
        "year": null
    },
    "What does classifying more than 10,000 image categories tell us?": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010",
            "게시자": "Springer Berlin Heidelberg",
            "설명": " Image classification is a critical task for both humans and computers. One of the challenges lies in the large scale of the semantic space. In particular, humans can recognize tens of thousands of object classes and scenes. No computer vision algorithm today has been tested at this scale. This paper presents a study of large scale categorization including a series of challenging experiments on classification with more than 10,000 image classes. We find that a) computational issues become crucial in algorithm design; b) conventional wisdom from a couple of hundred image categories on relative performance of different classifiers does not necessarily hold when the number of categories increases; c) there is a surprisingly strong relationship between the structure of WordNet (developed for studying language) and the difficulty of visual categorization; d) classification can be improved by exploiting the …",
            "저자": "Jia Deng, Alexander C Berg, Kai Li, Li Fei-Fei",
            "전체 인용횟수": "667회 인용2009201020112012201320142015201620172018201920202021202220233435588680635945513343423221",
            "컨퍼런스": "Computer Vision–ECCV 2010: 11th European Conference on Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part V 11",
            "페이지": "71-84",
            "학술 문서": "What does classifying more than 10,000 image categories tell us?J Deng, AC Berg, K Li, L Fei-Fei - Computer Vision–ECCV 2010: 11th European …, 2010667회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "What does classifying more than 10,000 image categories tell us?",
        "year": null
    },
    "Describable visual attributes for face verification and image search": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/3/10",
            "게시자": "IEEE",
            "권": "33",
            "설명": "We introduce the use of describable visual attributes for face verification and image search. Describable visual attributes are labels that can be given to an image to describe its appearance. This paper focuses on images of faces and the attributes used to describe them, although the concepts also apply to other domains. Examples of face attributes include gender, age, jaw shape, nose size, etc. The advantages of an attribute-based representation for vision tasks are manifold: They can be composed to create descriptions at various levels of specificity; they are generalizable, as they can be learned once and then applied to recognize new objects or categories without any further training; and they are efficient, possibly requiring exponentially fewer attributes (and training data) than explicitly naming each category. We show how one can create and label large data sets of real-world images to train classifiers which …",
            "저널": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "저자": "Neeraj Kumar, Alexander Berg, Peter N Belhumeur, Shree Nayar",
            "전체 인용횟수": "610회 인용20102011201220132014201520162017201820192020202120222023310274883606879605530262319",
            "페이지": "1962-1977",
            "학술 문서": "Describable visual attributes for face verification and image searchN Kumar, A Berg, PN Belhumeur, S Nayar - IEEE Transactions on Pattern Analysis and Machine …, 2011610회 인용 관련 학술자료 전체 21개의 버전 ",
            "호": "10"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Describable visual attributes for face verification and image search",
        "year": null
    },
    "Who are you with and where are you going?": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/6/20",
            "게시자": "IEEE",
            "설명": "We propose an agent-based behavioral model of pedestrians to improve tracking performance in realistic scenarios. In this model, we view pedestrians as decision-making agents who consider a plethora of personal, social, and environmental factors to decide where to go next. We formulate prediction of pedestrian behavior as an energy minimization on this model. Two of our main contributions are simple, yet effective estimates of pedestrian destination and social relationships (groups). Our final contribution is to incorporate these hidden properties into an energy formulation that results in accurate behavioral prediction. We evaluate both our estimates of destination and grouping, as well as our accuracy at prediction and tracking against state of the art behavioral model and show improvements, especially in the challenging observational situation of infrequent appearance observations-something that might occur …",
            "저자": "Kota Yamaguchi, Alexander C Berg, Luis E Ortiz, Tamara L Berg",
            "전체 인용횟수": "604회 인용20112012201320142015201620172018201920202021202220233163042374637586091665738",
            "컨퍼런스": "CVPR 2011",
            "페이지": "1345-1352",
            "학술 문서": "Who are you with and where are you going?K Yamaguchi, AC Berg, LE Ortiz, TL Berg - CVPR 2011, 2011604회 인용 관련 학술자료 전체 23개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Who are you with and where are you going?",
        "year": null
    },
    "Automatic attribute discovery and characterization from noisy web data": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010",
            "게시자": "Springer Berlin Heidelberg",
            "설명": " It is common to use domain specific terminology – attributes – to describe the visual appearance of objects. In order to scale the use of these describable visual attributes to a large number of categories, especially those not well studied by psychologists or linguists, it will be necessary to find alternative techniques for identifying attribute vocabularies and for learning to recognize attributes without hand labeled training data. We demonstrate that it is possible to accomplish both these tasks automatically by mining text and image data sampled from the Internet. The proposed approach also characterizes attributes according to their visual representation: global or local, and type: color, texture, or shape. This work focuses on discovering attributes and their visual appearance, and is as agnostic as possible about the textual description.",
            "저자": "Tamara L Berg, Alexander C Berg, Jonathan Shih",
            "전체 인용횟수": "558회 인용201120122013201420152016201720182019202020212022202320346172797355372221261820",
            "컨퍼런스": "Computer Vision–ECCV 2010: 11th European Conference on Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part I 11",
            "페이지": "663-676",
            "학술 문서": "Automatic attribute discovery and characterization from noisy web dataTL Berg, AC Berg, J Shih - Computer Vision–ECCV 2010: 11th European …, 2010558회 인용 관련 학술자료 전체 22개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Automatic attribute discovery and characterization from noisy web data",
        "year": null
    },
    "Midge: Generating image descriptions from computer vision detections": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/4",
            "설명": "This paper introduces a novel generation system that composes humanlike descriptions of images from computer vision detections. By leveraging syntactically informed word co-occurrence statistics, the generator filters and constrains the noisy detections output from a vision system to generate syntactic trees that detail what the computer vision system sees. Results show that the generation system outperforms state-of-the-art systems, automatically generating some of the most natural image descriptions to date.",
            "저자": "Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Yamaguchi, Karl Stratos, Xufeng Han, Alyssa Mensch, Alexander Berg, Tamara Berg, Hal Daumé III",
            "전체 인용횟수": "542회 인용20122013201420152016201720182019202020212022202351633544351646744485453",
            "컨퍼런스": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics",
            "페이지": "747-756",
            "학술 문서": "Midge: Generating image descriptions from computer vision detectionsM Mitchell, J Dodge, A Goyal, K Yamaguchi, K Stratos… - Proceedings of the 13th Conference of the European …, 2012542회 인용 관련 학술자료 전체 28개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Midge: Generating image descriptions from computer vision detections",
        "year": null
    },
    "Names and faces in the news": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004/6/27",
            "게시자": "IEEE",
            "권": "2",
            "설명": "We show quite good face clustering is possible for a dataset of inaccurately and ambiguously labelled face images. Our dataset is 44,773 face images, obtained by applying a face finder to approximately half a million captioned news images. This dataset is more realistic than usual face recognition datasets, because it contains faces captured \"in the wild\" in a variety of configurations with respect to the camera, taking a variety of expressions, and under illumination of widely varying color. Each face image is associated with a set of names, automatically extracted from the associated caption. Many, but not all such sets contain the correct name. We cluster face images in appropriate discriminant coordinates. We use a clustering procedure to break ambiguities in labelling and identify incorrectly labelled faces. A merging procedure then identifies variants of names that refer to the same individual. The resulting …",
            "저자": "Tamara L Berg, Alexander C Berg, Jaety Edwards, Michael Maire, Ryan White, Yee-Whye Teh, Erik Learned-Miller, David A Forsyth",
            "전체 인용횟수": "542회 인용20032004200520062007200820092010201120122013201420152016201720182019202020212022202337132935423833393435374941251719910117",
            "컨퍼런스": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.",
            "페이지": "II-II",
            "학술 문서": "Names and faces in the newsTL Berg, AC Berg, J Edwards, M Maire, R White… - Proceedings of the 2004 IEEE Computer Society …, 2004533회 인용 관련 학술자료 전체 47개의 버전 Faces and names in the news*T Miller, AC Berg, J Edwards, M Maire, R White… - Proceedings of the IEEE Conference on Computer …, 20049회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Names and faces in the news",
        "year": null
    },
    "Where to buy it: Matching street clothing photos in online shops": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "In this paper, we define a new task, Exact Street to Shop, where our goal is to match a real-world example of a garment item to the same item in an online shop. This is an extremely challenging task due to visual differences between street photos (pictures of people wearing clothing in everyday uncontrolled settings) and online shop photos (pictures of clothing items on people, mannequins, or in isolation, captured by professionals in more controlled settings). We collect a new dataset for this application containing 404,683 shop photos collected from 25 different online retailers and 20,357 street photos, providing a total of 39,479 clothing item matches between street and shop photos. We develop three different methods for Exact Street to Shop retrieval, including two deep learning baseline methods, and a method to learn a similarity measure between the street and shop domains. Experiments demonstrate that our learned similarity significantly outperforms our baselines that use existing deep learning based representations.",
            "저자": "M Hadi Kiapour, Xufeng Han, Svetlana Lazebnik, Alexander C Berg, Tamara L Berg",
            "전체 인용횟수": "518회 인용201620172018201920202021202220232375799774715435",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "3343-3351",
            "학술 문서": "Where to buy it: Matching street clothing photos in online shopsM Hadi Kiapour, X Han, S Lazebnik, AC Berg, TL Berg - Proceedings of the IEEE international conference on …, 2015518회 인용 관련 학술자료 전체 22개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Where to buy it: Matching street clothing photos in online shops",
        "year": null
    },
    "A taxonomy and evaluation of dense two-frame stereo correspondence algorithms": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2002/4",
            "게시자": "Kluwer Academic Publishers",
            "권": "47",
            "설명": " Stereo matching is one of the most active research areas in computer vision. While a large number of algorithms for stereo correspondence have been developed, relatively little work has been done on characterizing their performance. In this paper, we present a taxonomy of dense, two-frame stereo methods. Our taxonomy is designed to assess the different components and design decisions made in individual stereo algorithms. Using this taxonomy, we compare existing stereo methods and present experiments evaluating the performance of many different variants. In order to establish a common software platform and a collection of data sets for easy evaluation, we have designed a stand-alone, flexible C++ implementation that enables the evaluation of individual components and that can easily be extended to include new algorithms. We have also produced several new multi-frame stereo data sets with …",
            "저널": "International journal of computer vision",
            "저자": "Daniel Scharstein, Richard Szeliski",
            "전체 인용횟수": "10139회 인용200220032004200520062007200820092010201120122013201420152016201720182019202020212022202346104202267332380456533630651781670672658608583503454415438361265",
            "페이지": "7-42",
            "학술 문서": "A taxonomy and evaluation of dense two-frame stereo correspondence algorithmsD Scharstein, R Szeliski - International journal of computer vision, 200210139회 인용 관련 학술자료 전체 35개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A taxonomy and evaluation of dense two-frame stereo correspondence algorithms",
        "year": null
    },
    "Computer vision: algorithms and applications": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2022/1/3",
            "게시자": "Springer Nature",
            "설명": "Humans perceive the three-dimensional structure of the world with apparent ease. However, despite all of the recent advances in computer vision research, the dream of having a computer interpret an image at the same level as a two-year old remains elusive. Why is computer vision such a challenging problem and what is the current state of the art? Computer Vision: Algorithms and Applications explores the variety of techniques commonly used to analyze and interpret images. It also describes challenging real-world applications where vision is being successfully used, both for specialized applications such as medical imaging, and for fun, consumer-level tasks such as image editing and stitching, which students can apply to their own personal photos and videos. More than just a source of “recipes,” this exceptionally authoritative and comprehensive textbook/reference also takes a scientific approach to basic vision problems, formulating physical models of the imaging process before inverting them to produce descriptions of a scene. These problems are also analyzed using statistical models and solved using rigorous engineering techniques Topics and features: Structured to support active curricula and project-oriented courses, with tips in the Introduction for using the book in a variety of customized courses Presents exercises at the end of each chapter with a heavy emphasis on testing algorithms and containing numerous suggestions for small mid-term projects Provides additional material and more detailed mathematical topics in the Appendices, which cover linear algebra, numerical techniques, and Bayesian estimation theory Suggests …",
            "저자": "Richard Szeliski",
            "전체 인용횟수": "8775회 인용2010201120122013201420152016201720182019202020212022202337197353464572661724772788747862928836688",
            "학술 문서": "Computer vision: algorithms and applicationsR Szeliski - 20228731회 인용 관련 학술자료 전체 29개의 버전 Computer vision. Texts in computer science*R Szeliski - Texts in Computer Science, 201152회 인용 관련 학술자료 Computer vision: applications and algorithms*R Szeliski - 20116회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Computer vision: algorithms and applications",
        "year": null
    },
    "Photo tourism: exploring photo collections in 3D": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006/7/1",
            "도서": "ACM siggraph 2006 papers",
            "설명": "We present a system for interactively browsing and exploring large unstructured collections of photographs of a scene using a novel 3D interface. Our system consists of an image-based modeling front end that automatically computes the viewpoint of each photograph as well as a sparse 3D model of the scene and image to model correspondences. Our photo explorer uses image-based rendering techniques to smoothly transition between photographs, while also enabling full 3D navigation and exploration of the set of images and world geometry, along with auxiliary information such as overhead maps. Our system also makes it easy to construct photo tours of scenic or historic locations, and to annotate image details, which are automatically transferred to other relevant images. We demonstrate our system on several large personal photo collections as well as images gathered from Internet photo sharing sites.",
            "저자": "Noah Snavely, Steven M Seitz, Richard Szeliski",
            "전체 인용횟수": "4504회 인용2006200720082009201020112012201320142015201620172018201920202021202220231583137185232301359372367403358314286241221197179153",
            "페이지": "835-846",
            "학술 문서": "Photo tourism: exploring photo collections in 3DN Snavely, SM Seitz, R Szeliski - ACM siggraph 2006 papers, 20064504회 인용 관련 학술자료 전체 44개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Photo tourism: exploring photo collections in 3D",
        "year": null
    },
    "The lumigraph": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2023/8/1",
            "도서": "Seminal Graphics Papers: Pushing the Boundaries, Volume 2",
            "설명": "This paper discusses a new method for capturing the complete appearance of bothsynthetic and real world objects and scenes,representing this information, and then using this representation to render images of the object from new camera positions. Unlike the shape capture process traditionally used in computer vision and the rendering process traditionally used in computer graphics, our approach does not rely on geometric representations. Instead we sample and reconstruct a 4D function. which we call a Lumigraph. The Lumigraph is a subset o f the complete plenoptic fu nction that describes the flow of light at all positions in all directions. With the Lumigraph. new images of the object can be generated very quick]y,independentof the geometric or illumination complexity of the scene or object. The paper discusses a complete working system including the capture of sainples. the construction of the Lumigraph …",
            "저자": "Steven J Gortler, Radek Grzeszczuk, Richard Szeliski, Michael F Cohen",
            "전체 인용횟수": "3883회 인용1997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220236310014614417119921421921421715515112210595101105103114118130116137123110157149",
            "페이지": "453-464",
            "학술 문서": "The lumigraphSJ Gortler, R Grzeszczuk, R Szeliski, MF Cohen - Seminal Graphics Papers: Pushing the Boundaries …, 20233883회 인용 관련 학술자료 전체 22개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The lumigraph",
        "year": null
    },
    "A comparison and evaluation of multi-view stereo reconstruction algorithms": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006/6/17",
            "게시자": "IEEE",
            "권": "1",
            "설명": "This paper presents a quantitative comparison of several multi-view stereo reconstruction algorithms. Until now, the lack of suitable calibrated multi-view image datasets with known ground truth (3D shape models) has prevented such direct comparisons. In this paper, we first survey multi-view stereo algorithms and compare them qualitatively using a taxonomy that differentiates their key properties. We then describe our process for acquiring and calibrating multiview image datasets with high-accuracy ground truth and introduce our evaluation methodology. Finally, we present the results of our quantitative comparison of state-of-the-art multi-view stereo reconstruction algorithms on six benchmark datasets. The datasets, evaluation details, and instructions for submitting new models are available online at http://vision.middlebury.edu/mview.",
            "저자": "Steven M Seitz, Brian Curless, James Diebel, Daniel Scharstein, Richard Szeliski",
            "전체 인용횟수": "3206회 인용200620072008200920102011201220132014201520162017201820192020202120222023227288149174212220220243218216235196181172193163164",
            "컨퍼런스": "2006 IEEE computer society conference on computer vision and pattern recognition (CVPR'06)",
            "페이지": "519-528",
            "학술 문서": "A comparison and evaluation of multi-view stereo reconstruction algorithmsSM Seitz, B Curless, J Diebel, D Scharstein, R Szeliski - 2006 IEEE computer society conference on computer …, 20063206회 인용 관련 학술자료 전체 20개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A comparison and evaluation of multi-view stereo reconstruction algorithms",
        "year": null
    },
    "A database and evaluation methodology for optical flow": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/3",
            "게시자": "Springer US",
            "권": "92",
            "설명": " The quantitative evaluation of optical flow algorithms by Barron et al. (1994) led to significant advances in performance. The challenges for optical flow algorithms today go beyond the datasets and evaluation methods proposed in that paper. Instead, they center on problems associated with complex natural scenes, including nonrigid motion, real sensor noise, and motion discontinuities. We propose a new set of benchmarks and evaluation methods for the next generation of optical flow algorithms. To that end, we contribute four types of data to test different aspects of optical flow algorithms: (1) sequences with nonrigid motion where the ground-truth flow is determined by tracking hidden fluorescent texture, (2) realistic synthetic sequences, (3) high frame-rate video used to study interpolation error, and (4) modified stereo sequences of static scenes. In addition to the average angular error used by Barron et al …",
            "저널": "International journal of computer vision",
            "저자": "Simon Baker, Daniel Scharstein, JP Lewis, Stefan Roth, Michael J Black, Richard Szeliski",
            "전체 인용횟수": "3189회 인용20102011201220132014201520162017201820192020202120222023167176187222240262269248212222210205216171",
            "페이지": "1-31",
            "학술 문서": "A database and evaluation methodology for optical flowS Baker, D Scharstein, JP Lewis, S Roth, MJ Black… - International journal of computer vision, 20113189회 인용 관련 학술자료 전체 56개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A database and evaluation methodology for optical flow",
        "year": null
    },
    "Modeling the world from internet photo collections": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008/11",
            "게시자": "Springer US",
            "권": "80",
            "설명": "  There are billions of photographs on the Internet, comprising the largest and most diverse photo collection ever assembled. How can computer vision researchers exploit this imagery? This paper explores this question from the standpoint of 3D scene modeling and visualization. We present structure-from-motion and image-based rendering algorithms that operate on hundreds of images downloaded as a result of keyword-based image search queries like “Notre Dame” or “Trevi Fountain.” This approach, which we call Photo Tourism, has enabled reconstructions of numerous well-known world sites. This paper presents these algorithms and results as a first step towards 3D modeling of the world’s well-photographed sites, cities, and landscapes from Internet imagery, and discusses key open problems and challenges for the research community. ",
            "저널": "International journal of computer vision",
            "저자": "Noah Snavely, Steven M Seitz, Richard Szeliski",
            "전체 인용횟수": "2803회 인용20082009201020112012201320142015201620172018201920202021202220231163109138170191211267235263249207194183144126",
            "페이지": "189-210",
            "학술 문서": "Modeling the world from internet photo collectionsN Snavely, SM Seitz, R Szeliski - International journal of computer vision, 20082803회 인용 관련 학술자료 전체 31개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Modeling the world from internet photo collections",
        "year": null
    },
    "Image alignment and stitching: A tutorial": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2007/1/1",
            "게시자": "Now Publishers, Inc.",
            "권": "2",
            "설명": "This tutorial reviews image alignment and image stitching algorithms. Image alignment algorithms can discover the correspondence relationships among images with varying degrees of overlap. They are ideally suited for applications such as video stabilization, summarization, and the creation of panoramic mosaics. Image stitching algorithms take the alignment estimates produced by such registration algorithms and blend the images in a seamless manner, taking care to deal with potential problems such as blurring or ghosting caused by parallax and scene movement as well as varying image exposures. This tutorial reviews the basic motion models underlying alignment and stitching algorithms, describes effective direct (pixel-based) and feature-based alignment algorithms, and describes blending algorithms used to produce seamless mosaics. It ends with a discussion of open research problems in the area.",
            "저자": "Richard Szeliski",
            "전체 인용횟수": "2650회 인용2006200720082009201020112012201320142015201620172018201920202021202220232849879616116416719719818021518817819115314711495",
            "출처": "Foundations and Trends® in Computer Graphics and Vision",
            "페이지": "1-104",
            "학술 문서": "Image alignment and stitching: A tutorialR Szeliski - Foundations and Trends® in Computer Graphics and …, 20072610회 인용 관련 학술자료 전체 41개의 버전 Image alignment and stitching*R Szeliski, R Szeliski - Computer Vision: Algorithms and Applications, 202264회 인용 관련 학술자료 전체 11개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Image alignment and stitching: A tutorial",
        "year": null
    },
    "Building rome in a day": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/10/1",
            "게시자": "ACM",
            "권": "54",
            "설명": "We present a system that can reconstruct 3D geometry from large, unorganized collections of photographs such as those found by searching for a given city (e.g., Rome) on Internet photo-sharing sites. Our system is built on a set of new, distributed computer vision algorithms for image matching and 3D reconstruction, designed to maximize parallelism at each stage of the pipeline and to scale gracefully with both the size of the problem and the amount of available computation. Our experimental results demonstrate that it is now possible to reconstruct city-scale image collections with more than a hundred thousand images in less than a day.",
            "저널": "Communications of the ACM",
            "저자": "Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven M Seitz, Richard Szeliski",
            "전체 인용횟수": "2639회 인용2010201120122013201420152016201720182019202020212022202390126153195212252258232221180166173151168",
            "페이지": "105-112",
            "학술 문서": "Building rome in a dayS Agarwal, Y Furukawa, N Snavely, I Simon, B Curless… - Communications of the ACM, 20112639회 인용 관련 학술자료 전체 24개의 버전 ",
            "호": "10"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Building rome in a day",
        "year": null
    },
    "High-accuracy stereo depth maps using structured light": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2003/6/18",
            "게시자": "IEEE",
            "권": "1",
            "설명": "Progress in stereo algorithm performance is quickly outpacing the ability of existing stereo data sets to discriminate among the best-performing algorithms, motivating the need for more challenging scenes with accurate ground truth information. This paper describes a method for acquiring high-complexity stereo image pairs with pixel-accurate correspondence information using structured light. Unlike traditional range-sensing approaches, our method does not require the calibration of the light sources and yields registered disparity maps between all pairs of cameras and illumination projectors. We present new stereo data sets acquired with our method and demonstrate their suitability for stereo algorithm evaluation. Our results are available at http://www.middlebury.edu/stereo/.",
            "저자": "Daniel Scharstein, Richard Szeliski",
            "전체 인용횟수": "2290회 인용200420052006200720082009201020112012201320142015201620172018201920202021202220231761517476969114016414215715716015014813113512110875",
            "컨퍼런스": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings.",
            "페이지": "I-I",
            "학술 문서": "High-accuracy stereo depth maps using structured lightD Scharstein, R Szeliski - 2003 IEEE Computer Society Conference on Computer …, 20032290회 인용 관련 학술자료 전체 22개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "High-accuracy stereo depth maps using structured light",
        "year": null
    },
    "High-quality video view interpolation using a layered representation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004/8/1",
            "게시자": "ACM",
            "권": "23",
            "설명": "The ability to interactively control viewpoint while watching a video is an exciting application of image-based rendering. The goal of our work is to render dynamic scenes with interactive viewpoint control using a relatively small number of video cameras. In this paper, we show how high-quality video-based rendering of dynamic scenes can be accomplished using multiple synchronized video streams combined with novel image-based modeling and rendering algorithms. Once these video streams have been processed, we can synthesize any intermediate view between cameras at any time, with the potential for space-time manipulation.In our approach, we first use a novel color segmentation-based stereo algorithm to generate high-quality photoconsistent correspondences across all camera views. Mattes for areas near depth discontinuities are then automatically extracted to reduce artifacts during view synthesis …",
            "저널": "ACM transactions on graphics (TOG)",
            "저자": "C Lawrence Zitnick, Sing Bing Kang, Matthew Uyttendaele, Simon Winder, Richard Szeliski",
            "전체 인용횟수": "1945회 인용2004200520062007200820092010201120122013201420152016201720182019202020212022202312529712110115916215214615411210875101627358705543",
            "페이지": "600-608",
            "학술 문서": "High-quality video view interpolation using a layered representationCL Zitnick, SB Kang, M Uyttendaele, S Winder… - ACM transactions on graphics (TOG), 20041945회 인용 관련 학술자료 전체 14개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "High-quality video view interpolation using a layered representation",
        "year": null
    },
    "Layered depth images": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1998/7/24",
            "도서": "Proceedings of the 25th annual conference on Computer graphics and interactive techniques",
            "설명": "In this paper we present a set of efficient image based rendering methods capable of rendering multiple frames per second on a PC. The first method warps Sprites with Depth representing smooth surfaces without the gaps found in other techniques. A second method for more general scenes performs warping from an intermediate representation called a Layered Depth Image (LDI). An LDI is a view of the scene from a single input camera view, but with multiple pixels along each line of sight. The size of the representation grows only linearly with the observed depth complexity in the scene. Moreover, because the LDI data are represented in a single image coordinate system, McMillan’s warp ordering algorithm can be successfully adapted. As a result, pixels are drawn in the output image in backto-front order. No z-buffer is required, so alpha-compositing can be done efficiently without depth sorting. This makes …",
            "저자": "Jonathan Shade, Steven Gortler, Li-wei He, Richard Szeliski",
            "전체 인용횟수": "1814회 인용1998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202313527170951261111031109791808664647456433742374747565452",
            "페이지": "231-242",
            "학술 문서": "Layered depth imagesJ Shade, S Gortler, L He, R Szeliski - Proceedings of the 25th annual conference on …, 19981814회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Layered depth images",
        "year": null
    },
    "Edge-preserving decompositions for multi-scale tone and detail manipulation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008/8/1",
            "게시자": "ACM",
            "권": "27",
            "설명": "Many recent computational photography techniques decompose an image into a piecewise smooth base layer, containing large scale variations in intensity, and a residual detail layer capturing the smaller scale details in the image. In many of these applications, it is important to control the spatial scale of the extracted details, and it is often desirable to manipulate details at multiple scales, while avoiding visual artifacts. In this paper we introduce a new way to construct edge-preserving multi-scale image decompositions. We show that current basedetail decomposition techniques, based on the bilateral filter, are limited in their ability to extract detail at arbitrary scales. Instead, we advocate the use of an alternative edge-preserving smoothing operator, based on the weighted least squares optimization framework, which is particularly well suited for progressive coarsening of images and for multi-scale detail extraction …",
            "저널": "ACM transactions on graphics (TOG)",
            "저자": "Zeev Farbman, Raanan Fattal, Dani Lischinski, Richard Szeliski",
            "전체 인용횟수": "1761회 인용20092010201120122013201420152016201720182019202020212022202321433762103113174180176165155135143113121",
            "페이지": "1-10",
            "학술 문서": "Edge-preserving decompositions for multi-scale tone and detail manipulationZ Farbman, R Fattal, D Lischinski, R Szeliski - ACM transactions on graphics (TOG), 20081761회 인용 관련 학술자료 전체 22개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Edge-preserving decompositions for multi-scale tone and detail manipulation",
        "year": null
    },
    "Fast surface interpolation using hierarchical basis functions": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1990/6",
            "게시자": "IEEE",
            "권": "12",
            "설명": "An alternative to multigrid relaxation that is much easier to implement and more generally applicable is presented. Conjugate gradient descent is used in conjunction with a hierarchical (multiresolution) set of basis functions. The resultant algorithm uses a pyramid to smooth the residual vector before the direction is computed. Simulation results showing the speed of convergence and its dependence on the choice of interpolator, the number of smoothing levels, and other factors are presented. The relationship of this approach to other multiresolution relaxation and representation schemes is also discussed.< >",
            "저널": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "저자": "Richard Szeliski",
            "전체 인용횟수": "1503회 인용1989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202336415247568965899011189114995719232021212434333348443224124122231",
            "페이지": "513-528",
            "학술 문서": "Fast surface interpolation using hierarchical basis functionsR Szeliski - IEEE Transactions on Pattern Analysis and Machine …, 19901503회 인용 관련 학술자료 전체 10개의 버전 ",
            "호": "6"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Fast surface interpolation using hierarchical basis functions",
        "year": null
    },
    "Digital photography with flash and no-flash image pairs": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004/8/1",
            "게시자": "ACM",
            "권": "23",
            "설명": "Digital photography has made it possible to quickly and easily take a pair of images of low-light environments: one with flash to capture detail and one without flash to capture ambient illumination. We present a variety of applications that analyze and combine the strengths of such flash/no-flash image pairs. Our applications include denoising and detail transfer (to merge the ambient qualities of the no-flash image with the high-frequency flash detail), white-balancing (to change the color tone of the ambient image), continuous flash (to interactively adjust flash intensity), and red-eye removal (to repair artifacts in the flash image). We demonstrate how these applications can synthesize new images that are of higher quality than either of the originals.",
            "저널": "ACM transactions on graphics (TOG)",
            "저자": "Georg Petschnigg, Richard Szeliski, Maneesh Agrawala, Michael Cohen, Hugues Hoppe, Kentaro Toyama",
            "전체 인용횟수": "1491회 인용2005200620072008200920102011201220132014201520162017201820192020202120222023920265053797588971141441181198910486766748",
            "페이지": "664-672",
            "학술 문서": "Digital photography with flash and no-flash image pairsG Petschnigg, R Szeliski, M Agrawala, M Cohen… - ACM transactions on graphics (TOG), 20041491회 인용 관련 학술자료 전체 33개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Digital photography with flash and no-flash image pairs",
        "year": null
    },
    "Creating full view panoramic image mosaics and environment maps": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2023/8/1",
            "도서": "Seminal Graphics Papers: Pushing the Boundaries, Volume 2",
            "설명": "This paper presents a novel approach to creating full viewpanoramic mosaics from image sequences. Unlike current panoramic stitching methods, which usually require pure horizontal camera panning, our system does not require any controlled motions or constraints on how the images are taken (as long as there is no strong motion parallax). For example, images taken from a hand-held digital camera can be stitched seamlessly into panoramic mosaics. Because we represent our image mosaics using a set of transforms, there are no singularity problems such as those existing at the top and bottom of cylindrical or spherical maps. Our algorithm is fast and robust because it directly recovers 3D rotations instead of general 8 parameter planar perspective transforms. Methods to recover camera focal length are also presented. We also present an algorithm for efficiently extracting environment maps from our image …",
            "저자": "Richard Szeliski, Heung-Yeung Shum",
            "전체 인용횟수": "1446회 인용199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220233243627753779069776961626850586772435860435017241913",
            "페이지": "653-660",
            "학술 문서": "Creating full view panoramic image mosaics and environment mapsR Szeliski, HY Shum - Seminal Graphics Papers: Pushing the Boundaries …, 20231446회 인용 관련 학술자료 전체 28개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Creating full view panoramic image mosaics and environment maps",
        "year": null
    },
    "Video mosaics for virtual environments": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1996/3",
            "게시자": "IEEE",
            "권": "16",
            "설명": "As computer-based video becomes ubiquitous with the expansion of transmission, storage, and manipulation capabilities, it will offer a rich source of imagery for computer graphics applications. This article looks at one way to use video as a new source of high-resolution, photorealistic imagery for these applications. If you walked through an environment, such as a building interior, and filmed a video sequence of what you saw you could subsequently register and composite the video images together into large mosaics of the scene. In this way, you can achieve an essentially unlimited resolution. Furthermore, since you can acquire the images using any optical technology, you can reconstruct any scene regardless of its range or scale. Video mosaics can be used in many different applications, including the creation of virtual reality environments, computer-game settings, and movie special effects. I present …",
            "저널": "IEEE computer Graphics and Applications",
            "저자": "Richard Szeliski",
            "전체 인용횟수": "1435회 인용1996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023627547779736478858188866960575866565730433022151911105",
            "페이지": "22-30",
            "학술 문서": "Video mosaics for virtual environmentsR Szeliski - IEEE computer Graphics and Applications, 19961435회 인용 관련 학술자료 전체 21개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Video mosaics for virtual environments",
        "year": null
    },
    "Locally adapted hierarchical basis preconditioning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006/7/1",
            "도서": "ACM SIGGRAPH 2006 Papers",
            "설명": "This paper develops locally adapted hierarchical basis functions for effectively preconditioning large optimization problems that arise in computer graphics applications such as tone mapping, gradient-domain blending, colorization, and scattered data interpolation. By looking at the local structure of the coefficient matrix and performing a recursive set of variable eliminations, combined with a simplification of the resulting coarse level problems, we obtain bases better suited for problems with inhomogeneous (spatially varying) data, smoothness, and boundary constraints. Our approach removes the need to heuristically adjust the optimal number of preconditioning levels, significantly outperforms previously proposed approaches, and also maps cleanly onto data-parallel architectures such as modern GPUs.",
            "저자": "Richard Szeliski",
            "전체 인용횟수": "1368회 인용19881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202318343533354269548076968010486471412714172932434357483926148437744",
            "페이지": "1135-1143",
            "학술 문서": "Locally adapted hierarchical basis preconditioningR Szeliski - ACM SIGGRAPH 2006 Papers, 20061368회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Locally adapted hierarchical basis preconditioning",
        "year": null
    },
    "A comparative study of energy minimization methods for markov random fields with smoothness-based priors": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008/4/18",
            "게시자": "IEEE",
            "권": "30",
            "설명": "Among the most exciting advances in early vision has been the development of efficient energy minimization algorithms for pixel-labeling tasks such as depth or texture computation. It has been known for decades that such problems can be elegantly expressed as Markov random fields, yet the resulting energy minimization problems have been widely viewed as intractable. Algorithms such as graph cuts and loopy belief propagation (LBP) have proven to be very powerful: For example, such methods form the basis for almost all the top-performing stereo methods. However, the trade-offs among different energy minimization algorithms are still not well understood. In this paper, we describe a set of energy minimization benchmarks and use them to compare the solution quality and runtime of several common energy minimization algorithms. We investigate three promising methods-graph cuts, LBP, and tree …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Richard Szeliski, Ramin Zabih, Daniel Scharstein, Olga Veksler, Vladimir Kolmogorov, Aseem Agarwala, Marshall Tappen, Carsten Rother",
            "전체 인용횟수": "1339회 인용2008200920102011201220132014201520162017201820192020202120222023317810312113113413813010979694553313422",
            "페이지": "1068-1080",
            "학술 문서": "A comparative study of energy minimization methods for markov random fields with smoothness-based priorsR Szeliski, R Zabih, D Scharstein, O Veksler… - IEEE transactions on pattern analysis and machine …, 20081339회 인용 관련 학술자료 전체 14개의 버전 ",
            "호": "6"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A comparative study of energy minimization methods for markov random fields with smoothness-based priors",
        "year": null
    },
    "Synthesizing realistic facial expressions from photographs": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006/7/30",
            "도서": "Acm siggraph 2006 courses",
            "설명": "We present new techniques for creating photorealistic textured 3D facial models from photographs of a human subject, and for creating smooth transitions between different facial expressions by morphing between these different models. Starting from several uncalibrated views of a human subject, we employ a user-assisted technique to recover the camera poses corresponding to the views as well as the 3D coordinates of a sparse set of chosen locations on the subject's face. A scattered data interpolation technique is then used to deform a generic face mesh to fit the particular geometry of the subject's face. Having recovered the camera poses and the facial geometry, we extract from the input images one or more texture maps for the model. This process is repeated for several facial expressions of a particular subject. To generate transitions between these facial expressions we use 3D shape morphing between …",
            "저자": "Frédéric Pighin, Jamie Hecker, Dani Lischinski, Richard Szeliski, David H Salesin",
            "전체 인용횟수": "1320회 인용199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220235304772728594100122868865665948384137302225172012910",
            "페이지": "19-es",
            "학술 문서": "Synthesizing realistic facial expressions from photographsF Pighin, J Hecker, D Lischinski, R Szeliski, DH Salesin - Acm siggraph 2006 courses, 20061320회 인용 관련 학술자료 전체 31개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Synthesizing realistic facial expressions from photographs",
        "year": null
    },
    "Densely Connected Convolutional Networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections--one between each layer and its subsequent layer--our network has L (L+ 1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github. com/liuzhuang13/DenseNet.",
            "저자": "Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger",
            "전체 인용횟수": "40361회 인용2017201820192020202120222023387225344846547843295698343",
            "출처": "IEEE Conference on Computer Vision and Pattern Recognition",
            "학술 문서": "Densely connected convolutional networksG Huang, Z Liu, L Van Der Maaten, KQ Weinberger - Proceedings of the IEEE conference on computer …, 201740071회 인용 관련 학술자료 전체 37개의 버전 Densely connected convolutional networks. arXiv 2016*G Huang, Z Liu, L van der Maaten, KQ Weinberger - arXiv preprint arXiv:1608.06993, 2018263회 인용 관련 학술자료 Densely connected convolutional networks (2016)*G Huang, Z Liu, L van der Maaten, KQ Weinberger - arXiv preprint arXiv:1608.06993, 2016173회 인용 관련 학술자료 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*G Huang, Z Liu, L Van Der Maaten, KQ Weinberger - 2017105회 인용 관련 학술자료 van, and Weinberger*G Huang, Z Liu, L Maaten - Kilian Q,“Densely Connected Convolutional Networks,” …, 201686회 인용 관련 학술자료 Van Der, and Weinberger KQ. 2017*G Huang, Z Liu, L Maaten - … connected convolutional networks. In Proceedings of …, 201716회 인용 관련 학술자료 Densely Connected Convolutional Networks, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*G Huang, Z Liu, L Van Der Maaten, KQ Weinberger - 20179회 인용 관련 학술자료 vol 2017, Densely connected convolutional networks*G Huang, Z Liu, L Maaten, K Weinberger - 2017 IEEE conference on computer vision and pattern …, 20177회 인용 관련 학술자료 Dense Connected Convolutional Networks [C]//CVPR*G Huang, Z Liu, LVD Maaten - IEEE Computer Society, 20177회 인용 관련 학술자료 Densely connected convolutional networks. Paper presented at the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*G Huang, Z Liu, LVD Maaten, KQ Weinberger - 20175회 인용 관련 학술자료 van der & W*G Huang, Z Liu, L Maaten - KQ Densely connected convolutional networks. arXiv, 20184회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Densely Connected Convolutional Networks",
        "year": null
    },
    "Visualizing data using t-SNE": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008/11",
            "권": "9",
            "설명": "We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.",
            "저널": "The Journal of Machine Learning Research",
            "저자": "Laurens van der Maaten, Geoffrey Hinton",
            "전체 인용횟수": "38612회 인용20122013201420152016201720182019202020212022202311814928055410201663280840575088677377897845",
            "페이지": "85",
            "학술 문서": "Visualizing data using t-SNE.L Van der Maaten, G Hinton - Journal of machine learning research, 200838329회 인용 관련 학술자료 전체 45개의 버전 Visualizing data using t-SNE*G Hinton, L van der Maaten - J. Mach. Learn. Res., 2008409회 인용 관련 학술자료 Van Der; Hinton, G. 2008.“Visualizing data using t-SNE.”*L Maaten - Journal of machine learning research41회 인용 관련 학술자료 van der & Hinton, GE (2008)*L Maaten - J. Mach. Learn. Res18회 인용 관련 학술자료 t-SNE*L Van Der Maaten - Recuperado de https://lvdmaaten. github. io/tsne, 201916회 인용 관련 학술자료 Visualizing high-dimensional data using t-629 sne*L Van Der Maaten, G Hinton - Journal of Machine Learning Research, 20088회 인용 관련 학술자료 Journal of Machine Learning Research*L Maaten, G Hinton - 20083회 인용 관련 학술자료 ",
            "호": "2579-2605"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Visualizing data using t-SNE",
        "year": null
    },
    "Dimensionality reduction: A comparative review": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009",
            "설명": "In recent years, a variety of nonlinear dimensionality reduction techniques have been proposed that aim to address the limitations of traditional techniques such as PCA. The paper presents a review and systematic comparison of these techniques. The performances of the nonlinear techniques are investigated on artificial and natural tasks. The results of the experiments reveal that nonlinear techniques perform well on selected artificial tasks, but do not outperform the traditional PCA on real-world tasks. The paper explains these results by identifying weaknesses of current nonlinear techniques, and suggests how the performance of nonlinear dimensionality reduction techniques may be improved.",
            "저널": "Technical Report TiCC TR 2009-005",
            "저자": "LJP Van der Maaten, EO Postma, HJ Van den Herik",
            "전체 인용횟수": "3636회 인용2007200820092010201120122013201420152016201720182019202020212022202314283983117119163209228292264307342349414371259",
            "학술 문서": "Dimensionality reduction: A comparative reviewL Van Der Maaten, EO Postma, HJ van den Herik - Journal of Machine Learning Research, 20092444회 인용 관련 학술자료 전체 7개의 버전 Dimensionality reduction: a comparative*L Van Der Maaten, E Postma, J Van den Herik - J Mach Learn Res, 20091339회 인용 관련 학술자료 전체 14개의 버전 Vak*E Maanen - Informatie-Maandblad voor de Informatievoorziening, 200442회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Dimensionality reduction: A comparative review",
        "year": null
    },
    "Accelerating t-SNE using Tree-Based Algorithms": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/1/1",
            "게시자": "JMLR. org",
            "권": "15",
            "설명": "The paper investigates the acceleration of t-SNE—an embedding technique that is commonly used for the visualization of high-dimensional data in scatter plots—using two treebased algorithms. In particular, the paper develops variants of the Barnes-Hut algorithm and of the dual-tree algorithm that approximate the gradient used for learning t-SNE embeddings in O (N log N). Our experiments show that the resulting algorithms substantially accelerate t-SNE, and that they make it possible to learn embeddings of data sets with millions of objects. Somewhat counterintuitively, the Barnes-Hut variant of t-SNE appears to outperform the dual-tree variant.",
            "저널": "The Journal of Machine Learning Research",
            "저자": "Laurens Van Der Maaten",
            "전체 인용횟수": "2735회 인용20152016201720182019202020212022202336106183328457443406425321",
            "페이지": "3221-3245",
            "학술 문서": "Accelerating t-SNE using tree-based algorithmsL Van Der Maaten - The journal of machine learning research, 20142735회 인용 관련 학술자료 전체 11개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Accelerating t-SNE using Tree-Based Algorithms",
        "year": null
    },
    "Countering adversarial images using input transformations": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/10/31",
            "설명": "This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods",
            "저널": "arXiv preprint arXiv:1711.00117",
            "저자": "Chuan Guo, Mayank Rana, Moustapha Cisse, Laurens Van Der Maaten",
            "전체 인용횟수": "1356회 인용2017201820192020202120222023689174258278309241",
            "학술 문서": "Countering adversarial images using input transformationsC Guo, M Rana, M Cisse, L Van Der Maaten - arXiv preprint arXiv:1711.00117, 20171356회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Countering adversarial images using input transformations",
        "year": null
    },
    "Self-supervised learning of pretext-invariant representations": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020",
            "설명": "The goal of self-supervised learning from images is to construct image representations that are semantically meaningful via pretext tasks that do not require semantic annotations. Many pretext tasks lead to representations that are covariant with image transformations. We argue that, instead, semantic representations ought to be invariant under such transformations. Specifically, we develop Pretext-Invariant Representation Learning (PIRL, pronounced aspearl') that learns invariant representations based on pretext tasks. We use PIRL with a commonly used pretext task that involves solving jigsaw puzzles. We find that PIRL substantially improves the semantic quality of the learned image representations. Our approach sets a new state-of-the-art in self-supervised learning from images on several popular benchmarks for self-supervised learning. Despite being unsupervised, PIRL outperforms supervised pre-training in learning image representations for object detection. Altogether, our results demonstrate the potential of self-supervised representations with good invariance properties.",
            "저자": "Ishan Misra, Laurens van der Maaten",
            "전체 인용횟수": "1317회 인용2020202120222023128325461395",
            "컨퍼런스": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "페이지": "6707-6717",
            "학술 문서": "Self-supervised learning of pretext-invariant representationsI Misra, L Maaten - Proceedings of the IEEE/CVF conference on computer …, 20201317회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Self-supervised learning of pretext-invariant representations",
        "year": null
    },
    "3d semantic segmentation with submanifold sparse convolutional networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "Convolutional networks are the de-facto standard for analyzing spatio-temporal data such as images, videos, and 3D shapes. Whilst some of this data is naturally dense (eg, photos), many other data sources are inherently sparse. Examples include 3D point clouds that were obtained using a LiDAR scanner or RGB-D camera. Standard``dense''implementations of convolutional networks are very inefficient when applied on such sparse data. We introduce new sparse convolutional operations that are designed to process spatially-sparse data more efficiently, and use them to develop spatially-sparse convolutional networks. We demonstrate the strong performance of the resulting models, called submanifold sparse convolutional networks (SSCNs), on two tasks involving semantic segmentation of 3D point clouds. In particular, our models outperform all prior state-of-the-art on the test set of a recent semantic segmentation competition.",
            "저자": "Benjamin Graham, Martin Engelcke, Laurens Van Der Maaten",
            "전체 인용횟수": "1295회 인용2018201920202021202220231369155285374392",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "9224-9232",
            "학술 문서": "3d semantic segmentation with submanifold sparse convolutional networksB Graham, M Engelcke, L Van Der Maaten - Proceedings of the IEEE conference on computer …, 20181295회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "3d semantic segmentation with submanifold sparse convolutional networks",
        "year": null
    },
    "Feature denoising for improving adversarial robustness": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "Adversarial attacks to image classification systems present challenges to convolutional networks and opportunities for understanding them. This study suggests that adversarial perturbations on images lead to noise in the features constructed by these networks. Motivated by this observation, we develop new network architectures that increase adversarial robustness by performing feature denoising. Specifically, our networks contain blocks that denoise the features using non-local means or other filters; the entire networks are trained end-to-end. When combined with adversarial training, our feature denoising networks substantially improve the state-of-the-art in adversarial robustness in both white-box and black-box attack settings. On ImageNet, under 10-iteration PGD white-box attacks where prior art has 27.9% accuracy, our method achieves 55.7%; even under extreme 2000-iteration PGD white-box attacks, our method secures 42.6% accuracy. Our method was ranked first in Competition on Adversarial Attacks and Defenses (CAAD) 2018---it achieved 50.6% classification accuracy on a secret, ImageNet-like test dataset against 48 unknown attackers, surpassing the runner-up approach by 10%. Code is available at https://github. com/facebookresearch/ImageNet-Adversarial-Training.",
            "저자": "Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan L Yuille, Kaiming He",
            "전체 인용횟수": "882회 인용201820192020202120222023473196213199193",
            "컨퍼런스": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "페이지": "501-509",
            "학술 문서": "Feature denoising for improving adversarial robustnessC Xie, Y Wu, L Maaten, AL Yuille, K He - Proceedings of the IEEE/CVF conference on computer …, 2019882회 인용 관련 학술자료 전체 12개의 버전 Feature-level Denoising Improves Adversarial Robustness*C Xie, Y Wu, L van der Maaten, A Yuille, K He관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Feature denoising for improving adversarial robustness",
        "year": null
    },
    "Condensenet: An efficient densenet using learned group convolutions": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "Deep neural networks are increasingly used on mobile devices, where computational resources are limited. In this paper we develop CondenseNet, a novel network architecture with unprecedented efficiency. It combines dense connectivity with a novel module called learned group convolution. The dense connectivity facilitates feature re-use in the network, whereas learned group convolutions remove connections between layers for which this feature re-use is superfluous. At test time, our model can be implemented using standard group convolutions, allowing for efficient computation in practice. Our experiments show that CondenseNets are far more efficient than state-of-the-art compact convolutional networks such as MobileNets and ShuffleNets.",
            "저자": "Gao Huang, Shichen Liu, Laurens Van der Maaten, Kilian Q Weinberger",
            "전체 인용횟수": "878회 인용20182019202020212022202346133177196179142",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "2752-2761",
            "학술 문서": "Condensenet: An efficient densenet using learned group convolutionsG Huang, S Liu, L Van der Maaten, KQ Weinberger - Proceedings of the IEEE conference on computer …, 2018878회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Condensenet: An efficient densenet using learned group convolutions",
        "year": null
    },
    "Multi-scale dense networks for resource efficient image classification": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/3/29",
            "설명": "In this paper we investigate image classification with computational resource limits at test time. Two such settings are: 1. anytime classification, where the network's prediction for a test example is progressively updated, facilitating the output of a prediction at any time; and 2. budgeted batch classification, where a fixed amount of computation is available to classify a set of examples that can be spent unevenly across \"easier\" and \"harder\" inputs. In contrast to most prior work, such as the popular Viola and Jones algorithm, our approach is based on convolutional neural networks. We train multiple classifiers with varying resource demands, which we adaptively apply during test time. To maximally re-use computation between the classifiers, we incorporate them as early-exits into a single deep convolutional neural network and inter-connect them with dense connectivity. To facilitate high quality classification early on, we use a two-dimensional multi-scale network architecture that maintains coarse and fine level features all-throughout the network. Experiments on three image-classification tasks demonstrate that our framework substantially improves the existing state-of-the-art in both settings.",
            "저널": "arXiv preprint arXiv:1703.09844",
            "저자": "Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens Van Der Maaten, Kilian Q Weinberger",
            "전체 인용횟수": "670회 인용201820192020202120222023246989151162171",
            "학술 문서": "Multi-scale dense networks for resource efficient image classificationG Huang, D Chen, T Li, F Wu, L Van Der Maaten… - arXiv preprint arXiv:1703.09844, 2017670회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Multi-scale dense networks for resource efficient image classification",
        "year": null
    },
    "Learning a parametric embedding by preserving local structure": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009",
            "설명": "The paper presents a new unsupervised dimensionality reduction technique, called parametric t-SNE, that learns a parametric mapping between the high-dimensional data space and the low-dimensional latent space. Parametric t-SNE learns the parametric mapping in such a way that the local structure of the data is preserved as well as possible in the latent space. We evaluate the performance of parametric t-SNE in experiments on two datasets, in which we compare it to the performance of two other unsupervised parametric dimensionality reduction techniques. The results of experiments illustrate the strong performance of parametric t-SNE, in particular, in learning settings in which the dimensionality of the latent space is relatively low.",
            "저널": "Proceedings of AI-STATS",
            "저자": "Laurens van der Maaten",
            "전체 인용횟수": "670회 인용200920102011201220132014201520162017201820192020202120222023291121148233342688793978969",
            "학술 문서": "Learning a parametric embedding by preserving local structureL Van Der Maaten - Artificial intelligence and statistics, 2009670회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning a parametric embedding by preserving local structure",
        "year": null
    },
    "Rtsne: T-distributed stochastic neighbor embedding using Barnes-Hut implementation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/1/10",
            "저널": "R package version 0.13, URL https://github. com/jkrijthe/Rtsne",
            "저자": "Jesse H Krijthe, Laurens Van der Maaten",
            "전체 인용횟수": "472회 인용20152016201720182019202020212022202321018456590888271",
            "학술 문서": "Rtsne: T-distributed stochastic neighbor embedding using Barnes-Hut implementationJH Krijthe, L Van der Maaten - R package version 0.13, URL https://github. com/jkrijthe …, 2015427회 인용 관련 학술자료 Rtsne: T-distributed stochastic neighbor embedding using Barnes-Hut implementation. R package version 0.13*JH Krijthe, L Van der Maaten - Computer Software, 201555회 인용 관련 학술자료 Package ‘Rtsne’*J Krijthe, L van der Maaten, MJ Krijthe - R package version 0.13, 201820회 인용 관련 학술자료 전체 33개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Rtsne: T-distributed stochastic neighbor embedding using Barnes-Hut implementation",
        "year": null
    },
    "Submanifold sparse convolutional networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/6/5",
            "설명": "Convolutional network are the de-facto standard for analysing spatio-temporal data such as images, videos, 3D shapes, etc. Whilst some of this data is naturally dense (for instance, photos), many other data sources are inherently sparse. Examples include pen-strokes forming on a piece of paper, or (colored) 3D point clouds that were obtained using a LiDAR scanner or RGB-D camera. Standard \"dense\" implementations of convolutional networks are very inefficient when applied on such sparse data. We introduce a sparse convolutional operation tailored to processing sparse data that differs from prior work on sparse convolutional networks in that it operates strictly on submanifolds, rather than \"dilating\" the observation with every layer in the network. Our empirical analysis of the resulting submanifold sparse convolutional networks shows that they perform on par with state-of-the-art methods whilst requiring substantially less computation.",
            "저널": "arXiv preprint arXiv:1706.01307",
            "저자": "Benjamin Graham, Laurens Van der Maaten",
            "전체 인용횟수": "420회 인용20182019202020212022202321275886116110",
            "학술 문서": "Submanifold sparse convolutional networksB Graham, L Van der Maaten - arXiv preprint arXiv:1706.01307, 2017420회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Submanifold sparse convolutional networks",
        "year": null
    },
    "Learning Visual Features from Large Weakly Supervised Data": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": " Convolutional networks trained on large supervised datasets produce visual features which form the basis for the state-of-the-art in many computer-vision problems. Further improvements of these visual features will likely require even larger manually labeled data sets, which severely limits the pace at which progress can be made. In this paper, we explore the potential of leveraging massive, weakly-labeled image collections for learning good visual features. We train convolutional networks on a dataset of 100 million Flickr photos and comments, and show that these networks produce features that perform well in a range of vision problems. We also show that the networks appropriately capture word similarity and learn correspondences between different languages.",
            "저자": "Armand Joulin, Laurens van der Maaten, Allan Jabri, Nicolas Vasilache",
            "전체 인용횟수": "386회 인용201620172018201920202021202220231737374056626466",
            "컨퍼런스": "European Conference on Computer Vision",
            "학술 문서": "Learning visual features from large weakly supervised dataA Joulin, L Van Der Maaten, A Jabri, N Vasilache - Computer Vision–ECCV 2016: 14th European …, 2016386회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning Visual Features from Large Weakly Supervised Data",
        "year": null
    },
    "Convolutional networks with dense connectivity": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/5/23",
            "게시자": "IEEE",
            "권": "44",
            "설명": "Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with    layers have    connections—one between each layer and its subsequent layer—our network has    direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, encourage feature reuse and substantially improve parameter efficiency. We evaluate our proposed …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Gao Huang, Zhuang Liu, Geoff Pleiss, Laurens Van Der Maaten, Kilian Q Weinberger",
            "전체 인용횟수": "381회 인용2019202020212022202324698109125",
            "페이지": "8704-8716",
            "학술 문서": "Convolutional networks with dense connectivityG Huang, Z Liu, G Pleiss, L Van Der Maaten… - IEEE transactions on pattern analysis and machine …, 2019381회 인용 관련 학술자료 전체 14개의 버전 ",
            "호": "12"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Convolutional networks with dense connectivity",
        "year": null
    },
    "Revisiting visual question answering baselines": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/9/17",
            "게시자": "Springer International Publishing",
            "도서": "European conference on computer vision",
            "설명": " Visual question answering (VQA) is an interesting learning setting for evaluating the abilities and shortcomings of current systems for image understanding. Many of the recently proposed VQA systems include attention or memory mechanisms designed to perform “reasoning”. Furthermore, for the task of multiple-choice VQA, nearly all of these systems train a multi-class classifier on image and question features to predict an answer. This paper questions the value of these common practices and develops a simple alternative model based on binary classification. Instead of treating answers as competing choices, our model receives the answer as input and predicts whether or not an image-question-answer triplet is correct. We evaluate our model on the Visual7W Telling and the VQA Real Multiple Choice tasks, and find that even simple versions of our model perform competitively. Our best model achieves …",
            "저자": "Allan Jabri, Armand Joulin, Laurens Van Der Maaten",
            "전체 인용횟수": "305회 인용2015201620172018201920202021202220232541556147372823",
            "페이지": "727-739",
            "학술 문서": "Revisiting visual question answering baselinesA Jabri, A Joulin, L Van Der Maaten - European conference on computer vision, 2016305회 인용 관련 학술자료 전체 5개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Revisiting visual question answering baselines",
        "year": null
    },
    "Visualizing non-metric similarities in multiple maps": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012",
            "게시자": "Springer Netherlands",
            "설명": " Techniques for multidimensional scaling visualize objects as points in a low-dimensional metric map. As a result, the visualizations are subject to the fundamental limitations of metric spaces. These limitations prevent multidimensional scaling from faithfully representing non-metric similarity data such as word associations or event co-occurrences. In particular, multidimensional scaling cannot faithfully represent intransitive pairwise similarities in a visualization, and it cannot faithfully visualize “central” objects. In this paper, we present an extension of a recently proposed multidimensional scaling technique called t-SNE. The extension aims to address the problems of traditional multidimensional scaling techniques when these techniques are used to visualize non-metric similarities. The new technique, called multiple maps t-SNE, alleviates these problems by constructing a collection of maps that reveal …",
            "저널": "Machine Learning",
            "저자": "Laurens van der Maaten, Geoffrey Hinton",
            "전체 인용횟수": "300회 인용201220132014201520162017201820192020202120222023347171720264835493339",
            "페이지": "1-23",
            "학술 문서": "Visualizing non-metric similarities in multiple mapsL Van der Maaten, G Hinton - Machine learning, 2012300회 인용 관련 학술자료 전체 26개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Visualizing non-metric similarities in multiple maps",
        "year": null
    },
    "Nas-fpn: Learning scalable feature pyramid architecture for object detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "Current state-of-the-art convolutional architectures for object detection are manually designed. Here we aim to learn a better architecture of feature pyramid network for object detection. We adopt Neural Architecture Search and discover a new feature pyramid architecture in a novel scalable search space covering all cross-scale connections. The discovered architecture, named NAS-FPN, consists of a combination of top-down and bottom-up connections to fuse features across scales. NAS-FPN, combined with various backbone models in the RetinaNet framework, achieves better accuracy and latency tradeoff compared to state-of-the-art object detection models. NAS-FPN improves mobile detection accuracy by 2 AP compared to state-of-the-art SSDLite with MobileNetV2 model in [32] and achieves 48.3 AP which surpasses Mask R-CNN [10] detection accuracy with less computation time.",
            "저자": "Golnaz Ghiasi, Tsung-Yi Lin, Quoc V Le",
            "전체 인용횟수": "1454회 인용2019202020212022202333195424405384",
            "컨퍼런스": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "페이지": "7036-7045",
            "학술 문서": "Nas-fpn: Learning scalable feature pyramid architecture for object detectionG Ghiasi, TY Lin, QV Le - Proceedings of the IEEE/CVF conference on computer …, 20191454회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Nas-fpn: Learning scalable feature pyramid architecture for object detection",
        "year": null
    },
    "Learning to refine object segments": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "게시자": "Springer International Publishing",
            "설명": " Object segmentation requires both object-level information and low-level pixel data. This presents a challenge for feedforward networks: lower layers in convolutional nets capture rich spatial information, while upper layers encode object-level knowledge but are invariant to factors such as pose and appearance. In this work we propose to augment feedforward nets for object segmentation with a novel top-down refinement approach. The resulting bottom-up/top-down architecture is capable of efficiently generating high-fidelity object masks. Similarly to skip connections, our approach leverages features at all layers of the net. Unlike skip connections, our approach does not attempt to output independent predictions at each layer. Instead, we first output a coarse ‘mask encoding’ in a feedforward pass, then refine this mask encoding in a top-down pass utilizing features at successively lower layers. The …",
            "저자": "Pedro O Pinheiro, Tsung-Yi Lin, Ronan Collobert, Piotr Dollár",
            "전체 인용횟수": "1053회 인용20152016201720182019202020212022202361913016518317215412972",
            "컨퍼런스": "Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part I 14",
            "페이지": "75-91",
            "학술 문서": "Learning to refine object segmentsPO Pinheiro, TY Lin, R Collobert, P Dollár - Computer Vision–ECCV 2016: 14th European …, 20161053회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning to refine object segments",
        "year": null
    },
    "Dropblock: A regularization method for convolutional networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "권": "31",
            "설명": "Deep neural networks often work well when they are over-parameterized and trained with a massive amount of noise and regularization, such as weight decay and dropout. Although dropout is widely used as a regularization technique for fully connected layers, it is often less effective for convolutional layers. This lack of success of dropout for convolutional layers is perhaps due to the fact that activation units in convolutional layers are spatially correlated so information can still flow through convolutional networks despite dropout. Thus a structured form of dropout is needed to regularize convolutional networks. In this paper, we introduce DropBlock, a form of structured dropout, where units in a contiguous region of a feature map are dropped together. We found that applying DropbBlock in skip connections in addition to the convolution layers increases the accuracy. Also, gradually increasing number of dropped units during training leads to better accuracy and more robust to hyperparameter choices. Extensive experiments show that DropBlock works better than dropout in regularizing convolutional networks. On ImageNet classification, ResNet-50 architecture with DropBlock achieves  accuracy, which is more than  improvement on the baseline. On COCO detection, DropBlock improves Average Precision of RetinaNet from  to .",
            "저널": "Advances in neural information processing systems",
            "저자": "Golnaz Ghiasi, Tsung-Yi Lin, Quoc V Le",
            "전체 인용횟수": "1010회 인용2019202020212022202380157280264218",
            "학술 문서": "Dropblock: A regularization method for convolutional networksG Ghiasi, TY Lin, QV Le - Advances in neural information processing systems, 20181010회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Dropblock: A regularization method for convolutional networks",
        "year": null
    },
    "Bottleneck transformers for visual recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021",
            "설명": "We present BoTNet, a conceptually simple yet powerful backbone architecture that incorporates self-attention for multiple computer vision tasks including image classification, object detection and instance segmentation. By just replacing the spatial convolutions with global self-attention in the final three bottleneck blocks of a ResNet and no other changes, our approach improves upon the baselines significantly on instance segmentation and object detection while also reducing the parameters, with minimal overhead in latency. Through the design of BoTNet, we also point out how ResNet bottleneck blocks with self-attention can be viewed as Transformer blocks. Without any bells and whistles, BoTNet achieves 44.4% Mask AP and 49.7% Box AP on the COCO Instance Segmentation benchmark using the Mask R-CNN framework; surpassing the previous best published single model and single scale results of ResNeSt evaluated on the COCO validation set. Finally, we present a simple adaptation of the BoTNet design for image classification, resulting in models that achieve a strong performance of 84.7% top-1 accuracy on the ImageNet benchmark while being up to 1.64 x faster in compute time than the popular EfficientNet models on TPU-v3 hardware. We hope our simple and effective approach will serve as a strong baseline for future research in self-attention models for vision.",
            "저자": "Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, Ashish Vaswani",
            "전체 인용횟수": "845회 인용202120222023134341367",
            "컨퍼런스": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "페이지": "16519-16529",
            "학술 문서": "Bottleneck transformers for visual recognitionA Srinivas, TY Lin, N Parmar, J Shlens, P Abbeel… - Proceedings of the IEEE/CVF conference on computer …, 2021845회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Bottleneck transformers for visual recognition",
        "year": null
    },
    "Simple copy-paste is a strong data augmentation method for instance segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021",
            "설명": "Building instance segmentation models that are data-efficient and can handle rare object categories is an important challenge in computer vision. Leveraging data augmentations is a promising direction towards addressing this challenge. Here, we perform a systematic study of the Copy-Paste augmentation (eg,[13, 12]) for instance segmentation where we randomly paste objects onto an image. Prior studies on Copy-Paste relied on modeling the surrounding visual context for pasting the objects. However, we find that the simple mechanism of pasting objects randomly is good enough and can provide solid gains on top of strong baselines. Furthermore, we show Copy-Paste is additive with semi-supervised methods that leverage extra data through pseudo labeling (eg. self-training). On COCO instance segmentation, we achieve 49.1 mask AP and 57.3 box AP, an improvement of+ 0.6 mask AP and+ 1.5 box AP over the previous state-of-the-art. We further demonstrate that Copy-Paste can lead to significant improvements on the LVIS benchmark. Our baseline model outperforms the LVIS 2020 Challenge winning entry by+ 3.6 mask AP on rare categories.",
            "저자": "Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V Le, Barret Zoph",
            "전체 인용횟수": "729회 인용202120222023101275350",
            "컨퍼런스": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "페이지": "2918-2928",
            "학술 문서": "Simple copy-paste is a strong data augmentation method for instance segmentationG Ghiasi, Y Cui, A Srinivas, R Qian, TY Lin, ED Cubuk… - Proceedings of the IEEE/CVF conference on computer …, 2021729회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Simple copy-paste is a strong data augmentation method for instance segmentation",
        "year": null
    },
    "Rethinking pre-training and self-training": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020",
            "권": "33",
            "설명": "Pre-training is a dominant paradigm in computer vision. For example, supervised ImageNet pre-training is commonly used to initialize the backbones of object detection and segmentation models. He et al., however, show a striking result that ImageNet pre-training has limited impact on COCO object detection. Here we investigate self-training as another method to utilize additional data on the same setup and contrast it against ImageNet pre-training. Our study reveals the generality and flexibility of self-training with three additional insights: 1) stronger data augmentation and more labeled data further diminish the value of pre-training, 2) unlike pre-training, self-training is always helpful when using stronger data augmentation, in both low-data and high-data regimes, and 3) in the case that pre-training is helpful, self-training improves upon pre-training. For example, on the COCO object detection dataset, pre-training benefits when we use one fifth of the labeled data, and hurts accuracy when we use all labeled data. Self-training, on the other hand, shows positive improvements from+ 1.3 to+ 3.4 AP across all dataset sizes. In other words, self-training works well exactly on the same setup that pre-training does not work (using ImageNet to help COCO). On the PASCAL segmentation dataset, which is a much smaller dataset than COCO, though pre-training does help significantly, self-training improves upon the pre-trained model. On COCO object detection, we achieve 53.8 AP, an improvement of+ 1.7 AP over the strongest SpineNet model. On PASCAL segmentation, we achieve 90.5 mIOU, an improvement of+ 1.5 mIOU over the previous state-of-the …",
            "저널": "Advances in neural information processing systems",
            "저자": "Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin Dogus Cubuk, Quoc Le",
            "전체 인용횟수": "563회 인용202020212022202327147209179",
            "페이지": "3833-3845",
            "학술 문서": "Rethinking pre-training and self-trainingB Zoph, G Ghiasi, TY Lin, Y Cui, H Liu, ED Cubuk, Q Le - Advances in neural information processing systems, 2020563회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Rethinking pre-training and self-training",
        "year": null
    },
    "Learning data augmentation strategies for object detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020",
            "게시자": "Springer International Publishing",
            "설명": " Much research on object detection focuses on building better model architectures and detection algorithms. Changing the model architecture, however, comes at the cost of adding more complexity to inference, making models slower. Data augmentation, on the other hand, doesn’t add any inference complexity, but is insufficiently studied in object detection for two reasons. First it is more difficult to design plausible augmentation strategies for object detection than for classification, because one must handle the complexity of bounding boxes if geometric transformations are applied. Secondly, data augmentation attracts less research attention perhaps because it is believed to add less value and to transfer poorly compared to advances in network architectures. This paper serves two main purposes. First, we propose to use AutoAugment [3] to design better data augmentation strategies for object …",
            "저자": "Barret Zoph, Ekin D Cubuk, Golnaz Ghiasi, Tsung-Yi Lin, Jonathon Shlens, Quoc V Le",
            "전체 인용횟수": "531회 인용201920202021202220231887132150143",
            "컨퍼런스": "Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVII 16",
            "페이지": "566-583",
            "학술 문서": "Learning data augmentation strategies for object detectionB Zoph, ED Cubuk, G Ghiasi, TY Lin, J Shlens, QV Le - Computer Vision–ECCV 2020: 16th European …, 2020531회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning data augmentation strategies for object detection",
        "year": null
    },
    "Collaborative metric learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/4/3",
            "도서": "Proceedings of the 26th international conference on world wide web",
            "설명": "Metric learning algorithms produce distance metrics that capture the important relationships among data. In this work, we study the connection between metric learning and collaborative filtering. We propose Collaborative Metric Learning (CML) which learns a joint metric space to encode not only users' preferences but also the user-user and item-item similarity. The proposed algorithm outperforms state-of-the-art collaborative filtering algorithms on a wide range of recommendation tasks and uncovers the underlying spectrum of users' fine-grained preferences. CML also achieves significant speedup for Top-K recommendation tasks using off-the-shelf, approximate nearest-neighbor search, with negligible accuracy reduction.",
            "저자": "Cheng-Kang Hsieh, Longqi Yang, Yin Cui, Tsung-Yi Lin, Serge Belongie, Deborah Estrin",
            "전체 인용횟수": "518회 인용2017201820192020202120222023944879010010975",
            "페이지": "193-201",
            "학술 문서": "Collaborative metric learningCK Hsieh, L Yang, Y Cui, TY Lin, S Belongie, D Estrin - Proceedings of the 26th international conference on …, 2017518회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Collaborative metric learning",
        "year": null
    },
    "Open-vocabulary Object Detection via Vision and Language Knowledge Distillation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021/4/28",
            "설명": "We aim at advancing open-vocabulary object detection, which detects objects described by arbitrary text inputs. The fundamental challenge is the availability of training data. It is costly to further scale up the number of classes contained in existing object detection datasets. To overcome this challenge, we propose ViLD, a training method via Vision and Language knowledge Distillation. Our method distills the knowledge from a pretrained open-vocabulary image classification model (teacher) into a two-stage detector (student). Specifically, we use the teacher model to encode category texts and image regions of object proposals. Then we train a student detector, whose region embeddings of detected boxes are aligned with the text and image embeddings inferred by the teacher. We benchmark on LVIS by holding out all rare categories as novel categories that are not seen during training. ViLD obtains 16.1 mask AP with a ResNet-50 backbone, even outperforming the supervised counterpart by 3.8. When trained with a stronger teacher model ALIGN, ViLD achieves 26.3 AP. The model can directly transfer to other datasets without finetuning, achieving 72.2 AP on PASCAL VOC, 36.6 AP on COCO and 11.8 AP on Objects365. On COCO, ViLD outperforms the previous state-of-the-art by 4.8 on novel AP and 11.4 on overall AP. Code and demo are open-sourced at https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild.",
            "저널": "arXiv preprint arXiv:2104.13921",
            "저자": "Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, Yin Cui",
            "전체 인용횟수": "445회 인용2021202220236108331",
            "학술 문서": "Open-vocabulary object detection via vision and language knowledge distillationX Gu, TY Lin, W Kuo, Y Cui - arXiv preprint arXiv:2104.13921, 2021400회 인용 관련 학술자료 전체 5개의 버전 Zero-shot detection via vision and language knowledge distillation*X Gu, TY Lin, W Kuo, Y Cui - arXiv preprint arXiv:2104.13921, 202148회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Open-vocabulary Object Detection via Vision and Language Knowledge Distillation",
        "year": null
    },
    "Learning Deep Representations for Ground-to-Aerial Geolocalization": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "The recent availability of geo-tagged images and rich geospatial data has inspired a number of algorithms for image based geolocalization. Most approaches predict the location of a query image by matching to ground-level images with known locations (eg, street-view data). However, most of the Earth does not have ground-level reference photos available. Fortunately, more complete coverage is provided by oblique aerial or\" bird's eye\" imagery. In this work, we localize a ground-level query image by matching it to a reference database of aerial imagery. We use publicly available data to build a dataset of 78K aligned cross-view image pairs. The primary challenge for this task is that traditional computer vision approaches cannot handle the wide baseline and appearance variation of these cross-view pairs. We use our dataset to learn a feature representation in which matching views are near one another and mismatched views are far apart. Our proposed approach, Where-CNN, is inspired by deep learning success in face verification and achieves significant improvements over traditional hand-crafted features and existing deep features learned from other large-scale databases. We show the effectiveness of Where-CNN in finding matches between street view and aerial view imagery and demonstrate the ability of our learned features to generalize to novel locations.",
            "저자": "Tsung-Yi Lin, Yin Cui, Serge Belongie, James Hays",
            "전체 인용횟수": "382회 인용20142015201620172018201920202021202220232113544514739395250",
            "컨퍼런스": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "페이지": "5007-5015",
            "학술 문서": "Learning deep representations for ground-to-aerial geolocalizationTY Lin, Y Cui, S Belongie, J Hays - Proceedings of the IEEE conference on computer …, 2015382회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning Deep Representations for Ground-to-Aerial Geolocalization",
        "year": null
    },
    "A multipath network for object detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/4/7",
            "권": "2016",
            "설명": "The recent COCO object detection dataset presents several new challenges for object detection. In particular, it contains objects at a broad range of scales, less prototypical images, and requires more precise localization. To address these challenges, we test three modifications to the standard Fast R-CNN object detector: (1) skip connections that give the detector access to features at multiple network layers, (2) a foveal structure to exploit object context at multiple object resolutions, and (3) an integral loss function and corresponding network adjustment that improve localization. The result of these modifications is that information can flow along multiple paths in our network, including through features from multiple network layers and from multiple object views. We refer to our modified classifier as a \"MultiPath\" network. We couple our MultiPath network with DeepMask object proposals, which are well suited for localization and small objects, and adapt our pipeline to predict segmentation masks in addition to bounding boxes. The combined system improves results over the baseline Fast R-CNN detector with Selective Search by 66% overall and by 4x on small objects. It placed second in both the COCO 2015 detection and segmentation challenges.",
            "저널": "British Machine Vision Conference",
            "저자": "Sergey Zagoruyko, Adam Lerer, Tsung-Yi Lin, Pedro O Pinheiro, Sam Gross, Soumith Chintala, Piotr Dollár",
            "전체 인용횟수": "281회 인용2015201620172018201920202021202220231950543739392318",
            "학술 문서": "A multipath network for object detectionS Zagoruyko, A Lerer, TY Lin, PO Pinheiro, S Gross… - arXiv preprint arXiv:1604.02135, 2016281회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A multipath network for object detection",
        "year": null
    },
    "Revisiting ResNets: Improved Training and Scaling Strategies": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {},
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Revisiting ResNets: Improved Training and Scaling Strategies",
        "year": null
    },
    "Cross-view image geolocalization": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {},
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Cross-view image geolocalization",
        "year": null
    },
    "Magic3d: High-resolution text-to-3d content creation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {},
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Magic3d: High-resolution text-to-3d content creation",
        "year": null
    },
    "inerf: Inverting neural radiance fields for pose estimation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {},
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "inerf: Inverting neural radiance fields for pose estimation",
        "year": null
    },
    "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/4/27",
            "게시자": "IEEE",
            "권": "40",
            "설명": "In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or `atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L Yuille",
            "전체 인용횟수": "20594회 인용20152016201720182019202020212022202391368854165225863077369942013853",
            "페이지": "834-848",
            "학술 문서": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfsLC Chen, G Papandreou, I Kokkinos, K Murphy… - IEEE transactions on pattern analysis and machine …, 201717705회 인용 관련 학술자료 전체 10개의 버전 Semantic image segmentation with deep convolutional nets and fully connected crfs*LC Chen, G Papandreou, I Kokkinos, K Murphy… - arXiv preprint arXiv:1412.7062, 20145624회 인용 관련 학술자료 전체 23개의 버전 Semantic image segmentation with deep convolutional nets and fully connected crfs. arXiv 2014*LC Chen, G Papandreou, I Kokkinos, K Murphy… - arXiv preprint arXiv:1412.7062, 2014348회 인용 관련 학술자료 murphy, k., Yuille, A.: Semantic image segmentation with deep convolutional nets and fully connected CRFs*C Liang-Chieh, G Papandreou, I Kokkinos - … Conference on Learning Representations. Institute of …, 201515회 인용 관련 학술자료 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs",
        "year": null
    },
    "Feature extraction from faces using deformable templates": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1992/8",
            "게시자": "Kluwer Academic Publishers",
            "권": "8",
            "설명": " We propose a method for detecting and describing features of faces using deformable templates. The feature of interest, an eye for example, is described by a parameterized template. An energy function is defined which links edges, peaks, and valleys in the image intensity to corresponding properties of the template. The template then interacts dynamically with the image by altering its parameter values to minimize the energy function, thereby deforming itself to find the best fit. The final parametr values can be used as descriptors for the feature. We illustrate this method by showing deformable templates detecting eyes and mouths in real images. We demonstrate their ability for tracking features.",
            "저널": "International journal of computer vision",
            "저자": "Alan L Yuille, Peter W Hallinan, David S Cohen",
            "전체 인용횟수": "3162회 인용1991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220232333286296101110100104162119125148129180127141136137111116136106110986966623928363617",
            "페이지": "99-111",
            "학술 문서": "Feature extraction from faces using deformable templatesAL Yuille, PW Hallinan, DS Cohen - International journal of computer vision, 19923162회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Feature extraction from faces using deformable templates",
        "year": null
    },
    "Region competition: Unifying snakes, region growing, and Bayes/MDL for multiband image segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1996/9",
            "게시자": "IEEE",
            "권": "18",
            "설명": "We present a novel statistical and variational approach to image segmentation based on a new algorithm, named region competition. This algorithm is derived by minimizing a generalized Bayes/minimum description length (MDL) criterion using the variational principle. The algorithm is guaranteed to converge to a local minimum and combines aspects of snakes/balloons and region growing. The classic snakes/balloons and region growing algorithms can be directly derived from our approach. We provide theoretical analysis of region competition including accuracy of boundary location, criteria for initial conditions, and the relationship to edge detection using filters. It is straightforward to generalize the algorithm to multiband segmentation and we demonstrate it on gray level images, color images and texture images. The novel color model allows us to eliminate intensity gradients and shadows, thereby obtaining …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Song Chun Zhu, Alan Yuille",
            "전체 인용횟수": "3136회 인용199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202325455569869812414114018119717817917217216015316313413310784906741423514",
            "페이지": "884-900",
            "학술 문서": "Region competition: Unifying snakes, region growing, and Bayes/MDL for multiband image segmentationSC Zhu, A Yuille - IEEE transactions on pattern analysis and machine …, 19963136회 인용 관련 학술자료 전체 17개의 버전 ",
            "호": "9"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Region competition: Unifying snakes, region growing, and Bayes/MDL for multiband image segmentation",
        "year": null
    },
    "Transunet: Transformers make strong encoders for medical image segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021/2/8",
            "설명": "Medical image segmentation is an essential prerequisite for developing healthcare systems, especially for disease diagnosis and treatment planning. On various medical image segmentation tasks, the u-shaped architecture, also known as U-Net, has become the de-facto standard and achieved tremendous success. However, due to the intrinsic locality of convolution operations, U-Net generally demonstrates limitations in explicitly modeling long-range dependency. Transformers, designed for sequence-to-sequence prediction, have emerged as alternative architectures with innate global self-attention mechanisms, but can result in limited localization abilities due to insufficient low-level details. In this paper, we propose TransUNet, which merits both Transformers and U-Net, as a strong alternative for medical image segmentation. On one hand, the Transformer encodes tokenized image patches from a convolution neural network (CNN) feature map as the input sequence for extracting global contexts. On the other hand, the decoder upsamples the encoded features which are then combined with the high-resolution CNN feature maps to enable precise localization. We argue that Transformers can serve as strong encoders for medical image segmentation tasks, with the combination of U-Net to enhance finer details by recovering localized spatial information. TransUNet achieves superior performances to various competing methods on different medical applications including multi-organ segmentation and cardiac segmentation. Code and models are available at https://github.com/Beckschen/TransUNet.",
            "저널": "arXiv preprint arXiv:2102.04306",
            "저자": "Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille, Yuyin Zhou",
            "전체 인용횟수": "2262회 인용2021202220231347171378",
            "학술 문서": "Transunet: Transformers make strong encoders for medical image segmentationJ Chen, Y Lu, Q Yu, X Luo, E Adeli, Y Wang, L Lu… - arXiv preprint arXiv:2102.04306, 20212262회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Transunet: Transformers make strong encoders for medical image segmentation",
        "year": null
    },
    "Object perception as Bayesian inference": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004/2/4",
            "게시자": "Annual Reviews",
            "권": "55",
            "설명": "We perceive the shapes and material properties of objects quickly and reliably despite the complexity and objective ambiguities of natural images. Typical images are highly complex because they consist of many objects embedded in background clutter. Moreover, the image features of an object are extremely variable and ambiguous owing to the effects of projection, occlusion, background clutter, and illumination. The very success of everyday vision implies neural mechanisms, yet to be understood, that discount irrelevant information and organize ambiguous or noisy local image features into objects and surfaces. Recent work in Bayesian theories of visual perception has shown how complexity may be managed and ambiguity resolved through the task-dependent, probabilistic integration of prior object knowledge with image features.",
            "저자": "Daniel Kersten, Pascal Mamassian, Alan Yuille",
            "전체 인용횟수": "1545회 인용200420052006200720082009201020112012201320142015201620172018201920202021202220231332375162527564908285831031251149691918396",
            "출처": "Annu. Rev. Psychol.",
            "페이지": "271-304",
            "학술 문서": "Object perception as Bayesian inferenceD Kersten, P Mamassian, A Yuille - Annu. Rev. Psychol., 20041545회 인용 관련 학술자료 전체 33개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Object perception as Bayesian inference",
        "year": null
    },
    "Attention to scale: Scale-aware semantic image segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "Incorporating multi-scale features in fully convolutional neural networks (FCNs) has been a key element to achieving state-of-the-art performance on semantic image segmentation. One common way to extract multi-scale features is to feed multiple resized input images to a shared deep network and then merge the resulting features for pixel-wise classification. In this work, we propose an attention mechanism that learns to softly weight the multi-scale features at each pixel location. We adapt a state-of-the-art semantic image segmentation model, which we jointly train with multi-scale input images and the attention model. The proposed attention model not only outperforms average-and max-pooling, but allows us to diagnostically visualize the importance of features at different positions and scales. Moreover, we show that adding extra supervision to the output at each scale is essential to achieving excellent performance when merging multi-scale features. We demonstrate the effectiveness of our model with extensive experiments on three challenging datasets, including PASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO 2014.",
            "저자": "Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu, Alan L Yuille",
            "전체 인용횟수": "1538회 인용201620172018201920202021202220232984182239263275250192",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "3640-3649",
            "학술 문서": "Attention to scale: Scale-aware semantic image segmentationLC Chen, Y Yang, J Wang, W Xu, AL Yuille - Proceedings of the IEEE conference on computer …, 20161538회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Attention to scale: Scale-aware semantic image segmentation",
        "year": null
    },
    "Deep captioning with multimodal recurrent neural networks (m-rnn)": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/12/20",
            "설명": "In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval. The project page of this work is: www.stat.ucla.edu/~junhua.mao/m-RNN.html .",
            "저널": "arXiv preprint arXiv:1412.6632",
            "저자": "Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, Alan Yuille",
            "전체 인용횟수": "1460회 인용20152016201720182019202020212022202355123174207230170203154110",
            "학술 문서": "Deep captioning with multimodal recurrent neural networks (m-rnn)J Mao, W Xu, Y Yang, J Wang, Z Huang, A Yuille - arXiv preprint arXiv:1412.6632, 20141460회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep captioning with multimodal recurrent neural networks (m-rnn)",
        "year": null
    },
    "Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "Deep convolutional neural networks (DCNNs) trained on a large number of images with strong pixel-level annotations have recently significantly pushed the state-of-art in semantic image segmentation. We study the more challenging problem of learning DCNNs for semantic image segmentation from either (1) weakly annotated training data such as bounding boxes or image-level labels or (2) a combination of few strongly labeled and many weakly labeled images, sourced from one or multiple datasets. We develop Expectation-Maximization (EM) methods for semantic image segmentation model training under these weakly supervised and semi-supervised settings. Extensive experimental evaluation shows that the proposed techniques can learn models delivering competitive results on the challenging PASCAL VOC 2012 image segmentation benchmark, while requiring significantly less annotation effort. We share source code implementing the proposed system at https://bitbucket. org/deeplab/deeplab-public.",
            "저자": "George Papandreou, Liang-Chieh Chen, Kevin P Murphy, Alan L Yuille",
            "전체 인용횟수": "1454회 인용20152016201720182019202020212022202325106132157211245208200137",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "1742-1750",
            "학술 문서": "Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentationG Papandreou, LC Chen, KP Murphy, AL Yuille - Proceedings of the IEEE international conference on …, 20151454회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation",
        "year": null
    },
    "The secrets of salient object segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "설명": "In this paper we provide an extensive evaluation of fixation prediction and salient object segmentation algorithms as well as statistics of major datasets. Our analysis identifies serious design flaws of existing salient object benchmarks, called the dataset design bias, by over emphasising the stereotypical concepts of saliency. The dataset design bias does not only create the discomforting disconnection between fixations and salient object segmentation, but also misleads the algorithm designing. Based on our analysis, we propose a new high quality dataset that offers both fixation and salient object segmentation ground-truth. With fixations and salient object being presented simultaneously, we are able to bridge the gap between fixations and salient objects, and propose a novel method for salient object segmentation. Finally, we report significant benchmark progress on 3 existing datasets of segmenting salient objects.",
            "저자": "Yin Li, Xiaodi Hou, Christof Koch, James M Rehg, Alan L Yuille",
            "전체 인용횟수": "1451회 인용201420152016201720182019202020212022202355292129158177236219194160",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "280-287",
            "학술 문서": "The secrets of salient object segmentationY Li, X Hou, C Koch, JM Rehg, AL Yuille - Proceedings of the IEEE conference on computer …, 20141451회 인용 관련 학술자료 전체 29개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The secrets of salient object segmentation",
        "year": null
    },
    "The role of context for object detection and semantic segmentation in the wild": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "설명": "In this paper we study the role of context in existing state-of-the-art detection and segmentation approaches. Towards this goal, we label every pixel of PASCAL VOC 2010 detection challenge with a semantic category. We believe this data will provide plenty of challenges to the community, as it contains 520 additional classes for semantic segmentation and object detection. Our analysis shows that nearest neighbor based approaches perform poorly on semantic segmentation of contextual classes, showing the variability of PASCAL imagery. Furthermore, improvements of exist ing contextual models for detection is rather modest. In order to push forward the performance in this difficult scenario, we propose a novel deformable part-based model, which exploits both local context around each candidate detection as well as global context at the level of the scene. We show that this contextual reasoning significantly helps in detecting objects at all scales.",
            "저자": "Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, Alan Yuille",
            "전체 인용횟수": "1444회 인용201420152016201720182019202020212022202355484122132154185240208238",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "891-898",
            "학술 문서": "The role of context for object detection and semantic segmentation in the wildR Mottaghi, X Chen, X Liu, NG Cho, SW Lee, S Fidler… - Proceedings of the IEEE conference on computer …, 20141444회 인용 관련 학술자료 전체 30개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The role of context for object detection and semantic segmentation in the wild",
        "year": null
    },
    "The concave-convex procedure": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2003/4/1",
            "게시자": "MIT Press",
            "권": "15",
            "설명": "The concave-convex procedure (CCCP) is a way to construct discrete-time iterative dynamical systems that are guaranteed to decrease global optimization and energy functions monotonically. This procedure can be applied to almost any optimization problem, and many existing algorithms can be interpreted in terms of it. In particular, we prove that all expectation-maximization algorithms and classes of Legendre minimization and variational bounding algorithms can be reexpressed in terms of CCCP. We show that many existing neural network and mean-field theory algorithms are also examples of CCCP. The generalized iterative scaling algorithm and Sinkhorn's algorithm can also be expressed as CCCP by changing variables. CCCP can be used both as a new way to understand, and prove the convergence of, existing optimization algorithms and as a procedure for generating new algorithms.",
            "저널": "Neural computation",
            "저자": "Alan L Yuille, Anand Rangarajan",
            "전체 인용횟수": "1363회 인용200420052006200720082009201020112012201320142015201620172018201920202021202220235877233137629110810810611810510494917810660",
            "페이지": "915-936",
            "학술 문서": "The concave-convex procedureAL Yuille, A Rangarajan - Neural computation, 20031363회 인용 관련 학술자료 전체 9개의 버전 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The concave-convex procedure",
        "year": null
    },
    "Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "Recently, Neural Architecture Search (NAS) has successfully identified neural network architectures that exceed human designed ones on large-scale image classification. In this paper, we study NAS for semantic image segmentation. Existing works often focus on searching the repeatable cell structure, while hand-designing the outer network structure that controls the spatial resolution changes. This choice simplifies the search space, but becomes increasingly problematic for dense image prediction which exhibits a lot more network level architectural variations. Therefore, we propose to search the network level structure in addition to the cell level structure, which forms a hierarchical architecture search space. We present a network level search space that includes many popular designs, and develop a formulation that allows efficient gradient-based architecture search (3 P100 GPU days on Cityscapes images). We demonstrate the effectiveness of the proposed method on the challenging Cityscapes, PASCAL VOC 2012, and ADE20K datasets. Auto-DeepLab, our architecture searched specifically for semantic image segmentation, attains state-of-the-art performance without any ImageNet pretraining.",
            "저자": "Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua, Alan L Yuille, Li Fei-Fei",
            "전체 인용횟수": "1077회 인용2019202020212022202378253290266181",
            "컨퍼런스": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "페이지": "82-92",
            "학술 문서": "Auto-deeplab: Hierarchical neural architecture search for semantic image segmentationC Liu, LC Chen, F Schroff, H Adam, W Hua, AL Yuille… - Proceedings of the IEEE/CVF conference on computer …, 20191077회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation",
        "year": null
    },
    "Mitigating adversarial effects through randomization": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/11/6",
            "설명": "Convolutional neural networks have demonstrated high accuracy on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. For example, imperceptible perturbations added to clean images can cause convolutional neural networks to fail. In this paper, we propose to utilize randomization at inference time to mitigate adversarial effects. Specifically, we use two randomization operations: random resizing, which resizes the input images to a random size, and random padding, which pads zeros around the input images in a random manner. Extensive experiments demonstrate that the proposed randomization method is very effective at defending against both single-step and iterative attacks. Our method provides the following advantages: 1) no additional training or fine-tuning, 2) very few additional computations, 3) compatible with other adversarial defense methods. By combining the proposed randomization method with an adversarially trained model, it achieves a normalized score of 0.924 (ranked No.2 among 107 defense teams) in the NIPS 2017 adversarial examples defense challenge, which is far better than using adversarial training alone with a normalized score of 0.773 (ranked No.56). The code is public available at https://github.com/cihangxie/NIPS2017_adv_challenge_defense.",
            "저널": "arXiv preprint arXiv:1711.01991",
            "저자": "Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, Alan Yuille",
            "전체 인용횟수": "1050회 인용2017201820192020202120222023364138187218232204",
            "학술 문서": "Mitigating adversarial effects through randomizationC Xie, J Wang, Z Zhang, Z Ren, A Yuille - arXiv preprint arXiv:1711.01991, 20171050회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Mitigating adversarial effects through randomization",
        "year": null
    },
    "Vision as Bayesian inference: analysis by synthesis?": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006/7/1",
            "게시자": "Elsevier",
            "권": "10",
            "설명": "We argue that the study of human vision should be aimed at determining how humans perform natural tasks with natural images. Attempts to understand the phenomenology of vision from artificial stimuli, although worthwhile as a starting point, can lead to faulty generalizations about visual systems, because of the enormous complexity of natural images. Dealing with this complexity is daunting, but Bayesian inference on structured probability distributions offers the ability to design theories of vision that can deal with the complexity of natural images, and that use ‘analysis by synthesis' strategies with intriguing similarities to the brain. We examine these strategies using recent examples from computer vision, and outline some important imlications for cognitive science.",
            "저자": "Alan Yuille, Daniel Kersten",
            "전체 인용횟수": "1003회 인용20062007200820092010201120122013201420152016201720182019202020212022202382240714146508544495674755458716880",
            "출처": "Trends in cognitive sciences",
            "페이지": "301-308",
            "학술 문서": "Vision as Bayesian inference: analysis by synthesis?A Yuille, D Kersten - Trends in cognitive sciences, 20061003회 인용 관련 학술자료 전체 28개의 버전 ",
            "호": "7"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Vision as Bayesian inference: analysis by synthesis?",
        "year": null
    },
    "Adversarial examples for semantic segmentation and object detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "It has been well demonstrated that adversarial examples, ie, natural images with visually imperceptible perturbations added, cause deep networks to fail on image classification. In this paper, we extend adversarial examples to semantic segmentation and object detection which are much more difficult. Our observation is that both segmentation and detection are based on classifying multiple targets on an image (eg, the target is a pixel or a receptive field in segmentation, and an object proposal in detection). This inspires us to optimize a loss function over a set of targets for generating adversarial perturbations. Based on this, we propose a novel algorithm named Dense Adversary Generation (DAG), which applies to the state-of-the-art networks for segmentation and detection. We find that the adversarial perturbations can be transferred across networks with different training data, based on different architectures, and even for different recognition tasks. In particular, the transfer ability across networks with the same architecture is more significant than in other cases. Besides, we show that summing up heterogeneous perturbations often leads to better transfer performance, which provides an effective method of black-box adversarial attack.",
            "저자": "Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, Alan Yuille",
            "전체 인용횟수": "977회 인용20172018201920202021202220231573123199175196194",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "1369-1378",
            "학술 문서": "Adversarial examples for semantic segmentation and object detectionC Xie, J Wang, Z Zhang, Y Zhou, L Xie, A Yuille - Proceedings of the IEEE international conference on …, 2017977회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Adversarial examples for semantic segmentation and object detection",
        "year": null
    },
    "Scaling theorems for zero crossings": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1986/1",
            "게시자": "IEEE",
            "설명": "We characterize some properties of the zero crossings of the Laplacian of signals-in particular images-filtered with linear filters, as a function of the scale of the filter (extending recent work by Witkin [16]). We prove that in any dimension the only filter that does not create generic zero crossings as the scale increases is the Gaussian. This result can be generalized to apply to level crossings of any linear differential operator: it applies in particular to ridges and ravines in the image intensity. In the case of the second derivative along the gradient, there is no filter that avoids creation of zero crossings, unless the filtering is performed after the derivative is applied.",
            "저널": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "저자": "Alan L Yuille, Tomaso A Poggio",
            "전체 인용횟수": "977회 인용198519861987198819891990199119921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023101523443534384637604440363428192619203026183523242726222514231411968449",
            "페이지": "15-25",
            "학술 문서": "Scaling theorems for zero crossingsAL Yuille, TA Poggio - IEEE Transactions on Pattern Analysis and Machine …, 1986977회 인용 관련 학술자료 전체 23개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Scaling theorems for zero crossings",
        "year": null
    },
    "Detecting and reading text in natural scenes": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004/6/27",
            "게시자": "IEEE",
            "권": "2",
            "설명": "This paper gives an algorithm for detecting and reading text in natural images. The algorithm is intended for use by blind and visually impaired subjects walking through city scenes. We first obtain a dataset of city images taken by blind and normally sighted subjects. From this dataset, we manually label and extract the text regions. Next we perform statistical analysis of the text regions to determine which image features are reliable indicators of text and have low entropy (i.e. feature response is similar for all text images). We obtain weak classifiers by using joint probabilities for feature responses on and off text. These weak classifiers are used as input to an AdaBoost machine learning algorithm to train a strong classifier. In practice, we trained a cascade with 4 strong classifiers containing 79 features. An adaptive binarization and extension algorithm is applied to those regions selected by the cascade classifier …",
            "저자": "Xiangrong Chen, Alan L Yuille",
            "전체 인용횟수": "972회 인용20042005200620072008200920102011201220132014201520162017201820192020202120222023521282227313259648493929283714936332711",
            "컨퍼런스": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.",
            "페이지": "II-II",
            "학술 문서": "Detecting and reading text in natural scenesX Chen, AL Yuille - Proceedings of the 2004 IEEE Computer Society …, 2004972회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Detecting and reading text in natural scenes",
        "year": null
    },
    "Generation and comprehension of unambiguous object descriptions": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "We propose a method that can generate an unambiguous description (known as a referring expression) of a specific object or region in an image, and which can also comprehend or interpret such an expression to infer which object is being described. We show that our method outperforms previous methods that generate descriptions of objects without taking into account other potentially ambiguous objects in the scene. Our model is inspired by recent successes of deep learning methods for image captioning, but while image captioning is difficult to evaluate, our task allows for easy objective evaluation. We also present a new large-scale dataset for referring expressions, based on MS-COCO. We have released the dataset and a toolbox for visualization and evaluation, see https://github. com/mjhucla/Google_Refexp_toolbox.",
            "저자": "Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, Kevin Murphy",
            "전체 인용횟수": "965회 인용2015201620172018201920202021202220235146089103124125176254",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "11-20",
            "학술 문서": "Generation and comprehension of unambiguous object descriptionsJ Mao, J Huang, A Toshev, O Camburu, AL Yuille… - Proceedings of the IEEE conference on computer …, 2016965회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Generation and comprehension of unambiguous object descriptions",
        "year": null
    },
    "Genetic cnn": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "The deep convolutional neural network (CNN) is the state-of-the-art solution for large-scale visual recognition. Following some basic principles such as increasing network depth and constructing highway connections, researchers have manually designed a lot of fixed network architectures and verified their effectiveness. In this paper, we discuss the possibility of learning deep network structures automatically. Note that the number of possible network structures increases exponentially with the number of layers in the network, which motivates us to adopt the genetic algorithm to efficiently explore this large search space. The core idea is to propose an encoding method to represent each network structure in a fixed-length binary string. The genetic algorithm is initialized by generating a set of randomized individuals. In each generation, we define standard genetic operations, eg, selection, mutation and crossover, to generate competitive individuals and eliminate weak ones. The competitiveness of each individual is defined as its recognition accuracy, which is obtained via a standalone training process on a reference dataset. We run the genetic process on CIFAR10, a small-scale dataset, demonstrating its ability to find high-quality structures which are little studied before. The learned powerful structures are also transferrable to the ILSVRC2012 dataset for large-scale visual recognition.",
            "저자": "Lingxi Xie, Alan Yuille",
            "전체 인용횟수": "951회 인용2017201820192020202120222023870137159217177175",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "1379-1388",
            "학술 문서": "Genetic cnnL Xie, A Yuille - Proceedings of the IEEE international conference on …, 2017951회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Genetic cnn",
        "year": null
    },
    "The design and use of steerable filters": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1991/9/9",
            "권": "13",
            "설명": "Oriented ﬁlters are useful in many early vision and image processing tasks. One often needs to apply the same ﬁlter, rotated to different angles under adaptive control, or wishes to calculate the ﬁlter response at various orientations. We present an efﬁcient architecture to synthesize ﬁlters of arbitrary orientations from linear combinations of basis ﬁlters, allowing one to adaptively “steer” a ﬁlter to any orientation, and to determine analytically the ﬁlter output as a function of orientation. Steerable ﬁlters may be designed in quadrature pairs to allow adaptive control over phase as well as orientation. We show how to design and steer the ﬁlters and present examples of their use in several tasks: the analysis of orientation and phase, angularly adaptive ﬁltering, edge detection, and shape from shading. One can also build a self-similar steerable pyramid representation. The same concepts can be generalized to the design of 3 …",
            "저널": "IEEE Transactions on Pattern analysis and machine intelligence",
            "저자": "William T Freeman, Edward H Adelson",
            "전체 인용횟수": "4605회 인용19921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023151653666986103768794971131631631912372042522402492422332232082021801771301101118872",
            "페이지": "891-906",
            "학술 문서": "The design and use of steerable filtersWT Freeman, EH Adelson - IEEE Transactions on Pattern analysis and machine …, 19914605회 인용 관련 학술자료 전체 23개의 버전 ",
            "호": "9"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The design and use of steerable filters",
        "year": null
    },
    "LabelMe: a database and web-based tool for image annotation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008/5",
            "게시자": "Springer US",
            "권": "77",
            "설명": "  We seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web. ",
            "저널": "International journal of computer vision",
            "저자": "Bryan C Russell, Antonio Torralba, Kevin P Murphy, William T Freeman",
            "전체 인용횟수": "4255회 인용200720082009201020112012201320142015201620172018201920202021202220234995177210228219276242245273260233271302333397341",
            "페이지": "157-173",
            "학술 문서": "LabelMe: a database and web-based tool for image annotationBC Russell, A Torralba, KP Murphy, WT Freeman - International journal of computer vision, 20084255회 인용 관련 학술자료 전체 30개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "LabelMe: a database and web-based tool for image annotation",
        "year": null
    },
    "Example-based super-resolution": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2002/8/7",
            "게시자": "IEEE",
            "권": "22",
            "설명": "We call methods for achieving high-resolution enlargements of pixel-based images super-resolution algorithms. Many applications in graphics or image processing could benefit from such resolution independence, including image-based rendering (IBR), texture mapping, enlarging consumer photographs, and converting NTSC video content to high-definition television. We built on another training-based super-resolution algorithm and developed a faster and simpler algorithm for one-pass super-resolution. Our algorithm requires only a nearest-neighbor search in the training set for a vector derived from each patch of local image data. This one-pass super-resolution algorithm is a step toward achieving resolution independence in image-based representations. We don't expect perfect resolution independence-even the polygon representation doesn't have that-but increasing the resolution independence of pixel …",
            "저널": "IEEE Computer graphics and Applications",
            "저자": "William T Freeman, Thouis R Jones, Egon C Pasztor",
            "전체 인용횟수": "3497회 인용200320042005200620072008200920102011201220132014201520162017201820192020202120222023365143629010113716918024425528630828322724219619614712978",
            "페이지": "56-65",
            "학술 문서": "Example-based super-resolutionWT Freeman, TR Jones, EC Pasztor - IEEE Computer graphics and Applications, 20023497회 인용 관련 학술자료 전체 28개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Example-based super-resolution",
        "year": null
    },
    "First M87 event horizon telescope results. IV. Imaging the central supermassive black hole": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/4/10",
            "게시자": "IOP Publishing",
            "권": "875",
            "설명": "First M87 Event Horizon Telescope Results. IV. Imaging the Central Supermassive Black Hole \n- IOPscience This site uses cookies. By continuing to use this site you agree to our use of \ncookies. To find out more, see our Privacy and Cookies policy. Close this notification IOP \nScience home Skip to content Accessibility Help Search Journals Journals list Browse more \nthan 100 science journal titles Subject collections Read the very best research published in \nIOP journals Publishing partners Partner organisations and publications Open access IOP \nPublishing open access policy guide IOP Conference Series Read open access proceedings \nfrom science conferences worldwide Books Publishing Support Login IOPscience login / Sign \nUp Click here to close this panel. Primary search Search all IOPscience content Article Lookup \nSelect journal (required) Volume number: Issue number (if known): Article or page number: …",
            "저널": "The Astrophysical Journal Letters",
            "저자": "Kazunori Akiyama, Antxon Alberdi, Walter Alef, Keiichi Asada, Rebecca Azulay, Anne-Kathrin Baczko, David Ball, Mislav Baloković, John Barrett, Dan Bintley, Lindy Blackburn, Wilfred Boland, Katherine L Bouman, Geoffrey C Bower, Michael Bremer, Christiaan D Brinkerink, Roger Brissenden, Silke Britzen, Avery E Broderick, Dominique Broguiere, Thomas Bronzwaer, Do-Young Byun, John E Carlstrom, Andrew Chael, Chi-kwan Chan, Shami Chatterjee, Koushik Chatterjee, Ming-Tang Chen, Yongjun Chen, Ilje Cho, Pierre Christian, John E Conway, James M Cordes, Geoffrey B Crew, Yuzhu Cui, Jordy Davelaar, Mariafelicia De Laurentis, Roger Deane, Jessica Dempsey, Gregory Desvignes, Jason Dexter, Sheperd S Doeleman, Ralph P Eatough, Heino Falcke, Vincent L Fish, Ed Fomalont, Raquel Fraga-Encinas, William T Freeman, Per Friberg, Christian M Fromm, José L Gómez, Peter Galison, Charles F Gammie, Roberto García, Olivier Gentaz, Boris Georgiev, Ciriaco Goddi, Roman Gold, Minfeng Gu, Mark Gurwell, Kazuhiro Hada, Michael H Hecht, Ronald Hesper, Luis C Ho, Paul Ho, Mareki Honma, Chih-Wei L Huang, Lei Huang, David H Hughes, Shiro Ikeda, Makoto Inoue, Sara Issaoun, David J James, Buell T Jannuzi, Michael Janssen, Britton Jeter, Wu Jiang, Michael D Johnson, Svetlana Jorstad, Taehyun Jung, Mansour Karami, Ramesh Karuppusamy, Tomohisa Kawashima, Garrett K Keating, Mark Kettenis, Jae-Young Kim, Junhan Kim, Jongsoo Kim, Motoki Kino, Jun Yi Koay, Patrick M Koch, Shoko Koyama, Michael Kramer, Carsten Kramer, Thomas P Krichbaum, Cheng-Yu Kuo, Tod R Lauer, Sang-Sung Lee, Yan-Rong Li, Zhiyuan Li, Michael Lindqvist, Kuo Liu, Elisabetta Liuzzo, Wen-Ping Lo, Andrei P Lobanov, Laurent Loinard, Colin Lonsdale, Ru-Sen Lu, Nicholas R MacDonald, Jirong Mao, Sera Markoff, Daniel P Marrone, Alan P Marscher, Iván Martí-Vidal, Satoki Matsushita, Lynn D Matthews, Lia Medeiros, Karl M Menten, Yosuke Mizuno, Izumi Mizuno, James M Moran, Kotaro Moriyama, Monika Moscibrodzka, Cornelia Müller, Hiroshi Nagai, Neil M Nagar, Masanori Nakamura, Ramesh Narayan, Gopal Narayanan, Iniyan Natarajan, Roberto Neri, Chunchong Ni, Aristeidis Noutsos, Hiroki Okino, Héctor Olivares, Tomoaki Oyama, Feryal Özel, Daniel CM Palumbo, Nimesh Patel, Ue-Li Pen, Dominic W Pesce, Vincent Piétu, Richard Plambeck, Aleksandar PopStefanija, Oliver Porth, Ben Prather, Jorge A Preciado-López, Dimitrios Psaltis, Hung-Yi Pu, Venkatessh Ramakrishnan",
            "전체 인용횟수": "3445회 인용20192020202120222023343660754772889",
            "페이지": "L4",
            "학술 문서": "First M87 event horizon telescope results. IV. Imaging the central supermassive black holeK Akiyama, A Alberdi, W Alef, K Asada, R Azulay… - The Astrophysical Journal Letters, 20192764회 인용 관련 학술자료 전체 71개의 버전 First M87 event horizon telescope results. IV. Imaging the central supermassive black holeEvent Horizon Telescope Collaboration - arXiv preprint arXiv:1906.11241, 2019873회 인용 관련 학술자료 전체 3개의 버전 col.«First M87 Event Horizon Telescope Results. IV. Imaging the Central Supermassive Black Hole»K Akiyama - Astrophys. J, 20192회 인용 관련 학술자료 First M87 Event Horizon Telescope Results. IV. Imaging the Central Supermassive Black HoleK Asada, P Ho, CWL Huang, PM Koch, S Koyama… - The Astrophysical Journal Letters 875 (1), L4, 2019",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "First M87 event horizon telescope results. IV. Imaging the central supermassive black hole",
        "year": null
    },
    "Learning low-level vision": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2000/10",
            "게시자": "Kluwer Academic Publishers",
            "권": "40",
            "설명": " We describe a learning-based method for low-level vision problems—estimating scenes from images. We generate a synthetic world of scenes and their corresponding rendered images, modeling their relationships with a Markov network. Bayesian belief propagation allows us to efficiently find a local maximum of the posterior probability for the scene, given an image. We call this approach VISTA—Vision by Image/Scene TrAining. We apply VISTA to the “super-resolution” problem (estimating high frequency details from a low-resolution image), showing good results. To illustrate the potential breadth of the technique, we also apply it in two other problem domains, both simplified. We learn to distinguish shading from reflectance variations in a single image under particular lighting conditions. For the motion estimation problem in a “blobs world”, we show figure/ground discrimination, solution of the …",
            "저널": "International journal of computer vision",
            "저자": "William T Freeman, Egon C Pasztor, Owen T Carmichael",
            "전체 인용횟수": "2290회 인용200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023152330647888103108123125119142162141141121147921048970695951",
            "페이지": "25-47",
            "학술 문서": "Learning low-level visionWT Freeman, EC Pasztor, OT Carmichael - International journal of computer vision, 20002290회 인용 관련 학술자료 전체 38개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning low-level vision",
        "year": null
    },
    "Shiftable multiscale transforms": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1992/3",
            "게시자": "IEEE",
            "권": "38",
            "설명": "One of the major drawbacks of orthogonal wavelet transforms is their lack of translation invariance: the content of wavelet subbands is unstable under translations of the input signal. Wavelet transforms are also unstable with respect to dilations of the input signal and, in two dimensions, rotations of the input signal. The authors formalize these problems by defining a type of translation invariance called shiftability. In the spatial domain, shiftability corresponds to a lack of aliasing; thus, the conditions under which the property holds are specified by the sampling theorem. Shiftability may also be applied in the context of other domains, particularly orientation and scale. Jointly shiftable transforms that are simultaneously shiftable in more than one domain are explored. Two examples of jointly shiftable transforms are designed and implemented: a 1-D transform that is jointly shiftable in position and scale, and a 2-D …",
            "저널": "IEEE transactions on Information Theory",
            "저자": "Eero P Simoncelli, William T Freeman, Edward H Adelson, David J Heeger",
            "전체 인용횟수": "2058회 인용19921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023914253445496661676477779292104115951118963728178795865654044453821",
            "페이지": "587-607",
            "학술 문서": "Shiftable multiscale transformsEP Simoncelli, WT Freeman, EH Adelson, DJ Heeger - IEEE transactions on Information Theory, 19922054회 인용 관련 학술자료 전체 27개의 버전 Shiftable multiscale transformEP Siomoncelli, WT Freeman, EH Adelson, D Heege - IEEE Transactions on Information Theory, 19924회 인용 관련 학술자료 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Shiftable multiscale transforms",
        "year": null
    },
    "Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "권": "29",
            "설명": "We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.",
            "저널": "Advances in neural information processing systems",
            "저자": "Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, Josh Tenenbaum",
            "전체 인용횟수": "2018회 인용201620172018201920202021202220239107230339340348297319",
            "학술 문서": "Learning a probabilistic latent space of object shapes via 3d generative-adversarial modelingJ Wu, C Zhang, T Xue, B Freeman, J Tenenbaum - Advances in neural information processing systems, 20162018회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling",
        "year": null
    },
    "Understanding belief propagation and its generalizations": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2003/1/8",
            "권": "8",
            "설명": "“Inference” problems arise in statistical physics, computer vision, error-correcting coding theory, and AI. We explain the principles behind the belief propagation (BP) algorithm, which is an efficient way to solve inference problems based on passing local messages. We develop a unified approach, with examples, notation, and graphical models borrowed from the relevant disciplines. We explain the close connection between the BP algorithm and the Bethe approximation of statistical physics. In particular, we show that BP can only converge to a fixed point that is also a stationary point of the Bethe approximation to the free energy. This result helps expain the successes of the BP algorithm, and enables connections to be made with variational approaches to approximate inference. The connection of BP with the Bethe approximation also suggests a way to construct new message passing algorithms based on improvements to Bethe’s approximation introduced by Kikuchi and others. The new generalized belief propagation (GBP) algorithms are significantly more accurate than ordinary BP for some problems. We illustrate how to construct GBP algorithms with a detailed example.",
            "저널": "Exploring artificial intelligence in the new millennium",
            "저자": "Jonathan S Yedidia, William T Freeman, Yair Weiss",
            "전체 인용횟수": "2012회 인용2002200320042005200620072008200920102011201220132014201520162017201820192020202120222023123253761038992126111138149115109981131057191102687756",
            "페이지": "0018-9448",
            "학술 문서": "Understanding belief propagation and its generalizationsJS Yedidia, WT Freeman, Y Weiss - Exploring artificial intelligence in the new millennium, 20032012회 인용 관련 학술자료 전체 17개의 버전 ",
            "호": "236-239"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Understanding belief propagation and its generalizations",
        "year": null
    },
    "Constructing free-energy approximations and generalized belief propagation algorithms": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/6/27",
            "게시자": "IEEE",
            "권": "51",
            "설명": "Important inference problems in statistical physics, computer vision, error-correcting coding theory, and artificial intelligence can all be reformulated as the computation of marginal probabilities on factor graphs. The belief propagation (BP) algorithm is an efficient way to solve these problems that is exact when the factor graph is a tree, but only approximate when the factor graph has cycles. We show that BP fixed points correspond to the stationary points of the Bethe approximation of the free energy for a factor graph. We explain how to obtain region-based free energy approximations that improve the Bethe approximation, and corresponding generalized belief propagation (GBP) algorithms. We emphasize the conditions a free energy approximation must satisfy in order to be a \"valid\" or \"maxent-normal\" approximation. We describe the relationship between four different methods that can be used to generate valid …",
            "저널": "IEEE Transactions on information theory",
            "저자": "Jonathan S Yedidia, William T Freeman, Yair Weiss",
            "전체 인용횟수": "1881회 인용200420052006200720082009201020112012201320142015201620172018201920202021202220233056841011061171019914713010712893102787784836547",
            "페이지": "2282-2312",
            "학술 문서": "Constructing free-energy approximations and generalized belief propagation algorithmsJS Yedidia, WT Freeman, Y Weiss - IEEE Transactions on information theory, 20051881회 인용 관련 학술자료 전체 40개의 버전 ",
            "호": "7"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Constructing free-energy approximations and generalized belief propagation algorithms",
        "year": null
    },
    "Eulerian video magnification for revealing subtle changes in the world": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/7/1",
            "게시자": "ACM",
            "권": "31",
            "설명": "Our goal is to reveal temporal variations in videos that are difficult or impossible to see with the naked eye and display them in an indicative manner. Our method, which we call Eulerian Video Magnification, takes a standard video sequence as input, and applies spatial decomposition, followed by temporal filtering to the frames. The resulting signal is then amplified to reveal hidden information. Using our method, we are able to visualize the flow of blood as it fills the face and also to amplify and reveal small motions. Our technique can run in real time to show phenomena occurring at the temporal frequencies selected by the user.",
            "저널": "ACM transactions on graphics (TOG)",
            "저자": "Hao-Yu Wu, Michael Rubinstein, Eugene Shih, John Guttag, Frédo Durand, William Freeman",
            "전체 인용횟수": "1692회 인용201320142015201620172018201920202021202220234086138157183195185190153196139",
            "페이지": "1-8",
            "학술 문서": "Eulerian video magnification for revealing subtle changes in the worldHY Wu, M Rubinstein, E Shih, J Guttag, F Durand… - ACM transactions on graphics (TOG), 20121692회 인용 관련 학술자료 전체 13개의 버전 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Eulerian video magnification for revealing subtle changes in the world",
        "year": null
    },
    "The steerable pyramid: A flexible architecture for multi-scale derivative computation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1995/10/23",
            "게시자": "IEEE",
            "권": "3",
            "설명": "We describe an architecture for efficient and accurate linear decomposition of an image into scale and orientation subbands. The basis functions of this decomposition are directional derivative operators of any desired order. We describe the construction and implementation of the transform.",
            "저자": "Eero P Simoncelli, William T Freeman",
            "전체 인용횟수": "1574회 인용1996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023119131818282933335556576573596373807785931161047261676646",
            "컨퍼런스": "Proceedings., International Conference on Image Processing",
            "페이지": "444-447",
            "학술 문서": "The steerable pyramid: A flexible architecture for multi-scale derivative computationEP Simoncelli, WT Freeman - Proceedings., International Conference on Image …, 19951574회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The steerable pyramid: A flexible architecture for multi-scale derivative computation",
        "year": null
    },
    "Understanding and evaluating blind deconvolution algorithms": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/6/20",
            "게시자": "IEEE",
            "설명": "Blind deconvolution is the recovery of a sharp version of a blurred image when the blur kernel is unknown. Recent algorithms have afforded dramatic progress, yet many aspects of the problem remain challenging and hard to understand. The goal of this paper is to analyze and evaluate recent blind deconvolution algorithms both theoretically and experimentally. We explain the previously reported failure of the naive MAP approach by demonstrating that it mostly favors no-blur explanations. On the other hand we show that since the kernel size is often smaller than the image size a MAP estimation of the kernel alone can be well constrained and accurately recover the true blur. The plethora of recent deconvolution techniques makes an experimental evaluation on ground-truth data important. We have collected blur data with ground truth and compared recent algorithms under equal settings. Additionally, our data …",
            "저자": "Anat Levin, Yair Weiss, Fredo Durand, William T Freeman",
            "전체 인용횟수": "1543회 인용20092010201120122013201420152016201720182019202020212022202318396478105102134130116119106119126149115",
            "컨퍼런스": "2009 IEEE conference on computer vision and pattern recognition",
            "페이지": "1964-1971",
            "학술 문서": "Understanding and evaluating blind deconvolution algorithmsA Levin, Y Weiss, F Durand, WT Freeman - 2009 IEEE conference on computer vision and pattern …, 20091543회 인용 관련 학술자료 전체 19개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Understanding and evaluating blind deconvolution algorithms",
        "year": null
    },
    "Generalized belief propagation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2000",
            "권": "13",
            "설명": "Belief propagation (BP) was only supposed to work for tree-like networks but works surprisingly well in many applications involving networks with loops, including turbo codes. However, there has been little understanding of the algorithm or the nature of the solutions it finds for general graphs. We show that BP can only converge to a stationary point of an approximate free energy, known as the Bethe free energy in statis (cid: 173) tical physics. This result characterizes BP fixed-points and makes connections with variational approaches to approximate inference. More importantly, our analysis lets us build on the progress made in statistical physics since Bethe's approximation was introduced in 1935. Kikuchi and others have shown how to construct more ac (cid: 173) curate free energy approximations, of which Bethe's approximation is the simplest. Exploiting the insights from our analysis, we de (cid: 173) rive generalized belief propagation (GBP) versions ofthese Kikuchi approximations. These new message passing algorithms can be significantly more accurate than ordinary BP, at an adjustable in (cid: 173) crease in complexity. We illustrate such a new GBP algorithm on a grid Markov network and show that it gives much more accurate marginal probabilities than those found using ordinary BP.",
            "저널": "Advances in neural information processing systems",
            "저자": "Jonathan S Yedidia, William Freeman, Yair Weiss",
            "전체 인용횟수": "1469회 인용20002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202341939716283696898868983846873485740494851704847",
            "학술 문서": "Generalized belief propagationJS Yedidia, W Freeman, Y Weiss - Advances in neural information processing systems, 20001469회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Generalized belief propagation",
        "year": null
    },
    "Context-based vision system for place and object recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2003/10/13",
            "게시자": "IEEE",
            "설명": "While navigating in an environment, a vision system has to be able to recognize where it is and what the main objects in the scene are. We present a context-based vision system for place and object recognition. The goal is to identify familiar locations (e.g., office 610, conference room 941, main street), to categorize new environments (office, corridor, street) and to use that information to provide contextual priors for object recognition (e.g., tables are more likely in an office than a street). We present a low-dimensional global image representation that provides relevant information for place recognition and categorization, and show how such contextual information introduces strong priors that simplify object recognition. We have trained the system to recognize over 60 locations (indoors and outdoors) and to suggest the presence and locations of more than 20 different object types. The algorithm has been integrated …",
            "저널": "Proceedings Ninth IEEE International Conference on Computer Vision",
            "저자": "Torralba",
            "전체 인용횟수": "1276회 인용2004200520062007200820092010201120122013201420152016201720182019202020212022202328355965697296757610481756670646344553424",
            "페이지": "273-280 vol. 1",
            "학술 문서": "Context-based vision system for place and object recognitionTorralba - Proceedings Ninth IEEE International Conference on …, 20031276회 인용 관련 학술자료 전체 22개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Context-based vision system for place and object recognition",
        "year": null
    },
    "Hand gesture machine control system": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1997/1/14",
            "발명자": "William T Freeman, Craig D Weissman",
            "설명": "A system for the control from a distance of machines having displays incls hand gesture detection in which the hand gesture causes movement of an on-screen hand icon over an on-screen machine control icon, with the hand icon moving the machine control icon in accordance with sensed hand movements to effectuate machine control. In one embodiment, TV control led by hand signals includes detecting a single hand gesture and providing a hand icon on the screen along with the provision of icons representing TV controls such as volume, channel, color, density, etc., in which a television camera detects the hand in a noisy background through correlation techniques based on values of local image orientation. In order to trigger the system into operation, a trigger gesture such as the\" how\" sign is distinguished from the background through the utilization of orientation angle differences. From correlation values …",
            "전체 인용횟수": "1181회 인용199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202338710467111420294240781521591271309769595723198",
            "출원번호": "08391955",
            "특허 번호": "5594469",
            "특허청": "US",
            "학술 문서": "Hand gesture machine control systemWT Freeman, CD Weissman - US Patent 5,594,469, 19971181회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Hand gesture machine control system",
        "year": null
    },
    "Machine learning, a probabilistic perspective": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/4/3",
            "게시자": "Taylor & Francis",
            "권": "27",
            "설명": "The book also introduces the notion of a Bayesian likelihood function (p. 228), which “differs slightly from that in classical statistics.” The only difference I can spot is in the interpretation: Both functions of (θ, x) are numerically the same. Overall, the chapter on Bayesian inference does not spend much time on prior specification. There is a section on conjugate priors that does not mention picking the hyperparameters. While improper priors are introduced as limits of proper priors and as conveying “the least amount of information about [the parameters]”(p. 236), the difficulty in using improper priors for hypothesis testing is not mentioned. Both Chib’s method and the Savage-Dickey density ratio are suggested for the approximation of marginal likelihoods.",
            "저자": "Christian Robert",
            "전체 인용횟수": "15089회 인용201320142015201620172018201920202021202220231854587179691177149518012016219421141778",
            "출처": "CHANCE",
            "페이지": "62-63",
            "학술 문서": "Machine learning: a probabilistic perspective*KP Murphy - 201214700회 인용 관련 학술자료 전체 12개의 버전 Machine learning, a probabilistic perspectiveC Robert - 2014411회 인용 관련 학술자료 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Machine learning, a probabilistic perspective",
        "year": null
    },
    "Dynamic bayesian networks: representation, inference and learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2002",
            "기관": "University of California, Berkeley",
            "설명": "Modelling sequential data is important in many areas of science and engineering. Hidden Markov models (HMMs) and Kalman filter models (KFMs) are popular for this because they are simple and flexible. For example, HMMs have been used for speech recognition and bio-sequence analysis, and KFMs have been used for problems ranging from tracking planes and missiles to predicting the economy. However, HMMs and KFMs are limited in their “expressive power”. Dynamic Bayesian Networks (DBNs) generalize HMMs by allowing the state space to be represented in factored form, instead of as a single discrete random variable. DBNs generalize KFMs by allowing arbitrary probability distributions, not just (unimodal) linear-Gaussian. In this thesis, I will discuss how to represent many different kinds of models as DBNs, how to perform exact and approximate inference in DBNs, and how to learn DBN models from …",
            "저자": "Kevin Patrick Murphy",
            "전체 인용횟수": "3862회 인용2003200420052006200720082009201020112012201320142015201620172018201920202021202220236294164161170221258263237246228236242200179176159141150127105",
            "학술 문서": "Dynamic bayesian networks: representation, inference and learningKP Murphy - 20023862회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Dynamic bayesian networks: representation, inference and learning",
        "year": null
    },
    "Speed/accuracy trade-offs for modern convolutional object detectors": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-to-apples comparisons are difficult due to different base feature extractors (eg, VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN (Ren et al., 2015), R-FCN (Dai et al., 2016) and SSD (Liu et al., 2016) systems, which we view as\" meta-architectures\" and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task.",
            "저자": "Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, Kevin Murphy",
            "전체 인용횟수": "3206회 인용201720182019202020212022202397458631667608417281",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "7310-7311",
            "학술 문서": "Speed/accuracy trade-offs for modern convolutional object detectorsJ Huang, V Rathod, C Sun, M Zhu, A Korattikara… - Proceedings of the IEEE conference on computer …, 20173203회 인용 관련 학술자료 전체 13개의 버전 Speed and accuracy trade-offs for modern convolutional object detectors*A Fathi, A Korattikara, C Sun, I Fischer, J Huang… - 20173회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Speed/accuracy trade-offs for modern convolutional object detectors",
        "year": null
    },
    "Loopy belief propagation for approximate inference: An empirical study": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/1/23",
            "설명": "Recently, researchers have demonstrated that loopy belief propagation - the use of Pearls polytree algorithm IN a Bayesian network WITH loops OF error- correcting codes.The most dramatic instance OF this IS the near Shannon - limit performance OF Turbo Codes codes whose decoding algorithm IS equivalent TO loopy belief propagation IN a chain - structured Bayesian network. IN this paper we ask : IS there something special about the error - correcting code context, OR does loopy propagation WORK AS an approximate inference schemeIN a more general setting? We compare the marginals computed using loopy propagation TO the exact ones IN four Bayesian network architectures, including two real - world networks : ALARM AND QMR.We find that the loopy beliefs often converge AND WHEN they do, they give a good approximation TO the correct marginals.However,ON the QMR network, the loopy beliefs oscillated AND had no obvious relationship TO the correct posteriors. We present SOME initial investigations INTO the cause OF these oscillations, AND show that SOME simple methods OF preventing them lead TO the wrong results.",
            "저널": "arXiv preprint arXiv:1301.6725",
            "저자": "Kevin Murphy, Yair Weiss, Michael I Jordan",
            "전체 인용횟수": "2306회 인용2000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220237252350689693102120981039716914513614511612412210197949160",
            "학술 문서": "Loopy belief propagation for approximate inference: An empirical studyK Murphy, Y Weiss, MI Jordan - arXiv preprint arXiv:1301.6725, 20132306회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Loopy belief propagation for approximate inference: An empirical study",
        "year": null
    },
    "Knowledge Vault: A Web-scale approach to probabilistic knowledge fusion": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "설명": "Recent years have witnessed a proliferation of large-scale knowledge bases, including Wikipedia, Freebase, YAGO, Microsoft's Satori, and Google's Knowledge Graph. To increase the scale even further, we need to explore automatic methods for constructing knowledge bases. Previous approaches have primarily focused on text-based extraction, which can be very noisy. Here we introduce Knowledge Vault, a Web-scale probabilistic knowledge base that combines extractions from Web content (obtained via analysis of text, tabular data, page structure, and human annotations) with prior knowledge derived from existing knowledge repositories. We employ supervised machine learning methods for fusing these distinct information sources. The Knowledge Vault is substantially bigger than any previously published structured knowledge repository, and features a probabilistic inference system that computes …",
            "저자": "Xin Luna Dong, K Murphy, E Gabrilovich, G Heitz, W Horn, N Lao, Thomas Strohmann, Shaohua Sun, Wei Zhang",
            "전체 인용횟수": "2129회 인용201420152016201720182019202020212022202325123201240255269262301241188",
            "컨퍼런스": "KDD",
            "학술 문서": "Knowledge vault: A web-scale approach to probabilistic knowledge fusionX Dong, E Gabrilovich, G Heitz, W Horn, N Lao… - Proceedings of the 20th ACM SIGKDD international …, 20142129회 인용 관련 학술자료 전체 23개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Knowledge Vault: A Web-scale approach to probabilistic knowledge fusion",
        "year": null
    },
    "Rao-Blackwellised particle filtering for dynamic Bayesian networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2001/1",
            "게시자": "Springer New York",
            "도서": "Sequential Monte Carlo methods in practice",
            "설명": " Particle filtering in high dimensional state-spaces can be inefficient because a large number of samples is needed to represent the posterior. A standard technique to increase the efficiency of sampling techniques is to reduce the size of the state space by marginalizing out some of the variables analytically; this is called Rao-Blackwellisation (Casella and Robert 1996). Combining these two techniques results in Rao-Blackwellised particle filtering (RBPF) (Doucet 1998, Doucet, de Freitas, Murphy and Russell 2000). In this chapter, we explain RBPF, discuss when it can be used, and give a detailed example of its application to the problem of map learning for a mobile robot, which has a very large (~ 2100) discrete state space.",
            "저자": "Kevin Murphy, Stuart Russell",
            "전체 인용횟수": "1991회 인용2001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202362343507086112107107941031021041111071261091229289776964",
            "페이지": "499-515",
            "학술 문서": "Rao-Blackwellised particle filtering for dynamic Bayesian networksK Murphy, S Russell - Sequential Monte Carlo methods in practice, 20011991회 인용 관련 학술자료 전체 29개의 버전 Rao-blackwellised particle ﬁltering for dynamic bayesian networks*A Doucet, N de Freitas, K Murphy, S Russell - UAl-20003회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Rao-Blackwellised particle filtering for dynamic Bayesian networks",
        "year": null
    },
    "A review of relational machine learning for knowledge graphs": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/12/17",
            "게시자": "IEEE",
            "권": "104",
            "설명": "Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be “trained” on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive data sets. The first is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing …",
            "저자": "Maximilian Nickel, Kevin Murphy, Volker Tresp, Evgeniy Gabrilovich",
            "전체 인용횟수": "1770회 인용2015201620172018201920202021202220232777149197234301311262195",
            "출처": "Proceedings of the IEEE",
            "페이지": "11-33",
            "학술 문서": "A review of relational machine learning for knowledge graphsM Nickel, K Murphy, V Tresp, E Gabrilovich - Proceedings of the IEEE, 20151770회 인용 관련 학술자료 전체 19개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A review of relational machine learning for knowledge graphs",
        "year": null
    },
    "The bayes net toolbox for matlab": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2001",
            "권": "33",
            "저널": "Computing science and statistics",
            "저자": "Kevin Murphy",
            "전체 인용횟수": "1727회 인용2001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202351843557692113861071199497102981129685716862443629",
            "학술 문서": "The bayes net toolbox for matlabK Murphy - Computing science and statistics, 20011727회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The bayes net toolbox for matlab",
        "year": null
    },
    "Deep variational information bottleneck": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/12/1",
            "설명": "We present a variational approximation to the information bottleneck of Tishby et al. (1999). This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training. We call this method \"Deep Variational Information Bottleneck\", or Deep VIB. We show that models trained with the VIB objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack.",
            "저널": "arXiv preprint arXiv:1612.00410",
            "저자": "Alexander A Alemi, Ian Fischer, Joshua V Dillon, Kevin Murphy",
            "전체 인용횟수": "1466회 인용20172018201920202021202220231881146248290339337",
            "학술 문서": "Deep variational information bottleneckAA Alemi, I Fischer, JV Dillon, K Murphy - arXiv preprint arXiv:1612.00410, 20161466회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep variational information bottleneck",
        "year": null
    },
    "Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "Despite the steady progress in video analysis led by the adoption of convolutional neural networks (CNNs), the relative improvement has been less drastic as that in 2D static image classification. Three main challenges exist including spatial (image) feature representation, temporal information representation, and model/computation complexity. It was recently shown by Carreira and Zisserman that 3D CNNs, inflated from 2D networks and pretrained on ImageNet, could be a promising way for spatial and temporal representation learning. However, as for model/computation complexity, 3D CNNs are much more expensive than 2D CNNs and prone to overfit. We seek a balance between speed and accuracy by building an effective and efficient video classification system through systematic exploration of critical network design choices. In particular, we show that it is possible to replace many of the 3D convolutions by low-cost 2D convolutions. Rather surprisingly, best result (in both speed and accuracy) is achieved when replacing the 3D convolutions at the bottom of the network, suggesting that temporal representation learning on high-level``semantic''features is more useful. Our conclusion generalizes to datasets with very different properties. When combined with several other cost-effective designs including separable spatial/temporal convolution and feature gating, our system results in an effective video classification system that that produces very competitive results on several action classification benchmarks (Kinetics, Something-something, UCF101 and HMDB), as well as two action detection (localization) benchmarks (JHMDB and UCF101 …",
            "저자": "Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, Kevin Murphy",
            "전체 인용횟수": "1321회 인용2018201920202021202220231094219323347322",
            "컨퍼런스": "Proceedings of the European conference on computer vision (ECCV)",
            "페이지": "305-321",
            "학술 문서": "Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classificationS Xie, C Sun, J Huang, Z Tu, K Murphy - Proceedings of the European conference on computer …, 20181321회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification",
        "year": null
    },
    "Videobert: A joint model for video and language representation learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "Self-supervised learning has become increasingly important to leverage the abundance of unlabeled data available on platforms like YouTube. Whereas most existing approaches learn low-level representations, we propose a joint visual-linguistic model to learn high-level features without any explicit supervision. In particular, inspired by its recent success in language modeling, we build upon the BERT model to learn bidirectional joint distributions over sequences of visual and linguistic tokens, derived from vector quantization of video data and off-the-shelf speech recognition outputs, respectively. We use VideoBERT in numerous tasks, including action classification and video captioning. We show that it can be applied directly to open-vocabulary classification, and confirm that large amounts of training data and cross-modal information are critical to performance. Furthermore, we outperform the state-of-the-art on video captioning, and quantitative results verify that the model learns high-level semantic features.",
            "저자": "Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, Cordelia Schmid",
            "전체 인용횟수": "1139회 인용2019202020212022202317165276373305",
            "컨퍼런스": "Proceedings of the IEEE/CVF international conference on computer vision",
            "페이지": "7464-7473",
            "학술 문서": "Videobert: A joint model for video and language representation learningC Sun, A Myers, C Vondrick, K Murphy, C Schmid - Proceedings of the IEEE/CVF international conference …, 20191139회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Videobert: A joint model for video and language representation learning",
        "year": null
    },
    "Sharing visual features for multiclass and multiview object detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2007/3/19",
            "게시자": "IEEE",
            "권": "29",
            "설명": "We consider the problem of detecting a large number of different classes of objects in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, at multiple locations and scales. This can be slow and can require a lot of training data since each classifier requires the computation of many different image features. In particular, for independently trained detectors, the (runtime) computational complexity and the (training-time) sample complexity scale linearly with the number of classes to be detected. We present a multitask learning procedure, based on boosted decision stumps, that reduces the computational and sample complexity by finding common features that can be shared across the classes (and/or views). The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required and, therefore, the …",
            "저널": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "저자": "Antonio Torralba, Kevin P Murphy, William T Freeman",
            "전체 인용횟수": "973회 인용200620072008200920102011201220132014201520162017201820192020202120222023213963849186103977563623930261813236",
            "페이지": "854-869",
            "학술 문서": "Sharing visual features for multiclass and multiview object detectionA Torralba, KP Murphy, WT Freeman - IEEE Transactions on Pattern Analysis and Machine …, 2007973회 인용 관련 학술자료 전체 33개의 버전 ",
            "호": "5"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Sharing visual features for multiclass and multiview object detection",
        "year": null
    },
    "Towards accurate multi-person pose estimation in the wild": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "We propose a method for multi-person detection and 2-D pose estimation that achieves state-of-art results on the challenging COCO keypoints task. It is a simple, yet powerful, top-down approach consisting of two stages. In the first stage, we predict the location and scale of boxes which are likely to contain people; for this we use the Faster RCNN detector. In the second stage, we estimate the keypoints of the person potentially contained in each proposed bounding box. For each keypoint type we predict dense heatmaps and offsets using a fully convolutional ResNet. To combine these outputs we introduce a novel aggregation procedure to obtain highly localized keypoint predictions. We also use a novel form of keypoint-based Non-Maximum-Suppression (NMS), instead of the cruder box-level NMS, and a novel form of keypoint-based confidence score estimation, instead of box-level scoring. Trained on COCO data alone, our final system achieves average precision of 0.649 on the COCO test-dev set and the 0.643 test-standard sets, outperforming the winner of the 2016 COCO keypoints challenge and other recent state-of-art. Further, by using additional in-house labeled data we obtain an even higher average precision of 0.685 on the test-dev set and 0.673 on the test-standard set, more than 5% absolute improvement compared to the previous best performing method on the same dataset.",
            "저자": "George Papandreou, Tyler Zhu, Nori Kanazawa, Alexander Toshev, Jonathan Tompson, Chris Bregler, Kevin Murphy",
            "전체 인용횟수": "960회 인용20172018201920202021202220232680134151211204144",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "4903-4911",
            "학술 문서": "Towards accurate multi-person pose estimation in the wildG Papandreou, T Zhu, N Kanazawa, A Toshev… - Proceedings of the IEEE conference on computer …, 2017960회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Towards accurate multi-person pose estimation in the wild",
        "year": null
    },
    "Naive bayes classifiers": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006/10",
            "권": "18",
            "설명": "A classifier is a function f that maps input feature vectors x∈ X to output class labels y∈{1,..., C}, where X is the feature space. We will typically assume X= IRD or X={0, 1} D, ie, that the feature vector is a vector of D real numbers or D binary bits, but in general, we may mix discrete and continuous features. We assume the class labels are unordered (categorical) and mutually exclusive.(If we allow an input to belong to multiple classes, this is called a multi-label problem.) Our goal is to learn f from a labeled training set of N input-output pairs,(xn, yn), n= 1: N; this is an example of supervised learning.We will focus our attention on probabilistic classifiers, ie, methods that return p (y| x).(We will discuss the advantages of having probabilities below.) There are two main ways to do this. The first is to directly learn the function that computes the class posterior p (y| x): this is called a discriminative model, since it discriminates between different classes given the input. The alternative is to learn the class-conditional density p (x| y) for each value of y, and to learn the class priors p (y); then one can apply Bayes rule to compute the posterior",
            "저널": "University of British Columbia",
            "저자": "Kevin P Murphy",
            "전체 인용횟수": "900회 인용2011201220132014201520162017201820192020202120222023311111741638611210410710012999",
            "페이지": "1-8",
            "학술 문서": "Naive bayes classifiersKP Murphy - University of British Columbia, 2006900회 인용 관련 학술자료 전체 10개의 버전 ",
            "호": "60"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Naive bayes classifiers",
        "year": null
    },
    "Performance pay and top-management incentives": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1990/4/1",
            "게시자": "The University of Chicago Press",
            "권": "98",
            "설명": "Our estimates of the pay-performance relation (including pay, options, stockholdings, and dismissal) for chief executive officers indicate that CEO wealth changes $3.25 for every $1,000 change in shareholder wealth. Although the incentives generated by stock ownership are large relative to pay and dismissal incentives, most CEOs hold trivial fractions of their firm's stock, and ownership levels have declined over the past 50 years. We hypothesize that public and private political forces impose constraints that reduce the pay-performance sensitivity. Declines in both the pay-performance relation and the level of CEO pay since the 1930s are consistent with this hypothesis.",
            "저널": "Journal of political economy",
            "저자": "Michael C Jensen, Kevin J Murphy",
            "전체 인용횟수": "10775회 인용19911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202348636775100102123141173195189234300277329367384478473494514554591562545489486438387421344361309",
            "페이지": "225-264",
            "학술 문서": "Performance pay and top-management incentivesMC Jensen, KJ Murphy - Journal of political economy, 199010741회 인용 관련 학술자료 전체 25개의 버전 Murphy KJ (1990)*M Jensen - Performance pay and top-management incentives, 8160회 인용 관련 학술자료 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Performance pay and top-management incentives",
        "year": null
    },
    "Executive compensation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1999/1/1",
            "게시자": "Elsevier",
            "권": "3",
            "설명": "This chapter summarizes the empirical and theoretical research on executive compensation and provides a comprehensive and up-to-date description of pay practices (and trends in pay practices) for chief executive officers (CEOs). Topics discussed include the level and structure of CEO pay (including detailed analyses of annual bonus plans, executive stock options, and option valuation), international pay differences, the pay-setting process, the relation between CEO pay and firm performance (“pay-performance sensitivities”), the relation between sensitivities and subsequent firm performance, relative performance evaluation, executive turnover, and the politics of CEO pay.",
            "저자": "Kevin J Murphy",
            "전체 인용횟수": "5781회 인용19992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220232868104160220183229255270310300326339321326324285280251263214206155170120",
            "출처": "Handbook of labor economics",
            "페이지": "2485-2563",
            "학술 문서": "Executive compensationKJ Murphy - Handbook of labor economics, 19995753회 인용 관련 학술자료 전체 17개의 버전 Handbook of labor economics*KJ Murphy, O Ashenfelter, D Card - Executive compensation, 199992회 인용 관련 학술자료 pExecutive Compensationqin Orley Ashenfelter and David Card, eds*KJ Murphy - Handbook of Labor Economics, 19997회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Executive compensation",
        "year": null
    },
    "Compensation and incentives: Practice vs. theory": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1988/7",
            "게시자": "Blackwell Publishing Ltd",
            "권": "43",
            "설명": " A thorough understanding of internal incentive structures is critical to developing a viable theory of the firm, since these incentives determine to a large extent how individuals inside an organization behave. Many common features of organizational incentive systems are not easily explained by traditional economic theory—including egalitarian pay systems in which compensation is largely independent of performance, the overwhelming use of promotion‐based incentive systems, the absence of up‐front fees for jobs and effective bonding contracts, and the general reluctance of employers to fire, penalize, or give poor performance evaluations to employees. Typical explanations for these practices offered by behaviorists and practitioners are distinctly uneconomic—focusing on notions such as fairness, equity, morale, trust, social responsibility, and culture. The challenge to economists is to provide viable economic …",
            "저널": "The journal of Finance",
            "저자": "George P Baker, Michael C Jensen, Kevin J Murphy",
            "전체 인용횟수": "3389회 인용1989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202310382420405447595766537066728697971181191301491591531521681331291591331321441191049879",
            "페이지": "593-616",
            "학술 문서": "Compensation and incentives: Practice vs. theoryGP Baker, MC Jensen, KJ Murphy - The journal of Finance, 19883389회 인용 관련 학술자료 전체 14개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Compensation and incentives: Practice vs. theory",
        "year": null
    },
    "Corporate performance and managerial remuneration: An empirical analysis": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1985/4/1",
            "게시자": "North-Holland",
            "권": "7",
            "설명": "Economic theories of efficient compensation predict a positive relationship between executive pay and corporate performance, and yet efforts to document this relationship have been largely unsuccessful. In this paper, we argue that previous cross-sectional studies have omitted important variables which seriously bias their results. Using data that focus on individual executives over time, we find that executive compensation is strongly positively related to corporate performance as measured by shareholder return and growth in firm sales. The results are robust to the stock market performance measure utilized.",
            "저널": "Journal of accounting and economics",
            "저자": "Kevin J Murphy",
            "전체 인용횟수": "3258회 인용198519861987198819891990199119921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023910142520383631443759546969796683789085102113901271051241391671551341461471241409396748847",
            "페이지": "11-42",
            "학술 문서": "Corporate performance and managerial remuneration: An empirical analysisKJ Murphy - Journal of accounting and economics, 19853258회 인용 관련 학술자료 전체 5개의 버전 ",
            "호": "1-3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Corporate performance and managerial remuneration: An empirical analysis",
        "year": null
    },
    "Relational Contracts and the Theory of the Firm": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2002/2/1",
            "게시자": "MIT Press",
            "권": "117",
            "설명": " Relational contracts—informal agreements sustained by the value of future relationships—are prevalent within and between firms. We develop repeated-game models showing why and how relational contracts within firms (vertical integration) differ from those between (nonintegration). We show that integration affects the parties' temptations to renege on a given relational contract, and hence affects the best relational contract the parties can sustain. In this sense, the integration decision can be an instrument in the service of the parties' relationship. Our approach also has implications for joint ventures, alliances, and networks, and for the role of management within and between firms.",
            "저널": "The Quarterly Journal of Economics",
            "저자": "George Baker, Robert Gibbons, Kevin J Murphy",
            "전체 인용횟수": "2717회 인용20012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023273869699411213315516419617514712912011114511811611811610010789",
            "페이지": "39-84",
            "학술 문서": "Relational Contracts and the Theory of the FirmG Baker, R Gibbons, KJ Murphy - The Quarterly Journal of Economics, 20022717회 인용 관련 학술자료 전체 21개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Relational Contracts and the Theory of the Firm",
        "year": null
    },
    "Optimal incentive contracts in the presence of career concerns: Theory and evidence": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1992/6/1",
            "게시자": "The University of Chicago Press",
            "권": "100",
            "설명": "This paper studies optimal incentive contracts when workers have career concerns--concerns about the effects of current performance on future compensation. We show that the optimal compensation contract optimizes total incentives: the combination of the implicit incentives from career concerns and the explicit incentives from the compensation contract. Thus the explicit incentives from the optimal compensation contract should be strongest for workers close to retirement because career concerns are weakest for these workers. We find empirical support for this prediction in the relation between chief executive compensation and stock market performance.",
            "저널": "Journal of political Economy",
            "저자": "Robert Gibbons, Kevin J Murphy",
            "전체 인용횟수": "2615회 인용19921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023713192122222642384860876610110886921101111381181191229712612498113103104127117",
            "페이지": "468-505",
            "학술 문서": "Optimal incentive contracts in the presence of career concerns: Theory and evidenceR Gibbons, KJ Murphy - Journal of political Economy, 19922615회 인용 관련 학술자료 전체 17개의 버전 Optimal Incentive Contracts in the Presence of Career Concerns: Theory and Evidence*R Gibbons, KJ Murphy - … LIBRARY OF CRITICAL WRITINGS IN ECONOMICS, 1999",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Optimal incentive contracts in the presence of career concerns: Theory and evidence",
        "year": null
    },
    "CEO incentives: It's not how much you pay, but how": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1990/9",
            "설명": "Paying top executives better would eventually mean paying them more. The arrival of spring means yet another round in the national debate over executive compensation. Soon the business press will trumpet answers to the questions it asks every year: Who were the highest paid CEOs? How many executives made more than a million dollars? Who received the biggest raises? Political figures, union leaders, and consumer activists will issue now-familiar denunciations of executive salaries and urge that directors curb top-level pay in the interests of social equity and statesmanship. The critics have it wrong. There are serious problems with CEO compensation, but excessive pay is not the biggest issue. The relentless focus on how much CEOs are paid diverts public attention from the real problem--how CEOs are paid. In most publicly held companies, the compensation of top executives is virtually independent of performance. On average, corporate America pays its most important leaders like bureaucrats. Is it any wonder then that so many CEOs act like bureaucrats rather than the value-maximizing entrepreneurs companies need to enhance their standing in world markets? We recently completed an in-depth statistical analysis of executive compensation. Our study incorporates data on thousands of CEOs spanning five decades. The base sample consists of information on salaries and bonuses for 2,505 CEOs in 1,400 publicly held companies from 1974 through 1988. We also collected data on stock options and stock ownership for CEOs of the 430 largest publicly held companies in 1988. In addition, we drew on compensation data for …",
            "저자": "Michael C Jensen, Kevin J Murphy",
            "전체 인용횟수": "2238회 인용1991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220231213212329182434374048366344747567808995102134145111111118821029380806846",
            "학술 문서": "CEO incentives: It's not how much you pay, but howMC Jensen, KJ Murphy - 19902075회 인용 관련 학술자료 전체 13개의 버전 CEO incentives-It's not how much you pay, but how*MC Jensen, KJ Murphy - Journal of Applied Corporate Finance, 2010195회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "CEO incentives: It's not how much you pay, but how",
        "year": null
    },
    "Subjective performance measures in optimal incentive contracts": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1994/11/1",
            "게시자": "MIT Press",
            "권": "109",
            "설명": " Incentive contracts often include important subjective components that mitigate incentive distortions caused by imperfect objective measures. This paper explores the combined use of subjective and objective performance measures in (respectively) implicit and explicit incentive contracts. We show that the presence of sufficiently effective explicit contracts can render all implicit contracts infeasible, even those that would otherwise yield the first-best. We also show, however, that in some circumstances objective and subjective measures are complements: neither an explicit nor an implicit contract alone yields positive profit, but an appropriate combination of the two does. Finally, we consider subjective weights on objective measures.",
            "저널": "The quarterly journal of economics",
            "저자": "George Baker, Robert Gibbons, Kevin J Murphy",
            "전체 인용횟수": "2058회 인용199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220236611183128456273627310610297100106106102100868310589927072756159",
            "페이지": "1125-1156",
            "학술 문서": "Subjective performance measures in optimal incentive contractsG Baker, R Gibbons, KJ Murphy - The quarterly journal of economics, 19942058회 인용 관련 학술자료 전체 23개의 버전 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Subjective performance measures in optimal incentive contracts",
        "year": null
    },
    "Stock options for undiversified executives": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2002/2/1",
            "게시자": "North-Holland",
            "권": "33",
            "설명": "We employ a certainty-equivalence framework to analyze the cost, value and pay/performance sensitivity of non-tradable options held by undiversified, risk-averse executives. We derive “executive value” lines, the risk-adjusted analogues to Black–Scholes lines. We show that distinguishing between “executive value” and “company cost” provides insight into many issues regarding stock option practice including: executive views about Black–Scholes values; tradeoffs between options, restricted stock and cash; exercise price policies; option repricings; early exercise policies and decisions; and the length of vesting periods. It also leads to reinterpretations of both cross-sectional facts and longitudinal trends in the level of executive compensation.",
            "저널": "Journal of accounting and economics",
            "저자": "Brian J Hall, Kevin J Murphy",
            "전체 인용횟수": "1849회 인용200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220233344646510110110999109123110978490818069697066426449",
            "페이지": "3-42",
            "학술 문서": "Stock options for undiversified executivesBJ Hall, KJ Murphy - Journal of accounting and economics, 20021847회 인용 관련 학술자료 전체 20개의 버전 Stock Options for Undiversiﬁed Executives*BJ Hall, KJ Murphy - NBER working paper, 20006회 인용 관련 학술자료 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Stock options for undiversified executives",
        "year": null
    },
    "Relative performance evaluation for chief executive officers": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1990/4",
            "게시자": "SAGE Publications",
            "권": "43",
            "설명": "Relative performance evaluation (RPE) provides employees with an incentive to perform well while insulating their compensation from shocks that also affect the performances of other workers in the same firm, industry, or market. This paper reviews the benefits and costs of RPE and tests for the presence of RPE in the compensation contracts of chief executive officers (CEOs) using data on 1,668 CEOs from 1,049 corporations from 1974 to 1986. The results, in contrast to the findings of previous research, strongly support the hypothesis that RPE is used in compensation and retention decisions affecting CEOs: the revision in a CEO's pay and the probability that a CEO remains in his position for the following year are positively and significantly related to firm performance, but are negatively and significantly related to industry and market performance.",
            "저자": "Robert Gibbons, Kevin J Murphy",
            "전체 인용횟수": "1531회 인용199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202311131617182225372439474949615048615462596261728661598153412740284236",
            "출처": "ILR Review",
            "페이지": "30-S-51-S",
            "학술 문서": "Relative performance evaluation for chief executive officersR Gibbons, KJ Murphy - ILR Review, 19901531회 인용 관련 학술자료 전체 25개의 버전 Relative Performance Evaluation for Chief Executive Officers*R Gibbons, KJ Murphy - … LIBRARY OF CRITICAL WRITINGS IN ECONOMICS, 1999",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Relative performance evaluation for chief executive officers",
        "year": null
    },
    "Financial performance surrounding CEO turnover": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1993/1/1",
            "게시자": "North-Holland",
            "권": "16",
            "설명": "We document the behavior of a variety of financial variables surrounding CEO departures, and estimate the extent to which changes in potentially discretionary variables are explained by poor economic performance rather than direct managerial discretion. We conclude that turnover-related changes in R&D, advertising, capital expenditures, and accounting accruals are due mostly to poor performance. To the extent that outgoing or incoming managers exercise discretion over these variables, the discretion appears to be limited to firms where the CEO's departure is preceded by poor performance. We find no evidence of managerial discretion in strongly performing firms where the CEO retires as part of the normal succession process.",
            "저널": "Journal of Accounting and Economics",
            "저자": "Kevin J Murphy, Jerold L Zimmerman",
            "전체 인용횟수": "1397회 인용1994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220237810151820302532434045384754716469745453698367615958585953",
            "페이지": "273-315",
            "학술 문서": "Financial performance surrounding CEO turnoverKJ Murphy, JL Zimmerman - Journal of Accounting and Economics, 19931397회 인용 관련 학술자료 전체 4개의 버전 ",
            "호": "1-3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Financial performance surrounding CEO turnover",
        "year": null
    },
    "The trouble with stock options": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2003/9/1",
            "게시자": "American Economic Association",
            "권": "17",
            "설명": " The benefits of stock options are often not large enough to offset the inefficiency implied by the large divergence between the cost of options to companies and the value of options to risk-averse, undiversified executives and employees. Moreover, the benefits of options can often be achieved more effectively and economically through other means. Why are options so prevalent? Several explanations include changes in corporate governance, reporting requirements, taxes, the bull market and managerial rent-seeking. We offer an alternative hypothesis: boards and managers incorrectly perceive stock options to be inexpensive because options create no accounting charge and require no cash outlay.",
            "저널": "Journal of economic perspectives",
            "저자": "Brian J Hall, Kevin J Murphy",
            "전체 인용횟수": "1273회 인용200320042005200620072008200920102011201220132014201520162017201820192020202120222023214375768176858491726662556264524751353524",
            "페이지": "49-70",
            "학술 문서": "The trouble with stock optionsBJ Hall, KJ Murphy - Journal of economic perspectives, 20031273회 인용 관련 학술자료 전체 23개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The trouble with stock options",
        "year": null
    },
    "Remuneration: Where we've been, how we got to here, what are the problems, and how to fix them": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004/7/12",
            "게시자": "Harvard NOM working paper",
            "설명": "Currently, we are in the midst of a reexamination of chief executive officer (CEO) remuneration that has more than the usual amount of energy and substance. While much of the fury over CEO pay has been aimed at executives associated with accounting scandals and collapses in the prices of their company's shares, the controversies over GE CEO Jack Welch and NYSE CEO Richard Grasso signal a watershed. In their cases the competence and performance of both men were unquestioned: the issue seems to be the perception that they received\" too much\" and that there was inadequate disclosure.",
            "저자": "Michael C Jensen, Kevin J Murphy, Eric G Wruck",
            "전체 인용횟수": "1067회 인용20042005200620072008200920102011201220132014201520162017201820192020202120222023329466574767985806472656147542934303322",
            "학술 문서": "Remuneration: Where we've been, how we got to here, what are the problems, and how to fix themMC Jensen, KJ Murphy, EG Wruck - 20041067회 인용 관련 학술자료 전체 10개의 버전 ",
            "호": "04-28"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Remuneration: Where we've been, how we got to here, what are the problems, and how to fix them",
        "year": null
    },
    "The prince and the pauper? CEO pay in the United States and United Kingdom": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2000/11",
            "게시자": "Blackwell Publishers Ltd",
            "권": "110",
            "설명": "We document differences in CEO pay and incentives in the United States and the United Kingdom for 1997. After controlling for size, sector and other firm and executive characteristics, CEOs in the US earn 45% higher cash compensation and 190% higher total compensation. The calculated effective ownership percentage in the US implies that the median CEO receives 1.48% of any increase in shareholder wealth compared to 0.25% in the UK. The differences, can be largely attributed to greater share option awards in the US arising from institutional and cultural differences between the two countries.",
            "저널": "The Economic Journal",
            "저자": "Martin J Conyon, Kevin J Murphy",
            "전체 인용횟수": "938회 인용200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220231416412733485562455861534850425638433337252618",
            "페이지": "640-671",
            "학술 문서": "The prince and the pauper? CEO pay in the United States and United KingdomMJ Conyon, KJ Murphy - The Economic Journal, 2000938회 인용 관련 학술자료 전체 7개의 버전 ",
            "호": "467"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The prince and the pauper? CEO pay in the United States and United Kingdom",
        "year": null
    },
    "CEO pay and appointments: A market-based explanation for recent trends": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004/5/1",
            "게시자": "American Economic Association",
            "권": "94",
            "설명": "Very few business topics attract as much public attention as the paychecks of top executive officers in the largest US companies. Undoubtedly, part of this interest has been fueled by the large and continuous increases in chief executive officers’(CEOs’) compensation over the past three decades. Even ignoring the more recent escalation in the use of executive stock options (Brian Hall and Murphy, 2002, 2003), the base salaries and bonuses of Forbes 800 CEOs increased from an average of $700,000 in 1970 (in 2002-constant dollars) to over $2.2 million in 2000. 1 During the same period, the ratio of CEO cash compensation to average pay for production workers increased from about 25 in 1970 to nearly 90 in 2000. 2 The most prevalent explanation in the popular press for this trend is the “fat cat” theory, a variant of which has been espoused among academics by Lucian Bebchuk et al.(2002). 3 According to this …",
            "저널": "American economic review",
            "저자": "Kevin J Murphy, Jan Zabojnik",
            "전체 인용횟수": "891회 인용200520062007200820092010201120122013201420152016201720182019202020212022202314162130304254635962565359655751445644",
            "페이지": "192-196",
            "학술 문서": "CEO pay and appointments: A market-based explanation for recent trendsKJ Murphy, J Zabojnik - American economic review, 2004891회 인용 관련 학술자료 전체 10개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "CEO pay and appointments: A market-based explanation for recent trends",
        "year": null
    },
    "Executive compensation: Where we are, and how we got there": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/1/1",
            "게시자": "Elsevier",
            "권": "2",
            "도서": "Handbook of the Economics of Finance",
            "설명": "In this study, I summarize the current state of executive compensation, discuss measurement and incentive issues, document recent trends in executive pay in both U.S. and international firms, and analyze the evolution of executive pay over the past century. Most recent analyses of executive compensation have focused on efficient-contracting or managerial-power rationales for pay, while ignoring or downplaying the causes and consequences of disclosure requirements, tax policies, accounting rules, legislation, and the general political climate. A major theme of this study is that government intervention has been both a response to and a major driver of time trends in executive compensation over the past century, and that any explanation for pay that ignores political factors is critically incomplete.",
            "저자": "Kevin J Murphy",
            "전체 인용횟수": "822회 인용2012201320142015201620172018201920202021202220231035496063104887891818069",
            "페이지": "211-356",
            "학술 문서": "Executive compensation: Where we are, and how we got thereKJ Murphy - Handbook of the Economics of Finance, 2013822회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Executive compensation: Where we are, and how we got there",
        "year": null
    },
    "Performance standards in incentive contracts": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2000/12/1",
            "게시자": "North-Holland",
            "권": "30",
            "설명": "Research in incentives has focused on performance measures and pay-performance sensitivities but has largely ignored the “performance standard”, which generates important incentives whenever plan participants can influence the standard-setting process. “Internally determined” standards are directly affected by management actions in the current or prior year, while “externally determined” standards are less easily affected. I show that companies choose external standards when prior performance is a noisy estimate of contemporaneous performance. In addition, companies using budget based and other internally determined performance standards have less-variable bonus payouts, and are more likely to smooth earnings, than companies using externally determined standards.",
            "저널": "Journal of Accounting and Economics",
            "저자": "Kevin J Murphy",
            "전체 인용횟수": "804회 인용1999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023531210292224332838423832384343523945443637392834",
            "페이지": "245-278",
            "학술 문서": "Performance standards in incentive contractsKJ Murphy - Journal of Accounting and Economics, 2000804회 인용 관련 학술자료 전체 7개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Performance standards in incentive contracts",
        "year": null
    },
    "Incentives, learning, and compensation: A theoretical and empirical investigation of managerial labor contracts": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1986/4/1",
            "게시자": "The RAND Corporation",
            "설명": "This article analyzes properties and implications of multiperiod managerial labor contracts under two alternative hypotheses: incentives, in which productivity depends on unobservable effort, and learning, in which ability is unknown and is revealed over time. Shared and conflicting implications of these competing models are developed in terms of experience-earnings profiles and the relation between compensation and performance. I empirically examine these theoretical results by using a longitudinal sample of 1,488 chief executive officers followed from 1974-1985. The data yield mixed evidence that generally supports the learning hypothesis over the incentive hypothesis.",
            "저널": "The Rand Journal of Economics",
            "저자": "Kevin J Murphy",
            "전체 인용횟수": "763회 인용198719881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202334412551210161219192127233021123535243237383831402630231718141521178",
            "페이지": "59-76",
            "학술 문서": "Incentives, learning, and compensation: A theoretical and empirical investigation of managerial labor contractsKJ Murphy - The Rand Journal of Economics, 1986763회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Incentives, learning, and compensation: A theoretical and empirical investigation of managerial labor contracts",
        "year": null
    },
    "Managerial capital and the market for CEOs": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2007/4",
            "설명": "This paper reconciles three pronounced trends in US corporate governance: the increase in pay levels for top executives, the increasing prevalence of appointing CEOs through external hiring rather than internal promotions, and the increased prevalence of hiring outside CEOs with prior experience as CEOs. We propose that these trends reflect a shift in the relative importance of\" managerial ability\"(CEO skills transferable across companies) and\" firm-specific human capital\"(valuable only within the organization). We show that if the supply of workers in the corporate sector is relatively elastic, an increase in the relative importance of managerial ability leads to fewer promotions, more external hires, and an increase in equilibrium average wages for CEOs. We test our model using CEO pay and turnover data from 1970 to 2000. We show that CEO compensation is higher for CEOs hired from outside their firm, and for CEOs in industries where outside hiring is prevalent.",
            "저널": "Available at SSRN 984376",
            "저자": "Kevin J Murphy, Jan Zabojnik",
            "전체 인용횟수": "683회 인용200620072008200920102011201220132014201520162017201820192020202120222023132029244050435252393749384123333436",
            "학술 문서": "Managerial capital and the market for CEOsKJ Murphy, J Zabojnik - Available at SSRN 984376, 2007683회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Managerial capital and the market for CEOs",
        "year": null
    },
    "Informal authority in organizations": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1999/3/1",
            "게시자": "Oxford University Press",
            "권": "15",
            "설명": " We assert that decision rights in organizations are not contractible: the boss can always overturn a subordinate's decision, so formal authority resides only at the top. Although decision rights cannot be formally delegated, they might be informally delegated through self-enforcing relational contracts. We examine the feasibility of informal authority in two informational environments. We show that different information structures produce different decisions not only because different information is brought to bear in the decision-making process, but also because different information creates different temptations to renege on relational contracts. In addition, we explore the implications of formal delegation achieved through divestitures.",
            "저널": "Journal of Law, Economics, and organization",
            "저자": "George Baker, Robert Gibbons, Kevin J Murphy",
            "전체 인용횟수": "669회 인용19992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220237112425241835272930323030263736333619382521301714",
            "페이지": "56-73",
            "학술 문서": "Informal authority in organizationsG Baker, R Gibbons, KJ Murphy - Journal of Law, Economics, and organization, 1999669회 인용 관련 학술자료 전체 17개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Informal authority in organizations",
        "year": null
    },
    "Psychological testing": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1988",
            "권": "18",
            "설명": "Psychological Testing Page 1 Psychological Testing Principles and Applications Fourth \nEdition Kevin R. Murphy Colorado State University Charles O. Davidshofer Colorado State \nUniversity Prentice Hall, Upper Saddle River, New Jersey 07458 Page 2 Contents \nPREFACE xiii Part I Introduction to Psychological Testing 1 TESTS AND MEASUREMENTS \n1 Psychological Tests—A Definition 3 Types of Tests 6 Tests and Decisions—Uses of \nPsychological Tests 9 Testing Activities of Psychologists 12 Information about Tests 13 \nStandards for Testing 15 Critical Discussion: Alternatives to Psychological Tests 17 \nSummary 18 Key Terms 18 2 DEFINING AND MEASURING PSYCHOLOGICAL \nATTRIBUTES: ABILITY, INTERESTS, AND PERSONALITY 1 9 Psychological Attributes and \nDecisions 19 Intelligence—General Mental Ability 20 Page 3 vi Contents Critical Discussion: \nIQorEQ? 34 Interests 35 Personality 39 Critical …",
            "저널": "Principles, and Applications, Englewood Cliffs",
            "저자": "Kevin R Murphy, Charles O Davidshofer",
            "전체 인용횟수": "3032회 인용1993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023151810242632494249526774761051098412010113314315816217718215216215516313112880",
            "학술 문서": "Psychological testingKR Murphy, CO Davidshofer - Principles, and Applications, Englewood Cliffs, 19883032회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Psychological testing",
        "year": null
    },
    "Understanding performance appraisal: Social, organizational, and goal-based perspectives": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1995/1/3",
            "게시자": "Sage",
            "설명": "Based on a previous book by the same authors, Understanding Performance Appraisal delineates a social-psychological model of the appraisal process that emphasizes the goals pursued by raters, ratees, and the various users of performance appraisal. The authors apply this goal-oriented perspective to developing, implementing, and evaluating performance appraisal systems. This perspective also emphasizes the context in which appraisal occurs and demonstrates that the shortcomings of performance appraisal are in fact sensible adaptations to its various requirements, pressures, and demands. Relevant research is summarized and recommendations are offered for future research and applications. Graduate-level students, organizational development consultants and trainers, human resource managers, faculty and scholars, and psychologists in human resource management as well as other professionals who conduct research on performance appraisal programs will find this book not only interesting but also a valuable resource.",
            "저자": "Kevin R Murphy, Jeanette N Cleveland",
            "전체 인용횟수": "2738회 인용19961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220231434304957565180708881881021081071351401511301601581451461281201068480",
            "학술 문서": "Understanding performance appraisal: Social, organizational, and goal-based perspectivesKR Murphy, JN Cleveland - 19952738회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Understanding performance appraisal: Social, organizational, and goal-based perspectives",
        "year": null
    },
    "Statistical power analysis: A simple and general model for traditional and modern hypothesis tests": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/10/18",
            "게시자": "Routledge",
            "설명": "Noted for its accessible approach, this bestseller applies power analysis to both null hypothesis and minimum-effect testing using the same basic model. Through the use of a few relatively simple procedures and examples from the behavioral and social sciences, the authors show readers with little expertise in statistical analysis how to quickly obtain the values needed to carry out the power analysis for their research. Illustrations of how these analyses work and how they can be used to understand problems of study design, to evaluate research, and to choose the appropriate criterion for defining \"statistically significant\" outcomes are sprinkled throughout. The book presents a simple and general model for statistical power analysis that is based on the F statistic.  Statistical Power Analysis reviews how to determine:  The sample size needed to achieve desired levels of power  The level of power needed in a study …",
            "저자": "Brett Myors, Kevin R Murphy, Allen Wolach",
            "전체 인용횟수": "2049회 인용19981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023661629364957658172897385958599106108105131136115921068690",
            "학술 문서": "Statistical power analysis: A simple and general model for traditional and modern hypothesis testsB Myors, KR Murphy, A Wolach - 20102049회 인용 관련 학술자료 전체 10개의 버전 Statistical power analysis: A simple and general model for traditional and modern hypothesis tests*N Hagenes, F Lang - 1999"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Statistical power analysis: A simple and general model for traditional and modern hypothesis tests",
        "year": null
    },
    "Performance appraisal: An organizational perspective.": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1991",
            "게시자": "Allyn & Bacon",
            "설명": "Jan Cleveland suggested that a simpler model of the appraisal process was needed.... That model, with several modifications and elaborations, became the four-component model of the appraisal process in organizations that is used to organize this book.",
            "저자": "Kevin R Murphy, Jeanette N Cleveland",
            "전체 인용횟수": "1195회 인용199119921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023111524312936402630303727304237292923463545536057424952443750293229",
            "학술 문서": "Performance appraisal: An organizational perspective.KR Murphy, JN Cleveland - 19911189회 인용 관련 학술자료 전체 2개의 버전 Performance appraisal.*KR Murphy, PJ Deckert - 20136회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Performance appraisal: An organizational perspective.",
        "year": null
    },
    "Reconsidering the use of personality tests in personnel selection contexts": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2007/9",
            "게시자": "Blackwell Publishing Inc",
            "권": "60",
            "설명": "Although long thought to be unrelated to job performance, research in the early 1990s provided evidence that personality can predict job performance. Accompanying this research was a resurgence of interest in the use of personality tests in high‐stakes selection environments. Yet there are numerous potential problems associated with the current operational use of personality. As such, 5 former journal editors from Personnel Psychology and the Journal of Applied Psychology (2 primary outlets for such research), who have collectively reviewed over 7,000 manuscripts and who have no vested interest in personality testing, reconsider the research on the use of personality tests in environments where important selection decisions are made. Their comments are based on a panel discussion held at the 2004 SIOP conference. Collectively, they come to several conclusions. First, faking on self‐report personality tests …",
            "저널": "Personnel psychology",
            "저자": "Frederick P Morgeson, Michael A Campion, Robert L Dipboye, John R Hollenbeck, Kevin Murphy, Neal Schmitt",
            "전체 인용횟수": "1032회 인용20072008200920102011201220132014201520162017201820192020202120222023737435278927295715893585348646230",
            "페이지": "683-729",
            "학술 문서": "Reconsidering the use of personality tests in personnel selection contextsFP Morgeson, MA Campion, RL Dipboye… - Personnel psychology, 20071032회 인용 관련 학술자료 전체 11개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Reconsidering the use of personality tests in personnel selection contexts",
        "year": null
    },
    "Multiple uses of performance appraisal: Prevalence and correlates.": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1989/2",
            "게시자": "American Psychological Association",
            "권": "74",
            "설명": "Performance appraisal is used in organizations for a variety of purposes. However, little empirical research has been conducted to determine (a) the extent to which performance appraisal is used for each of several purposes in industry,(b) the extent to which appraisal data may be used for multiple and possibly conflicting uses within the same organization, and (c) organizational correlates of these uses. A survey questionnaire designed to answer these questions was mailed to 243 members of Division 14 of the American Psychological Association who were employed in industry. A factor analysis of the 106 completed questionnaires indicated four general uses of information from performance appraisals. The use of performance appraisal to simultaneously make distinctions between and within individuals is common. Canonical correlation analyses indicated that organizational characteristics were significantly …",
            "저널": "Journal of applied psychology",
            "저자": "Jeanette N Cleveland, Kevin R Murphy, Richard E Williams",
            "전체 인용횟수": "970회 인용19901991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220238491671519192110161919181618183417231933446859585959494136394125",
            "페이지": "130",
            "학술 문서": "Multiple uses of performance appraisal: Prevalence and correlates.JN Cleveland, KR Murphy, RE Williams - Journal of applied psychology, 1989970회 인용 관련 학술자료 전체 10개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Multiple uses of performance appraisal: Prevalence and correlates.",
        "year": null
    },
    "Performance evaluation in work settings": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1998/2",
            "게시자": "Annual Reviews",
            "권": "49",
            "설명": "▪ Abstract  Recent research from 1993 on performance evaluations in work settings is reviewed and integrated with the prior reset and historical bases. Contemporary research reflects several themes: General models of job performance are being developed, the job performance domain is being expanded, research continues to explore the psychometric characteristics of performance ratings, research is developing on potential bias in ratings, rater training is examined, and research continues in terms of efforts to attach utility values to rated performance. We conclude that research is progressing in traditional content areas as well in the exploration of new ground. Researchers are recognizing that job performance is more than just the execution of specific tasks and that it involves a wider array of important organizational activities. There is also an increased optimism regarding the use of supervisory ratings and …",
            "저자": "Richard D Arvey, Kevin R Murphy",
            "전체 인용횟수": "940회 인용19992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220233132522203024223152402848495454465454475441384337",
            "출처": "Annual review of psychology",
            "페이지": "141-168",
            "학술 문서": "Performance evaluation in work settingsRD Arvey, KR Murphy - Annual review of psychology, 1998940회 인용 관련 학술자료 전체 13개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Performance evaluation in work settings",
        "year": null
    },
    "Performance appraisal and performance management: 100 years of progress?": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/3",
            "게시자": "American Psychological Association",
            "권": "102",
            "설명": "We review 100 years of research on performance appraisal and performance management, highlighting the articles published in JAP, but including significant work from other journals as well. We discuss trends in eight substantive areas:(1) scale formats,(2) criteria for evaluating ratings,(3) training,(4) reactions to appraisal,(5) purpose of rating,(6) rating sources,(7) demographic differences in ratings, and (8) cognitive processes, and discuss what we have learned from research in each area. We also focus on trends during the heyday of performance appraisal research in JAP (1970-2000), noting which were more productive and which potentially hampered progress. Our overall conclusion is that JAP’s role in this literature has not been to propose models and new ideas, but has been primarily to test ideas and models proposed elsewhere. Nonetheless, we conclude that the papers published in JAP made important …",
            "저자": "Angelo S DeNisi, Kevin R Murphy",
            "전체 인용횟수": "728회 인용2017201820192020202120222023114893117141151150",
            "출처": "Journal of applied psychology",
            "페이지": "421",
            "학술 문서": "Performance appraisal and performance management: 100 years of progress?AS DeNisi, KR Murphy - Journal of applied psychology, 2017728회 인용 관련 학술자료 전체 7개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Performance appraisal and performance management: 100 years of progress?",
        "year": null
    },
    "Honesty in the workplace.": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1993",
            "게시자": "Thomson Brooks/Cole Publishing Co",
            "설명": "Why do some companies have loyal, trustworthy employees while others struggle to control theft, time-cheating, and chronic low production?",
            "저자": "Kevin R Murphy",
            "전체 인용횟수": "727회 인용1993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023251012201118171520232215342627222335412841452833323928132510",
            "학술 문서": "Honesty in the workplace.KR Murphy - 1993727회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Honesty in the workplace.",
        "year": null
    },
    "Correlates of perceived fairness and accuracy of performance evaluation.": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1978/12",
            "게시자": "American Psychological Association",
            "권": "63",
            "설명": "Employee perceptions of the fairness and accuracy of a performance evaluation system were examined by means of a questionnaire administered to all exempt managerial and professional employees of a large manufacturing organization. Primary (N= 355) and hold-out (N= 356) samples were identified, and a forward stepwise multiple regression analysis was performed on the primary sample. Cross-validation indicated that a 5-variable linear composite accounted for 29% of the variance in the dependent variable. Frequency of evaluation, identification of goals to eliminate weaknesses, and supervisor knowledge of a subordinate's level of performance and job duties were significantly related to perceptions of fairness and accuracy of performance evaluation.(3 ref)(PsycINFO Database Record (c) 2016 APA, all rights reserved)",
            "저널": "Journal of Applied psychology",
            "저자": "Frank J Landy, Janet L Barnes, Kevin R Murphy",
            "전체 인용횟수": "579회 인용1984198519861987198819891990199119921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023869979712101861316201412141212111413111315201316272621192214152213121410",
            "페이지": "751",
            "학술 문서": "Correlates of perceived fairness and accuracy of performance evaluation.FJ Landy, JL Barnes, KR Murphy - Journal of Applied psychology, 1978579회 인용 관련 학술자료 전체 4개의 버전 ",
            "호": "6"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Correlates of perceived fairness and accuracy of performance evaluation.",
        "year": null
    },
    "Is the relationship between cognitive ability and job performance stable over time?": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1989/9/1",
            "게시자": "Lawrence Erlbaum Associates, Inc.",
            "권": "2",
            "설명": "Schmidt, Hunter, and Outerbridge's (1986) causal model of job performance suggests that cognitive ability is the most important cause of job performance and that the relationship between ability and performance is stable over time. Research on both the stability of skilled performance and the ability requirements of tasks is inconsistent with this model. Our article describes an alternative model that ascribes a critical importance to ability during stages where workers are learning new tasks and performing unfamiliar functions (i.e., transition stages) but less so during stages where workers are performing well-learned, familiar tasks (i.e., maintenance stages). The alternative model is shown to account for the findings explained by the Schmidt et al. (1986) model, as well as for findings that cannot be accounted for by their model.",
            "저널": "Human performance",
            "저자": "Kevin R Murphy",
            "전체 인용횟수": "536회 인용1990199119921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023235674392541046101342123151918233028292730333231322418",
            "페이지": "183-200",
            "학술 문서": "Is the relationship between cognitive ability and job performance stable over time?KR Murphy - Human performance, 1989536회 인용 관련 학술자료 전체 3개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Is the relationship between cognitive ability and job performance stable over time?",
        "year": null
    },
    "Dimensions of job performance": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1989",
            "설명": "This report describes and applies a method of determining the dimensions that make up the performance of work throughout the Navys enlisted ratings and grades. The dimensions of work performance are defined within a hierarchy from micro to macro and the relationships between such dimensions and input variables such as ability, experience, and motivation are described. Key inclusions and omissions from current research within DoD on the prediction of job performance are identified. Keywords Job Performance, Performance measures, Performance dimensions, Naval personnel, Enlisted personnel.Descriptors:",
            "저널": "Testing: Applied and theoretical perspectives",
            "저자": "Kevin R Murphy, Leonard P Kroeker",
            "전체 인용횟수": "498회 인용1997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220234338663103118912241723243039344153454726",
            "페이지": "218-247",
            "학술 문서": "Dimensions of job performanceKR Murphy, LP Kroeker - Testing: Applied and theoretical perspectives, 1989498회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Dimensions of job performance",
        "year": null
    },
    "Women and men in organizations: Sex and gender issues at work": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2000",
            "게시자": "Psychology Press",
            "설명": "The gender and racial composition of the American workforce is rapidly changing. As more women in particular enter the workforce and as they enter jobs that have traditionally been dominated by men, issues related to sex and gender in work settings have become increasingly important and complex. Research addressing sex and gender in the workplace is conducted in several distinct disciplines, ranging from psychology and sociology to management and economics. Further, books on gender at work often reflect either a more traditional management perspective or a more recent feminist perspective; rarely however, are these two orientations on women and work acknowledged within the same text. Thus, the principle goal of the book is to communicate a variety of social psychological literatures and research on gender issues that affect work behaviors to upper-level undergraduate and graduate students in applied psychology and business.",
            "저자": "Jeanette N Cleveland, Margaret Stockdale, Kevin R Murphy, Barbara A Gutek",
            "전체 인용횟수": "471회 인용2001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202344212019213017202826291823283228281418151111",
            "학술 문서": "Women and men in organizations: Sex and gender issues at workJN Cleveland, M Stockdale, KR Murphy, BA Gutek - 2000471회 인용 관련 학술자료 전체 5개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Women and men in organizations: Sex and gender issues at work",
        "year": null
    },
    "Nature and consequences of halo error: A critical analysis.": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1993/4",
            "게시자": "American Psychological Association",
            "권": "78",
            "설명": "The definition of halo error that dominated researchers' thinking for most of this century implied that (1) halo error was common,(2) it was a rater error, with true and illusory components,(3) it led to inflated correlations among rating dimensions and was due to the influence of a general evaluation on specific judgments, and (4) it had negative consequences and should be avoided or removed. Research is reviewed showing that all of the major elements of this conception of halo are either wrong or problematic. Because of unresolved confounds of true and illusory halo and the often unclear consequences of halo errors, the authors suggest a moratorium on the use of halo indices as dependent measures in applied research. They suggest specific directions for further research on halo that take into account the context in which judgments are formed and ratings are obtained and that more clearly distinguish between …",
            "저널": "Journal of Applied psychology",
            "저자": "Kevin R Murphy, Robert A Jako, Rebecca L Anhalt",
            "전체 인용횟수": "433회 인용1994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220236768189713169715161113151711151317232318221819191816",
            "페이지": "218",
            "학술 문서": "Nature and consequences of halo error: A critical analysis.KR Murphy, RA Jako, RL Anhalt - Journal of Applied psychology, 1993433회 인용 관련 학술자료 전체 10개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Nature and consequences of halo error: A critical analysis.",
        "year": null
    },
    "The Changing Nature of Performance: Implications for Staffing, Motivation, and Development. Frontiers of Industrial and Organizational Psychology.": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1999",
            "게시자": "Jossey-Bass Inc., Publishers, 350 Sansome Street, San Francisco, CA 94104",
            "설명": "This volume provides a discussion of the relationship between the changing nature of work and the understanding, measurement, and influence of human performance. Chapter 1, Employee Performance in Today's Organizations (Daniel R. Ilgen, Elaine D. Pulakos), introduces seven key changes in the nature of work--changes in technology, job design, type of workforce, training methodology, external control, leadership, and work structure--and how they affect job performance and the following three emphasized human resource domains: staffing, motivation, and training and development. The remainder of the book elaborates on performance in organizations from two perspectives. The seven chapters in part 1 describe in detail the seven key changes that are strongly influencing work and work  settings and discuss their implications for performance:",
            "저자": "Daniel R Ilgen, Elaine D Pulakos",
            "전체 인용횟수": "408회 인용1998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202322381810112517151520151514141420312718221224275",
            "학술 문서": "The Changing Nature of Performance: Implications for Staffing, Motivation, and Development. Frontiers of Industrial and Organizational Psychology.DR Ilgen, ED Pulakos - 1999408회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The Changing Nature of Performance: Implications for Staffing, Motivation, and Development. Frontiers of Industrial and Organizational Psychology.",
        "year": null
    },
    "Toward narrowing the research‐practice gap in performance appraisal": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1985/6",
            "게시자": "Blackwell Publishing Ltd",
            "권": "38",
            "설명": "Recently, performance appraisal researchers have adopted a cognitive approach to analyzing judgment processes in performance appraisal. While this approach allows researchers to tap a wealth of knowledge applicable to the appraisal context, this line of research is likely to widen the already existing gap between research and practice. We argue that coordination of the talents of researchers and practitioners is essential for narrowing the gap. Specifically, we suggest researchers focus their research on the best methods of ensuring use of relevant and valid data in appraisal, given organizational constraints. We also suggest practitioners focus on determining observable and measurable aspects of performance, and thus, specify appropriate appraisal content. We also note that cognitive process research has promise for increasing raters' ability to judge accurately, but that this approach does not necessarily …",
            "저널": "Personnel Psychology",
            "저자": "Cristina G Banks, Kevin R Murphy",
            "전체 인용횟수": "407회 인용1986198719881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202358181115101317121167781010781176121294914312513119121412314",
            "페이지": "335-345",
            "학술 문서": "Toward narrowing the research‐practice gap in performance appraisalCG Banks, KR Murphy - Personnel Psychology, 1985407회 인용 관련 학술자료 전체 4개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Toward narrowing the research‐practice gap in performance appraisal",
        "year": null
    },
    "Are we getting fooled again? Coming to terms with limitations in the use of personality tests for personnel selection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2007/12",
            "게시자": "Blackwell Publishing Inc",
            "권": "60",
            "설명": "We recently published an article in which we highlighted a number of issues associated with the use of self‐report personality tests in personnel selection contexts (Morgeson et al., 2007). Both Ones, Dilchert, Viswesvaran, and Judge (2007) and Tett and Christiansen (2007) have written responses to this article. In our response to these articles we address many of the issues raised by Ones et al. and Tett and Christiansen. In addition to a detailed response, we make the following 4 key points: (1) Our criticisms of personality testing apply only to the selection context, not to all research on personality; (2) the observed validities of personality tests predicting job performance criteria are low and have not changed much over time; (3) when evaluating the usefulness of using personality tests to select applicants, one must not ignore the observed, uncorrected validity; and (4) when discussing the value of personality tests …",
            "저널": "Personnel Psychology",
            "저자": "Frederick P Morgeson, Michael A Campion, Robert L Dipboye, John R Hollenbeck, Kevin Murphy, Neal Schmitt",
            "전체 인용횟수": "374회 인용2007200820092010201120122013201420152016201720182019202020212022202331620223427332325272524252017209",
            "페이지": "1029-1049",
            "학술 문서": "Are we getting fooled again? Coming to terms with limitations in the use of personality tests for personnel selectionFP Morgeson, MA Campion, RL Dipboye… - Personnel Psychology, 2007374회 인용 관련 학술자료 전체 6개의 버전 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Are we getting fooled again? Coming to terms with limitations in the use of personality tests for personnel selection",
        "year": null
    },
    "A critique of emotional intelligence: What are the problems and how can they be fixed?": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/2/25",
            "게시자": "Psychology Press",
            "설명": "This book analyzes important criticisms of the current research on Emotional Intelligence (EI), a topic of growing interest in the behavioral and social sciences. It looks at emotional intelligence research and EI interventions from a scientific and measurement perspective and identifies ways of improving the often shaky foundations of our current conceptions of emotional intelligence. With a balanced viewpoint, A Critique of Emotional Intelligence includes contributions from leading critics of EI research and practice (eg, Frank Landy, Mark Schmit, Chockalingam Viswesvaran), proponents of EI (eg, Neal Ashkanasy, Catherine Daus), as well as a broad range of well-informed authors. Proponents claim that EI is more important in life than academic intelligence, while opponents claim that there is no such thing as emotional intelligence. Three key criticisms that have been leveled at emotional intelligence include:(1) EI is poorly defined and poorly measured;(2) EI is a new name for familiar constructs that have been studied for decades; and (3) claims about EI are overblown. While the book presents these criticisms, the final section proposes ways of improving EI research and practice with EI theories, tests, and applications.",
            "저자": "Kevin R Murphy",
            "전체 인용횟수": "361회 인용200520062007200820092010201120122013201420152016201720182019202020212022202315823161817212313272524333126171212",
            "학술 문서": "A critique of emotional intelligence: What are the problems and how can they be fixed?KR Murphy - 2014361회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A critique of emotional intelligence: What are the problems and how can they be fixed?",
        "year": null
    },
    "SSD: Single Shot MultiBox Detector": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/12/9",
            "설명": " We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component …",
            "저널": "arXiv preprint arXiv:1512.02325",
            "저자": "Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg",
            "전체 인용횟수": "34148회 인용2017201820192020202120222023760231343765477697775696342",
            "학술 문서": "Ssd: Single shot multibox detectorW Liu, D Anguelov, D Erhan, C Szegedy, S Reed… - Computer Vision–ECCV 2016: 14th European …, 201634148회 인용 관련 학술자료 전체 34개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "SSD: Single Shot MultiBox Detector",
        "year": null
    },
    "ParseNet: Looking Wider to See Better": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "We present a technique for adding global context to deep convolutional networks for semantic segmentation. The approach is simple, using the average feature for a layer to augment the features at each location. In addition, we study several idiosyncrasies of training, significantly increasing the performance of baseline networks (e.g. from FCN). When we add our proposed global feature, and a technique for learning normalization parameters, accuracy increases consistently even over our improved versions of the baselines. Our proposed approach, ParseNet, achieves state-of-the-art performance on SiftFlow and PASCAL-Context with small additional computational cost over baselines, and near current state-of-the-art performance on PASCAL VOC 2012 semantic segmentation with a simple approach. Code is available at https://github.com/weiliu89/caffe/tree/fcn .",
            "저널": "arXiv preprint arXiv:1506.04579",
            "저자": "Wei Liu, Andrew Rabinovich, Alexander Berg",
            "전체 인용횟수": "1401회 인용201520162017201820192020202120222023440112153208241266211148",
            "학술 문서": "Parsenet: Looking wider to see betterW Liu, A Rabinovich, AC Berg - arXiv preprint arXiv:1506.04579, 20151401회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "ParseNet: Looking Wider to See Better",
        "year": null
    },
    "Pointpwc-net: A coarse-to-fine network for supervised and self-supervised scene flow estimation on 3d point clouds": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/11/27",
            "설명": "We propose a novel end-to-end deep scene flow model, called PointPWC-Net, on 3D point clouds in a coarse-to-fine fashion. Flow computed at the coarse level is upsampled and warped to a finer level, enabling the algorithm to accommodate for large motion without a prohibitive search space. We introduce novel cost volume, upsampling, and warping layers to efficiently handle 3D point cloud data. Unlike traditional cost volumes that require exhaustively computing all the cost values on a high-dimensional grid, our point-based formulation discretizes the cost volume onto input 3D points, and a PointConv operation efficiently computes convolutions on the cost volume. Experiment results on FlyingThings3D outperform the state-of-the-art by a large margin. We further explore novel self-supervised losses to train our model and achieve comparable results to state-of-the-art trained with supervised loss. Without any fine-tuning, our method also shows great generalization ability on KITTI Scene Flow 2015 dataset, outperforming all previous methods.",
            "저널": "arXiv preprint arXiv:1911.12408",
            "저자": "Wenxuan Wu, Zhiyuan Wang, Zhuwen Li, Wei Liu, Li Fuxin",
            "전체 인용횟수": "203회 인용20192020202120222023112376687",
            "학술 문서": "Pointpwc-net: Cost volume on point clouds for (self-) supervised scene flow estimation*W Wu, ZY Wang, Z Li, W Liu, L Fuxin - Computer Vision–ECCV 2020: 16th European …, 2020130회 인용 관련 학술자료 전체 5개의 버전 Pointpwc-net: A coarse-to-fine network for supervised and self-supervised scene flow estimation on 3d point cloudsW Wu, Z Wang, Z Li, W Liu, L Fuxin - arXiv preprint arXiv:1911.12408, 201974회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pointpwc-net: A coarse-to-fine network for supervised and self-supervised scene flow estimation on 3d point clouds",
        "year": null
    },
    "Fast single shot detection and pose estimation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/10/25",
            "게시자": "IEEE",
            "설명": "For applications in navigation and robotics, estimating the 3D pose of objects is as important as detection. Many approaches to pose estimation rely on detecting or tracking parts or keypoints [11, 21]. In this paper we build on a recent state-of-the-art convolutional network for sliding-window detection [10] to provide detection and rough pose estimation in a single shot, without intermediate stages of detecting parts or initial bounding boxes. While not the first system to treat pose estimation as a categorization problem, this is the first attempt to combine detection and pose estimation at the same level using a deep learning approach. The key to the architecture is a deep convolutional network where scores for the presence of an object category, the offset for its location, and the approximate pose are all estimated on a regular grid of locations in the image. The resulting system is as accurate as recent work on pose …",
            "저자": "Patrick Poirson, Phil Ammirato, Cheng-Yang Fu, Wei Liu, Jana Kosecka, Alexander C Berg",
            "전체 인용횟수": "145회 인용201520162017201820192020202120222023121532272015238",
            "컨퍼런스": "2016 Fourth International Conference on 3D Vision (3DV)",
            "페이지": "676-684",
            "학술 문서": "Fast single shot detection and pose estimationP Poirson, P Ammirato, CY Fu, W Liu, J Kosecka… - 2016 Fourth International Conference on 3D Vision …, 2016145회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Fast single shot detection and pose estimation",
        "year": null
    },
    "Multimedia classification and event detection using double fusion": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/7",
            "게시자": "Springer US",
            "권": "71",
            "설명": " Multimedia Event Detection(MED) is a multimedia retrieval task with the goal of finding videos of a particular event in video archives, given example videos and event descriptions; different from MED, multimedia classification is a task that classifies given videos into specified classes. Both tasks require mining features of example videos to learn the most discriminative features, with best performance resulting from a combination of multiple complementary features. How to combine different features is the focus of this paper. Generally, early fusion and late fusion are two popular combination strategies. The former one fuses features before performing classification and the latter one combines output of classifiers from different features. Early fusion can better capture the relationship among features yet is prone to over-fit the training data. Late fusion deals with the over-fitting problem better but does not allow …",
            "저널": "Multimedia tools and applications",
            "저자": "Zhen-zhong Lan, Lei Bao, Shoou-I Yu, Wei Liu, Alexander G Hauptmann",
            "전체 인용횟수": "114회 인용20132014201520162017201820192020202120222023191011711810161415",
            "페이지": "333-347",
            "학술 문서": "Multimedia classification and event detection using double fusionZ Lan, L Bao, SI Yu, W Liu, AG Hauptmann - Multimedia tools and applications, 2014114회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Multimedia classification and event detection using double fusion",
        "year": null
    },
    "Double fusion for multimedia event detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012",
            "게시자": "Springer Berlin Heidelberg",
            "설명": " Multimedia Event Detection is a multimedia retrieval task with the goal of finding videos of a particular event in an internet video archive, given example videos and descriptions. We focus here on mining features of example videos to learn the most characteristic features, which requires a combination of multiple complementary types of features. Generally, early fusion and late fusion are two popular combination strategies. The former one fuses features before performing classification and the latter one combines output of classifiers from different features. In this paper, we introduce a fusion scheme named double fusion, which combines early fusion and late fusion together to incorporate their advantages. Results are reported on TRECVID MED 2010 and 2011 data sets. For MED 2010, we get a mean minimal normalized detection cost (MNDC) of 0.49, which exceeds the state of the art performance by …",
            "저자": "Zhen-zhong Lan, Lei Bao, Shoou-I Yu, Wei Liu, Alexander G Hauptmann",
            "전체 인용횟수": "96회 인용201220132014201520162017201820192020202120222023615161165868714",
            "컨퍼런스": "Advances in Multimedia Modeling: 18th International Conference, MMM 2012, Klagenfurt, Austria, January 4-6, 2012. Proceedings 18",
            "페이지": "173-185",
            "학술 문서": "Double fusion for multimedia event detectionZ Lan, L Bao, SI Yu, W Liu, AG Hauptmann - Advances in Multimedia Modeling: 18th International …, 201296회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Double fusion for multimedia event detection",
        "year": null
    },
    "Leveraging long-range temporal relationships between proposals for video object detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "Single-frame object detectors perform well on videos sometimes, even without temporal context. However, challenges such as occlusion, motion blur, and rare poses of objects are hard to resolve without temporal awareness. Thus, there is a strong need to improve video object detection by considering long-range temporal dependencies. In this paper, we present a light-weight modification to a single-frame detector that accounts for arbitrary long dependencies in a video. It improves the accuracy of a single-frame detector significantly with negligible compute overhead. The key component of our approach is a novel temporal relation module, operating on object proposals, that learns the similarities between proposals from different frames and selects proposals from past and/or future to support current proposals. Our final\" causal\" model, without any offline post-processing steps, runs at a similar speed as a single-frame detector and achieves state-of-the-art video object detection on ImageNet VID dataset.",
            "저자": "Mykhailo Shvets, Wei Liu, Alexander C Berg",
            "전체 인용횟수": "90회 인용202020212022202311263221",
            "컨퍼런스": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "페이지": "9756-9764",
            "학술 문서": "Leveraging long-range temporal relationships between proposals for video object detectionM Shvets, W Liu, AC Berg - Proceedings of the IEEE/CVF International Conference …, 201990회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Leveraging long-range temporal relationships between proposals for video object detection",
        "year": null
    },
    "Extending semi-supervised learning methods for inductive transfer learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/12/6",
            "게시자": "IEEE",
            "설명": "Inductive transfer learning and semi-supervised learning are two different branches of machine learning. The former tries to reuse knowledge in labeled out-of-domain instances while the later attempts to exploit the usefulness of unlabeled in-domain instances. In this paper, we bridge the two branches by pointing out that many semi-supervised learning methods can be extended for inductive transfer learning, if the step of labeling an unlabeled instance is replaced by re-weighting a diff-distribution instance. Based on this recognition, we develop a new transfer learning method, namely COITL, by extending the co-training method in semi-supervised learning. Experimental results reveal that COITL can achieve significantly higher generalization and robustness, compared with two state-of-the-art methods in inductive transfer learning.",
            "저자": "Yuan Shi, Zhenzhong Lan, Wei Liu, Wei Bi",
            "전체 인용횟수": "44회 인용20112012201320142015201620172018201920202021202220235373352311513",
            "컨퍼런스": "2009 Ninth IEEE international conference on data mining",
            "페이지": "483-492",
            "학술 문서": "Extending semi-supervised learning methods for inductive transfer learningY Shi, Z Lan, W Liu, W Bi - 2009 Ninth IEEE international conference on data …, 200944회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Extending semi-supervised learning methods for inductive transfer learning",
        "year": null
    },
    "Low-power image recognition challenge": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/1/16",
            "게시자": "IEEE",
            "설명": "Significant progress has been made in recent years using computer programs recognizing objects in images. Meanwhile, many cameras are embedded in battery-powered systems (such as mobile phones, wearable devices, and drones) and energy efficiency is essential. Even though many research papers have been published on the topics related to low power and image recognition, there does not exist a common metric for comparing different solutions in terms of (1) energy efficiency and (2) accuracy in recognition. Low-Power Image Recognition Challenge (LPIRC) is, to our knowledge, the only on-site competition that considers both energy consumption and recognition accuracy. LPIRC was held as one-day workshops in the Design Automation Conference in 2015 and 2016. Each participating team brought their own system to the workshops. The referee system of LPIRC includes (1) an intranet, (2) a power …",
            "저자": "Kent Gauen, Rohit Rangan, Anup Mohan, Yung-Hsiang Lu, Wei Liu, Alexander C Berg",
            "전체 인용횟수": "41회 인용20172018201920202021202220232693972",
            "컨퍼런스": "2017 22nd Asia and South pacific design automation conference (ASP-DAC)",
            "페이지": "99-104",
            "학술 문서": "Low-power image recognition challengeK Gauen, R Rangan, A Mohan, YH Lu, W Liu, AC Berg - 2017 22nd Asia and South pacific design automation …, 201741회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Low-power image recognition challenge",
        "year": null
    },
    "Constrained variational policy optimization for safe reinforcement learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2022/6/28",
            "게시자": "PMLR",
            "설명": "Safe reinforcement learning (RL) aims to learn policies that satisfy certain constraints before deploying them to safety-critical applications. Previous primal-dual style approaches suffer from instability issues and lack optimality guarantees. This paper overcomes the issues from the perspective of probabilistic inference. We introduce a novel Expectation-Maximization approach to naturally incorporate constraints during the policy learning: 1) a provable optimal non-parametric variational distribution could be computed in closed form after a convex optimization (E-step); 2) the policy parameter is improved within the trust region based on the optimal variational distribution (M-step). The proposed algorithm decomposes the safe RL problem into a convex optimization phase and a supervised learning phase, which yields a more stable training performance. A wide range of experiments on continuous robotic tasks shows that the proposed method achieves significantly better constraint satisfaction performance and better sample efficiency than baselines. The code is available at https://github. com/liuzuxin/cvpo-safe-rl.",
            "저자": "Zuxin Liu, Zhepeng Cen, Vladislav Isenbaev, Wei Liu, Steven Wu, Bo Li, Ding Zhao",
            "전체 인용횟수": "40회 인용2021202220232829",
            "컨퍼런스": "International Conference on Machine Learning",
            "페이지": "13644-13668",
            "학술 문서": "Constrained variational policy optimization for safe reinforcement learningZ Liu, Z Cen, V Isenbaev, W Liu, S Wu, B Li, D Zhao - International Conference on Machine Learning, 202240회 인용 관련 학술자료 전체 5개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Constrained variational policy optimization for safe reinforcement learning",
        "year": null
    },
    "Lecture notes in computer science": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "권": "9905",
            "저널": "Computer Vision–ECCV",
            "저자": "Bastian Leibe, Jiri Matas, Nicu Sebe, L Wei, A Dragomir, E Dumitru, S Christian, R Scott, F Cheng-Yang",
            "전체 인용횟수": "39회 인용20152016201720182019202020212022202311556109",
            "페이지": "21-37",
            "학술 문서": "Lecture notes in computer scienceB Leibe, J Matas, N Sebe, L Wei, A Dragomir… - Computer Vision–ECCV, 201639회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Lecture notes in computer science",
        "year": null
    },
    "Predicting Entry-Level Categories": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "게시자": "Springer",
            "설명": " Entry-level categories—the labels people use to name an object—were originally defined and studied by psychologists in the 1970s and 1980s. In this paper we extend these ideas to study entry-level categories at a larger scale and to learn models that can automatically predict entry-level categories for images. Our models combine visual recognition predictions with linguistic resources like WordNet and proxies for word “naturalness” mined from the enormous amount of text on the web. We demonstrate the usefulness of our models for predicting nouns (entry-level words) associated with images by people, and for learning mappings between concepts predicted by existing visual recognition systems and entry-level concepts. In this work we make use of recent successful efforts on convolutional network models for visual recognition by training classifiers for 7404 object categories on ConvNet activation …",
            "저널": "International Journal of Computer Vision",
            "저자": "Vicente Ordonez, Wei Liu, Jia Deng, Yejin Choi, Alexander C. Berg, Tamara L. Berg",
            "전체 인용횟수": "24회 인용2016201720182019202020212022202347332221",
            "페이지": "1-15",
            "학술 문서": "Predicting entry-level categoriesV Ordonez, W Liu, J Deng, Y Choi, AC Berg, TL Berg - International Journal of Computer Vision, 201524회 인용 관련 학술자료 전체 25개의 버전 ",
            "호": "Marr Prize Special Issue"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Predicting Entry-Level Categories",
        "year": null
    },
    "Informedia@ trecvid 2010": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010",
            "게시자": "Carnegie Mellon University",
            "설명": "The Informedia group participated in four tasks this year, including Semantic indexing, Known-item search, Surveillance event detection and Event detection in Internet multimedia pilot. For semantic indexing, except for training traditional SVM classifiers for each high level feature by using different low level features, a kind of cascade classifier was trained which including four layers with different visual features respectively. For Known Item Search task, we built a text-based video retrieval and a visual-based video retrieval system, and then query-class dependent late fusion was used to combine the runs from these two systems. For surveillance event detection, we especially put our focus on analyzing motions and human in videos. We detected the events by three channels. Firstly, we adopted a robust new descriptor called MoSIFT, which explicitly encodes appearance features together with motion information. And then we trained event classifiers in sliding windows using a bag-of-video-word approach. Secondly, we used the human detection and tracking algorithms to detect and track the regions of human, and then just focus on the MoSIFT points in the human regions. Thirdly, after getting the decision, we also borrow the results of human detection to filter the decision. In addition, to reduce the number of false alarms further, we aggregated short positive windows to favor long segmentation and applied a cascade classi- fier approach. The performance shows dramatic improvement over last year on the event detection task. For event detection in internet multimedia pilot, our system is purely based on textual information in the form of Automatic …",
            "저자": "Huan Li, Lei Bao, Arnold Overwijk, Wei Liu, Long-Fei Zhang, Shoou-I Yu, Ming-Yu Chen, Florian Metze, Alexander Hauptmann",
            "전체 인용횟수": "20회 인용20112012201320142015201620172018201920202021753121",
            "학술 문서": "Informedia@ trecvid 2010H Li, L Bao, A Overwijk, W Liu, LF Zhang, SI Yu… - 201020회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Informedia@ trecvid 2010",
        "year": null
    },
    "Unsupervised summarization of rushes videos": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/10/25",
            "도서": "Proceedings of the 18th ACM international conference on Multimedia",
            "설명": "This paper proposes a new framework to formulate summarization of rushes video as an unsupervised learning problem. We pose the problem of video summarization as one of time-series clustering, and proposed Constrained Aligned Cluster Analysis (CACA). CACA combines kernel k-means, Dynamic Time Alignment Kernel (DTAK), and unlike previous work, CACA jointly optimizes video segmentation and shot clustering. CACA is effciently solved via dynamic programming. Experimental results on the TRECVID 2007 and 2008 BBC rushes video summarization databases validate the accuracy and effectiveness of CACA.",
            "저자": "Yang Liu, Feng Zhou, Wei Liu, Fernando De la Torre, Yan Liu",
            "전체 인용횟수": "18회 인용201120122013201420152016201720182019202013124331",
            "페이지": "751-754",
            "학술 문서": "Unsupervised summarization of rushes videosY Liu, F Zhou, W Liu, F De la Torre, Y Liu - Proceedings of the 18th ACM international conference …, 201018회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Unsupervised summarization of rushes videos",
        "year": null
    },
    "Reed Scott E., Anguelov Dragomir, Erhan Dumitru, Vanhoucke Vincent, and Rabinovich Andrew. 2015": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "저널": "Going deeper with convolutions. In CVPR",
            "저자": "Szegedy Christian, Liu Wei, Jia Yangqing, Sermanet Pierre",
            "전체 인용횟수": "11회 인용201820192020202120222023211124",
            "페이지": "1-9",
            "학술 문서": "Reed Scott E., Anguelov Dragomir, Erhan Dumitru, Vanhoucke Vincent, and Rabinovich Andrew. 2015S Christian, L Wei, J Yangqing, S Pierre - Going deeper with convolutions. In CVPR, 201411회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Reed Scott E., Anguelov Dragomir, Erhan Dumitru, Vanhoucke Vincent, and Rabinovich Andrew. 2015",
        "year": null
    },
    "European Conference on Computer Vision (ECCV)": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012",
            "게시자": "Springer",
            "저자": "L Wei, A Dragomir, E Dumitru",
            "전체 인용횟수": "10회 인용201920202021202220231243",
            "페이지": "679-692",
            "학술 문서": "European Conference on Computer Vision (ECCV)L Wei, A Dragomir, E Dumitru - 201210회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "European Conference on Computer Vision (ECCV)",
        "year": null
    },
    "Learning to name objects": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/2/25",
            "게시자": "ACM",
            "권": "59",
            "설명": "We have seen remarkable recent progress in computational visual recognition, producing systems that can classify objects into thousands of different categories with increasing accuracy. However, one question that has received relatively less attention is \"what labels should recognition systems output?\" This paper looks at the problem of predicting category labels that mimic how human observers would name objects. This goal is related to the concept of entry-level categories first introduced by psychologists in the 1970s and 1980s. We extend these seminal ideas to study human naming at large scale and to learn computational models for predicting entry-level categories. Practical applications of this work include improving human-focused computer vision applications such as automatically generating a natural language description for an image or text-based image search.",
            "저널": "Communications of the ACM",
            "저자": "Vicente Ordonez, Wei Liu, Jia Deng, Yejin Choi, Alexander C Berg, Tamara L Berg",
            "전체 인용횟수": "9회 인용20162017201820192020202120222023131121",
            "페이지": "108-115",
            "학술 문서": "Learning to name objectsV Ordonez, W Liu, J Deng, Y Choi, AC Berg, TL Berg - Communications of the ACM, 20169회 인용 관련 학술자료 전체 11개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning to name objects",
        "year": null
    },
    "CosFace: Large Margin Cosine Loss for Deep Face Recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/6",
            "게시자": "IEEE",
            "설명": "Face recognition has made extraordinary progress owing to the advancement of deep convolutional neural networks (CNNs). The central task of face recognition, including face verification and identification, involves face feature discrimination. However, the traditional softmax loss of deep CNNs usually lacks the power of discrimination. To address this problem, recently several loss functions such as center loss, large margin softmax loss, and angular softmax loss have been proposed. All these improved losses share the same idea: maximizing inter-class variance and minimizing intra-class variance. In this paper, we propose a novel loss function, namely large margin cosine loss (LMCL), to realize this idea from a different perspective. More specifically, we reformulate the softmax loss as a cosine loss by L2 normalizing both features and weight vectors to remove radial variations, based on which a cosine margin term is introduced to further maximize the decision margin in the angular space. As a result, minimum intra-class variance and maximum inter-class variance are achieved by virtue of normalization and cosine decision margin maximization. We refer to our model trained with LMCL as CosFace. Extensive experimental evaluations are conducted on the most popular public-domain face recognition datasets such as MegaFace Challenge, Youtube Faces (YTF) and Labeled Face in the Wild (LFW). We achieve the state-of-the-art performance on these benchmarks, which confirms the effectiveness of our proposed approach.",
            "저자": "Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, Wei Liu",
            "전체 인용횟수": "2468회 인용20182019202020212022202336221447562633555",
            "컨퍼런스": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "학술 문서": "Cosface: Large margin cosine loss for deep face recognitionH Wang, Y Wang, Z Zhou, X Ji, D Gong, J Zhou, Z Li… - Proceedings of the IEEE conference on computer …, 20182463회 인용 관련 학술자료 전체 14개의 버전 CosFace: large margin cosine loss for deep face recognition. CoRR abs/1801.09414 (2018)*H Wang, Y Wang, Z Zhou, X Ji, Z Li, D Gong, J Zhou… - arXiv preprint arXiv:1801.09414, 20185회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "CosFace: Large Margin Cosine Loss for Deep Face Recognition",
        "year": null
    },
    "SCA-CNN: Spatial and Channel-wise Attention in Convolutional Networks for Image Captioning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/7",
            "게시자": "IEEE",
            "설명": "Visual attention has been successfully applied in structural prediction tasks such as visual captioning and question answering. Existing visual attention models are generally spatial, ie, the attention is modeled as spatial probabilities that re-weight the last conv-layer feature map of a CNN encoding an input image. However, we argue that such spatial attention does not necessarily conform to the attention mechanism---a dynamic feature extractor that combines contextual fixations over time, as CNN features are naturally spatial, channel-wise and multi-layer. In this paper, we introduce a novel convolutional neural network dubbed SCA-CNN that incorporates Spatial and Channel-wise Attentions in a CNN. In the task of image captioning, SCA-CNN dynamically modulates the sentence generation context in multi-layer feature maps, encoding where (ie, attentive spatial locations at multiple layers) and what (ie, attentive channels) the visual attention is. We evaluate the proposed SCA-CNN architecture on three benchmark image captioning datasets: Flickr8K, Flickr30K, and MSCOCO. It is consistently observed that SCA-CNN significantly outperforms state-of-the-art visual attention-based image captioning methods.",
            "저자": "Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian Shao, Wei Liu, Tat-Seng Chua",
            "전체 인용횟수": "1898회 인용201720182019202020212022202318106265355394417332",
            "컨퍼런스": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "학술 문서": "Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioningL Chen, H Zhang, J Xiao, L Nie, J Shao, W Liu… - Proceedings of the IEEE conference on computer …, 20171898회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "SCA-CNN: Spatial and Channel-wise Attention in Convolutional Networks for Image Captioning",
        "year": null
    },
    "Supervised Hashing with Kernels": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012",
            "게시자": "IEEE Computer Society",
            "설명": "Recent years have witnessed the growing popularity of hashing in large-scale vision problems. It has been shown that the hashing quality could be boosted by leveraging supervised information into hash function learning. However, the existing supervised methods either lack adequate performance or often incur cumbersome model training. In this paper, we propose a novel kernel-based supervised hashing model which requires a limited amount of supervised information, i.e., similar and dissimilar data pairs, and a feasible training cost in achieving high quality hashing. The idea is to map the data to compact binary codes whose Hamming distances are minimized on similar pairs and simultaneously maximized on dissimilar pairs. Our approach is distinct from prior works by utilizing the equivalence between optimizing the code inner products and the Hamming distances. This enables us to sequentially and …",
            "저널": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "저자": "W Liu, J Wang, R Ji, YG Jiang, SF Chang",
            "전체 인용횟수": "1649회 인용20122013201420152016201720182019202020212022202317567812516523021720918116011371",
            "학술 문서": "Supervised hashing with kernelsW Liu, J Wang, R Ji, YG Jiang, SF Chang - 2012 IEEE conference on computer vision and pattern …, 20121649회 인용 관련 학술자료 전체 19개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Supervised Hashing with Kernels",
        "year": null
    },
    "Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/9",
            "설명": "We propose an end-to-end deep learning architecture that produces a 3D shape in triangular mesh from a single color image. Limited by the nature of deep neural network, previous methods usually represent a 3D shape in volume or point cloud, and it is non-trivial to convert them to the more ready-to-use mesh model. Unlike the existing methods, our network represents 3D mesh in a graph-based convolutional neural network and produces correct geometry by progressively deforming an ellipsoid, leveraging perceptual features extracted from the input image. We adopt a coarse-to-fine strategy to make the whole deformation procedure stable, and define various of mesh related losses to capture properties of different levels to guarantee visually appealing and physically accurate 3D geometry. Extensive experiments show that our method not only qualitatively produces mesh model with better details, but also achieves higher 3D shape estimation accuracy compared to the state-of-the-art.",
            "저자": "Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, Yu-Gang Jiang",
            "전체 인용횟수": "1251회 인용2018201920202021202220236118231296308283",
            "컨퍼런스": "European Conference on Computer Vision (ECCV)",
            "페이지": "arXiv preprint arXiv:1804.01654",
            "학술 문서": "Pixel2mesh: Generating 3d mesh models from single rgb imagesN Wang, Y Zhang, Z Li, Y Fu, W Liu, YG Jiang - Proceedings of the European conference on computer …, 20181251회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images",
        "year": null
    },
    "Supervised Discrete Hashing": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/6",
            "게시자": "IEEE",
            "설명": "Recently, learning based hashing techniques have attracted broad research interests due to the resulting efficient storage and retrieval of images, videos, documents, etc. However, a major difficulty of learning to hash lies in handling the discrete constraints imposed on the needed hash codes, which typically makes hash optimizations very challenging (NP-hard in general). In this work, we propose a new supervised hashing framework, where the learning objective for hashing is to make the optimal binary hash codes for classification. By introducing an auxiliary variable, we reformulate the objective such that it can be solved substantially efficiently by using a regularization algorithm. One of the key steps in the algorithm is to solve the regularization sub-problem associated with the NP-hard binary optimization. We show that with cyclic coordinate descent, the sub-problem admits an analytical solution. As such, a high-quality discrete solution can eventually be obtained in an efficient computing manner, which enables to tackle massive datasets. We evaluate the proposed approach, dubbed Supervised Discrete Hashing (SDH), on four large image datasets, and demonstrate that SDH outperforms the state-of-the-art hashing methods in large-scale image retrieval.",
            "저자": "Fumin Shen, Chunhua Shen, Wei Liu, Heng Tao Shen",
            "전체 인용횟수": "1229회 인용201520162017201820192020202120222023169015117121218516112498",
            "컨퍼런스": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "학술 문서": "Supervised discrete hashingF Shen, C Shen, W Liu, H Tao Shen - Proceedings of the IEEE conference on computer …, 20151229회 인용 관련 학술자료 전체 24개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Supervised Discrete Hashing",
        "year": null
    },
    "Hashing with Graphs": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011",
            "게시자": "ACM",
            "설명": "Hashing is becoming increasingly popular for efficient nearest neighbor search in massive databases. However, learning short codes that yield good search performance is still a challenge. Moreover, in many cases realworld data lives on a low-dimensional manifold, which should be taken into account to capture meaningful nearest neighbors. In this paper, we propose a novel graph-based hashing method which automatically discovers the neighborhood structure inherent in the data to learn appropriate compact codes. To make such an approach computationally feasible, we utilize Anchor Graphs to obtain tractable low-rank adjacency matrices. Our formulation allows constant time hashing of a new data point by extrapolating graph Laplacian eigenvectors to eigenfunctions. Finally, we describe a hierarchical threshold learning procedure in which each eigenfunction yields multiple bits, leading to higher search accuracy. Experimental comparison with the other state-of-the-art methods on two large datasets demonstrates the efficacy of the proposed method.",
            "저자": "W. Liu, J. Wang, S. Kumar, S. Chang",
            "전체 인용횟수": "1190회 인용20102011201220132014201520162017201820192020202120222023353365761201301531441181151016840",
            "컨퍼런스": "Proceedings of International Conference on Machine Learning (ICML)",
            "페이지": "1-8",
            "학술 문서": "Hashing with graphsW Liu, J Wang, S Kumar, SF Chang - 20111190회 인용 관련 학술자료 전체 18개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Hashing with Graphs",
        "year": null
    },
    "Multiple object tracking: A literature review": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021/4",
            "게시자": "Elsevier",
            "권": "293",
            "설명": "Multiple Object Tracking (MOT) has gained increasing attention due to its academic and commercial potential. Although different approaches have been proposed to tackle this problem, it still remains challenging due to factors like abrupt appearance changes and severe object occlusions. In this work, we contribute the first comprehensive and most recent review on this problem. We inspect the recent advances in various aspects and propose some interesting directions for future research. To the best of our knowledge, there has not been any extensive review on this topic in the community. We endeavor to provide a thorough review on the development of this problem in recent decades. The main contributions of this review are fourfold: 1) Key aspects in an MOT system, including formulation, categorization, key principles, evaluation of MOT are discussed; 2) Instead of enumerating individual works, we discuss …",
            "저널": "Artificial Intelligence",
            "저자": "Wenhan Luo, Junliang Xing, Anton Milan, Xiaoqin Zhang, Wei Liu, Tae-kyun Kim",
            "전체 인용횟수": "853회 인용201620172018201920202021202220235315089111142223196",
            "페이지": "arXiv preprint arXiv:1409.7618v4",
            "학술 문서": "Multiple object tracking: A literature reviewW Luo, J Xing, A Milan, X Zhang, W Liu, TK Kim - Artificial intelligence, 2021853회 인용 관련 학술자료 전체 8개의 버전 ",
            "호": "103448"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Multiple object tracking: A literature review",
        "year": null
    },
    "Attentive collaborative filtering: Multimedia recommendation with item-and component-level attention": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/8/7",
            "도서": "Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval",
            "설명": "Multimedia content is dominating today's Web information. The nature of multimedia user-item interactions is 1/0 binary implicit feedback (e.g., photo likes, video views, song downloads, etc.), which can be collected at a larger scale with a much lower cost than explicit feedback (e.g., product ratings). However, the majority of existing collaborative filtering (CF) systems are not well-designed for multimedia recommendation, since they ignore the implicitness in users' interactions with multimedia content. We argue that, in multimedia recommendation, there exists item- and component-level implicitness which blurs the underlying users' preferences. The item-level implicitness means that users' preferences on items (e.g. photos, videos, songs, etc.) are unknown, while the component-level implicitness means that inside each item users' preferences on different components (e.g. regions in an image, frames of a video, etc …",
            "저자": "Jingyuan Chen, Hanwang Zhang, Xiangnan He, Liqiang Nie, Wei Liu, Tat-Seng Chua",
            "전체 인용횟수": "832회 인용20172018201920202021202220231086153162151128139",
            "페이지": "335-344",
            "학술 문서": "Attentive collaborative filtering: Multimedia recommendation with item-and component-level attentionJ Chen, H Zhang, X He, L Nie, W Liu, TS Chua - Proceedings of the 40th International ACM SIGIR …, 2017832회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Attentive collaborative filtering: Multimedia recommendation with item-and component-level attention",
        "year": null
    },
    "Gated Fusion Network for Single Image Dehazing": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/6",
            "설명": "In this paper, we propose an efficient algorithm to directly restore a clear image from a hazy input. The proposed algorithm hinges on an end-to-end trainable neural network that consists of an encoder and a decoder. The encoder is exploited to capture the context of the derived input images, while the decoder is employed to estimate the contribution of each input to the final dehazed result using the learned representations attributed to the encoder. The constructed network adopts a novel fusion-based strategy which derives three inputs from an original hazy image by applying White Balance (WB), Contrast Enhancing (CE), and Gamma Correction (GC). We compute pixel-wise confidence maps based on the appearance differences between these different inputs to blend the information of the derived inputs and preserve the regions with pleasant visibility. The final dehazed image is yielded by gating the important features of the derived inputs. To train the network, we introduce a multi-scale based approach so that the halo artifacts can be avoided. Extensive experimental results on both synthetic and real-world images demonstrate that the proposed algorithm performs favorably against the state-of-the-art algorithms.",
            "저자": "Wenqi Ren, Lin Ma, Jiawei Zhang, Jinshan Pan, Xiaochun Cao, Wei Liu, Ming-Hsuan Yang",
            "전체 인용횟수": "763회 인용2018201920202021202220231970129185185173",
            "컨퍼런스": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "학술 문서": "Gated fusion network for single image dehazingW Ren, L Ma, J Zhang, J Pan, X Cao, W Liu, MH Yang - Proceedings of the IEEE conference on computer …, 2018763회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Gated Fusion Network for Single Image Dehazing",
        "year": null
    },
    "Large graph construction for scalable semi-supervised learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010",
            "게시자": "ACM",
            "설명": "In this paper, we address the scalability issue plaguing graph-based semi-supervised learning via a small number of anchor points which adequately cover the entire point cloud. Critically, these anchor points enable nonparametric regression that predicts the label for each data point as a locally weighted average of the labels on anchor points. Because conventional graph construction is inefficient in large scale, we propose to construct a tractable large graph by coupling anchorbased label prediction and adjacency matrix design. Contrary to the Nyström approximation of adjacency matrices which results in indefinite graph Laplacians and in turn leads to potential non-convex optimization over graphs, the proposed graph construction approach based on a unique idea called AnchorGraph provides nonnegative adjacency matrices to guarantee positive semidefinite graph Laplacians. Our approach scales linearly with the data size and in practice usually produces a large sparse graph. Experiments on large datasets demonstrate the significant accuracy improvement and scalability of the proposed approach.",
            "저자": "Wei Liu, Junfeng He, Shih-Fu Chang",
            "전체 인용횟수": "682회 인용20102011201220132014201520162017201820192020202120222023413234047608074556254575751",
            "컨퍼런스": "Proceedings of International Conference on Machine Learning (ICML)",
            "페이지": "679-686",
            "학술 문서": "Large graph construction for scalable semi-supervised learningW Liu, J He, SF Chang - Proceedings of the 27th international conference on …, 2010682회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Large graph construction for scalable semi-supervised learning",
        "year": null
    },
    "Tensor Robust Principal Component Analysis with A New Tensor Nuclear Norm": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020/4",
            "게시자": "IEEE",
            "권": "42",
            "설명": "In this paper, we consider the Tensor Robust Principal Component Analysis (TRPCA) problem, which aims to exactly recover the low-rank and sparse components from their sum. Our model is based on the recently proposed tensor-tensor product (or t-product) [14]. Induced by the t-product, we first rigorously deduce the tensor spectral norm, tensor nuclear norm, and tensor average rank, and show that the tensor nuclear norm is the convex envelope of the tensor average rank within the unit ball of the tensor spectral norm. These definitions, their relationships and properties are consistent with matrix cases. Equipped with the new tensor nuclear norm, we then solve the TRPCA problem by solving a convex program and provide the theoretical guarantee for the exact recovery. Our TRPCA model and recovery guarantee include matrix RPCA as a special case. Numerical experiments verify our results, and the …",
            "저널": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)",
            "저자": "Canyi Lu, Jiashi Feng, Yudong Chen, Wei Liu, Zhouchen Lin, Shuicheng Yan",
            "전체 인용횟수": "674회 인용20182019202020212022202354486144204187",
            "페이지": "925-938",
            "학술 문서": "Tensor robust principal component analysis with a new tensor nuclear normC Lu, J Feng, Y Chen, W Liu, Z Lin, S Yan - IEEE transactions on pattern analysis and machine …, 2019674회 인용 관련 학술자료 전체 11개의 버전 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Tensor Robust Principal Component Analysis with A New Tensor Nuclear Norm",
        "year": null
    },
    "Discrete Graph Hashing": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/12",
            "게시자": "MIT press",
            "권": "27",
            "설명": "Hashing has emerged as a popular technique for fast nearest neighbor search in gigantic databases. In particular, learning based hashing has received considerable attention due to its appealing storage and search efficiency. However, the performance of most unsupervised learning based hashing methods deteriorates rapidly as the hash code length increases. We argue that the degraded performance is due to inferior optimization procedures used to achieve discrete binary codes. This paper presents a graph-based unsupervised hashing model to preserve the neighborhood structure of massive data in a discrete code space. We cast the graph hashing problem into a discrete optimization framework which directly learns the binary codes. A tractable alternating maximization algorithm is then proposed to explicitly deal with the discrete constraints, yielding high-quality codes to well capture the local neighborhoods. Extensive experiments performed on four large datasets with up to one million samples show that our discrete optimization based graph hashing method obtains superior search accuracy over state-of-the-art unsupervised hashing methods, especially for longer codes.",
            "저자": "Wei Liu, Cun Mu, Sanjiv Kumar, Shih-Fu Chang",
            "전체 인용횟수": "594회 인용20142015201620172018201920202021202220232195773748685816340",
            "컨퍼런스": "Advances in Neural Information Processing Systems (NIPS)",
            "학술 문서": "Discrete graph hashingW Liu, C Mu, S Kumar, SF Chang - Advances in neural information processing systems, 2014594회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Discrete Graph Hashing",
        "year": null
    },
    "Learning to hash for indexing big data—A survey": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/1",
            "게시자": "IEEE",
            "권": "104",
            "설명": "The explosive growth in Big Data has attracted much attention in designing efficient indexing and search methods recently. In many critical applications such as large-scale search and pattern matching, finding the nearest neighbors to a query is a fundamental research problem. However, the straightforward solution using exhaustive comparison is infeasible due to the prohibitive computational complexity and memory requirement. In response, approximate nearest neighbor (ANN) search based on hashing techniques has become popular due to its promising performance in both efficiency and accuracy. Prior randomized hashing methods, e.g., locality-sensitive hashing (LSH), explore data-independent hash functions with random projections or permutations. Although having elegant theoretic guarantees on the search quality in certain metric spaces, performance of randomized hashing has been shown …",
            "저널": "Proceedings of the IEEE",
            "저자": "Jun Wang, Wei Liu, Sanjiv Kumar, Shih-Fu Chang",
            "전체 인용횟수": "573회 인용20152016201720182019202020212022202323492859881776631",
            "페이지": "34-57",
            "학술 문서": "Learning to hash for indexing big data—A surveyJ Wang, W Liu, S Kumar, SF Chang - Proceedings of the IEEE, 2015573회 인용 관련 학술자료 전체 9개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning to hash for indexing big data—A survey",
        "year": null
    },
    "Bi-Real Net: Enhancing the Performance of 1-Bit CNNs with Improved Representational Capability and Advanced Training Algorithm": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/9",
            "설명": "In this work, we study the 1-bit convolutional neural networks (CNNs), of which both the weights and activations are binary. While being efficient, the classification accuracy of the current 1-bit CNNs is much worse compared with their counterpart real-valued CNN models on the large-scale dataset, like ImageNet. To shrink the performance gap between the 1-bit and real-valued CNN models, we propose a novel model, dubbed Bi-Real net, which connects the real activations (after the 1-bit convolution and/or BatchNorm layer, before the sign function) to that of the consecutive block, through an identity shortcut. Consequently, compared to the standard 1-bit CNN, the representational capability of the Bi-Real net is significantly enhanced, only with a negligible additional cost on computation. Moreover, we develop a specific training algorithm including three technical novelties for 1-bit CNNs. First, we derive a tight approximation to the derivative of the non-differentiable sign function with respect to activation. Second, we propose a magnitude-aware gradient with respect to weight to update the weight parameter. Last, we pre-train the real-valued CNN model with a clip function, rather than the ReLU function, to provide a better initialization for Bi-Real net. Experiments on ImageNet show that the Bi-Real net with proposed training algorithm achieves 56.4% and 62.2% top-1 accuracy with 18 layers and 34 layers, respectively, and achieves up to 23.9 X memory saving and 17.0 X computational reduction.",
            "저자": "Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, Kwang-Ting Cheng",
            "전체 인용횟수": "524회 인용201820192020202120222023637100121140116",
            "컨퍼런스": "European Conference on Computer Vision (ECCV)",
            "학술 문서": "Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithmZ Liu, B Wu, W Luo, X Yang, W Liu, KT Cheng - Proceedings of the European conference on computer …, 2018524회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Bi-Real Net: Enhancing the Performance of 1-Bit CNNs with Improved Representational Capability and Advanced Training Algorithm",
        "year": null
    },
    "Tensor Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Tensors via Convex Optimization": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "게시자": "IEEE",
            "설명": "This paper studies the Tensor Robust Principal Component (TRPCA) problem which extends the known Robust PCA to the tensor case. Our model is based on a new tensor Singular Value Decomposition (t-SVD) and its induced tensor tubal rank and tensor nuclear norm. Consider that we have a 3-way tensor X in R^ n* n* n_3 such that X= L_0+ S_0, where L_0 has low tubal rank and S_0 is sparse. Is that possible to recover both components? In this work, we prove that under certain suitable assumptions, we can recover both the low-rank and the sparse components exactly by simply solving a convex program whose objective is a weighted combination of the tensor nuclear norm and the l1-norm, ie, min L, E st|| L|| _*+ lambda|| E|| _1 st X= L+ E. where lambda= 1/sqrtmax (n_1, n_2) n_3. Interestingly, TRPCA involves RPCA as a special case when n_3= 1 and thus it is a simple and elegant tensor extension of RPCA. Also numerical experiments verify our theory and the application for the image denoising demonstrates the effectiveness of our method.",
            "저자": "Canyi Lu, Jiashi Feng, Yudong Chen, Wei Liu, Zhouchen Lin, Shuicheng Yan",
            "전체 인용횟수": "499회 인용20162017201820192020202120222023533519391709160",
            "컨퍼런스": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "페이지": "5249-5257",
            "학술 문서": "Tensor robust principal component analysis: Exact recovery of corrupted low-rank tensors via convex optimizationC Lu, J Feng, Y Chen, W Liu, Z Lin, S Yan - Proceedings of the IEEE conference on computer …, 2016499회 인용 관련 학술자료 전체 20개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Tensor Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Tensors via Convex Optimization",
        "year": null
    },
    "Self-Supervised Adversarial Hashing Networks for Cross-Modal Retrieval": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/6",
            "설명": "Thanks to the success of deep learning, cross-modal retrieval has made significant progress recently. However, there still remains a crucial bottleneck: how to bridge the modality gap to further enhance the retrieval accuracy. In this paper, we propose a self-supervised adversarial hashing (SSAH) approach, which lies among the early attempts to incorporate adversarial learning into cross-modal hashing in a self-supervised fashion. The primary contribution of this work is that two adversarial networks are leveraged to maximize the semantic correlation and consistency of the representations between different modalities. In addition, we harness a self-supervised semantic network to discover high-level semantic information in the form of multi-label annotations. Such information guides the feature learning process and preserves the modality relationships in both the common semantic space and the Hamming space. Extensive experiments carried out on three benchmark datasets validate that the proposed SSAH surpasses the state-of-the-art methods.",
            "저자": "Chao Li, Cheng Deng, Ning Li, Wei Liu, Xinbo Gao, Dacheng Tao",
            "전체 인용횟수": "418회 인용201820192020202120222023948819510872",
            "컨퍼런스": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "학술 문서": "Self-supervised adversarial hashing networks for cross-modal retrievalC Li, C Deng, N Li, W Liu, X Gao, D Tao - Proceedings of the IEEE conference on computer …, 2018418회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Self-Supervised Adversarial Hashing Networks for Cross-Modal Retrieval",
        "year": null
    },
    "Learning distance metrics with contextual constraints for image retrieval": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006",
            "게시자": "IEEE",
            "권": "2",
            "설명": "Relevant Component Analysis (RCA) has been proposed for learning distance metrics with contextual constraints for image retrieval. However, RCA has two important disadvantages. One is the lack of exploiting negative constraints which can also be informative, and the other is its incapability of capturing complex nonlinear relationships between data instances with the contextual information. In this paper, we propose two algorithms to overcome these two disadvantages, i.e., Discriminative Component Analysis (DCA) and Kernel DCA. Compared with other complicated methods for distance metric learning, our algorithms are rather simple to understand and very easy to solve. We evaluate the performance of our algorithms on image retrieval in which experimental results show that our algorithms are effective and promising in learning good quality distance metrics for image retrieval.",
            "저자": "Steven CH Hoi, Wei Liu, Michael R Lyu, Wei-Ying Ma",
            "전체 인용횟수": "407회 인용20072008200920102011201220132014201520162017201820192020202120222023101113302428363541333427261819155",
            "컨퍼런스": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)",
            "페이지": "2072-2078",
            "학술 문서": "Learning distance metrics with contextual constraints for image retrievalSCH Hoi, W Liu, MR Lyu, WY Ma - 2006 IEEE Computer Society Conference on Computer …, 2006407회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning distance metrics with contextual constraints for image retrieval",
        "year": null
    },
    "Learning to Compose Dynamic Tree Structures for Visual Contexts": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/6",
            "설명": "We propose to compose dynamic tree structures that place the objects in an image into a visual context, helping visual reasoning tasks such as scene graph generation and visual Q&A. Our visual context tree model, dubbed VCTree, has two key advantages over existing structured object representations including chains and fully-connected graphs: 1) The efficient and expressive binary tree encodes the inherent parallel/hierarchical relationships among objects, eg,\" clothes\" and\" pants\" are usually co-occur and belong to\" person\"; 2) the dynamic structure varies from image to image and task to task, allowing more content-/task-specific message passing among objects. To construct a VCTree, we design a score function that calculates the task-dependent validity between each object pair, and the tree is the binary version of the maximum spanning tree from the score matrix. Then, visual contexts are encoded by bidirectional TreeLSTM and decoded by task-specific models. We develop a hybrid learning procedure which integrates end-task supervised learning and the tree structure reinforcement learning, where the former's evaluation result serves as a self-critic for the latter's structure exploration. Experimental results on two benchmarks, which require reasoning over contexts: Visual Genome for scene graph generation and VQA2. 0 for visual Q&A, show that VCTree outperforms state-of-the-art results while discovering interpretable visual context structures.",
            "저자": "Kaihua Tang, Hanwang Zhang, Baoyuan Wu, Wenhan Luo, Wei Liu",
            "전체 인용횟수": "400회 인용20192020202120222023124187125135",
            "컨퍼런스": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "페이지": "arXiv preprint arXiv:1812.01880",
            "학술 문서": "Learning to compose dynamic tree structures for visual contextsK Tang, H Zhang, B Wu, W Luo, W Liu - Proceedings of the IEEE/CVF conference on computer …, 2019400회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning to Compose Dynamic Tree Structures for Visual Contexts",
        "year": null
    },
    "Unsupervised Deep Tracking": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/6",
            "설명": "We propose an unsupervised visual tracking method in this paper. Different from existing approaches using extensive annotated data for supervised learning, our CNN model is trained on large-scale unlabeled videos in an unsupervised manner. Our motivation is that a robust tracker should be effective in both the forward and backward predictions (ie, the tracker can forward localize the target object in successive frames and backtrace to its initial position in the first frame). We build our framework on a Siamese correlation filter network, which is trained using unlabeled raw videos. Meanwhile, we propose a multiple-frame validation method and a cost-sensitive loss to facilitate unsupervised learning. Without bells and whistles, the proposed unsupervised tracker achieves the baseline accuracy of fully supervised trackers, which require complete and accurate labels during training. Furthermore, unsupervised framework exhibits a potential in leveraging unlabeled or weakly labeled data to further improve the tracking accuracy.",
            "저자": "Ning Wang, Yibing Song, Chao Ma, Wengang Zhou, Wei Liu, Houqiang Li",
            "전체 인용횟수": "387회 인용2019202020212022202312871228974",
            "컨퍼런스": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "페이지": "arXiv preprint arXiv:1904.01828",
            "학술 문서": "Unsupervised deep trackingN Wang, Y Song, C Ma, W Zhou, W Liu, H Li - Proceedings of the IEEE/CVF conference on computer …, 2019387회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Unsupervised Deep Tracking",
        "year": null
    },
    "A Sufficient Condition for Convergences of Adam and RMSProp": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/6",
            "설명": "Adam and RMSProp are two of the most influential adaptive stochastic algorithms for training deep neural networks, which have been pointed out to be divergent even in the convex setting via a few simple counterexamples. Many attempts, such as decreasing an adaptive learning rate, adopting a big batch size, incorporating a temporal decorrelation technique, seeking an analogous surrogate, etc., have been tried to promote Adam/RMSProp-type algorithms to converge. In contrast with existing approaches, we introduce an alternative easy-to-check sufficient condition, which merely depends on the parameters of the base learning rate and combinations of historical second-order moments, to guarantee the global convergence of generic Adam/RMSProp for solving large-scale non-convex stochastic optimization. Moreover, we show that the convergences of several variants of Adam, such as AdamNC, AdaEMA, etc., can be directly implied via the proposed sufficient condition in the non-convex setting. In addition, we illustrate that Adam is essentially a specifically weighted AdaGrad with exponential moving average momentum, which provides a novel perspective for understanding Adam and RMSProp. This observation coupled with this sufficient condition gives much deeper interpretations on their divergences. At last, we validate the sufficient condition by applying Adam and RMSProp to tackle a certain counterexample and train deep neural networks. Numerical results are exactly in accord with our theoretical analysis.",
            "저자": "Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, Wei Liu",
            "전체 인용횟수": "378회 인용201820192020202120222023121519511097",
            "컨퍼런스": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "페이지": "arXiv preprint arXiv:1811.09358",
            "학술 문서": "A sufficient condition for convergences of adam and rmspropF Zou, L Shen, Z Jie, W Zhang, W Liu - Proceedings of the IEEE/CVF Conference on computer …, 2019378회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A Sufficient Condition for Convergences of Adam and RMSProp",
        "year": null
    },
    "Structures of the CXCR4 chemokine GPCR with small-molecule and cyclic peptide antagonists": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/11/19",
            "게시자": "American Association for the Advancement of Science",
            "권": "330",
            "설명": "Chemokine receptors are critical regulators of cell migration in the context of immune surveillance, inflammation, and development. The G protein–coupled chemokine receptor CXCR4 is specifically implicated in cancer metastasis and HIV-1 infection. Here we report five independent crystal structures of CXCR4 bound to an antagonist small molecule IT1t and a cyclic peptide CVX15 at 2.5 to 3.2 angstrom resolution. All structures reveal a consistent homodimer with an interface including helices V and VI that may be involved in regulating signaling. The location and shape of the ligand-binding sites differ from other G protein–coupled receptors and are closer to the extracellular surface. These structures provide new clues about the interactions between CXCR4 and its natural ligand CXCL12, and with the HIV-1 glycoprotein gp120.",
            "저널": "Science",
            "저자": "Beili Wu, Ellen YT Chien, Clifford D Mol, Gustavo Fenalti, Wei Liu, Vsevolod Katritch, Ruben Abagyan, Alexei Brooun, Peter Wells, F Christopher Bi, Damon J Hamel, Peter Kuhn, Tracy M Handel, Vadim Cherezov, Raymond C Stevens",
            "전체 인용횟수": "2050회 인용2011201220132014201520162017201820192020202120222023219284273218199160119128103103748364",
            "페이지": "1066-1071",
            "학술 문서": "Structures of the CXCR4 chemokine GPCR with small-molecule and cyclic peptide antagonistsB Wu, EYT Chien, CD Mol, G Fenalti, W Liu, V Katritch… - Science, 20102045회 인용 관련 학술자료 전체 19개의 버전 Structures of the CXCR4 chemokine receptor in complex with small molecule and cyclic peptide antagonists*B Wu, EYT Chien, CD Mol, G Fenalti, W Liu, R Abagyan… - Science6회 인용 관련 학술자료 Chien*B Wu - EY, Mol, CD, Fenalti, G., Liu, W., Katritch, V., Abagyan …, 20102회 인용 관련 학술자료 TM 45 Handel, V. Cherezov and RC Stevens*B Wu, EYT Chien, CD Mol, G Fenalti, W Liu, V Katritch… - Science, 20102회 인용 관련 학술자료 ",
            "호": "6007"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Structures of the CXCR4 chemokine GPCR with small-molecule and cyclic peptide antagonists",
        "year": null
    },
    "Structure of the human dopamine D3 receptor in complex with a D2/D3 selective antagonist": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/11/19",
            "게시자": "American Association for the Advancement of Science",
            "권": "330",
            "설명": "Dopamine modulates movement, cognition, and emotion through activation of dopamine G protein–coupled receptors in the brain. The crystal structure of the human dopamine D3 receptor (D3R) in complex with the small molecule D2R/D3R-specific antagonist eticlopride reveals important features of the ligand binding pocket and extracellular loops. On the intracellular side of the receptor, a locked conformation of the ionic lock and two distinctly different conformations of intracellular loop 2 are observed. Docking of R-22, a D3R-selective antagonist, reveals an extracellular extension of the eticlopride binding site that comprises a second binding pocket for the aryl amide of R-22, which differs between the highly homologous D2R and D3R. This difference provides direction to the design of D3R-selective agents for treating drug abuse and other neuropsychiatric indications.",
            "저널": "Science",
            "저자": "Ellen YT Chien, Wei Liu, Qiang Zhao, Vsevolod Katritch, Gye Won Han, Michael A Hanson, Lei Shi, Amy Hauck Newman, Jonathan A Javitch, Vadim Cherezov, Raymond C Stevens",
            "전체 인용횟수": "1312회 인용20112012201320142015201620172018201920202021202220231421931681401348067837669605237",
            "페이지": "1091-1095",
            "학술 문서": "Structure of the human dopamine D3 receptor in complex with a D2/D3 selective antagonistEYT Chien, W Liu, Q Zhao, V Katritch, G Won Han… - Science, 20101312회 인용 관련 학술자료 전체 22개의 버전 ",
            "호": "6007"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Structure of the human dopamine D3 receptor in complex with a D2/D3 selective antagonist",
        "year": null
    },
    "Structure of the human κ-opioid receptor in complex with JDTic": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/5/17",
            "게시자": "Nature Publishing Group UK",
            "권": "485",
            "설명": "Opioid receptors mediate the actions of endogenous and exogenous opioids on many physiological processes, including the regulation of pain, respiratory drive, mood, and—in the case of κ-opioid receptor (κ-OR)—dysphoria and psychotomimesis. Here we report the crystal structure of the human κ-OR in complex with the selective antagonist JDTic, arranged in parallel dimers, at 2.9 Å resolution. The structure reveals important features of the ligand-binding pocket that contribute to the high affinity and subtype selectivity of JDTic for the human κ-OR. Modelling of other important κ-OR-selective ligands, including the morphinan-derived antagonists norbinaltorphimine and 5′-guanidinonaltrindole, and the diterpene agonist salvinorin A analogue RB-64, reveals both common and distinct features for binding these diverse chemotypes. Analysis of site-directed mutagenesis and ligand structure–activity relationships …",
            "저널": "Nature",
            "저자": "Huixian Wu, Daniel Wacker, Mauro Mileni, Vsevolod Katritch, Gye Won Han, Eyal Vardy, Wei Liu, Aaron A Thompson, Xi-Ping Huang, F Ivy Carroll, S Wayne Mascarella, Richard B Westkaemper, Philip D Mosier, Bryan L Roth, Vadim Cherezov, Raymond C Stevens",
            "전체 인용횟수": "1045회 인용201220132014201520162017201820192020202120222023661781291329067987459634533",
            "페이지": "327-332",
            "학술 문서": "Structure of the human κ-opioid receptor in complex with JDTicH Wu, D Wacker, M Mileni, V Katritch, GW Han, E Vardy… - Nature, 20121045회 인용 관련 학술자료 전체 29개의 버전 ",
            "호": "7398"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Structure of the human κ-opioid receptor in complex with JDTic",
        "year": null
    },
    "Structural basis for allosteric regulation of GPCRs by sodium ions": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/7/13",
            "게시자": "American Association for the Advancement of Science",
            "권": "337",
            "설명": "Pharmacological responses of G protein–coupled receptors (GPCRs) can be fine-tuned by allosteric modulators. Structural studies of such effects have been limited due to the medium resolution of GPCR structures. We reengineered the human A2A adenosine receptor by replacing its third intracellular loop with apocytochrome b562RIL and solved the structure at 1.8 angstrom resolution. The high-resolution structure allowed us to identify 57 ordered water molecules inside the receptor comprising three major clusters. The central cluster harbors a putative sodium ion bound to the highly conserved aspartate residue Asp2.50. Additionally, two cholesterols stabilize the conformation of helix VI, and one of 23 ordered lipids intercalates inside the ligand-binding pocket. These high-resolution details shed light on the potential role of structured water molecules, sodium ions, and lipids/cholesterol in GPCR stabilization and …",
            "저널": "Science",
            "저자": "Wei Liu, Eugene Chun, Aaron A Thompson, Pavel Chubukov, Fei Xu, Vsevolod Katritch, Gye Won Han, Christopher B Roth, Laura H Heitman, Adriaan P IJzerman, Vadim Cherezov, Raymond C Stevens",
            "전체 인용횟수": "975회 인용2012201320142015201620172018201920202021202220231477971081071029611176546856",
            "페이지": "232-236",
            "학술 문서": "Structural basis for allosteric regulation of GPCRs by sodium ionsW Liu, E Chun, AA Thompson, P Chubukov, F Xu… - Science, 2012975회 인용 관련 학술자료 전체 22개의 버전 ",
            "호": "6091"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Structural basis for allosteric regulation of GPCRs by sodium ions",
        "year": null
    },
    "Structure of the human histamine H1 receptor complex with doxepin": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/7/7",
            "게시자": "Nature Publishing Group UK",
            "권": "475",
            "설명": "The biogenic amine histamine is an important pharmacological mediator involved in pathophysiological processes such as allergies and inflammations. Histamine H1 receptor (H1R) antagonists are very effective drugs alleviating the symptoms of allergic reactions. Here we show the crystal structure of the H1R complex with doxepin, a first-generation H1R antagonist. Doxepin sits deep in the ligand-binding pocket and directly interacts with Trp 4286.48, a highly conserved key residue in G-protein-coupled-receptor activation. This well-conserved pocket with mostly hydrophobic nature contributes to the low selectivity of the first-generation compounds. The pocket is associated with an anion-binding region occupied by a phosphate ion. Docking of various second-generation H1R antagonists reveals that the unique carboxyl group present in this class of compounds interacts with Lys 1915.39 and/or Lys 179ECL2 …",
            "저널": "Nature",
            "저자": "Tatsuro Shimamura, Mitsunori Shiroishi, Simone Weyand, Hirokazu Tsujimoto, Graeme Winter, Vsevolod Katritch, Ruben Abagyan, Vadim Cherezov, Wei Liu, Gye Won Han, Takuya Kobayashi, Raymond C Stevens, So Iwata",
            "전체 인용횟수": "932회 인용2011201220132014201520162017201820192020202120222023381111421051036861605751503842",
            "페이지": "65-70",
            "학술 문서": "Structure of the human histamine H1 receptor complex with doxepinT Shimamura, M Shiroishi, S Weyand, H Tsujimoto… - Nature, 2011932회 인용 관련 학술자료 전체 24개의 버전 Structure of the human histamine H1 receptor complex with doxepin*V Katritch, R Abagyan, V Cherezov, W Liu, GW Han… - Nature, 20116회 인용 관련 학술자료 ",
            "호": "7354"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Structure of the human histamine H1 receptor complex with doxepin",
        "year": null
    },
    "Crystal structure of rhodopsin bound to arrestin by femtosecond X-ray laser": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/7/30",
            "게시자": "Nature Publishing Group UK",
            "권": "523",
            "설명": "G-protein-coupled receptors (GPCRs) signal primarily through G proteins or arrestins. Arrestin binding to GPCRs blocks G protein interaction and redirects signalling to numerous G-protein-independent pathways. Here we report the crystal structure of a constitutively active form of human rhodopsin bound to a pre-activated form of the mouse visual arrestin, determined by serial femtosecond X-ray laser crystallography. Together with extensive biochemical and mutagenesis data, the structure reveals an overall architecture of the rhodopsin–arrestin assembly in which rhodopsin uses distinct structural elements, including transmembrane helix 7 and helix 8, to recruit arrestin. Correspondingly, arrestin adopts the pre-activated conformation, with a ∼20° rotation between the amino and carboxy domains, which opens up a cleft in arrestin to accommodate a short helix formed by the second intracellular loop of rhodopsin …",
            "저널": "Nature",
            "저자": "Yanyong Kang, X Edward Zhou, Xiang Gao, Yuanzheng He, Wei Liu, Andrii Ishchenko, Anton Barty, Thomas A White, Oleksandr Yefanov, Gye Won Han, Qingping Xu, Parker W De Waal, Jiyuan Ke, MH Eileen Tan, Chenghai Zhang, Arne Moeller, Graham M West, Bruce D Pascal, Ned Van Eps, Lydia N Caro, Sergey A Vishnivetskiy, Regina J Lee, Kelly M Suino-Powell, Xin Gu, Kuntal Pal, Jinming Ma, Xiaoyong Zhi, Sébastien Boutet, Garth J Williams, Marc Messerschmidt, Cornelius Gati, Nadia A Zatsepin, Dingjie Wang, Daniel James, Shibom Basu, Shatabdi Roy-Chowdhury, Chelsie E Conrad, Jesse Coe, Haiguang Liu, Stella Lisova, Christopher Kupitz, Ingo Grotjohann, Raimund Fromme, Yi Jiang, Minjia Tan, Huaiyu Yang, Jun Li, Meitian Wang, Zhong Zheng, Dianfan Li, Nicole Howe, Yingming Zhao, Jörg Standfuss, Kay Diederichs, Yuhui Dong, Clinton S Potter, Bridget Carragher, Martin Caffrey, Hualiang Jiang, Henry N Chapman, John CH Spence, Petra Fromme, Uwe Weierstall, Oliver P Ernst, Vsevolod Katritch, Vsevolod V Gurevich, Patrick R Griffin, Wayne L Hubbell, Raymond C Stevens, Vadim Cherezov, Karsten Melcher, H Eric Xu",
            "전체 인용횟수": "795회 인용201420152016201720182019202020212022202322010113412510892965853",
            "페이지": "561-567",
            "학술 문서": "Crystal structure of rhodopsin bound to arrestin by femtosecond X-ray laserY Kang, XE Zhou, X Gao, Y He, W Liu, A Ishchenko… - Nature, 2015795회 인용 관련 학술자료 전체 28개의 버전 ",
            "호": "7562"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Crystal structure of rhodopsin bound to arrestin by femtosecond X-ray laser",
        "year": null
    },
    "Structural features for functional selectivity at serotonin receptors": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/5/3",
            "게시자": "American Association for the Advancement of Science",
            "권": "340",
            "설명": "Drugs active at G protein–coupled receptors (GPCRs) can differentially modulate either canonical or noncanonical signaling pathways via a phenomenon known as functional selectivity or biased signaling. We report biochemical studies showing that the hallucinogen lysergic acid diethylamide, its precursor ergotamine (ERG), and related ergolines display strong functional selectivity for β-arrestin signaling at the 5-HT2B 5-hydroxytryptamine (5-HT) receptor, whereas they are relatively unbiased at the 5-HT1B receptor. To investigate the structural basis for biased signaling, we determined the crystal structure of the human 5-HT2B receptor bound to ERG and compared it with the 5-HT1B/ERG structure. Given the relatively poor understanding of GPCR structure and function to date, insight into different GPCR signaling pathways is important to better understand both adverse and favorable therapeutic activities.",
            "저널": "Science",
            "저자": "Daniel Wacker, Chong Wang, Vsevolod Katritch, Gye Won Han, Xi-Ping Huang, Eyal Vardy, John D McCorvy, Yi Jiang, Meihua Chu, Fai Yiu Siu, Wei Liu, H Eric Xu, Vadim Cherezov, Bryan L Roth, Raymond C Stevens",
            "전체 인용횟수": "759회 인용2013201420152016201720182019202020212022202341103965867886860794253",
            "페이지": "615-619",
            "학술 문서": "Structural features for functional selectivity at serotonin receptorsD Wacker, C Wang, V Katritch, GW Han, XP Huang… - Science, 2013759회 인용 관련 학술자료 전체 18개의 버전 ",
            "호": "6132"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Structural features for functional selectivity at serotonin receptors",
        "year": null
    },
    "Lipidic cubic phase injector facilitates membrane protein serial femtosecond crystallography": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/2/14",
            "게시자": "Nature Publishing Group UK",
            "권": "5",
            "설명": "Lipidic cubic phase (LCP) crystallization has proven successful for high-resolution structure determination of challenging membrane proteins. Here we present a technique for extruding gel-like LCP with embedded membrane protein microcrystals, providing a continuously renewed source of material for serial femtosecond crystallography. Data collected from sub-10-μm-sized crystals produced with less than 0.5 mg of purified protein yield structural insights regarding cyclopamine binding to the Smoothened receptor.",
            "저널": "Nature communications",
            "저자": "Uwe Weierstall, Daniel James, Chong Wang, Thomas A White, Dingjie Wang, Wei Liu, John CH Spence, R Bruce Doak, Garrett Nelson, Petra Fromme, Raimund Fromme, Ingo Grotjohann, Christopher Kupitz, Nadia A Zatsepin, Haiguang Liu, Shibom Basu, Daniel Wacker, Gye Won Han, Vsevolod Katritch, Sébastien Boutet, Marc Messerschmidt, Garth J Williams, Jason E Koglin, M Marvin Seibert, Markus Klinker, Cornelius Gati, Robert L Shoeman, Anton Barty, Henry N Chapman, Richard A Kirian, Kenneth R Beyerlein, Raymond C Stevens, Dianfan Li, Syed TA Shah, Nicole Howe, Martin Caffrey, Vadim Cherezov",
            "전체 인용횟수": "581회 인용201420152016201720182019202020212022202336776466747177354432",
            "페이지": "3309",
            "학술 문서": "Lipidic cubic phase injector facilitates membrane protein serial femtosecond crystallographyU Weierstall, D James, C Wang, TA White, D Wang… - Nature communications, 2014581회 인용 관련 학술자료 전체 18개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Lipidic cubic phase injector facilitates membrane protein serial femtosecond crystallography",
        "year": null
    },
    "Structural basis for molecular recognition at serotonin receptors": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/5/3",
            "게시자": "American Association for the Advancement of Science",
            "권": "340",
            "설명": "Serotonin or 5-hydroxytryptamine (5-HT) regulates a wide spectrum of human physiology through the 5-HT receptor family. We report the crystal structures of the human 5-HT1B G protein–coupled receptor bound to the agonist antimigraine medications ergotamine and dihydroergotamine. The structures reveal similar binding modes for these ligands, which occupy the orthosteric pocket and an extended binding pocket close to the extracellular loops. The orthosteric pocket is formed by residues conserved in the 5-HT receptor family, clarifying the family-wide agonist activity of 5-HT. Compared with the structure of the 5-HT2B receptor, the 5-HT1B receptor displays a 3 angstrom outward shift at the extracellular end of helix V, resulting in a more open extended pocket that explains subtype selectivity. Together with docking and mutagenesis studies, these structures provide a comprehensive structural basis for …",
            "저널": "Science",
            "저자": "Chong Wang, Yi Jiang, Jinming Ma, Huixian Wu, Daniel Wacker, Vsevolod Katritch, Gye Won Han, Wei Liu, Xi-Ping Huang, Eyal Vardy, John D McCorvy, Xiang Gao, X Edward Zhou, Karsten Melcher, Chenghai Zhang, Fang Bai, Huaiyu Yang, Linlin Yang, Hualiang Jiang, Bryan L Roth, Vadim Cherezov, Raymond C Stevens, H Eric Xu",
            "전체 인용횟수": "577회 인용201320142015201620172018201920202021202220233079835553633942554432",
            "페이지": "610-614",
            "학술 문서": "Structural basis for molecular recognition at serotonin receptorsC Wang, Y Jiang, J Ma, H Wu, D Wacker, V Katritch… - Science, 2013577회 인용 관련 학술자료 전체 20개의 버전 ",
            "호": "6132"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Structural basis for molecular recognition at serotonin receptors",
        "year": null
    },
    "Structure of the nociceptin/orphanin FQ receptor in complex with a peptide mimetic": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/5/17",
            "게시자": "Nature Publishing Group UK",
            "권": "485",
            "설명": "Members of the opioid receptor family of G-protein-coupled receptors (GPCRs) are found throughout the peripheral and central nervous system, where they have key roles in nociception and analgesia. Unlike the ‘classical’ opioid receptors, δ, κ and μ (δ-OR, κ-OR and μ-OR), which were delineated by pharmacological criteria in the 1970s and 1980s, the nociceptin/orphanin FQ (N/OFQ) peptide receptor (NOP, also known as ORL-1) was discovered relatively recently by molecular cloning and characterization of an orphan GPCR. Although it shares high sequence similarity with classical opioid GPCR subtypes (∼60%), NOP has a markedly distinct pharmacology, featuring activation by the endogenous peptide N/OFQ, and unique selectivity for exogenous ligands,. Here we report the crystal structure of human NOP, solved in complex with the peptide mimetic antagonist compound-24 (C-24) (ref. ), revealing atomic …",
            "저널": "Nature",
            "저자": "Aaron A Thompson, Wei Liu, Eugene Chun, Vsevolod Katritch, Huixian Wu, Eyal Vardy, Xi-Ping Huang, Claudio Trapella, Remo Guerrini, Girolamo Calo, Bryan L Roth, Vadim Cherezov, Raymond C Stevens",
            "전체 인용횟수": "576회 인용2012201320142015201620172018201920202021202220233611384884531443725302713",
            "페이지": "395-399",
            "학술 문서": "Structure of the nociceptin/orphanin FQ receptor in complex with a peptide mimeticAA Thompson, W Liu, E Chun, V Katritch, H Wu… - Nature, 2012576회 인용 관련 학술자료 전체 25개의 버전 ",
            "호": "7398"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Structure of the nociceptin/orphanin FQ receptor in complex with a peptide mimetic",
        "year": null
    },
    "Structure of the human smoothened receptor bound to an antitumour agent": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/5/16",
            "게시자": "Nature Publishing Group UK",
            "권": "497",
            "설명": "The smoothened (SMO) receptor, a key signal transducer in the hedgehog signalling pathway, is responsible for the maintenance of normal embryonic development and is implicated in carcinogenesis. It is classified as a class frizzled (class F) G-protein-coupled receptor (GPCR), although the canonical hedgehog signalling pathway involves the GLI transcription factors and the sequence similarity with class A GPCRs is less than 10%. Here we report the crystal structure of the transmembrane domain of the human SMO receptor bound to the small-molecule antagonist LY2940680 at 2.5 Å resolution. Although the SMO receptor shares the seven-transmembrane helical fold, most of the conserved motifs for class A GPCRs are absent, and the structure reveals an unusually complex arrangement of long extracellular loops stabilized by four disulphide bonds. The ligand binds at the extracellular end of the seven …",
            "저널": "Nature",
            "저자": "Chong Wang, Huixian Wu, Vsevolod Katritch, Gye Won Han, Xi-Ping Huang, Wei Liu, Fai Yiu Siu, Bryan L Roth, Vadim Cherezov, Raymond C Stevens",
            "전체 인용횟수": "529회 인용2013201420152016201720182019202020212022202329841005843634940272111",
            "페이지": "338-343",
            "학술 문서": "Structure of the human smoothened receptor bound to an antitumour agentC Wang, H Wu, V Katritch, GW Han, XP Huang, W Liu… - Nature, 2013529회 인용 관련 학술자료 전체 18개의 버전 ",
            "호": "7449"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Structure of the human smoothened receptor bound to an antitumour agent",
        "year": null
    },
    "Serial femtosecond crystallography of G protein–coupled receptors": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/12/20",
            "게시자": "American Association for the Advancement of Science",
            "권": "342",
            "설명": "X-ray crystallography of G protein–coupled receptors and other membrane proteins is hampered by difficulties associated with growing sufficiently large crystals that withstand radiation damage and yield high-resolution data at synchrotron sources. We used an x-ray free-electron laser (XFEL) with individual 50-femtosecond-duration x-ray pulses to minimize radiation damage and obtained a high-resolution room-temperature structure of a human serotonin receptor using sub-10-micrometer microcrystals grown in a membrane mimetic matrix known as lipidic cubic phase. Compared with the structure solved by using traditional microcrystallography from cryo-cooled crystals of about two orders of magnitude larger volume, the room-temperature XFEL structure displays a distinct distribution of thermal motions and conformations of residues that likely more accurately represent the receptor structure and dynamics in a …",
            "저널": "Science",
            "저자": "Wei Liu, Daniel Wacker, Cornelius Gati, Gye Won Han, Daniel James, Dingjie Wang, Garrett Nelson, Uwe Weierstall, Vsevolod Katritch, Anton Barty, Nadia A Zatsepin, Dianfan Li, Marc Messerschmidt, Sébastien Boutet, Garth J Williams, Jason E Koglin, M Marvin Seibert, Chong Wang, Syed TA Shah, Shibom Basu, Raimund Fromme, Christopher Kupitz, Kimberley N Rendek, Ingo Grotjohann, Petra Fromme, Richard A Kirian, Kenneth R Beyerlein, Thomas A White, Henry N Chapman, Martin Caffrey, John CH Spence, Raymond C Stevens, Vadim Cherezov",
            "전체 인용횟수": "512회 인용201420152016201720182019202020212022202349897664615144291923",
            "페이지": "1521-1524",
            "학술 문서": "Serial femtosecond crystallography of G protein–coupled receptorsW Liu, D Wacker, C Gati, GW Han, D James, D Wang… - Science, 2013512회 인용 관련 학술자료 전체 15개의 버전 ",
            "호": "6165"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Serial femtosecond crystallography of G protein–coupled receptors",
        "year": null
    },
    "Structure of the human glucagon class B G-protein-coupled receptor": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/7/25",
            "게시자": "Nature Publishing Group UK",
            "권": "499",
            "설명": "Binding of the glucagon peptide to the glucagon receptor (GCGR) triggers the release of glucose from the liver during fasting; thus GCGR plays an important role in glucose homeostasis. Here we report the crystal structure of the seven transmembrane helical domain of human GCGR at 3.4 Å resolution, complemented by extensive site-specific mutagenesis, and a hybrid model of glucagon bound to GCGR to understand the molecular recognition of the receptor for its native ligand. Beyond the shared seven transmembrane fold, the GCGR transmembrane domain deviates from class A G-protein-coupled receptors with a large ligand-binding pocket and the first transmembrane helix having a ‘stalk’ region that extends three alpha-helical turns above the plane of the membrane. The stalk positions the extracellular domain (∼12 kilodaltons) relative to the membrane to form the glucagon-binding site that captures the …",
            "저널": "Nature",
            "저자": "Fai Yiu Siu, Min He, Chris De Graaf, Gye Won Han, Dehua Yang, Zhiyun Zhang, Caihong Zhou, Qingping Xu, Daniel Wacker, Jeremiah S Joseph, Wei Liu, Jesper Lau, Vadim Cherezov, Vsevolod Katritch, Ming-Wei Wang, Raymond C Stevens",
            "전체 인용횟수": "446회 인용201320142015201620172018201920202021202220231875935554452630151712",
            "페이지": "444-449",
            "학술 문서": "Structure of the human glucagon class B G-protein-coupled receptorFY Siu, M He, C De Graaf, GW Han, D Yang, Z Zhang… - Nature, 2013446회 인용 관련 학술자료 전체 23개의 버전 ",
            "호": "7459"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Structure of the human glucagon class B G-protein-coupled receptor",
        "year": null
    },
    "Fusion partner toolchest for the stabilization and crystallization of G protein-coupled receptors": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/6/6",
            "게시자": "Elsevier",
            "권": "20",
            "설명": "Structural studies of human G protein-coupled receptors (GPCRs) have recently been accelerated through the use of a fusion partner that was inserted into the third intracellular loop. Using chimeras of the human β2-adrenergic and human A2A adenosine receptors, we present the methodology and data for the initial selection of an expanded set of fusion partners for crystallizing GPCRs. In particular, use of the thermostabilized apocytochrome b562RIL as a fusion partner displays certain advantages over previously utilized fusion proteins, resulting in a significant improvement in stability and structure of GPCR-fusion constructs.",
            "저널": "Structure",
            "저자": "Eugene Chun, Aaron A Thompson, Wei Liu, Christopher B Roth, Mark T Griffith, Vsevolod Katritch, Joshua Kunken, Fei Xu, Vadim Cherezov, Michael A Hanson, Raymond C Stevens",
            "전체 인용횟수": "421회 인용20122013201420152016201720182019202020212022202333237423038363450414234",
            "페이지": "967-976",
            "학술 문서": "Fusion partner toolchest for the stabilization and crystallization of G protein-coupled receptorsE Chun, AA Thompson, W Liu, CB Roth, MT Griffith… - Structure, 2012421회 인용 관련 학술자료 전체 13개의 버전 ",
            "호": "6"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Fusion partner toolchest for the stabilization and crystallization of G protein-coupled receptors",
        "year": null
    },
    "Structure of the angiotensin receptor revealed by serial femtosecond crystallography": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/5/7",
            "게시자": "Elsevier",
            "권": "161",
            "설명": "Angiotensin II type 1 receptor (AT1R) is a G protein-coupled receptor that serves as a primary regulator for blood pressure maintenance. Although several anti-hypertensive drugs have been developed as AT1R blockers (ARBs), the structural basis for AT1R ligand-binding and regulation has remained elusive, mostly due to the difficulties of growing high-quality crystals for structure determination using synchrotron radiation. By applying the recently developed method of serial femtosecond crystallography at an X-ray free-electron laser, we successfully determined the room-temperature crystal structure of the human AT1R in complex with its selective antagonist ZD7155 at 2.9-Å resolution. The AT1R-ZD7155 complex structure revealed key structural features of AT1R and critical interactions for ZD7155 binding. Docking simulations of the clinically used ARBs into the AT1R structure further elucidated both the common …",
            "저널": "Cell",
            "저자": "Haitao Zhang, Hamiyet Unal, Cornelius Gati, Gye Won Han, Wei Liu, Nadia A Zatsepin, Daniel James, Dingjie Wang, Garrett Nelson, Uwe Weierstall, Michael R Sawaya, Qingping Xu, Marc Messerschmidt, Garth J Williams, Sébastien Boutet, Oleksandr M Yefanov, Thomas A White, Chong Wang, Andrii Ishchenko, Kalyan C Tirupula, Russell Desnoyer, Jesse Coe, Chelsie E Conrad, Petra Fromme, Raymond C Stevens, Vsevolod Katritch, Sadashiva S Karnik, Vadim Cherezov",
            "전체 인용횟수": "367회 인용20142015201620172018201920202021202220231244454564240383927",
            "페이지": "833-844",
            "학술 문서": "Structure of the angiotensin receptor revealed by serial femtosecond crystallographyH Zhang, H Unal, C Gati, GW Han, W Liu, NA Zatsepin… - Cell, 2015367회 인용 관련 학술자료 전체 22개의 버전 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Structure of the angiotensin receptor revealed by serial femtosecond crystallography",
        "year": null
    },
    "Structural basis for selectivity and diversity in angiotensin II receptors": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/4/20",
            "게시자": "Nature Publishing Group UK",
            "권": "544",
            "설명": "The angiotensin II receptors AT1R and AT2R serve as key components of the renin–angiotensin–aldosterone system. AT1R has a central role in the regulation of blood pressure, but the function of AT2R is unclear and it has a variety of reported effects. To identify the mechanisms that underlie the differences in function and ligand selectivity between these receptors, here we report crystal structures of human AT2R bound to an AT2R-selective ligand and to an AT1R/AT2R dual ligand, capturing the receptor in an active-like conformation. Unexpectedly, helix VIII was found in a non-canonical position, stabilizing the active-like state, but at the same time preventing the recruitment of G proteins or β-arrestins, in agreement with the lack of signalling responses in standard cellular assays. Structure–activity relationship, docking and mutagenesis studies revealed the crucial interactions for ligand binding and selectivity. Our …",
            "저널": "Nature",
            "저자": "Haitao Zhang, Gye Won Han, Alexander Batyuk, Andrii Ishchenko, Kate L White, Nilkanth Patel, Anastasiia Sadybekov, Beata Zamlynny, Michael T Rudd, Kaspar Hollenstein, Alexandra Tolstikova, Thomas A White, Mark S Hunter, Uwe Weierstall, Wei Liu, Kerim Babaoglu, Eric L Moore, Ryan D Katz, Jennifer M Shipman, Margarita Garcia-Calvo, Sujata Sharma, Payal Sheth, Stephen M Soisson, Raymond C Stevens, Vsevolod Katritch, Vadim Cherezov",
            "전체 인용횟수": "207회 인용201720182019202020212022202319383333283223",
            "페이지": "327-332",
            "학술 문서": "Structural basis for selectivity and diversity in angiotensin II receptorsH Zhang, GW Han, A Batyuk, A Ishchenko, KL White… - Nature, 2017207회 인용 관련 학술자료 전체 16개의 버전 ",
            "호": "7650"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Structural basis for selectivity and diversity in angiotensin II receptors",
        "year": null
    },
    "Structure of the full-length glucagon class B G-protein-coupled receptor": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/6/8",
            "게시자": "Nature Publishing Group UK",
            "권": "546",
            "설명": "The human glucagon receptor, GCGR, belongs to the class B G-protein-coupled receptor family and plays a key role in glucose homeostasis and the pathophysiology of type 2 diabetes. Here we report the 3.0 Å crystal structure of full-length GCGR containing both the extracellular domain and transmembrane domain in an inactive conformation. The two domains are connected by a 12-residue segment termed the stalk, which adopts a β-strand conformation, instead of forming an α-helix as observed in the previously solved structure of the GCGR transmembrane domain. The first extracellular loop exhibits a β-hairpin conformation and interacts with the stalk to form a compact β-sheet structure. Hydrogen–deuterium exchange, disulfide crosslinking and molecular dynamics studies suggest that the stalk and the first extracellular loop have critical roles in modulating peptide ligand binding and receptor activation …",
            "저널": "Nature",
            "저자": "Haonan Zhang, Anna Qiao, Dehua Yang, Linlin Yang, Antao Dai, Chris De Graaf, Steffen Reedtz-Runge, Venkatasubramanian Dharmarajan, Hui Zhang, Gye Won Han, Thomas D Grant, Raymond G Sierra, Uwe Weierstall, Garrett Nelson, Wei Liu, Yanhong Wu, Limin Ma, Xiaoqing Cai, Guangyao Lin, Xiaoai Wu, Zhi Geng, Yuhui Dong, Gaojie Song, Patrick R Griffin, Jesper Lau, Vadim Cherezov, Huaiyu Yang, Michael A Hanson, Raymond C Stevens, Qiang Zhao, Hualiang Jiang, Ming-Wei Wang, Beili Wu",
            "전체 인용횟수": "190회 인용201720182019202020212022202319423838182113",
            "페이지": "259-264",
            "학술 문서": "Structure of the full-length glucagon class B G-protein-coupled receptorH Zhang, A Qiao, D Yang, L Yang, A Dai, C De Graaf… - Nature, 2017190회 인용 관련 학술자료 전체 18개의 버전 ",
            "호": "7657"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Structure of the full-length glucagon class B G-protein-coupled receptor",
        "year": null
    },
    "Structural basis for bifunctional peptide recognition at human δ-opioid receptor": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/3",
            "게시자": "Nature Publishing Group US",
            "권": "22",
            "설명": "Bifunctional μ- and δ-opioid receptor (OR) ligands are potential therapeutic alternatives, with diminished side effects, to alkaloid opiate analgesics. We solved the structure of human δ-OR bound to the bifunctional δ-OR antagonist and μ-OR agonist tetrapeptide H-Dmt-Tic-Phe-Phe-NH2 (DIPP-NH2) by serial femtosecond crystallography, revealing a cis-peptide bond between H-Dmt and Tic. The observed receptor-peptide interactions are critical for understanding of the pharmacological profiles of opioid peptides and for development of improved analgesics.",
            "저널": "Nature structural & molecular biology",
            "저자": "Gustavo Fenalti, Nadia A Zatsepin, Cecilia Betti, Patrick Giguere, Gye Won Han, Andrii Ishchenko, Wei Liu, Karel Guillemyn, Haitao Zhang, Daniel James, Dingjie Wang, Uwe Weierstall, John CH Spence, Sébastien Boutet, Marc Messerschmidt, Garth J Williams, Cornelius Gati, Oleksandr M Yefanov, Thomas A White, Dominik Oberthuer, Markus Metz, Chun Hong Yoon, Anton Barty, Henry N Chapman, Shibom Basu, Jesse Coe, Chelsie E Conrad, Raimund Fromme, Petra Fromme, Dirk Tourwé, Peter W Schiller, Bryan L Roth, Steven Ballet, Vsevolod Katritch, Raymond C Stevens, Vadim Cherezov",
            "전체 인용횟수": "186회 인용2014201520162017201820192020202120222023116282432252212167",
            "페이지": "265-268",
            "학술 문서": "Structural basis for bifunctional peptide recognition at human δ-opioid receptorG Fenalti, NA Zatsepin, C Betti, P Giguere, GW Han… - Nature structural & molecular biology, 2015186회 인용 관련 학술자료 전체 21개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Structural basis for bifunctional peptide recognition at human δ-opioid receptor",
        "year": null
    },
    "Structural basis of ligand recognition at the human MT1 melatonin receptor": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/5/9",
            "게시자": "Nature Publishing Group UK",
            "권": "569",
            "설명": "Melatonin (N-acetyl-5-methoxytryptamine) is a neurohormone that maintains circadian rhythms by synchronization to environmental cues and is involved in diverse physiological processes such as the regulation of blood pressure and core body temperature, oncogenesis, and immune function. Melatonin is formed in the pineal gland in a light-regulated manner by enzymatic conversion from 5-hydroxytryptamine (5-HT or serotonin), and modulates sleep and wakefulness by activating two high-affinity G-protein-coupled receptors, type 1A (MT1) and type 1B (MT2),. Shift work, travel, and ubiquitous artificial lighting can disrupt natural circadian rhythms; as a result, sleep disorders affect a substantial population in modern society and pose a considerable economic burden. Over-the-counter melatonin is widely used to alleviate jet lag and as a safer alternative to benzodiazepines and other sleeping aids,, and is one of …",
            "저널": "Nature",
            "저자": "Benjamin Stauch, Linda C Johansson, John D McCorvy, Nilkanth Patel, Gye Won Han, Xi-Ping Huang, Cornelius Gati, Alexander Batyuk, Samuel T Slocum, Andrii Ishchenko, Wolfgang Brehm, Thomas A White, Nairie Michaelian, Caleb Madsen, Lan Zhu, Thomas D Grant, Jessica M Grandner, Anna Shiriaeva, Reid HJ Olsen, Alexandra R Tribo, Saïd Yous, Raymond C Stevens, Uwe Weierstall, Vsevolod Katritch, Bryan L Roth, Wei Liu, Vadim Cherezov",
            "전체 인용횟수": "165회 인용201920202021202220231739383831",
            "페이지": "284-288",
            "학술 문서": "Structural basis of ligand recognition at the human MT1 melatonin receptorB Stauch, LC Johansson, JD McCorvy, N Patel… - Nature, 2019165회 인용 관련 학술자료 전체 28개의 버전 ",
            "호": "7755"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Structural basis of ligand recognition at the human MT1 melatonin receptor",
        "year": null
    },
    "Serial millisecond crystallography of membrane and soluble protein microcrystals using synchrotron radiation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/7/1",
            "게시자": "International Union of Crystallography",
            "권": "4",
            "설명": "Crystal structure determination of biological macromolecules using the novel technique of serial femtosecond crystallography (SFX) is severely limited by the scarcity of X-ray free-electron laser (XFEL) sources. However, recent and future upgrades render microfocus beamlines at synchrotron-radiation sources suitable for room-temperature serial crystallography data collection also. Owing to the longer exposure times that are needed at synchrotrons, serial data collection is termed serial millisecond crystallography (SMX). As a result, the number of SMX experiments is growing rapidly, with a dozen experiments reported so far. Here, the first high-viscosity injector-based SMX experiments carried out at a US synchrotron source, the Advanced Photon Source (APS), are reported. Microcrystals (5–20 µm) of a wide variety of proteins, including lysozyme, thaumatin, phycocyanin, the human A2A adenosine receptor …",
            "저널": "IUCrJ",
            "저자": "Jose M Martin-Garcia, Chelsie E Conrad, Garrett Nelson, Natasha Stander, Nadia A Zatsepin, James Zook, Lan Zhu, James Geiger, Eugene Chun, David Kissick, Mark C Hilgart, Craig Ogata, Andrii Ishchenko, Nirupa Nagaratnam, Shatabdi Roy-Chowdhury, Jesse Coe, Ganesh Subramanian, Alexander Schaffer, Daniel James, Gihan Ketwala, Nagarajan Venugopalan, Shenglan Xu, Stephen Corcoran, Dale Ferguson, Uwe Weierstall, John CH Spence, Vadim Cherezov, Petra Fromme, Robert F Fischetti, Wei Liu",
            "전체 인용횟수": "145회 인용20172018201920202021202220237183040191611",
            "페이지": "439-454",
            "학술 문서": "Serial millisecond crystallography of membrane and soluble protein microcrystals using synchrotron radiationJM Martin-Garcia, CE Conrad, G Nelson, N Stander… - IUCrJ, 2017145회 인용 관련 학술자료 전체 16개의 버전 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Serial millisecond crystallography of membrane and soluble protein microcrystals using synchrotron radiation",
        "year": null
    },
    "YOLO9000: Better, Faster, Stronger.": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time.",
            "저자": "Joseph Redmon, Ali Farhadi",
            "전체 인용횟수": "19143회 인용2017201820192020202120222023356139623613000386444593546",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "학술 문서": "YOLO9000: better, faster, strongerJ Redmon, A Farhadi - Proceedings of the IEEE conference on computer …, 201718997회 인용 관련 학술자료 전체 23개의 버전 ieee 2017 ieee conference on computer Vision and Pattern Recognition (cvpr)-honolulu, hi (2017.7. 21-2017.7. 26)*J Redmon, A Farhadi - 2017 ieee conference on computer vision and pattern …, 2017165회 인용 관련 학술자료 YOLO9000: Better, faster, stronger. arXiv 2016*J Redmon, A Farhadi - arXiv preprint arXiv:1612.08242, 2016143회 인용 관련 학술자료 YOLO9000: Better, faster, stronger. arXiv*J Redmon, A Farhadi - arXiv preprint arXiv:1612.08242, 201683회 인용 관련 학술자료 YOLO9000: better, faster, stronger. CoRR abs/1612.08242 (2016)*J Redmon, A Farhadi - arXiv preprint arXiv:1612.08242, 201633회 인용 관련 학술자료 YOLO9000: Better, Faster, Stronger. University of Washington*J Redmon, A Farhadi - Allen Institute for AI, Facebook AI Research, 20178회 인용 관련 학술자료 YOLO9000: Be er, faster, stronger*J Redmon, A Farhadi - arXiv preprint arXiv:1612.08242, 20166회 인용 관련 학술자료 YOLO9000: better, faster, stronger. arXiv [Preprint]. 2017*J Redmon, A Farhadi - All rights reserved. No reuse allowed without …, 20175회 인용 관련 학술자료 YOLO9000: Better, Faster, Stronger, Dec. 2016*J Redmon, A Farhadi - URL: http://arxiv. org/abs/1612.082425회 인용 관련 학술자료 Yolov2: Real‐time object detection*J Redmon, A Farhadi - 20174회 인용 관련 학술자료 YOLO9000: Better, stronger, faster*J Redmon, A Farhadi - 2017 IEEE Conference on Computer Vision and …, 20174회 인용 관련 학술자료 YOLO9000: Better, Faster, Stronger. arXiv e-prints, page*J Redmon, A Farhadi - arXiv preprint arXiv:1612.08242, 20162회 인용 관련 학술자료 YOLO9000: Better, Faster, Stronger [Электронный ресурс]/Joseph Redmon, Ali Farhadi–University of Washington*J Redmon - Allen Institute for AI–2017. Режим доступа: https …2회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "YOLO9000: Better, Faster, Stronger.",
        "year": null
    },
    "Xnor-net: Imagenet classification using binary convolutional neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/9/17",
            "게시자": "Springer International Publishing",
            "도서": "European conference on computer vision",
            "설명": " We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values resulting in 32 memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58 faster convolutional operations (in terms of number of the high precision operations) and 32 memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is the same as the full-precision AlexNet. We …",
            "저자": "Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, Ali Farhadi",
            "전체 인용횟수": "5128회 인용2016201720182019202020212022202357284654809962908819591",
            "페이지": "525-542",
            "학술 문서": "Xnor-net: Imagenet classification using binary convolutional neural networksM Rastegari, V Ordonez, J Redmon, A Farhadi - European conference on computer vision, 20165106회 인용 관련 학술자료 전체 9개의 버전 XNOR-Net: ImageNet classification using binary convolutional neural networks. CoRR abs/1603.05279 (2016)*M Rastegari, V Ordonez, J Redmon, A Farhadi - arXiv preprint arXiv:1603.05279, 201619회 인용 관련 학술자료 Computer Vision—ECCV 2016*M Rastegari, V Ordonez, J Redmon, A Farhadi - Proceedings of the 14th European Conference …, 20169회 인용 관련 학술자료 Xnor-net: Imagenet classification using binary convolutional neural networks. ArXiv e-prints*M Rastegari, V Ordonez, J Redmon, A Farhadi - arXiv preprint arXiv:1603.05279, 20167회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Xnor-net: Imagenet classification using binary convolutional neural networks",
        "year": null
    },
    "Darknet: Open source neural networks in c": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "저자": "J Redmon",
            "전체 인용횟수": "1576회 인용201620172018201920202021202220231349156235290318322176",
            "출처": "http://pjreddie.com/darknet",
            "학술 문서": "Darknet: Open source neural networks in cJ Redmon - 20131201회 인용 관련 학술자료 YOLOv3: an incremental improvement. CoRR abs/1804.02767 (2018)*J Redmon, A Farhadi - arXiv preprint arXiv:1804.02767, 1804145회 인용 관련 학술자료 Darknet: open source neural networks in C. 2013–2016*J Redmon - 2016139회 인용 관련 학술자료 2016. Darknet: Open Source Neural Networks in C*J Redmon - 201371회 인용 관련 학술자료 Darknet*J Redmon - pjreddie. com/darknet/.[Accessed 15 September 2018], 201832회 인용 관련 학술자료 2013–2016 Darknet: Open Source Neural Networks in C*J Redmon - See http://pjreddie. com/darknet15회 인용 관련 학술자료 Tiny darknet*J Redmon - URL: https://pjreddie. com/darknet/tinydarknet/(visited …, 201811회 인용 관련 학술자료 Darknet: Open source neural networks in c. h ttp*J Redmon - pjreddie. com/darknet, 201311회 인용 관련 학술자료 Darknet: Open source neural networks in c. Pjreddie. com*J Redmon - 20169회 인용 관련 학술자료 Tiny yolo*J Redmon, A Farhadi - Tiny yolo, 20174회 인용 관련 학술자료 Joseph Chet Redmon*J Redmon, A Farhadi - survival strategies for the Robot Rebellion. pjreddie …3회 인용 관련 학술자료 2016. Darknet: Open source neural networks in c. 2013-2016*J Redmon - 20132회 인용 관련 학술자료 Darknet, 2017*JC Redmon2회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Darknet: Open source neural networks in c",
        "year": null
    },
    "Real-time grasp detection using convolutional neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/5/26",
            "게시자": "IEEE",
            "설명": "We present an accurate, real-time approach to robotic grasp detection based on convolutional neural networks. Our network performs single-stage regression to graspable bounding boxes without using standard sliding window or region proposal techniques. The model outperforms state-of-the-art approaches by 14 percentage points and runs at 13 frames per second on a GPU. Our network can simultaneously perform classification so that in a single step it recognizes the object and finds a good grasp rectangle. A modification to this model predicts multiple grasps per object by using a locally constrained prediction mechanism. The locally constrained model performs significantly better, especially on objects that can be grasped in a variety of ways.",
            "저자": "Joseph Redmon, Anelia Angelova",
            "전체 인용횟수": "877회 인용2015201620172018201920202021202220234265676120147142156137",
            "컨퍼런스": "2015 IEEE international conference on robotics and automation (ICRA)",
            "페이지": "1316-1322",
            "학술 문서": "Real-time grasp detection using convolutional neural networksJ Redmon, A Angelova - 2015 IEEE international conference on robotics and …, 2015873회 인용 관련 학술자료 전체 12개의 버전 Real-Time Grasp Detection Using Convolutional Neural Networks, 2014*J Redmon, A Angelova - arXiv preprint arXiv:1412.3128, 20185회 인용 관련 학술자료 Real-time grasp detection using convolutional neural networks. InRobotics and Automation (ICRA)*J Redmon, A Angelova - 2015 IEEE International Conference on2회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Real-time grasp detection using convolutional neural networks",
        "year": null
    },
    "IQA: Visual Question Answering in Interactive Environments": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "We introduce Interactive Question Answering (IQA), the task of answering questions that require an autonomous agent to interact with a dynamic visual environment. IQA presents the agent with a scene and a question, like:“Are there any apples in the fridge?” The agent must navigate around the scene, acquire visual understanding of scene elements, interact with objects (eg open refrigerators) and plan for a series of actions conditioned on the question. Popular reinforcement learning approaches with a single controller perform poorly on IQA owing to the large and diverse state space. We propose the Hierarchical Interactive Memory Network (HIMN), consisting of a factorized set of controllers, allowing the system to operate at multiple levels of temporal abstraction. To evaluate HIMN, we introduce IQUAD V1, a new dataset built upon AI2-THOR [35], a simulated photo-realistic environment of configurable indoor scenes with interactive objects. IQUAD V1 has 75,000 questions, each paired with a unique scene configuration. Our experiments show that our proposed model outperforms popular single controller based methods on IQUAD V1. For sample questions and results, please view our video: https://youtu. be/pXd3C-1jr98.",
            "저널": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "저자": "Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, Ali Farhadi",
            "전체 인용횟수": "386회 인용20172018201920202021202220234296674887748",
            "학술 문서": "Iqa: Visual question answering in interactive environmentsD Gordon, A Kembhavi, M Rastegari, J Redmon, D Fox… - Proceedings of the IEEE conference on computer …, 2018386회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "IQA: Visual Question Answering in Interactive Environments",
        "year": null
    },
    "An incremental improvement": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/4",
            "게시자": "Retrieved from",
            "권": "8",
            "저널": "arXiv preprint arXiv:1804.02767",
            "저자": "Joseph Redmon, Ali Farhadi YOLO",
            "전체 인용횟수": "312회 인용201920202021202220233348838562",
            "학술 문서": "An incremental improvementJ Redmon, AF YOLO - arXiv preprint arXiv:1804.02767, 2018312회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "An incremental improvement",
        "year": null
    },
    "YOLO9000: Better": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/12",
            "게시자": "ar Xiv preprint ar",
            "권": "14",
            "저널": "Faster, Stronger",
            "저자": "Joseph Redmon, Ali Farhadi",
            "전체 인용횟수": "271회 인용201720182019202020212022202316504244434924",
            "페이지": "08242",
            "학술 문서": "YOLO9000: BetterJ Redmon, A Farhadi - Faster, Stronger, 2016271회 인용 관련 학술자료 ",
            "호": "1612"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "YOLO9000: Better",
        "year": null
    },
    "YOLO9000: better, faster, stronger": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/7",
            "저자": "Redmon Joseph, Farhadi Ali",
            "전체 인용횟수": "254회 인용20172018201920202021202220234193046505747",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "7263-7271",
            "학술 문서": "YOLO9000: better, faster, strongerR Joseph, F Ali - Proceedings of the IEEE conference on computer …, 2017254회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "YOLO9000: better, faster, stronger",
        "year": null
    },
    "You Only Look Once: Unified": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "권": "1",
            "저널": "Real-Time Object Detection",
            "저자": "Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi",
            "전체 인용횟수": "231회 인용20172018201920202021202220238272529395346",
            "학술 문서": "You Only Look Once: UnifiedJ Redmon, S Divvala, R Girshick, A Farhadi - Real-Time Object Detection, 2016231회 인용 관련 학술자료 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "You Only Look Once: Unified",
        "year": null
    },
    "Yolov3: an incremental improvement. arXiv [Preprint]. doi: 10.48550": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {},
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Yolov3: an incremental improvement. arXiv [Preprint]. doi: 10.48550",
        "year": null
    },
    "Yolov3: An incremental im-provement": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {},
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Yolov3: An incremental im-provement",
        "year": null
    },
    "FARHADIA": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {},
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "FARHADIA",
        "year": null
    },
    "Image based object detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {},
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Image based object detection",
        "year": null
    },
    "Who Let The Dogs Out? Modeling Dog Behavior From Visual Data": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {},
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Who Let The Dogs Out? Modeling Dog Behavior From Visual Data",
        "year": null
    },
    "YOLOv3: An Incremental Improvement. arXiv; 2018;[DOI: https://dx. doi. org/10.48550/arXiv": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {},
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "YOLOv3: An Incremental Improvement. arXiv; 2018;[DOI: https://dx. doi. org/10.48550/arXiv",
        "year": null
    },
    "The cityscapes dataset for semantic urban scene understanding": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.",
            "저자": "Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele",
            "전체 인용횟수": "11224회 인용201620172018201920202021202220236030077013291685217424812342",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2016. IEEE Conference on",
            "페이지": "3213-3223",
            "학술 문서": "The cityscapes dataset for semantic urban scene understandingM Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler… - Proceedings of the IEEE conference on computer …, 201611224회 인용 관련 학술자료 전체 22개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The cityscapes dataset for semantic urban scene understanding",
        "year": null
    },
    "Generative adversarial text to image synthesis": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/6/11",
            "게시자": "PMLR",
            "설명": "Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories such as faces, album covers, room interiors and flowers. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.",
            "저자": "Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, Honglak Lee",
            "전체 인용횟수": "3484회 인용2016201720182019202020212022202329216486561531543544528",
            "컨퍼런스": "International conference on machine learning",
            "페이지": "1060-1069",
            "학술 문서": "Generative adversarial text to image synthesisS Reed, Z Akata, X Yan, L Logeswaran, B Schiele… - International conference on machine learning, 20163484회 인용 관련 학술자료 전체 24개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Generative adversarial text to image synthesis",
        "year": null
    },
    "2d human pose estimation: New benchmark and state of the art analysis": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "설명": "Human pose estimation has made significant progress during the last years. However current datasets are limited in their coverage of the overall pose estimation challenges. Still these serve as the common sources to evaluate, train and compare different models on. In this paper we introduce a novel benchmark\" MPII Human Pose\" that makes a significant advance in terms of diversity and difficulty, a contribution that we feel is required for future developments in human body models. This comprehensive dataset was collected using an established taxonomy of over 800 human activities. The collected images cover a wider variety of human activities than previous datasets including various recreational, occupational and householding activities, and capture people from a wider range of viewpoints. We provide a rich set of labels including positions of body joints, full 3D torso and head orientation, occlusion labels for joints and body parts, and activity labels. For each image we provide adjacent video frames to facilitate the use of motion information. Given these rich annotations we perform a detailed analysis of leading human pose estimation approaches and gaining insights for the success and failures of these methods.",
            "저자": "Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, Bernt Schiele",
            "전체 인용횟수": "2756회 인용2015201620172018201920202021202220232870175223378398483510440",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2014. IEEE Conference on",
            "페이지": "3686-3693",
            "학술 문서": "2d human pose estimation: New benchmark and state of the art analysisM Andriluka, L Pishchulin, P Gehler, B Schiele - Proceedings of the IEEE Conference on computer …, 20142756회 인용 관련 학술자료 전체 19개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "2d human pose estimation: New benchmark and state of the art analysis",
        "year": null
    },
    "A tutorial on human activity recognition using body-worn inertial sensors": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/1/1",
            "게시자": "ACM",
            "권": "46",
            "설명": "The last 20 years have seen ever-increasing research activity in the field of human activity recognition. With activity recognition having considerably matured, so has the number of challenges in designing, implementing, and evaluating activity recognition systems. This tutorial aims to provide a comprehensive hands-on introduction for newcomers to the field of human activity recognition. It specifically focuses on activity recognition using on-body inertial sensors. We first discuss the key research challenges that human activity recognition shares with general pattern recognition and identify those challenges that are specific to human activity recognition. We then describe the concept of an Activity Recognition Chain (ARC) as a general-purpose framework for designing and evaluating activity recognition systems. We detail each component of the framework, provide references to related research, and introduce the …",
            "저자": "Andreas Bulling, Ulf Blanke, Bernt Schiele",
            "전체 인용횟수": "1638회 인용20142015201620172018201920202021202220232589155158198220232213201132",
            "출처": "ACM Computing Surveys (CSUR)",
            "페이지": "1-33",
            "학술 문서": "A tutorial on human activity recognition using body-worn inertial sensorsA Bulling, U Blanke, B Schiele - ACM Computing Surveys (CSUR), 20141638회 인용 관련 학술자료 전체 18개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A tutorial on human activity recognition using body-worn inertial sensors",
        "year": null
    },
    "Zero-shot learning-a comprehensive evaluation of the good, the bad and the ugly": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/7/19",
            "게시자": "IEEE",
            "설명": "Due to the importance of zero-shot learning, i.e., classifying images where there is a lack of labeled training data, the number of proposed approaches has recently increased steadily. We argue that it is time to take a step back and to analyze the status quo of the area. The purpose of this paper is three-fold. First, given the fact that there is no agreed upon zero-shot learning benchmark, we first define a new benchmark by unifying both the evaluation protocols and data splits of publicly available datasets used for this task. This is an important contribution as published results are often not comparable and sometimes even flawed due to, e.g., pre-training on zero-shot test classes. Moreover, we propose a new zero-shot learning dataset, the Animals with Attributes 2 (AWA2) dataset which we make publicly available both in terms of image features and the images themselves. Second, we compare and analyze a …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Yongqin Xian, Christoph H Lampert, Bernt Schiele, Zeynep Akata",
            "전체 인용횟수": "1471회 인용2017201820192020202120222023868156260316353303",
            "학술 문서": "Zero-shot learning—a comprehensive evaluation of the good, the bad and the uglyY Xian, CH Lampert, B Schiele, Z Akata - IEEE transactions on pattern analysis and machine …, 20181471회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Zero-shot learning-a comprehensive evaluation of the good, the bad and the ugly",
        "year": null
    },
    "Deepercut: A deeper, stronger, and faster multi-person pose estimation model": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "게시자": "Springer International Publishing",
            "설명": " The goal of this paper is to advance the state-of-the-art of articulated pose estimation in scenes with multiple people. To that end we contribute on three fronts. We propose (1) improved body part detectors that generate effective bottom-up proposals for body parts; (2) novel image-conditioned pairwise terms that allow to assemble the proposals into a variable number of consistent body part configurations; and (3) an incremental optimization strategy that explores the search space more efficiently thus leading both to better performance and significant speed-up factors. Evaluation is done on two single-person and two multi-person pose estimation benchmarks. The proposed approach significantly outperforms best known multi-person pose estimation results while demonstrating competitive performance on the task of single person pose estimation (Models and code available at                      http://pose.mpi-inf …",
            "저자": "Eldar Insafutdinov, Leonid Pishchulin, Bjoern Andres, Mykhaylo Andriluka, Bernt Schiele",
            "전체 인용횟수": "1330회 인용201620172018201920202021202220231183159196233231217173",
            "컨퍼런스": "Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI 14",
            "페이지": "34-50",
            "학술 문서": "Deepercut: A deeper, stronger, and faster multi-person pose estimation modelE Insafutdinov, L Pishchulin, B Andres, M Andriluka… - Computer Vision–ECCV 2016: 14th European …, 20161330회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deepercut: A deeper, stronger, and faster multi-person pose estimation model",
        "year": null
    },
    "Robust object detection with interleaved categorization and segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008/5/1",
            "게시자": "Springer US",
            "권": "77",
            "설명": "  This paper presents a novel method for detecting and localizing objects of a visual category in cluttered real-world scenes. Our approach considers object categorization and figure-ground segmentation as two interleaved processes that closely collaborate towards a common goal. As shown in our work, the tight coupling between those two processes allows them to benefit from each other and improve the combined performance.   The core part of our approach is a highly flexible learned representation for object shape that can combine the information observed on different training examples in a probabilistic extension of the Generalized Hough Transform. The resulting approach can detect categorical objects in novel images and automatically infer a probabilistic segmentation from the recognition result. This segmentation is then in turn used to again improve recognition by allowing the system to …",
            "저널": "International Journal of Computer Vision (IJCV)",
            "저자": "Bastian Leibe, Aleš Leonardis, Bernt Schiele",
            "전체 인용횟수": "1295회 인용200720082009201020112012201320142015201620172018201920202021202220237327713014013415113711710272413531233011",
            "페이지": "259-289",
            "학술 문서": "Robust object detection with interleaved categorization and segmentationB Leibe, A Leonardis, B Schiele - International journal of computer vision, 20081295회 인용 관련 학술자료 전체 21개의 버전 ",
            "호": "1-3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Robust object detection with interleaved categorization and segmentation",
        "year": null
    },
    "Pedestrian detection in crowded scenes": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/6/20",
            "게시자": "IEEE",
            "설명": "In this paper, we address the problem of detecting pedestrians in crowded real-world scenes with severe overlaps. Our basic premise is that this problem is too difficult for any type of model or feature alone. Instead, we present an algorithm that integrates evidence in multiple iterations and from different sources. The core part of our method is the combination of local and global cues via probabilistic top-down segmentation. Altogether, this approach allows examining and comparing object hypotheses with high precision down to the pixel level. Qualitative and quantitative results on a large data set confirm that our method is able to reliably detect pedestrians in crowded scenes, even when they overlap and partially occlude each other. In addition, the flexible nature of our approach allows it to operate on very small training sets.",
            "저자": "Bastian Leibe, Edgar Seemann, Bernt Schiele",
            "전체 인용횟수": "1227회 인용2005200620072008200920102011201220132014201520162017201820192020202120222023114264991038997857967615452456059495440",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2005. IEEE Conference on",
            "페이지": "878-885",
            "학술 문서": "Pedestrian detection in crowded scenesB Leibe, E Seemann, B Schiele - 2005 IEEE computer society conference on computer …, 20051227회 인용 관련 학술자료 전체 25개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pedestrian detection in crowded scenes",
        "year": null
    },
    "Combined object categorization and segmentation with an implicit shape model": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004/5/15",
            "권": "2",
            "설명": "We present a method for object categorization in real-world scenes. Following a common consensus in the field, we do not assume that a figureground segmentation is available prior to recognition. However, in contrast to most standard approaches for object class recognition, our approach automatically segments the object as a result of the categorization. This combination of recognition and segmentation into one process is made possible by our use of an Implicit Shape Model, which integrates both into a common probabilistic framework. In addition to the recognition and segmentation result, it also generates a per-pixel confidence measure specifying the area that supports a hypothesis and how much it can be trusted. We use this confidence to derive a natural extension of the approach to handle multiple objects in a scene and resolve ambiguities between overlapping hypotheses with a novel MDL-based criterion. In addition, we present an extensive evaluation of our method on a standard dataset for car detection and compare its performance to existing methods from the literature. Our results show that the proposed method significantly outperforms previously published methods while needing one order of magnitude less training examples. Finally, we present results for articulated objects, which show that the proposed method can categorize and segment unfamiliar objects in different articulations and with widely varying texture patterns, even under significant partial occlusion.",
            "저널": "Workshop on statistical learning in computer vision, ECCV",
            "저자": "Bastian Leibe, Ales Leonardis, Bernt Schiele",
            "전체 인용횟수": "1218회 인용20042005200620072008200920102011201220132014201520162017201820192020202120222023640596399891041189110810075525327422521219",
            "페이지": "7",
            "학술 문서": "Combined object categorization and segmentation with an implicit shape modelB Leibe, A Leonardis, B Schiele - Workshop on statistical learning in computer vision …, 20041218회 인용 관련 학술자료 전체 23개의 버전 ",
            "호": "5"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Combined object categorization and segmentation with an implicit shape model",
        "year": null
    },
    "Deepcut: Joint subset partition and labeling for multi person pose estimation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "This paper considers the task of articulated human pose estimation of multiple people in real world images. We propose an approach that jointly solves the tasks of detection and pose estimation: it infers the number of persons in a scene, identifies occluded body parts, and disambiguates body parts between people in close proximity of each other. This joint formulation is in contrast to previous strategies, that address the problem by first detecting people and subsequently estimating their body pose. We propose a partitioning and labeling formulation of a set of body-part hypotheses generated with CNN-based part detectors. Our formulation, an instance of an integer linear program, implicitly performs non-maximum suppression on the set of part candidates and groups them to form configurations of body parts respecting geometric and appearance constraints. Experiments on four different datasets demonstrate state-of-the-art results for both single person and multi person pose estimation.",
            "저자": "Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern Andres, Mykhaylo Andriluka, Peter V Gehler, Bernt Schiele",
            "전체 인용횟수": "1209회 인용201620172018201920202021202220232179145179202230176157",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "4929-4937",
            "학술 문서": "Deepcut: Joint subset partition and labeling for multi person pose estimationL Pishchulin, E Insafutdinov, S Tang, B Andres… - Proceedings of the IEEE conference on computer …, 20161209회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deepcut: Joint subset partition and labeling for multi person pose estimation",
        "year": null
    },
    "People-tracking-by-detection and people-detection-by-tracking": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008/6/23",
            "게시자": "IEEE",
            "설명": "Both detection and tracking people are challenging problems, especially in complex real world scenes that commonly involve multiple people, complicated occlusions, and cluttered or even moving backgrounds. People detectors have been shown to be able to locate pedestrians even in complex street scenes, but false positives have remained frequent. The identification of particular individuals has remained challenging as well. Tracking methods are able to find a particular individual in image sequences, but are severely challenged by real-world scenarios such as crowded street scenes. In this paper, we combine the advantages of both detection and tracking in a single framework. The approximate articulation of each person is detected in every frame based on local features that model the appearance of individual body parts. Prior knowledge on possible articulations and temporal coherency within a walking …",
            "저자": "Mykhaylo Andriluka, Stefan Roth, Bernt Schiele",
            "전체 인용횟수": "1207회 인용200820092010201120122013201420152016201720182019202020212022202363769879112511412312188877160413530",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2008. IEEE Conference on",
            "페이지": "1-8",
            "학술 문서": "People-tracking-by-detection and people-detection-by-trackingM Andriluka, S Roth, B Schiele - 2008 IEEE Conference on computer vision and pattern …, 20081207회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "People-tracking-by-detection and people-detection-by-tracking",
        "year": null
    },
    "Pictorial structures revisited: People detection and articulated pose estimation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/6/20",
            "게시자": "IEEE",
            "설명": "Non-rigid object detection and articulated pose estimation are two related and challenging problems in computer vision. Numerous models have been proposed over the years and often address different special cases, such as pedestrian detection or upper body pose estimation in TV footage. This paper shows that such specialization may not be necessary, and proposes a generic approach based on the pictorial structures framework. We show that the right selection of components for both appearance and spatial modeling is crucial for general applicability and overall performance of the model. The appearance of body parts is modeled using densely sampled shape context descriptors and discriminatively trained AdaBoost classifiers. Furthermore, we interpret the normalized margin of each classifier as likelihood in a generative model. Non-Gaussian relationships between parts are represented as Gaussians in …",
            "저자": "Mykhaylo Andriluka, Stefan Roth, Bernt Schiele",
            "전체 인용횟수": "1155회 인용2008200920102011201220132014201520162017201820192020202120222023495379999511310210094817556505653",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2009. IEEE Conference on",
            "페이지": "1014-1021",
            "학술 문서": "Pictorial structures revisited: People detection and articulated pose estimationM Andriluka, S Roth, B Schiele - 2009 IEEE conference on computer vision and pattern …, 20091155회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pictorial structures revisited: People detection and articulated pose estimation",
        "year": null
    },
    "Meta-transfer learning for few-shot learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "Meta-learning has been proposed as a framework to address the challenging few-shot learning setting. The key idea is to leverage a large number of similar few-shot tasks in order to learn how to adapt a base-learner to a new task for which only a few labeled samples are available. As deep neural networks (DNNs) tend to overfit using a few samples only, meta-learning typically uses shallow neural networks (SNNs), thus limiting its effectiveness. In this paper we propose a novel few-shot learning method called meta-transfer learning (MTL) which learns to adapt a deep NN for few shot learning tasks. Specifically,\" meta\" refers to training multiple tasks, and\" transfer\" is achieved by learning scaling and shifting functions of DNN weights for each task. In addition, we introduce the hard task (HT) meta-batch scheme as an effective learning curriculum for MTL. We conduct experiments using (5-class, 1-shot) and (5-class, 5-shot) recognition tasks on two challenging few-shot learning benchmarks: miniImageNet and Fewshot-CIFAR100. Extensive comparisons to related works validate that our meta-transfer learning approach trained with the proposed HT meta-batch scheme achieves top performance. An ablation study also shows that both components contribute to fast convergence and high accuracy.",
            "저자": "Qianru Sun, Yaoyao Liu, Tat-Seng Chua, Bernt Schiele",
            "전체 인용횟수": "1147회 인용2019202020212022202333183290333300",
            "컨퍼런스": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "페이지": "403-412",
            "학술 문서": "Meta-transfer learning for few-shot learningQ Sun, Y Liu, TS Chua, B Schiele - Proceedings of the IEEE/CVF conference on computer …, 20191147회 인용 관련 학술자료 전체 18개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Meta-transfer learning for few-shot learning",
        "year": null
    },
    "Evaluation of output embeddings for fine-grained image classification": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "Image classification has advanced significantly in recent years with the availability of large-scale image sets. However, fine-grained classification remains a major challenge due to the annotation cost of large numbers of fine-grained categories. This project shows that compelling classification performance can be achieved on such categories even without labeled training data. Given image and class embeddings, we learn a compatibility function such that matching embeddings are assigned a higher score than mismatching ones; zero-shot classification of an image proceeds by finding the label yielding the highest joint compatibility score. We use state-of-the-art image features and focus on different supervised attributes and unsupervised output embeddings either derived from hierarchies or learned from unlabeled text corpora. We establish a substantially improved state-of-the-art on the Animals with Attributes and Caltech-UCSD Birds datasets. Most encouragingly, we demonstrate that purely unsupervised output embeddings (learned from Wikipedia and improved with fine-grained text) achieve compelling results, even outperforming the previous supervised state-of-the-art. By combining different output embeddings, we further improve results.",
            "저자": "Zeynep Akata, Scott Reed, Daniel Walter, Honglak Lee, Bernt Schiele",
            "전체 인용횟수": "1134회 인용201520162017201820192020202120222023114787130182185195149129",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "2927-2936",
            "학술 문서": "Evaluation of output embeddings for fine-grained image classificationZ Akata, S Reed, D Walter, H Lee, B Schiele - Proceedings of the IEEE conference on computer …, 20151134회 인용 관련 학술자료 전체 21개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Evaluation of output embeddings for fine-grained image classification",
        "year": null
    },
    "Analyzing appearance and contour based methods for object categorization": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2003/6/18",
            "게시자": "IEEE",
            "권": "2",
            "설명": "Object recognition has reached a level where we can identify a large number of previously seen and known objects. However, the more challenging and important task of categorizing previously unseen objects remains largely unsolved. Traditionally, contour and shape based methods are regarded most adequate for handling the generalization requirements needed for this task. Appearance based methods, on the other hand, have been successful in object identification and detection scenarios. Today little work is done to systematically compare existing methods and characterize their relative capabilities for categorizing objects. In order to compare different methods we present a new database specifically tailored to the task of object categorization. It contains high-resolution color images of 80 objects from 8 different categories, for a total of 3280 images. It is used to analyze the performance of several …",
            "저자": "Bastian Leibe, Bernt Schiele",
            "전체 인용횟수": "1014회 인용20032004200520062007200820092010201120122013201420152016201720182019202020212022202343232563641496148685762706656586442503219",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2003. IEEE Computer Society Conference on",
            "페이지": "II-409-15 vol. 2",
            "학술 문서": "Analyzing appearance and contour based methods for object categorizationB Leibe, B Schiele - 2003 IEEE Computer Society Conference on Computer …, 20031014회 인용 관련 학술자료 전체 19개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Analyzing appearance and contour based methods for object categorization",
        "year": null
    },
    "Feature generating networks for zero-shot learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "Suffering from the extreme training data imbalance between seen and unseen classes, most of existing state-of-the-art approaches fail to achieve satisfactory results for the challenging generalized zero-shot learning task. To circumvent the need for labeled examples of unseen classes, we propose a novel generative adversarial network (GAN) that synthesizes CNN features conditioned on class-level semantic information, offering a shortcut directly from a semantic descriptor of a class to a class-conditional feature distribution. Our proposed approach, pairing a Wasserstein GAN with a classification loss, is able to generate sufficiently discriminative CNN features to train softmax classifiers or any multimodal embedding method. Our experimental results demonstrate a significant boost in accuracy over the state of the art on five challenging datasets--CUB, FLO, SUN, AWA and ImageNet--in both the zero-shot learning and generalized zero-shot learning settings.",
            "저자": "Yongqin Xian, Tobias Lorenz, Bernt Schiele, Zeynep Akata",
            "전체 인용횟수": "948회 인용20182019202020212022202326111184228219176",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "5542-5551",
            "학술 문서": "Feature generating networks for zero-shot learningY Xian, T Lorenz, B Schiele, Z Akata - Proceedings of the IEEE conference on computer …, 2018948회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Feature generating networks for zero-shot learning",
        "year": null
    },
    "Learning deep representations of fine-grained visual descriptions": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "State-of-the-art methods for zero-shot visual recognition formulate learning as a joint embedding problem of images and side information. In these formulations the current best complement to visual features are attributes: manually-encoded vectors describing shared characteristics among categories. Despite good performance, attributes have limitations:(1) finer-grained recognition requires commensurately more attributes, and (2) attributes do not provide a natural language interface. We propose to overcome these limitations by training neural language models from scratch; ie without pre-training and only consuming words and characters. Our proposed models train end-to-end to align with the fine-grained and category-specific content of images. Natural language provides a flexible and compact way of encoding only the salient visual aspects for distinguishing categories. By training on raw text, our model can do inference on raw text as well, providing humans a familiar mode both for annotation and retrieval. Our model achieves strong performance on zero-shot text-based image retrieval and significantly outperforms the attribute-based state-of-the-art for zero-shot classification on the Caltech-UCSD Birds 200-2011 dataset.",
            "저자": "Scott Reed, Zeynep Akata, Honglak Lee, Bernt Schiele",
            "전체 인용횟수": "914회 인용201620172018201920202021202220231060110138163171126117",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "49-58",
            "학술 문서": "Learning deep representations of fine-grained visual descriptionsS Reed, Z Akata, H Lee, B Schiele - Proceedings of the IEEE conference on computer …, 2016914회 인용 관련 학술자료 전체 18개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning deep representations of fine-grained visual descriptions",
        "year": null
    },
    "Face recognition: A literature survey": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2003/12/1",
            "게시자": "ACM",
            "권": "35",
            "설명": "As one of the most successful applications of image analysis and understanding, face recognition has recently received significant attention, especially during the past several years. At least two reasons account for this trend: the first is the wide range of commercial and law enforcement applications, and the second is the availability of feasible technologies after 30 years of research. Even though current machine recognition systems have reached a certain level of maturity, their success is limited by the conditions imposed by many real applications. For example, recognition of face images acquired in an outdoor environment with changes in illumination and/or pose remains a largely unsolved problem. In other words, current systems are still far away from the capability of the human perception system.This paper provides an up-to-date critical survey of still- and video-based face recognition research. There are two …",
            "저자": "Wenyi Zhao, Rama Chellappa, P Jonathon Phillips, Azriel Rosenfeld",
            "전체 인용횟수": "9450회 인용20022003200420052006200720082009201020112012201320142015201620172018201920202021202220234287184385406489538632667652679664660635562419431333255225201150",
            "출처": "ACM computing surveys (CSUR)",
            "페이지": "399-458",
            "학술 문서": "Face recognition: A literature surveyW Zhao, R Chellappa, PJ Phillips, A Rosenfeld - ACM computing surveys (CSUR), 20039450회 인용 관련 학술자료 전체 33개의 버전 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Face recognition: A literature survey",
        "year": null
    },
    "Human and machine recognition of faces: A survey": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1995/5",
            "게시자": "IEEE",
            "권": "83",
            "설명": "The goal of this paper is to present a critical survey of existing literature on human and machine recognition of faces. Machine recognition of faces has several applications, ranging from static matching of controlled photographs as in mug shots matching and credit card verification to surveillance video images. Such applications have different constraints in terms of complexity of processing requirements and thus present a wide range of different technical challenges. Over the last 20 years researchers in psychophysics, neural sciences and engineering, image processing analysis and computer vision have investigated a number of issues related to face recognition by humans and machines. Ongoing research activities have been given a renewed emphasis over the last five years. Existing techniques and systems have been tested on different sets of images of varying complexities. But very little synergism exists …",
            "저널": "Proceedings of the IEEE",
            "저자": "Rama Chellappa, Charles L Wilson, Saad Sirohey",
            "전체 인용횟수": "4391회 인용19951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023187186135149158148184245272311247211202210216183200181156133132100956365535133",
            "페이지": "705-741",
            "학술 문서": "Human and machine recognition of faces: A surveyR Chellappa, CL Wilson, S Sirohey - Proceedings of the IEEE, 19954391회 인용 관련 학술자료 전체 10개의 버전 ",
            "호": "5"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Human and machine recognition of faces: A survey",
        "year": null
    },
    "Soft-NMS--improving object detection with one line of code": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "Non-maximum suppression is an integral part of the object detection pipeline. First, it sorts all detection boxes on the basis of their scores. The detection box M with the maximum score is selected and all other detection boxes with a significant overlap (using a pre-defined threshold) with M are suppressed. This process is recursively applied on the remaining boxes. As per the design of the algorithm, if an object lies within the predefined overlap threshold, it leads to a miss. To this end, we propose Soft-NMS, an algorithm which decays the detection scores of all other objects as a continuous function of their overlap with M. Hence, no object is eliminated in this process. Soft-NMS obtains consistent improvements for the coco-style mAP metric on standard datasets like PASCAL VOC 2007 (1.7% for both R-FCN and Faster-RCNN) and MS-COCO (1.3% for R-FCN and 1.1% for Faster-RCNN) by just changing the NMS algorithm without any additional hyper-parameters. Using Deformable-RFCN, Soft-NMS improves state-of-the-art in object detection from 39.8% to 40.9% with a single model. Further, the computational complexity of Soft-NMS is the same as traditional NMS and hence it can be efficiently implemented. Since Soft-NMS does not require any extra training and is simple to implement, it can be easily integrated into any object detection pipeline. Code for Soft-NMS is publicly available on GitHub",
            "저자": "Navaneeth Bodla, Bharat Singh, Rama Chellappa, Larry S Davis",
            "전체 인용횟수": "1887회 인용2017201820192020202120222023988191306404469413",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "5561-5569",
            "학술 문서": "Soft-NMS--improving object detection with one line of codeN Bodla, B Singh, R Chellappa, LS Davis - Proceedings of the IEEE international conference on …, 20171887회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Soft-NMS--improving object detection with one line of code",
        "year": null
    },
    "Machine recognition of human activities: A survey": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008/9/26",
            "게시자": "IEEE",
            "권": "18",
            "설명": "The past decade has witnessed a rapid proliferation of video cameras in all walks of life and has resulted in a tremendous explosion of video content. Several applications such as content-based video annotation and retrieval, highlight extraction and video summarization require recognition of the activities occurring in the video. The analysis of human activities in videos is an area with increasingly important consequences from security and surveillance to entertainment and personal archiving. Several challenges at various levels of processing-robustness against errors in low-level processing, view and rate-invariant representations at midlevel processing and semantic representation of human activities at higher level processing-make this problem hard to solve. In this review paper, we present a comprehensive survey of efforts in the past couple of decades to address the problems of representation, recognition …",
            "저자": "Pavan Turaga, Rama Chellappa, Venkatramana S Subrahmanian, Octavian Udrea",
            "전체 인용횟수": "1828회 인용200920102011201220132014201520162017201820192020202120222023451091381642392151741571481118273564941",
            "출처": "IEEE Transactions on Circuits and Systems for Video technology",
            "페이지": "1473-1488",
            "학술 문서": "Machine recognition of human activities: A surveyP Turaga, R Chellappa, VS Subrahmanian, O Udrea - IEEE Transactions on Circuits and Systems for Video …, 20081828회 인용 관련 학술자료 전체 22개의 버전 ",
            "호": "11"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Machine recognition of human activities: A survey",
        "year": null
    },
    "Human action recognition by representing 3d skeletons as points in a lie group": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "설명": "Recently introduced cost-effective depth sensors coupled with the real-time skeleton estimation algorithm of Shotton et al.[16] have generated a renewed interest in skeleton-based human action recognition. Most of the existing skeleton-based approaches use either the joint locations or the joint angles to represent a human skeleton. In this paper, we propose a new skeletal representation that explicitly models the 3D geometric relationships between various body parts using rotations and translations in 3D space. Since 3D rigid body motions are members of the special Euclidean group SE (3), the proposed skeletal representation lies in the Lie group SE (3)×...× SE (3), which is a curved manifold. Using the proposed representation, human actions can be modeled as curves in this Lie group. Since classification of curves in this Lie group is not an easy task, we map the action curves from the Lie group to its Lie algebra, which is a vector space. We then perform classification using a combination of dynamic time warping, Fourier temporal pyramid representation and linear SVM. Experimental results on three action datasets show that the proposed representation performs better than many existing skeletal representations. The proposed approach also outperforms various state-of-the-art skeleton-based human action recognition approaches.",
            "저자": "Raviteja Vemulapalli, Felipe Arrate, Rama Chellappa",
            "전체 인용횟수": "1655회 인용2014201520162017201820192020202120222023869126160217210222250193165",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "588-595",
            "학술 문서": "Human action recognition by representing 3d skeletons as points in a lie groupR Vemulapalli, F Arrate, R Chellappa - Proceedings of the IEEE conference on computer …, 20141655회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Human action recognition by representing 3d skeletons as points in a lie group",
        "year": null
    },
    "Discriminant analysis for recognition of human face images": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1997/8/1",
            "게시자": "Optica Publishing Group",
            "권": "14",
            "설명": "The discrimination power of various human facial features is studied and a new scheme for automatic face recognition (AFR) is proposed. The first part of the paper focuses on the linear discriminant analysis (LDA) of different aspects of human faces in the spatial as well as in the wavelet domain. This analysis allows objective evaluation of the significance of visual information in different parts (features) of the face for identifying the human subject. The LDA of faces also provides us with a small set of features that carry the most relevant information for classification purposes. The features are obtained through eigenvector analysis of scatter matrices with the objective of maximizing between-class variations and minimizing within-class variations. The result is an efficient projection-based feature-extraction and classification scheme for AFR. Each projection creates a decision axis with a certain level of discrimination …",
            "저널": "Josa a",
            "저자": "Kamran Etemad, Rama Chellappa",
            "전체 인용횟수": "1486회 인용1997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220235352220302941507053606675767896113107847574635432251817",
            "페이지": "1724-1733",
            "학술 문서": "Discriminant analysis for recognition of human face imagesK Etemad, R Chellappa - Josa a, 19971486회 인용 관련 학술자료 전체 25개의 버전 ",
            "호": "8"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Discriminant analysis for recognition of human face images",
        "year": null
    },
    "Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/12/8",
            "게시자": "IEEE",
            "권": "41",
            "설명": "We present an algorithm for simultaneous face detection, landmarks localization, pose estimation and gender recognition using deep convolutional neural networks (CNN). The proposed method called, HyperFace, fuses the intermediate layers of a deep CNN using a separate CNN followed by a multi-task learning algorithm that operates on the fused features. It exploits the synergy among the tasks which boosts up their individual performances. Additionally, we propose two variants of HyperFace: (1) HyperFace-ResNet that builds on the ResNet-101 model and achieves significant improvement in performance, and (2) Fast-HyperFace that uses a high recall fast face detector for generating region proposals to improve the speed of the algorithm. Extensive experiments show that the proposed models are able to capture both global and local information in faces and performs significantly better than many competitive …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Rajeev Ranjan, Vishal M Patel, Rama Chellappa",
            "전체 인용횟수": "1413회 인용201620172018201920202021202220231287172272276237193141",
            "페이지": "121-135",
            "학술 문서": "Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognitionR Ranjan, VM Patel, R Chellappa - IEEE transactions on pattern analysis and machine …, 20171413회 인용 관련 학술자료 전체 10개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition",
        "year": null
    },
    "A method for enforcing integrability in shape from shading algorithms": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1988/7",
            "게시자": "IEEE",
            "권": "10",
            "설명": "An approach for enforcing integrability, a particular implementation of the approach, an example of its application to extending an existing shape-from-shading algorithm, and experimental results showing the improvement that results from enforcing integrability are presented. A possibly nonintegrable estimate of surface slopes is represented by a finite set of basis functions, and integrability is enforced by calculating the orthogonal projection onto a vector subspace spanning the set of integrable slopes. The integrability projection constraint was applied to extending an iterative shape-from-shading algorithm of M.J. Brooks and B.K.P. Horn (1985). Experimental results show that the extended algorithm converges faster and with less error than the original version. Good surface reconstructions were obtained with and without known boundary conditions and for fairly complicated surfaces.< >",
            "저널": "IEEE Transactions on pattern analysis and machine intelligence",
            "저자": "Robert T.  Frankot, Rama Chellappa",
            "전체 인용횟수": "1384회 인용19881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202310141511191521141312131192116364457657865615360646869645354544243383945",
            "페이지": "439-451",
            "학술 문서": "A method for enforcing integrability in shape from shading algorithmsRT Frankot, R Chellappa - IEEE Transactions on pattern analysis and machine …, 19881384회 인용 관련 학술자료 전체 25개의 버전 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A method for enforcing integrability in shape from shading algorithms",
        "year": null
    },
    "Domain adaptation for object recognition: An unsupervised approach": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/11/6",
            "게시자": "IEEE",
            "설명": "Adapting the classifier trained on a source domain to recognize instances from a new target domain is an important problem that is receiving recent attention. In this paper, we present one of the first studies on unsupervised domain adaptation in the context of object recognition, where we have labeled data only from the source domain (and therefore do not have correspondences between object categories across domains). Motivated by incremental learning, we create intermediate representations of data between the two domains by viewing the generative subspaces (of same dimension) created from these domains as points on the Grassmann manifold, and sampling points along the geodesic between them to obtain subspaces that provide a meaningful description of the underlying domain shift. We then obtain the projections of labeled source domain data onto these subspaces, from which a discriminative …",
            "저자": "Raghuraman Gopalan, Ruonan Li, Rama Chellappa",
            "전체 인용횟수": "1265회 인용2012201320142015201620172018201920202021202220232459821001371181311601471259066",
            "컨퍼런스": "2011 international conference on computer vision",
            "페이지": "999-1006",
            "학술 문서": "Domain adaptation for object recognition: An unsupervised approachR Gopalan, R Li, R Chellappa - 2011 international conference on computer vision, 20111265회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Domain adaptation for object recognition: An unsupervised approach",
        "year": null
    },
    "Defense-gan: Protecting classifiers against adversarial attacks using generative models": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/5/17",
            "설명": "In recent years, deep neural network approaches have been widely adopted for machine learning tasks, including classification. However, they were shown to be vulnerable to adversarial perturbations: carefully crafted small perturbations can cause misclassification of legitimate images. We propose Defense-GAN, a new framework leveraging the expressive capability of generative models to defend deep neural networks against such attacks. Defense-GAN is trained to model the distribution of unperturbed images. At inference time, it finds a close output to a given image which does not contain the adversarial changes. This output is then fed to the classifier. Our proposed method can be used with any classification model and does not modify the classifier structure or training procedure. It can also be used as a defense against any attack as it does not assume knowledge of the process for generating the adversarial examples. We empirically show that Defense-GAN is consistently effective against different attack methods and improves on existing defense strategies. Our code has been made publicly available at https://github.com/kabkabm/defensegan",
            "저널": "arXiv preprint arXiv:1805.06605",
            "저자": "Pouya Samangouei, Maya Kabkab, Rama Chellappa",
            "전체 인용횟수": "1223회 인용20182019202020212022202386196215256261202",
            "학술 문서": "Defense-gan: Protecting classifiers against adversarial attacks using generative modelsP Samangouei, M Kabkab, R Chellappa - arXiv preprint arXiv:1805.06605, 20181223회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Defense-gan: Protecting classifiers against adversarial attacks using generative models",
        "year": null
    },
    "Discriminant analysis of principal components for face recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1998",
            "게시자": "Springer Berlin Heidelberg",
            "설명": " In this paper we describe a face recognition method based on PCA (Principal Component Analysis) and LDA (Linear Discriminant Analysis). The method consists of two steps: first we project the face image from the original vector space to a face subspace via PCA, second we use LDA to obtain a linear classifier. The basic idea of combining PCA and LDA is to improve the generalization capability of LDA when only few samples per class are available. Using FERET dataset we demonstrate a significant improvement when principal components rather than original images are fed to the LDA classifier. The hybrid classifier using PCA and LDA provides a useful framework for other image recognition tasks as well.",
            "저널": "Face recognition: From theory to applications",
            "저자": "Wenyi Zhao, Arvindh Krishnaswamy, Rama Chellappa, Daniel L Swets, John Weng",
            "전체 인용횟수": "1184회 인용1998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202391215272543497273694273626962806349594046423421239",
            "페이지": "73-85",
            "학술 문서": "Discriminant analysis of principal components for face recognitionW Zhao, A Krishnaswamy, R Chellappa, DL Swets… - Face recognition: From theory to applications, 19981184회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Discriminant analysis of principal components for face recognition",
        "year": null
    },
    "Entropy rate superpixel segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/6/20",
            "게시자": "IEEE",
            "설명": "We propose a new objective function for superpixel segmentation. This objective function consists of two components: entropy rate of a random walk on a graph and a balancing term. The entropy rate favors formation of compact and homogeneous clusters, while the balancing function encourages clusters with similar sizes. We present a novel graph construction for images and show that this construction induces a matroid - a combinatorial structure that generalizes the concept of linear independence in vector spaces. The segmentation is then given by the graph topology that maximizes the objective function under the matroid constraint. By exploiting submodular and mono-tonic properties of the objective function, we develop an efficient greedy algorithm. Furthermore, we prove an approximation bound of ½ for the optimality of the solution. Extensive experiments on the Berkeley segmentation benchmark show that …",
            "저자": "Ming-Yu Liu, Oncel Tuzel, Srikumar Ramalingam, Rama Chellappa",
            "전체 인용횟수": "1115회 인용201220132014201520162017201820192020202120222023183554831021231201401201349877",
            "컨퍼런스": "CVPR 2011",
            "페이지": "2097-2104",
            "학술 문서": "Entropy rate superpixel segmentationMY Liu, O Tuzel, S Ramalingam, R Chellappa - CVPR 2011, 20111115회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Entropy rate superpixel segmentation",
        "year": null
    },
    "Visual tracking and recognition using appearance-adaptive models in particle filters": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004/10/18",
            "게시자": "IEEE",
            "권": "13",
            "설명": "We present an approach that incorporates appearance-adaptive models in a particle filter to realize robust visual tracking and recognition algorithms. Tracking needs modeling interframe motion and appearance changes, whereas recognition needs modeling appearance changes between frames and gallery images. In conventional tracking algorithms, the appearance model is either fixed or rapidly changing, and the motion model is simply a random walk with fixed noise variance. Also, the number of particles is typically fixed. All these factors make the visual tracker unstable. To stabilize the tracker, we propose the following modifications: an observation model arising from an adaptive appearance model, an adaptive velocity motion model with adaptive noise variance, and an adaptive number of particles. The adaptive-velocity model is derived using a first-order linear predictor based on the appearance …",
            "저널": "IEEE Transactions on Image Processing",
            "저자": "Shaohua Kevin Zhou, Rama Chellappa, Baback Moghaddam",
            "전체 인용횟수": "951회 인용2004200520062007200820092010201120122013201420152016201720182019202020212022202310355978849272707474575654233620181177",
            "페이지": "1491-1506",
            "학술 문서": "Visual tracking and recognition using appearance-adaptive models in particle filtersSK Zhou, R Chellappa, B Moghaddam - IEEE Transactions on Image Processing, 2004951회 인용 관련 학술자료 전체 19개의 버전 ",
            "호": "11"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Visual tracking and recognition using appearance-adaptive models in particle filters",
        "year": null
    },
    "Visual domain adaptation: A survey of recent advances": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/4/2",
            "게시자": "IEEE",
            "권": "32",
            "설명": "In pattern recognition and computer vision, one is often faced with scenarios where the training data used to learn a model have different distribution from the data on which the model is applied. Regardless of the cause, any distributional change that occurs after learning a classifier can degrade its performance at test time. Domain adaptation tries to mitigate this degradation. In this article, we provide a survey of domain adaptation methods for visual recognition. We discuss the merits and drawbacks of existing domain adaptation approaches and identify promising avenues for research in this rapidly evolving field.",
            "저널": "IEEE signal processing magazine",
            "저자": "Vishal M Patel, Raghuraman Gopalan, Ruonan Li, Rama Chellappa",
            "전체 인용횟수": "937회 인용20152016201720182019202020212022202319649911615313913911484",
            "페이지": "53-69",
            "학술 문서": "Visual domain adaptation: A survey of recent advancesVM Patel, R Gopalan, R Li, R Chellappa - IEEE signal processing magazine, 2015937회 인용 관련 학술자료 전체 6개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Visual domain adaptation: A survey of recent advances",
        "year": null
    },
    "Identification of humans using gait": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004/8/16",
            "게시자": "IEEE",
            "권": "13",
            "설명": "We propose a view-based approach to recognize humans from their gait. Two different image features have been considered: the width of the outer contour of the binarized silhouette of the walking person and the entire binary silhouette itself. To obtain the observation vector from the image features, we employ two different methods. In the first method, referred to as the indirect approach, the high-dimensional image feature is transformed to a lower dimensional space by generating what we call the frame to exemplar (FED) distance. The FED vector captures both structural and dynamic traits of each individual. For compact and effective gait representation and recognition, the gait information in the FED vector sequences is captured in a hidden Markov model (HMM). In the second method, referred to as the direct approach, we work with the feature vector directly (as opposed to computing the FED) and train an HMM …",
            "저널": "IEEE Transactions on image processing",
            "저자": "Amit Kale, Aravind Sundaresan, AN Rajagopalan, Naresh P Cuntoor, Amit K Roy-Chowdhury, Volker Kruger, Rama Chellappa",
            "전체 인용횟수": "808회 인용20032004200520062007200820092010201120122013201420152016201720182019202020212022202321142487040496147475062495238322524181516",
            "페이지": "1163-1173",
            "학술 문서": "Identification of humans using gaitA Kale, A Sundaresan, AN Rajagopalan, NP Cuntoor… - IEEE Transactions on image processing, 2004808회 인용 관련 학술자료 전체 22개의 버전 ",
            "호": "9"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Identification of humans using gait",
        "year": null
    },
    "Classification of textures using Gaussian Markov random fields": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1985/8",
            "게시자": "IEEE",
            "권": "33",
            "설명": "The problem of texture classification arises in several disciplines such as remote sensing, computer vision, and image analysis. In this paper we present two feature extraction methods for the classification of textures using two-dimensional (2-D) Markov random field (MRF) models. It is assumed that the given M × M texture is generated by a Gaussian MRF model. In the first method, the least square (LS) estimates of model parameters are used as features. In the second method, using the notion of sufficient statistics, it is shown that the sample correlations over a symmetric window including the origin are optimal features for classification. Simple minimum distance classifiers using these two feature sets yield good classification accuracies for a seven class problem.",
            "저널": "IEEE Transactions on Acoustics, Speech, and Signal Processing",
            "저자": "Rama Chellappa, Shankar Chatterjee",
            "전체 인용횟수": "754회 인용198619871988198919901991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220233157421211161192718241824152129373225344029353430332730152322131577",
            "페이지": "959-963",
            "학술 문서": "Classification of textures using Gaussian Markov random fieldsR Chellappa, S Chatterjee - IEEE Transactions on Acoustics, Speech, and Signal …, 1985754회 인용 관련 학술자료 전체 7개의 버전 Classification of textures using Gaussain Markov random fields*R CHELLAPPA - IEEE Trans. Pattern Anal. Machine Intell., 1991",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Classification of textures using Gaussian Markov random fields",
        "year": null
    },
    "Estimation of object motion parameters from noisy images": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1986/1",
            "게시자": "IEEE",
            "설명": "An approach is presented for the estimation of object motion parameters based on a sequence of noisy images. The problem considered is that of a rigid body undergoing unknown rotational and translational motion. The measurement data consists of a sequence of noisy image coordinates of two or more object correspondence points. By modeling the object dynamics as a function of time, estimates of the model parameters (including motion parameters) can be extracted from the data using recursive and/or batch techniques. This permits a desired degree of smoothing to be achieved through the use of an arbitrarily large number of images. Some assumptions regarding object structure are presently made. Results are presented for a recursive estimation procedure: the case considered here is that of a sequence of one dimensional images of a two dimensional object. Thus, the object moves in one transverse …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Ted J Broida, Rama Chellappa",
            "전체 인용횟수": "729회 인용198619871988198919901991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220236711203635363226232526241117172181449111126282435294221241517175234",
            "페이지": "90-99",
            "학술 문서": "Estimation of object motion parameters from noisy imagesTJ Broida, R Chellappa - IEEE transactions on pattern analysis and machine …, 1986729회 인용 관련 학술자료 전체 17개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Estimation of object motion parameters from noisy images",
        "year": null
    },
    "Generate to adapt: Aligning domains using generative adversarial networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "Domain Adaptation is an actively researched problem in Computer Vision. In this work, we propose an approach that leverages unsupervised data to bring the source and target distributions closer in a learned joint feature space. We accomplish this by inducing a symbiotic relationship between the learned embedding and a generative adversarial network. This is in contrast to methods which use the adversarial framework for realistic data generation and retraining deep models with such data. We demonstrate the strength and generality of our approach by performing experiments on three different tasks with varying levels of difficulty:(1) Digit classification (MNIST, SVHN and USPS datasets)(2) Object recognition using OFFICE dataset and (3) Domain adaptation from synthetic to real data. Our method achieves state-of-the art performance in most experimental settings and by far the only GAN-based method that has been shown to work well across different datasets such as OFFICE and DIGITS.",
            "저자": "Swami Sankaranarayanan, Yogesh Balaji, Carlos D Castillo, Rama Chellappa",
            "전체 인용횟수": "723회 인용2017201820192020202120222023724101166168134121",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "8503-8512",
            "학술 문서": "Generate to adapt: Aligning domains using generative adversarial networksS Sankaranarayanan, Y Balaji, CD Castillo… - Proceedings of the IEEE conference on computer …, 2018723회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Generate to adapt: Aligning domains using generative adversarial networks",
        "year": null
    },
    "Frontal to profile face verification in the wild": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/3/7",
            "게시자": "IEEE",
            "설명": "We have collected a new face data set that will facilitate research in the problem of frontal to profile face verification `in the wild'. The aim of this data set is to isolate the factor of pose variation in terms of extreme poses like profile, where many features are occluded, along with other `in the wild' variations. We call this data set the Celebrities in Frontal-Profile (CFP) data set. We find that human performance on Frontal-Profile verification in this data set is only slightly worse (94.57% accuracy) than that on Frontal-Frontal verification (96.24% accuracy). However we evaluated many state-of-the-art algorithms, including Fisher Vector, Sub-SML and a Deep learning algorithm. We observe that all of them degrade more than 10% from Frontal-Frontal to Frontal-Profile verification. The Deep learning implementation, which performs comparable to humans on Frontal-Frontal, performs significantly worse (84.91% accuracy) on …",
            "저자": "Soumyadip Sengupta, Jun-Cheng Chen, Carlos Castillo, Vishal M Patel, Rama Chellappa, David W Jacobs",
            "전체 인용횟수": "699회 인용201620172018201920202021202220235103481106146155154",
            "컨퍼런스": "2016 IEEE winter conference on applications of computer vision (WACV)",
            "페이지": "1-9",
            "학술 문서": "Frontal to profile face verification in the wildS Sengupta, JC Chen, C Castillo, VM Patel… - 2016 IEEE winter conference on applications of …, 2016699회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Frontal to profile face verification in the wild",
        "year": null
    },
    "Estimation of illuminant direction, albedo, and shape from shading": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2002",
            "설명": "A robust approach to recovery of shape from shading information is presented. Assuming uniform albedo and Lambertian ods for the estimation surface for of the illuminant imaging direction model, we and first surface present albedo. meth-",
            "저자": "Qinfen Zheng, Rama Chellappa",
            "전체 인용횟수": "698회 인용199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202326101721111914162624203633433746433720282626302092112777755",
            "페이지": "540-545",
            "학술 문서": "Estimation of illuminant direction, albedo, and shape from shadingQ Zheng, R Chellappa - 2002698회 인용 관련 학술자료 전체 23개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Estimation of illuminant direction, albedo, and shape from shading",
        "year": null
    },
    "Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/2/1",
            "게시자": "IEEE",
            "설명": "The discriminative model learning for image denoising has been recently attracting considerable attentions due to its favorable denoising performance. In this paper, we take one step forward by investigating the construction of feed-forward denoising convolutional neural networks (DnCNNs) to embrace the progress in very deep architecture, learning algorithm, and regularization method into image denoising. Specifically, residual learning and batch normalization are utilized to speed up the training process as well as boost the denoising performance. Different from the existing discriminative denoising models which usually train a specific model for additive white Gaussian noise at a certain noise level, our DnCNN model is able to handle Gaussian denoising with unknown noise level (i.e., blind Gaussian denoising). With the residual learning strategy, DnCNN implicitly removes the latent clean image in the hidden …",
            "저널": "IEEE Transactions on Image Processing",
            "저자": "Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, Lei Zhang",
            "전체 인용횟수": "7012회 인용20172018201920202021202220231074708731168139115751352",
            "학술 문서": "Beyond a gaussian denoiser: Residual learning of deep cnn for image denoisingK Zhang, W Zuo, Y Chen, D Meng, L Zhang - IEEE transactions on image processing, 20177012회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising",
        "year": null
    },
    "FSIM: A feature similarity index for image quality assessment": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/8",
            "게시자": "IEEE",
            "권": "20",
            "설명": "Image quality assessment (IQA) aims to use computational models to measure the image quality consistently with subjective evaluations. The well-known structural similarity index brings IQA from pixel- to structure-based stage. In this paper, a novel feature similarity (FSIM) index for full reference IQA is proposed based on the fact that human visual system (HVS) understands an image mainly according to its low-level features. Specifically, the phase congruency (PC), which is a dimensionless measure of the significance of a local structure, is used as the primary feature in FSIM. Considering that PC is contrast invariant while the contrast information does affect HVS' perception of image quality, the image gradient magnitude (GM) is employed as the secondary feature in FSIM. PC and GM play complementary roles in characterizing the image local quality. After obtaining the local quality map, we use PC again as a …",
            "저널": "Image Processing, IEEE Transactions on",
            "저자": "Lin Zhang, Lei Zhang, X Mou, D Zhang",
            "전체 인용횟수": "4765회 인용20112012201320142015201620172018201920202021202220232780111219302394416475475529566604523",
            "페이지": "2378-2386",
            "학술 문서": "FSIM: A feature similarity index for image quality assessmentL Zhang, L Zhang, X Mou, D Zhang - IEEE transactions on Image Processing, 20114765회 인용 관련 학술자료 전체 19개의 버전 ",
            "호": "8"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "FSIM: A feature similarity index for image quality assessment",
        "year": null
    },
    "A completed modeling of local binary pattern operator for texture classification": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/6",
            "게시자": "IEEE",
            "권": "19",
            "설명": "In this correspondence, a completed modeling of the local binary pattern (LBP) operator is proposed and an associated completed LBP (CLBP) scheme is developed for texture classification. A local region is represented by its center pixel and a local difference sign-magnitude transform (LDSMT). The center pixels represent the image gray level and they are converted into a binary code, namely CLBP-Center (CLBP_C), by global thresholding. LDSMT decomposes the image local differences into two complementary components: the signs and the magnitudes, and two operators, namely CLBP-Sign (CLBP_S) and CLBP-Magnitude (CLBP_M), are proposed to code them. The traditional LBP is equivalent to the CLBP_S part of CLBP, and we show that CLBP_S preserves more information of the local structure than CLBP_M, which explains why the simple LBP operator can extract the texture features reasonably well …",
            "저널": "Image Processing, IEEE Transactions on",
            "저자": "Zhenhua Guo, Lei Zhang, David Zhang",
            "전체 인용횟수": "2559회 인용20112012201320142015201620172018201920202021202220234380147201226241272292270239210177136",
            "페이지": "1657-1663",
            "학술 문서": "A completed modeling of local binary pattern operator for texture classificationZ Guo, L Zhang, D Zhang - IEEE transactions on image processing, 20102559회 인용 관련 학술자료 전체 17개의 버전 ",
            "호": "6"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A completed modeling of local binary pattern operator for texture classification",
        "year": null
    },
    "Fast compressive tracking": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/4/7",
            "게시자": "IEEE",
            "권": "36",
            "설명": "It is a challenging task to develop effective and efficient appearance models for robust object tracking due to factors such as pose variation, illumination change, occlusion, and motion blur. Existing online tracking algorithms often update models with samples from observations in recent frames. Despite much success has been demonstrated, numerous issues remain to be addressed. First, while these adaptive appearance models are data-dependent, there does not exist sufficient amount of data for online algorithms to learn at the outset. Second, online tracking algorithms often encounter the drift problems. As a result of self-taught learning, misaligned samples are likely to be added and degrade the appearance models. In this paper, we propose a simple yet effective and efficient tracking algorithm with an appearance model based on features extracted from a multiscale image feature space with dataindependent …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Kaihua Zhang, Lei Zhang, Ming-Hsuan Yang",
            "전체 인용횟수": "2471회 인용20132014201520162017201820192020202120222023104243411443352333219149805231",
            "페이지": "2002-2015",
            "학술 문서": "Fast compressive trackingK Zhang, L Zhang, MH Yang - IEEE transactions on pattern analysis and machine …, 20142445회 인용 관련 학술자료 전체 31개의 버전 Fast compressive tracking*Z Kaihua, Z Lei, Y Ming-Hsuan - IEEE transactions on pattern analysis and machine …, 201451회 인용 관련 학술자료 ",
            "호": "10"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Fast compressive tracking",
        "year": null
    },
    "Real-time Compressive Tracking": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012",
            "게시자": "Springer",
            "저널": "ECCV 2012",
            "저자": "Kaihua Zhang, Lei Zhang, Ming-Hsuan Yang",
            "전체 인용횟수": "2445회 인용20132014201520162017201820192020202120222023104242407436347330215148805131",
            "학술 문서": "Fast compressive tracking*K Zhang, L Zhang, MH Yang - IEEE transactions on pattern analysis and machine …, 20142445회 인용 관련 학술자료 전체 31개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Real-time Compressive Tracking",
        "year": null
    },
    "Sparse Representation or Collaborative Representation: Which Helps Face Recognition?": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011",
            "게시자": "IEEE",
            "설명": "As a recently proposed technique, sparse representation based classification (SRC) has been widely used for face recognition (FR). SRC first codes a testing sample as a sparse linear combination of all the training samples, and then classifies the testing sample by evaluating which class leads to the minimum representation error. While the importance of sparsity is much emphasized in SRC and many related works, the use of collaborative representation (CR) in SRC is ignored by most literature. However, is it really the l 1 -norm sparsity that improves the FR accuracy? This paper devotes to analyze the working mechanism of SRC, and indicates that it is the CR but not the l1-norm sparsity that makes SRC powerful for face classification. Consequently, we propose a very simple yet much more efficient face classification scheme, namely CR based classification with regularized least square (CRC_RLS). The …",
            "저자": "Lei Zhang, Meng Yang, Xiangchu Feng",
            "전체 인용횟수": "2426회 인용20122013201420152016201720182019202020212022202356132194260324279292248202158140105",
            "컨퍼런스": "Int. Conf. on Comput. Vis",
            "페이지": "543 – 550",
            "학술 문서": "Sparse representation or collaborative representation: Which helps face recognition?L Zhang, M Yang, X Feng - 2011 International conference on computer vision, 20112426회 인용 관련 학술자료 전체 18개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Sparse Representation or Collaborative Representation: Which Helps Face Recognition?",
        "year": null
    },
    "Weighted Nuclear Norm Minimization with Application to Image Denoising": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "게시자": "IEEE",
            "설명": "As a convex relaxation of the low rank matrix factorization problem, the nuclear norm minimization has been attracting significant research interest in recent years. The standard nuclear norm minimization regularizes each singular value equally to pursue the convexity of the objective function. However, this greatly restricts its capability and flexibility in dealing with many practical problems (eg, denoising), where the singular values have clear physical meanings and should be treated differently. In this paper we study the weighted nuclear norm minimization (WNNM) problem, where the singular values are assigned different weights. The solutions of the WNNM problem are analyzed under different weighting conditions. We then apply the proposed WNNM algorithm to image denoising by exploiting the image nonlocal self-similarity. Experimental results clearly show that the proposed WNNM algorithm outperforms many state-of-the-art denoising algorithms such as BM3D in terms of both quantitative measure and visual perception quality.",
            "저자": "Shuhang Gu, Lei Zhang, Wangmeng Zuo, Xiangchu Feng",
            "전체 인용횟수": "2159회 인용201420152016201720182019202020212022202363979160200292340330353336",
            "컨퍼런스": "IEEE Conf. on Computer Vision and Pattern Recognition 2014",
            "학술 문서": "Weighted nuclear norm minimization with application to image denoisingS Gu, L Zhang, W Zuo, X Feng - Proceedings of the IEEE conference on computer …, 20142159회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Weighted Nuclear Norm Minimization with Application to Image Denoising",
        "year": null
    },
    "FFDNet: Toward a fast and flexible solution for CNN-based image denoising": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/9",
            "게시자": "IEEE",
            "권": "27",
            "설명": "Due to the fast inference and good performance, discriminative learning methods have been widely studied in image denoising. However, these methods mostly learn a specific model for each noise level, and require multiple models for denoising images with different noise levels. They also lack flexibility to deal with spatially variant noise, limiting their applications in practical denoising. To address these issues, we present a fast and flexible denoising convolutional neural network, namely FFDNet, with a tunable noise level map as the input. The proposed FFDNet works on downsampled sub-images, achieving a good trade-off between inference speed and denoising performance. In contrast to the existing discriminative denoisers, FFDNet enjoys several desirable properties, including: 1) the ability to handle a wide range of noise levels (i.e., [0, 75]) effectively with a single network; 2) the ability to remove spatially …",
            "저널": "IEEE Transactions on Image Processing",
            "저자": "Kai Zhang, Wangmeng Zuo, Lei Zhang",
            "전체 인용횟수": "2005회 인용20182019202020212022202329178321454518490",
            "페이지": "4608-4622",
            "학술 문서": "FFDNet: Toward a fast and flexible solution for CNN-based image denoisingK Zhang, W Zuo, L Zhang - IEEE Transactions on Image Processing, 20182005회 인용 관련 학술자료 전체 7개의 버전 ",
            "호": "9"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "FFDNet: Toward a fast and flexible solution for CNN-based image denoising",
        "year": null
    },
    "Learning Deep CNN Denoiser Prior for Image Restoration": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/4/11",
            "설명": "Model-based optimization methods and discriminative learning methods have been the two dominant strategies for solving various inverse problems in low-level vision. Typically, those two kinds of methods have their respective merits and drawbacks, eg, model-based optimization methods are flexible for handling different inverse problems but are usually time-consuming with sophisticated priors for the purpose of good performance; in the meanwhile, discriminative learning methods have fast testing speed but their application range is greatly restricted by the specialized task. Recent works have revealed that, with the aid of variable splitting techniques, denoiser prior can be plugged in as a modular part of model-based optimization methods to solve other inverse problems (eg, deblurring). Such an integration induces considerable advantage when the denoiser is obtained via discriminative learning. However, the study of integration with fast discriminative denoiser prior is still lacking. To this end, this paper aims to train a set of fast and effective CNN (convolutional neural network) denoisers and integrate them into model-based optimization method to solve other inverse problems. Experimental results demonstrate that the learned set of denoisers can not only achieve promising Gaussian denoising results but also can be used as prior to deliver good performance for various low-level vision applications.",
            "저널": "arXiv preprint arXiv:1704.03264",
            "저자": "Kai Zhang, Wangmeng Zuo, Shuhang Gu, Lei Zhang",
            "전체 인용횟수": "1935회 인용201720182019202020212022202315146252342415390359",
            "학술 문서": "Learning deep CNN denoiser prior for image restorationK Zhang, W Zuo, S Gu, L Zhang - Proceedings of the IEEE conference on computer …, 20171935회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning Deep CNN Denoiser Prior for Image Restoration",
        "year": null
    },
    "Nonlocally Centralized Sparse Representation for Image Restoration": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/4",
            "게시자": "IEEE",
            "권": "22",
            "설명": "Sparse representation models code an image patch as a linear combination of a few atoms chosen out from an over-complete dictionary, and they have shown promising results in various image restoration applications. However, due to the degradation of the observed image (e.g., noisy, blurred, and/or down-sampled), the sparse representations by conventional models may not be accurate enough for a faithful reconstruction of the original image. To improve the performance of sparse representation-based image restoration, in this paper the concept of sparse coding noise is introduced, and the goal of image restoration turns to how to suppress the sparse coding noise. To this end, we exploit the image nonlocal self-similarity to obtain good estimates of the sparse coding coefficients of the original image, and then centralize the sparse coding coefficients of the observed image to those estimates. The so-called …",
            "저널": "IEEE Trans. on Image Processing",
            "저자": "Weisheng Dong, Lei Zhang, Guangming Shi, Xin Li",
            "전체 인용횟수": "1595회 인용20132014201520162017201820192020202120222023860115148159187226221198144113",
            "페이지": "1620-1630",
            "학술 문서": "Nonlocally centralized sparse representation for image restorationW Dong, L Zhang, G Shi, X Li - IEEE transactions on Image Processing, 20121595회 인용 관련 학술자료 전체 20개의 버전 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Nonlocally Centralized Sparse Representation for Image Restoration",
        "year": null
    },
    "Image deblurring and super-resolution by adaptive sparse domain selection and adaptive regularization": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/7",
            "게시자": "IEEE",
            "권": "20",
            "설명": "As a powerful statistical image modeling technique, sparse representation has been successfully used in various image restoration applications. The success of sparse representation owes to the development of the  l 1 -norm optimization techniques and the fact that natural images are intrinsically sparse in some domains. The image restoration quality largely depends on whether the employed sparse domain can represent well the underlying image. Considering that the contents can vary significantly across different images or different patches in a single image, we propose to learn various sets of bases from a precollected dataset of example image patches, and then, for a given patch to be processed, one set of bases are adaptively selected to characterize the local sparse domain. We further introduce two adaptive regularization terms into the sparse representation framework. First, a set of autoregressive (AR …",
            "저널": "Image Processing, IEEE Transactions on",
            "저자": "Weisheng Dong, Lei Zhang, Guangming Shi, Xiaolin Wu",
            "전체 인용횟수": "1523회 인용2010201120122013201420152016201720182019202020212022202351754881331552021671651351301137965",
            "페이지": "1838-1857",
            "학술 문서": "Image deblurring and super-resolution by adaptive sparse domain selection and adaptive regularizationW Dong, L Zhang, G Shi, X Wu - IEEE Transactions on image processing, 20111523회 인용 관련 학술자료 전체 25개의 버전 ",
            "호": "7"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Image deblurring and super-resolution by adaptive sparse domain selection and adaptive regularization",
        "year": null
    },
    "Gradient Magnitude Similarity Deviation: A Highly Efficient Perceptual Image Quality Index": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/2",
            "게시자": "IEEE",
            "권": "23",
            "설명": "It is an important task to faithfully evaluate the perceptual quality of output images in many applications, such as image compression, image restoration, and multimedia streaming. A good image quality assessment (IQA) model should not only deliver high quality prediction accuracy, but also be computationally efficient. The efficiency of IQA metrics is becoming particularly important due to the increasing proliferation of high-volume visual data in high-speed networks. We present a new effective and efficient IQA model, called gradient magnitude similarity deviation (GMSD). The image gradients are sensitive to image distortions, while different local structures in a distorted image suffer different degrees of degradations. This motivates us to explore the use of global variation of gradient based local quality map for overall image quality prediction. We find that the pixel-wise gradient magnitude similarity (GMS) between …",
            "저널": "IEEE Transactions on Image Processing",
            "저자": "Wufeng Xue, Lei Zhang, Xuanqin Mou, Alan Bovik",
            "전체 인용횟수": "1446회 인용20142015201620172018201920202021202220233178104114156189169176220191",
            "페이지": "684 - 695",
            "학술 문서": "Gradient magnitude similarity deviation: A highly efficient perceptual image quality indexW Xue, L Zhang, X Mou, AC Bovik - IEEE transactions on image processing, 20131446회 인용 관련 학술자료 전체 29개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Gradient Magnitude Similarity Deviation: A Highly Efficient Perceptual Image Quality Index",
        "year": null
    },
    "Second-order attention network for single image super-resolution": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "Recently, deep convolutional neural networks (CNNs) have been widely explored in single image super-resolution (SISR) and obtained remarkable performance. However, most of the existing CNN-based SISR methods mainly focus on wider or deeper architecture design, neglecting to explore the feature correlations of intermediate layers, hence hindering the representational power of CNNs. To address this issue, in this paper, we propose a second-order attention network (SAN) for more powerful feature expression and feature correlation learning. Specifically, a novel train-able second-order channel attention (SOCA) module is developed to adaptively rescale the channel-wise features by using second-order feature statistics for more discriminative representations. Furthermore, we present a non-locally enhanced residual group (NLRG) structure, which not only incorporates non-local operations to capture long-distance spatial contextual information, but also contains repeated local-source residual attention groups (LSRAG) to learn increasingly abstract feature representations. Experimental results demonstrate the superiority of our SAN network over state-of-the-art SISR methods in terms of both quantitative metrics and visual quality.",
            "저자": "Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, Lei Zhang",
            "전체 인용횟수": "1401회 인용2019202020212022202316175372432398",
            "컨퍼런스": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "페이지": "11065-11074",
            "학술 문서": "Second-order attention network for single image super-resolutionT Dai, J Cai, Y Zhang, ST Xia, L Zhang - Proceedings of the IEEE/CVF conference on computer …, 20191401회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Second-order attention network for single image super-resolution",
        "year": null
    },
    "An edge-guided image interpolation algorithm via directional filtering and data fusion": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006/8",
            "게시자": "IEEE",
            "권": "15",
            "설명": "Preserving edge structures is a challenge to image interpolation algorithms that reconstruct a high-resolution image from a low-resolution counterpart. We propose a new edge-guided nonlinear interpolation technique through directional filtering and data fusion. For a pixel to be interpolated, two observation sets are defined in two orthogonal directions, and each set produces an estimate of the pixel value. These directional estimates, modeled as different noisy measurements of the missing pixel are fused by the linear minimum mean square-error estimation (LMMSE) technique into a more robust estimate, using the statistics of the two observation sets. We also present a simplified version of the LMMSE-based interpolation algorithm to reduce computational cost without sacrificing much the interpolation performance. Experiments show that the new interpolation techniques can preserve edge sharpness and reduce …",
            "저널": "Image Processing, IEEE Transactions on",
            "저자": "Lei Zhang, Xiaolin Wu",
            "전체 인용횟수": "1210회 인용200720082009201020112012201320142015201620172018201920202021202220238123741515481819684698792971299286",
            "페이지": "2226-2238",
            "학술 문서": "An edge-guided image interpolation algorithm via directional filtering and data fusionL Zhang, X Wu - IEEE transactions on Image Processing, 20061210회 인용 관련 학술자료 전체 16개의 버전 ",
            "호": "8"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "An edge-guided image interpolation algorithm via directional filtering and data fusion",
        "year": null
    },
    "Fisher Discrimination Dictionary Learning for Sparse Representation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011",
            "게시자": "IEEE",
            "설명": "Sparse representation based classification has led to interesting image recognition results, while the dictionary used for sparse coding plays a key role in it. This paper presents a novel dictionary learning (DL) method to improve the pattern classification performance. Based on the Fisher discrimination criterion, a structured dictionary, whose dictionary atoms have correspondence to the class labels, is learned so that the reconstruction error after sparse coding can be used for pattern classification. Meanwhile, the Fisher discrimination criterion is imposed on the coding coefficients so that they have small within-class scatter but big between-class scatter. A new classification scheme associated with the proposed Fisher discrimination DL (FDDL) method is then presented by using both the discriminative information in the reconstruction error and sparse coding coefficients. The proposed FDDL is extensively evaluated …",
            "저자": "Meng Yang, Lei Zhang, Xiangchu Feng, David Zhang",
            "전체 인용횟수": "1174회 인용201220132014201520162017201820192020202120222023288012014616814212710589594840",
            "컨퍼런스": "ICCV 2011",
            "페이지": "543 – 550",
            "학술 문서": "Fisher discrimination dictionary learning for sparse representationM Yang, L Zhang, X Feng, D Zhang - 2011 international conference on computer vision, 20111174회 인용 관련 학술자료 전체 18개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Fisher Discrimination Dictionary Learning for Sparse Representation",
        "year": null
    },
    "Rotation invariant texture classification using LBP variance (LBPV) with global matching": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/3/31",
            "게시자": "Pergamon",
            "권": "43",
            "설명": "Local or global rotation invariant feature extraction has been widely used in texture classification. Local invariant features, e.g. local binary pattern (LBP), have the drawback of losing global spatial information, while global features preserve little local texture information. This paper proposes an alternative hybrid scheme, globally rotation invariant matching with locally variant LBP texture features. Using LBP distribution, we first estimate the principal orientations of the texture image and then use them to align LBP histograms. The aligned histograms are then in turn used to measure the dissimilarity between images. A new texture descriptor, LBP variance (LBPV), is proposed to characterize the local contrast information into the one-dimensional LBP histogram. LBPV does not need any quantization and it is totally training-free. To further speed up the proposed matching scheme, we propose a method to reduce …",
            "저널": "Pattern Recognition",
            "저자": "Zhenhua Guo, Lei Zhang, David Zhang",
            "전체 인용횟수": "1045회 인용2010201120122013201420152016201720182019202020212022202383855102971171301061247860464130",
            "페이지": "706-719",
            "학술 문서": "Rotation invariant texture classification using LBP variance (LBPV) with global matchingZ Guo, L Zhang, D Zhang - Pattern recognition, 20101045회 인용 관련 학술자료 전체 8개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Rotation invariant texture classification using LBP variance (LBPV) with global matching",
        "year": null
    },
    "Fast Tracking via Dense Spatio-Temporal Context Learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "게시자": "Springer",
            "저널": "ECCV 2014",
            "저자": "Kaihua Zhang, Lei Zhang, Qingshan Liu, David Zhang, Ming-Hsuan Yang",
            "전체 인용횟수": "1016회 인용2014201520162017201820192020202120222023158013717519916497553935",
            "학술 문서": "Fast visual tracking via dense spatio-temporal context learning*K Zhang, L Zhang, Q Liu, D Zhang, MH Yang - Computer Vision–ECCV 2014: 13th European …, 2014991회 인용 관련 학술자료 전체 22개의 버전 Fast tracking via spatio-temporal context Learning [J]*K Zhang, L Zhang, MH Yang, D Zhang - Computer Science, 201334회 인용 관련 학술자료 Fast Tracking Via Dense Spatio-temporal Context Learning*K Zhang, L Zhang, Q Liu, D Zhang, MH Yang - Proceedings of the IEEE European Conference on …2회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Fast Tracking via Dense Spatio-Temporal Context Learning",
        "year": null
    },
    "Active contours with selective local or global segmentation: A new formulation and level set method": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/4/30",
            "게시자": "Elsevier",
            "권": "28",
            "설명": "A novel region-based active contour model (ACM) is proposed in this paper. It is implemented with a special processing named Selective Binary and Gaussian Filtering RegularizedLevel Set(SBGFRLS) method, which first selectively penalizes the level set function to be binary, and then uses a Gaussian smoothing kernel to regularize it. The advantages of our method are as follows. First, a new region-based signed pressure force (SPF) function is proposed, which can efficiently stop the contours at weak or blurred edges. Second, the exterior and interior boundaries can be automatically detected with the initial contour being anywhere in the image. Third, the proposed ACM with SBGFRLS has the property of selective local or global segmentation. It can segment not only the desired object but also the other objects. Fourth, the level set function can be easily initialized with a binary function, which is more efficient to …",
            "저널": "Image and Vision Computing",
            "저자": "Kaihua Zhang, Lei Zhang, Huihui Song, Wengang Zhou",
            "전체 인용횟수": "1015회 인용2010201120122013201420152016201720182019202020212022202314417911910011210195868359563626",
            "페이지": "668-676",
            "학술 문서": "Active contours with selective local or global segmentation: a new formulation and level set methodK Zhang, L Zhang, H Song, W Zhou - Image and Vision computing, 20101015회 인용 관련 학술자료 전체 13개의 버전 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Active contours with selective local or global segmentation: A new formulation and level set method",
        "year": null
    },
    "Learning a single convolutional super-resolution network for multiple degradations": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "Recent years have witnessed the unprecedented success of deep convolutional neural networks (CNNs) in single image super-resolution (SISR). However, existing CNN-based SISR methods mostly assume that a low-resolution (LR) image is bicubicly downsampled from a high-resolution (HR) image, thus inevitably giving rise to poor performance when the true degradation does not follow this assumption. Moreover, they lack scalability in learning a single model to non-blindly deal with multiple degradations. To address these issues, we propose a general framework with dimensionality stretching strategy that enables a single convolutional super-resolution network to take two key factors of the SISR degradation process, ie, blur kernel and noise level, as input. Consequently, the super-resolver can handle multiple and even spatially variant degradations, which significantly improves the practicability. Extensive experimental results on synthetic and real LR images show that the proposed convolutional super-resolution network not only can produce favorable results on multiple degradations but also is computationally efficient, providing a highly effective and scalable solution to practical SISR applications.",
            "저자": "Kai Zhang, Wangmeng Zuo, Lei Zhang",
            "전체 인용횟수": "971회 인용20182019202020212022202318101164232234215",
            "컨퍼런스": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "페이지": "3262-3271",
            "학술 문서": "Learning a single convolutional super-resolution network for multiple degradationsK Zhang, W Zuo, L Zhang - Proceedings of the IEEE conference on computer …, 2018971회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning a single convolutional super-resolution network for multiple degradations",
        "year": null
    },
    "Bottom-up and top-down attention for image captioning and visual question answering": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr/SPICE/BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.",
            "저자": "Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang",
            "전체 인용횟수": "4665회 인용20172018201920202021202220231416154780210351124960",
            "컨퍼런스": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "페이지": "6077-6086",
            "학술 문서": "Bottom-up and top-down attention for image captioning and visual question answeringP Anderson, X He, C Buehler, D Teney, M Johnson… - Proceedings of the IEEE conference on computer …, 20184663회 인용 관련 학술자료 전체 18개의 버전 Bottom-up and top-down attention for image captioning and visual question answering, arXiv*P Anderson, X He, C Buehler, D Teney, M Johnson… - arXiv preprint arXiv:1707.07998, 20185회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Bottom-up and top-down attention for image captioning and visual question answering",
        "year": null
    },
    "MS-Celeb-1M: A dataset and benchmark for large-scale face recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/10/8",
            "게시자": "Springer, Cham",
            "설명": " In this paper, we design a benchmark task and provide the associated datasets for recognizing face images and link them to corresponding entity keys in a knowledge base. More specifically, we propose a benchmark task to recognize one million celebrities from their face images, by using all the possibly collected face images of this individual on the web as training data. The rich information provided by the knowledge base helps to conduct disambiguation and improve the recognition accuracy, and contributes to various real-world applications, such as image captioning and news video analysis. Associated with this task, we design and provide concrete measurement set, evaluation protocol, as well as training data. We also present in details our experiment setup and report promising baseline results. Our benchmark task could lead to one of the largest classification problems in computer vision. To the …",
            "저자": "Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, Jianfeng Gao",
            "전체 인용횟수": "1990회 인용201620172018201920202021202220231273166239326418394350",
            "컨퍼런스": "European Conference on Computer Vision",
            "페이지": "87-102",
            "학술 문서": "Ms-celeb-1m: A dataset and benchmark for large-scale face recognitionY Guo, L Zhang, Y Hu, X He, J Gao - Computer Vision–ECCV 2016: 14th European …, 20161990회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "MS-Celeb-1M: A dataset and benchmark for large-scale face recognition",
        "year": null
    },
    "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020/8/23",
            "게시자": "Springer, Cham",
            "설명": " Large-scale pre-training methods of learning cross-modal representations on image-text pairs are becoming popular for vision-language tasks. While existing methods simply concatenate image region features and text features as input to the model to be pre-trained and use self-attention to learn image-text semantic alignments in a brute force manner, in this paper, we propose a new learning method Oscar (Object-Semantics Aligned Pre-training), which uses object tags detected in images as anchor points to significantly ease the learning of alignments. Our method is motivated by the observation that the salient objects in an image can be accurately detected, and are often mentioned in the paired text. We pre-train an Oscar model on the public corpus of 6.5 million text-image pairs, and fine-tune it on downstream tasks, creating new state-of-the-arts on six well-established vision-language understanding …",
            "저자": "Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, Jianfeng Gao",
            "전체 인용횟수": "1502회 인용202020212022202324244600628",
            "컨퍼런스": "European Conference on Computer Vision",
            "페이지": "121-137",
            "학술 문서": "Oscar: Object-semantics aligned pre-training for vision-language tasksX Li, X Yin, C Li, P Zhang, X Hu, L Zhang, L Wang… - Computer Vision–ECCV 2020: 16th European …, 20201502회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks",
        "year": null
    },
    "Cvt: Introducing convolutions to vision transformers": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021",
            "설명": "We present in this paper a new architecture, named Convolutional vision Transformer (CvT), that improves Vision Transformer (ViT) in performance and efficiency by introducing convolutions into ViT to yield the best of both designs. This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding, and a convolutional Trasnsformer block leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural networks (CNNs) to the ViT architecture (ie shift, scale, and distortion invariance) while maintaining the merits of Transformers (ie dynamic attention, global context, and better generalization). We validate CvT by conducting extensive experiments, showing that this approach achieves state-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k, with less parameters and lower FLOPs. In addition, performance gains are maintained when pretrained on larger datasets (eg ImageNet-22k) and fine-tuned to downstream tasks. Finally, our results show that the positional encoding, a crucial component in existing Vision Transformers, can be safely removed in our model, simplifying the design for higher resolution vision tasks. Code will be released at https://github. com/microsoft/CvT.",
            "저자": "Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang",
            "전체 인용횟수": "1357회 인용202120222023122523702",
            "컨퍼런스": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "페이지": "22-31",
            "학술 문서": "Cvt: Introducing convolutions to vision transformersH Wu, B Xiao, N Codella, M Liu, X Dai, L Yuan… - Proceedings of the IEEE/CVF international conference …, 20211349회 인용 관련 학술자료 전체 8개의 버전 Cvt: Introducing convolutions to vision transformers. arXiv 2021*H Wu, B Xiao, N Codella, M Liu, X Dai, L Yuan… - arXiv preprint arXiv:2103.1580815회 인용 관련 학술자료 CvT: Introducing convolutions to vision transformers (2021)*H Wu, B Xiao, N Codella, M Liu, X Dai, L Yuan… - arXiv preprint arXiv:2103.15808, 20217회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Cvt: Introducing convolutions to vision transformers",
        "year": null
    },
    "VinVL: Revisiting visual representations in vision-language models": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021",
            "설명": "This paper presents a detailed study of improving vision features and develops an improved object detection model for vision language (VL) tasks. Compared to the most widely used bottom-up and top-down model [2], the new model is bigger, pre-trained on much larger training corpora that combine multiple public annotated object detection datasets, and thus can generate representations of a richer collection of visual objects and concepts. While previous VL research focuses solely on improving the vision-language fusion model and leaves the object detection model improvement untouched, we present an empirical study to show that vision features matter significantly in VL models. In our experiments we feed the vision features generated by the new object detection model into a pre-trained transformer-based VL fusion model Oscar+, and fine-tune Oscar+ on a wide range of downstream VL tasks. Our results show that the new vision features significantly improve the performance across all VL tasks, creating new state-of-the-art results on seven public benchmarks. We will release the new object detection model to public.",
            "저자": "Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, Jianfeng Gao",
            "전체 인용횟수": "814회 인용20212022202368338405",
            "컨퍼런스": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "페이지": "5579-5588",
            "학술 문서": "Vinvl: Revisiting visual representations in vision-language modelsP Zhang, X Li, X Hu, J Yang, L Zhang, L Wang, Y Choi… - Proceedings of the IEEE/CVF conference on computer …, 2021674회 인용 관련 학술자료 전체 8개의 버전 Vinvl: Making visual representations matter in vision-language models*P Zhang, X Li, X Hu, J Yang, L Zhang, L Wang, Y Choi… - arXiv preprint arXiv:2101.00529, 2021146회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "VinVL: Revisiting visual representations in vision-language models",
        "year": null
    },
    "Unified Vision-Language Pre-Training for Image Captioning and VQA.": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020",
            "설명": "This paper presents a unified Vision-Language Pre-training (VLP) model. The model is unified in that (1) it can be fine-tuned for either vision-language generation (eg, image captioning) or understanding (eg, visual question answering) tasks, and (2) it uses a shared multi-layer transformer network for both encoding and decoding, which differs from many existing methods where the encoder and decoder are implemented using separate models. The unified VLP model is pre-trained on a large amount of image-text pairs using the unsupervised learning objectives of two tasks: bidirectional and sequence-to-sequence (seq2seq) masked vision-language prediction. The two tasks differ solely in what context the prediction conditions on. This is controlled by utilizing specific self-attention masks for the shared transformer network. To the best of our knowledge, VLP is the first reported model that achieves state-of-the-art results on both vision-language generation and understanding tasks, as disparate as image captioning and visual question answering, across three challenging benchmark datasets: COCO Captions, Flickr30k Captions, and VQA 2.0. The code and the pre-trained models are available at https://github. com/LuoweiZhou/VLP.",
            "저자": "Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J Corso, Jianfeng Gao",
            "전체 인용횟수": "760회 인용20192020202120222023271180256247",
            "컨퍼런스": "AAAI",
            "페이지": "13041-13049",
            "학술 문서": "Unified vision-language pre-training for image captioning and vqaL Zhou, H Palangi, L Zhang, H Hu, J Corso, J Gao - Proceedings of the AAAI conference on artificial …, 2020760회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Unified Vision-Language Pre-Training for Image Captioning and VQA.",
        "year": null
    },
    "HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020",
            "설명": "Bottom-up human pose estimation methods have difficulties in predicting the correct pose for small persons due to challenges in scale variation. In this paper, we present HigherHRNet: a novel bottom-up human pose estimation method for learning scale-aware representations using high-resolution feature pyramids. Equipped with multi-resolution supervision for training and multi-resolution aggregation for inference, the proposed approach is able to solve the scale variation challenge in bottom-up multi-person pose estimation and localize keypoints more precisely, especially for small person. The feature pyramid in HigherHRNet consists of feature map outputs from HRNet and upsampled higher-resolution outputs through a transposed convolution. HigherHRNet outperforms the previous best bottom-up method by 2.5% AP for medium person on COCO test-dev, showing its effectiveness in handling scale variation. Furthermore, HigherHRNet achieves new state-of-the-art result on COCO test-dev (70.5% AP) without using refinement or other post-processing techniques, surpassing all existing bottom-up methods. HigherHRNet even surpasses all top-down methods on CrowdPose test (67.6% AP), suggesting its robustness in crowded scene.",
            "저자": "Bowen Cheng, Bin Xiao, Jingdong Wang, Honghui Shi, Thomas S Huang, Lei Zhang",
            "전체 인용횟수": "631회 인용202020212022202324148204251",
            "컨퍼런스": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "페이지": "5386-5395",
            "학술 문서": "Higherhrnet: Scale-aware representation learning for bottom-up human pose estimationB Cheng, B Xiao, J Wang, H Shi, TS Huang, L Zhang - Proceedings of the IEEE/CVF conference on computer …, 2020631회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation",
        "year": null
    },
    "Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "Vision-language navigation (VLN) is the task of navigating an embodied agent to carry out natural language instructions inside real 3D environments. In this paper, we study how to address three critical challenges for this task: the cross-modal grounding, the ill-posed feedback, and the generalization problems. First, we propose a novel Reinforced Cross-Modal Matching (RCM) approach that enforces cross-modal grounding both locally and globally via reinforcement learning (RL). Particularly, a matching critic is used to provide an intrinsic reward to encourage global matching between instructions and trajectories, and a reasoning navigator is employed to perform cross-modal grounding in the local visual scene. Evaluation on a VLN benchmark dataset shows that our RCM model significantly outperforms previous methods by 10% on SPL and achieves the new state-of-the-art performance. To improve the generalizability of the learned policy, we further introduce a Self-Supervised Imitation Learning (SIL) method to explore unseen environments by imitating its own past, good decisions. We demonstrate that SIL can approximate a better and more efficient policy, which tremendously minimizes the success rate performance gap between seen and unseen environments (from 30.7% to 11.7%).",
            "저자": "Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, Lei Zhang",
            "전체 인용횟수": "470회 인용2018201920202021202220232389112193124",
            "컨퍼런스": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "페이지": "6629-6638",
            "학술 문서": "Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigationX Wang, Q Huang, A Celikyilmaz, J Gao, D Shen… - Proceedings of the IEEE/CVF conference on computer …, 2019470회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation",
        "year": null
    },
    "Cleannet: Transfer learning for scalable image classifier training with label noise": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "In this paper, we study the problem of learning image classification models with label noise. Existing approaches depending on human supervision are generally not scalable as manually identifying correct or incorrect labels is time-consuming, whereas approaches not relying on human supervision are scalable but less effective. To reduce the amount of human supervision for label noise cleaning, we introduce CleanNet, a joint neural embedding network, which only requires a fraction of the classes being manually verified to provide the knowledge of label noise that can be transferred to other classes. We further integrate CleanNet and conventional convolutional neural network classifier into one framework for image classification learning. We demonstrate the effectiveness of the proposed algorithm on both of the label noise detection task and the image classification on noisy data task on several large-scale datasets. Experimental results show that CleanNet can reduce label noise detection error rate on held-out classes where no human supervision available by 41.5% compared to current weakly supervised methods. It also achieves 47% of the performance gain of verifying all images with only 3.2% images verified on an image classification task. Source code and dataset will be available at kuanghuei. github. io/CleanNetProject.",
            "저자": "Kuang-Huei Lee, Xiaodong He, Lei Zhang, Linjun Yang",
            "전체 인용횟수": "444회 인용2018201920202021202220234397513011084",
            "컨퍼런스": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "페이지": "5447-5456",
            "학술 문서": "Cleannet: Transfer learning for scalable image classifier training with label noiseKH Lee, X He, L Zhang, L Yang - Proceedings of the IEEE conference on computer …, 2018444회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Cleannet: Transfer learning for scalable image classifier training with label noise",
        "year": null
    },
    "Support vector machine learning for image retrieval": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2001/10/7",
            "게시자": "IEEE",
            "권": "2",
            "저자": "Lei Zhang, Fuzong Lin, Bo Zhang",
            "전체 인용횟수": "426회 인용2001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202333152411232326343434232024292215816111165",
            "컨퍼런스": "Image Processing, 2001. Proceedings. 2001 International Conference on",
            "페이지": "721-724 vol. 2",
            "학술 문서": "Segmenting human knee cartilage automatically from multi-contrast MR images using support vector machines and discriminative random fields*K Zhang, J Deng, W Lu - 2011 18th IEEE International Conference on Image …, 2011426회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Support vector machine learning for image retrieval",
        "year": null
    },
    "DINO: DETR with improved denoising anchor boxes for end-to-end object detection": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2023/4",
            "설명": "We present DINO (\\textbf{D}ETR with \\textbf{I}mproved de\\textbf{N}oising anch\\textbf{O}r boxes), a state-of-the-art end-to-end object detector. % in this paper. DINO improves over previous DETR-like models in performance and efficiency by using a contrastive way for denoising training, a mixed query selection method for anchor initialization, and a look forward twice scheme for box prediction. DINO achieves AP in  epochs and AP in  epochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a significant improvement of \\textbf{AP} and \\textbf{AP}, respectively, compared to DN-DETR, the previous best DETR-like model. DINO scales well in both model size and data size. Without bells and whistles, after pre-training on the Objects365 dataset with a SwinL backbone, DINO obtains the best results on both COCO \\texttt{val2017} (\\textbf{AP}) and \\texttt{test-dev} (\\textbf{AP}). Compared to other models on the leaderboard, DINO significantly reduces its model size and pre-training data size while achieving better results. Our code will be available at \\url{https://github.com/IDEACVR/DINO}.",
            "저자": "Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel Ni, Harry Shum",
            "전체 인용횟수": "425회 인용2022202374346",
            "컨퍼런스": "International Conference on Learning Representations",
            "학술 문서": "Dino: Detr with improved denoising anchor boxes for end-to-end object detectionH Zhang, F Li, S Liu, L Zhang, H Su, J Zhu, LM Ni… - arXiv preprint arXiv:2203.03605, 2022425회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "DINO: DETR with improved denoising anchor boxes for end-to-end object detection",
        "year": null
    },
    "AnnoSearch: Image auto-annotation by search": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006",
            "게시자": "Ieee",
            "권": "2",
            "설명": "Although it has been studied for several years by computer vision and machine learning communities, image annotation is still far from practical. In this paper, we present AnnoSearch, a novel way to annotate images using search and data mining technologies. Leveraging the Web-scale images, we solve this problem in two-steps: 1) searching for semantically and visually similar images on the Web, 2) and mining annotations from them. Firstly, at least one accurate keyword is required to enable text-based search for a set of semantically similar images. Then content-based search is performed on this set to retrieve visually similar images. At last, annotations are mined from the descriptions (titles, URLs and surrounding texts) of these images. It worth highlighting that to ensure the efficiency, high dimensional visual features are mapped to hash codes which significantly speed up the content-based search process …",
            "저자": "Xin-Jing Wang, Lei Zhang, Feng Jing, Wei-Ying Ma",
            "전체 인용횟수": "422회 인용20062007200820092010201120122013201420152016201720182019202020212022202351632313840363429511724191116727",
            "컨퍼런스": "Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on",
            "페이지": "1483-1490",
            "학술 문서": "Annosearch: Image auto-annotation by searchXJ Wang, L Zhang, F Jing, WY Ma - 2006 IEEE Computer Society Conference on Computer …, 2006422회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "AnnoSearch: Image auto-annotation by search",
        "year": null
    },
    "Multilinear discriminant analysis for face recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2007/1",
            "게시자": "IEEE",
            "권": "16",
            "설명": " There is a growing interest in subspace learning techniques for face recognition; however, the excessive dimension of the data space often brings the algorithms into the curse of dimensionality dilemma. In this paper, we present a novel approach to solve the supervised dimensionality reduction problem by encoding an image object as a general tensor of second or even higher order. First, we propose a discriminant tensor criterion, whereby multiple interrelated lower dimensional discriminative subspaces are derived for feature extraction. Then, a novel approach, called -mode optimization, is presented to iteratively learn these subspaces by unfolding the tensor along different tensor directions. We call this algorithm multilinear discriminant analysis (MDA), which has the following characteristics: 1) multiple interrelated subspaces can collaborate to discriminate different classes, 2) for classification problems involving …",
            "저널": "IEEE Transactions on Image Processing",
            "저자": "Shuicheng Yan, Dong Xu, Qiang Yang, Lei Zhang, Xiaoou Tang, Hong-Jiang Zhang",
            "전체 인용횟수": "417회 인용2007200820092010201120122013201420152016201720182019202020212022202331224224234413335322123282714159",
            "페이지": "212-220",
            "학술 문서": "Multilinear discriminant analysis for face recognitionS Yan, D Xu, Q Yang, L Zhang, X Tang, HJ Zhang - IEEE Transactions on image processing, 2006417회 인용 관련 학술자료 전체 18개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Multilinear discriminant analysis for face recognition",
        "year": null
    },
    "Grounded language-image pre-training": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2022",
            "설명": "This paper presents a grounded language-image pre-training (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP unifies object detection and phrase grounding for pre-training. The unification brings two benefits: 1) it allows GLIP to learn from both detection and grounding data to improve both tasks and bootstrap a good grounding model; 2) GLIP can leverage massive image-text pairs by generating grounding boxes in a self-training fashion, making the learned representations semantic-rich. In our experiments, we pre-train GLIP on 27M grounding data, including 3M human-annotated and 24M web-crawled image-text pairs. The learned representations demonstrate strong zero-shot and few-shot transferability to various object-level recognition tasks. 1) When directly evaluated on COCO and LVIS (without seeing any images in COCO during pre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many supervised baselines. 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val and 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13 downstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised Dynamic Head.",
            "저자": "Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, Jianfeng Gao",
            "전체 인용횟수": "404회 인용202120222023271331",
            "컨퍼런스": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "페이지": "10965-10975",
            "학술 문서": "Grounded language-image pre-trainingLH Li, P Zhang, H Zhang, J Yang, C Li, Y Zhong… - Proceedings of the IEEE/CVF Conference on Computer …, 2022404회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Grounded language-image pre-training",
        "year": null
    },
    "Photo2trip: Generating travel routes from geo-tagged photos for trip planning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/10/25",
            "게시자": "ACM",
            "설명": "Travel route planning is an important step for a tourist to prepare his/her trip. As a common scenario, a tourist usually asks the following questions when he/she is planning his/her trip in an unfamiliar place: 1) Are there any travel route suggestions for a one-day or three-day trip in Beijing? 2) What is the most popular travel path within the Forbidden City? To facilitate a tourist's trip planning, in this paper, we target at solving the problem of automatic travel route planning. We propose to leverage existing travel clues recovered from 20 million geo-tagged photos collected from www.panoramio.com to suggest customized travel route plans according to users' preferences. As the footprints of tourists at memorable destinations, the geo-tagged photos could be naturally used to discover the travel paths within a destination (attractions/landmarks) and travel routes between destinations. Based on the information discovered …",
            "저자": "Xin Lu, Changhu Wang, Jiang-Ming Yang, Yanwei Pang, Lei Zhang",
            "전체 인용횟수": "402회 인용2010201120122013201420152016201720182019202020212022202311026433649505436251823176",
            "컨퍼런스": "Proceedings of the international conference on Multimedia",
            "페이지": "143-152",
            "학술 문서": "Photo2trip: generating travel routes from geo-tagged photos for trip planningX Lu, C Wang, JM Yang, Y Pang, L Zhang - Proceedings of the 18th ACM international conference …, 2010402회 인용 관련 학술자료 전체 5개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Photo2trip: Generating travel routes from geo-tagged photos for trip planning",
        "year": null
    },
    "Pairwise rotation invariant co-occurrence local binary pattern": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/1/1",
            "게시자": "Springer Berlin Heidelberg",
            "도서": "Computer Vision–ECCV 2012",
            "설명": "Designing effective features is a fundamental problem in computer vision. However, it is usually difficult to achieve a great tradeoff between discriminative power and robustness. Previous works shown that spatial co-occurrence can boost the discriminative power of features. However the current existing co-occurrence features are taking few considerations to the robustness and hence suffering from sensitivity to geometric and photometric variations. In this work, we study the Transform Invariance (TI) of co-occurrence features. Concretely we formally introduce a Pairwise Transform Invariance (PTI) principle, and then propose a novel Pairwise Rotation Invariant Co-occurrence Local Binary Pattern (PRICoLBP) feature, and further extend it to incorporate multi-scale, multi-orientation, and multi-channel information. Different from other LBP variants, PRICoLBP can not only capture the spatial context co-occurrence …",
            "저자": "Xianbiao Qi, Rong Xiao, Jun Guo, Lei Zhang",
            "전체 인용횟수": "370회 인용20132014201520162017201820192020202120222023614315755545038271912",
            "페이지": "158-171",
            "학술 문서": "Pairwise rotation invariant co-occurrence local binary patternX Qi, R Xiao, CG Li, Y Qiao, J Guo, X Tang - IEEE transactions on pattern analysis and machine …, 2014370회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pairwise rotation invariant co-occurrence local binary pattern",
        "year": null
    },
    "Efficient 3D reconstruction for face recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005/6/1",
            "게시자": "Pergamon",
            "권": "38",
            "설명": "Face recognition with variant pose, illumination and expression (PIE) is a challenging problem. In this paper, we propose an analysis-by-synthesis framework for face recognition with variant PIE. First, an efficient two-dimensional (2D)-to-three-dimensional (3D) integrated face reconstruction approach is introduced to reconstruct a personalized 3D face model from a single frontal face image with neutral expression and normal illumination. Then, realistic virtual faces with different PIE are synthesized based on the personalized 3D face to characterize the face subspace. Finally, face recognition is conducted based on these representative virtual faces. Compared with other related work, this framework has following advantages: (1) only one single frontal face is required for face recognition, which avoids the burdensome enrollment work; (2) the synthesized face samples provide the capability to conduct recognition …",
            "저널": "Pattern Recognition",
            "저자": "Dalong Jiang, Yuxiao Hu, Shuicheng Yan, Lei Zhang, Hongjiang Zhang, Wen Gao",
            "전체 인용횟수": "367회 인용200520062007200820092010201120122013201420152016201720182019202020212022202321315202428212938302922172118121483",
            "페이지": "787-798",
            "학술 문서": "Efficient 3D reconstruction for face recognitionD Jiang, Y Hu, S Yan, L Zhang, H Zhang, W Gao - Pattern Recognition, 2005367회 인용 관련 학술자료 전체 11개의 버전 ",
            "호": "6"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Efficient 3D reconstruction for face recognition",
        "year": null
    },
    "Face annotation for photo management": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2007/9/25",
            "발명자": "Lei Zhang, Longbin Chen, Mingjing Li, Hong-Jiang Zhang",
            "설명": "Systems and methods for annotating a face in a digital image are described. In one aspect, a probability model is trained by mapping one or more sets of sample facial features to corresponding names of individuals. A face from an input data set of at least one the digital image is then detected. Facial features are then automatically extracted from the detected face. A similarity measure is them modeled as a posterior probability that the facial features match a particular set of features identified in the probability model. The similarity measure is statistically learned. A name is then inferred as a function of the similarity measure. The face is then annotated with the name.",
            "전체 인용횟수": "349회 인용2006200720082009201020112012201320142015201620172018201920202021202220236110152126333034322119132116181913",
            "출원번호": "10609974",
            "특허 번호": "7274822",
            "특허청": "US",
            "학술 문서": "Face annotation for photo managementL Zhang, L Chen, M Li, HJ Zhang - US Patent 7,274,822, 2007349회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Face annotation for photo management",
        "year": null
    },
    "Spatial-bag-of-features": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/6/13",
            "게시자": "IEEE",
            "설명": "In this paper, we study the problem of large scale image retrieval by developing a new class of bag-of-features to encode geometric information of objects within an image. Beyond existing orderless bag-of-features, local features of an image are first projected to different directions or points to generate a series of ordered bag-of-features, based on which different families of spatial bag-of-features are designed to capture the invariance of object translation, rotation, and scaling. Then the most representative features are selected based on a boosting-like method to generate a new bag-of-features-like vector representation of an image. The proposed retrieval framework works well in image retrieval task owing to the following three properties: 1) the encoding of geometric information of objects for capturing objects' spatial transformation, 2) the supervised feature selection and combination strategy for enhancing the …",
            "저자": "Yang Cao, Changhu Wang, Zhiwei Li, Liqing Zhang, Lei Zhang",
            "전체 인용횟수": "335회 인용20092010201120122013201420152016201720182019202020212022202311263352585230311866645",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on",
            "페이지": "3352-3359",
            "학술 문서": "Spatial-bag-of-featuresY Cao, C Wang, Z Li, L Zhang, L Zhang - 2010 IEEE Computer Society Conference on Computer …, 2010335회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Spatial-bag-of-features",
        "year": null
    },
    "Dynamic Head: Unifying Object Detection Heads with Attentions": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021",
            "설명": "The complex nature of combining localization and classification in object detection has resulted in the flourished development of methods. Previous works tried to improve the performance in various object detection heads but failed to present a unified view. In this paper, we present a novel dynamic head framework to unify object detection heads with attentions. By coherently combining multiple self-attention mechanisms between feature levels for scale-awareness, among spatial locations for spatial-awareness, and within output channels for task-awareness, the proposed approach significantly improves the representation ability of object detection heads without any computational overhead. Further experiments demonstrate that the effectiveness and efficiency of the proposed dynamic head on the COCO benchmark. With a standard ResNeXt-101-DCN backbone, we largely improve the performance over popular object detectors and achieve a new state-of-the-art at 54.0 AP. The code will be released at https://github. com/microsoft/DynamicHead.",
            "저자": "Xiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen, Mengchen Liu, Lu Yuan, Lei Zhang",
            "전체 인용횟수": "314회 인용20212022202322116172",
            "컨퍼런스": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "페이지": "7373-7382",
            "학술 문서": "Dynamic head: Unifying object detection heads with attentionsX Dai, Y Chen, B Xiao, D Chen, M Liu, L Yuan, L Zhang - Proceedings of the IEEE/CVF conference on computer …, 2021314회 인용 관련 학술자료 전체 5개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Dynamic Head: Unifying Object Detection Heads with Attentions",
        "year": null
    },
    "A review of electrode materials for electrochemical supercapacitors": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012",
            "게시자": "Royal Society of Chemistry",
            "권": "41",
            "설명": "In this critical review, metal oxides-based materials for electrochemical supercapacitor (ES) electrodes are reviewed in detail together with a brief review of carbon materials and conducting polymers. Their advantages, disadvantages, and performance in ES electrodes are discussed through extensive analysis of the literature, and new trends in material development are also reviewed. Two important future research directions are indicated and summarized, based on results published in the literature: the development of composite and nanostructured ES materials to overcome the major challenge posed by the low energy density of ES (476 references).",
            "저자": "Guoping Wang, Lei Zhang, Jiujun Zhang",
            "전체 인용횟수": "8964회 인용2012201320142015201620172018201920202021202220236233956287995299810441040962830699558",
            "출처": "Chemical Society Reviews",
            "페이지": "797-828",
            "학술 문서": "A review of electrode materials for electrochemical supercapacitorsG Wang, L Zhang, J Zhang - Chemical Society Reviews, 20128964회 인용 관련 학술자료 전체 12개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A review of electrode materials for electrochemical supercapacitors",
        "year": null
    },
    "A review of electrolyte materials and compositions for electrochemical supercapacitors": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "게시자": "Royal Society of Chemistry",
            "권": "44",
            "설명": "Electrolytes have been identified as some of the most influential components in the performance of electrochemical supercapacitors (ESs), which include: electrical double-layer capacitors, pseudocapacitors and hybrid supercapacitors. This paper reviews recent progress in the research and development of ES electrolytes. The electrolytes are classified into several categories, including: aqueous, organic, ionic liquids, solid-state or quasi-solid-state, as well as redox-active electrolytes. Effects of electrolyte properties on ES performance are discussed in detail. The principles and methods of designing and optimizing electrolytes for ES performance and application are highlighted through a comprehensive analysis of the literature. Interaction among the electrolytes, electro-active materials and inactive components (current collectors, binders, and separators) is discussed. The challenges in producing high-performing …",
            "저자": "Cheng Zhong, Yida Deng, Wenbin Hu, Jinli Qiao, Lei Zhang, Jiujun Zhang",
            "전체 인용횟수": "3075회 인용20162017201820192020202120222023110231344465445549483425",
            "출처": "Chemical Society Reviews",
            "페이지": "7484-7539",
            "학술 문서": "A review of electrolyte materials and compositions for electrochemical supercapacitorsC Zhong, Y Deng, W Hu, J Qiao, L Zhang, J Zhang - Chemical Society Reviews, 20153075회 인용 관련 학술자료 전체 7개의 버전 ",
            "호": "21"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A review of electrolyte materials and compositions for electrochemical supercapacitors",
        "year": null
    },
    "A review of anode catalysis in the direct methanol fuel cell": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006/4/21",
            "게시자": "Elsevier",
            "권": "155",
            "설명": "In this paper, more than 100 articles related to anode catalysts for the direct methanol fuel cell (DMFC) are reviewed, mainly focusing on the three most active areas: (1) progress in preparation methods of Pt–Ru catalysts with respect to activity improvement and utilization optimization; (2) preparation of novel carbon materials as catalyst supports to create a highly dispersed and stably supported catalysts; (3) exploration of new catalysts with a low noble metal content and non-noble metal elements through fast activity down-selection methods such as combinatorial methods. Suggested research and development (R&D) directions for new DMFC anode catalysis are also discussed.",
            "저자": "Hansan Liu, Chaojie Song, Lei Zhang, Jiujun Zhang, Haijiang Wang, David P Wilkinson",
            "전체 인용횟수": "2137회 인용2006200720082009201020112012201320142015201620172018201920202021202220231579109138141127146159170187141146138118116816253",
            "출처": "Journal of Power Sources",
            "페이지": "95-110",
            "학술 문서": "A review of anode catalysis in the direct methanol fuel cellH Liu, C Song, L Zhang, J Zhang, H Wang… - Journal of Power Sources, 20062137회 인용 관련 학술자료 전체 8개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A review of anode catalysis in the direct methanol fuel cell",
        "year": null
    },
    "A review on non-precious metal electrocatalysts for PEM fuel cells": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011",
            "게시자": "Royal Society of Chemistry",
            "권": "4",
            "설명": "With the approaching commercialization of PEM fuel cell technology, developing active, inexpensive non-precious metal ORR catalyst materials to replace currently used Pt-based catalysts is a necessary and essential requirement in order to reduce the overall system cost. This review paper highlights the progress made over the past 40 years with a detailed discussion of recent works in the area of non-precious metal electrocatalysts for oxygen reduction reaction, a necessary reaction at the PEM fuel cell cathode. Several important kinds of unsupported or carbon supported non-precious metal electrocatalysts for ORR are reviewed, including non-pyrolyzed and pyrolyzed transition metal nitrogen-containing complexes, conductive polymer-based catalysts, transition metal chalcogenides, metal oxides/carbides/nitrides/oxynitrides/carbonitrides, and enzymatic compounds. Among these candidates, pyrolyzed …",
            "저자": "Zhongwei Chen, Drew Higgins, Aiping Yu, Lei Zhang, Jiujun Zhang",
            "전체 인용횟수": "1878회 인용20112012201320142015201620172018201920202021202220235431271812012262162001811441669295",
            "출처": "Energy & Environmental Science",
            "페이지": "3167-3192",
            "학술 문서": "A review on non-precious metal electrocatalysts for PEM fuel cellsZ Chen, D Higgins, A Yu, L Zhang, J Zhang - Energy & Environmental Science, 20111878회 인용 관련 학술자료 전체 4개의 버전 ",
            "호": "9"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A review on non-precious metal electrocatalysts for PEM fuel cells",
        "year": null
    },
    "A review of Fe–N/C and Co–N/C catalysts for the oxygen reduction reaction": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008/6/1",
            "게시자": "Pergamon",
            "권": "53",
            "설명": "This paper reviews over 100 articles related to heat-treated Fe– and Co–N/C catalysts for the oxygen reduction reaction. The literature shows that through several decades’ effort in the development of non-noble catalysts such as heat-treated Fe– and Co–N/C catalysts, tremendous progress has been made in catalyst synthesis methodologies and the understanding of the mechanism. A heat-treatment step has been identified as necessary for catalyst activity and stability improvement. The enhanced performance of the catalysts is strongly dependent on the carbon support, the source of metal and nitrogen, and the thermal treatment conditions. The metal content in these catalysts also plays an important role in their activity and stability. A saturated metal content has been identified as a major limiting factor for further improvement of catalyst activity. The nitrogen content and the presence of a disordered or …",
            "저자": "Cicero WB Bezerra, Lei Zhang, Kunchan Lee, Hansan Liu, Aldaléa LB Marques, Edmar P Marques, Haijiang Wang, Jiujun Zhang",
            "전체 인용횟수": "1219회 인용2008200920102011201220132014201520162017201820192020202120222023528637389831041111091251039584654532",
            "출처": "Electrochimica Acta",
            "페이지": "4937-4951",
            "학술 문서": "A review of Fe–N/C and Co–N/C catalysts for the oxygen reduction reactionCWB Bezerra, L Zhang, K Lee, H Liu, ALB Marques… - Electrochimica Acta, 20081219회 인용 관련 학술자료 전체 7개의 버전 ",
            "호": "15"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A review of Fe–N/C and Co–N/C catalysts for the oxygen reduction reaction",
        "year": null
    },
    "Nanostructured Pt-alloy electrocatalysts for PEM fuel cell oxygen reduction reaction": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010",
            "게시자": "Royal Society of Chemistry",
            "권": "39",
            "설명": "In this critical review, we present the current technological advances in proton exchange membrane (PEM) fuel cell catalysis, with a focus on strategies for developing nanostructured Pt-alloys as electrocatalysts for the oxygen reduction reaction (ORR). The achievements are reviewed and the major challenges, including high cost, insufficient activity and low stability, are addressed and discussed. The nanostructured Pt-alloy catalysts can be grouped into different clusters: (i) Pt-alloy nanoparticles, (ii) Pt-alloy nanotextures such as Pt-skins/monolayers on top of base metals, and (iii) branched or anisotropic elongated Pt or Pt-alloy nanostructures. Although some Pt-alloy catalysts with advanced nanostructures have shown remarkable activity levels, the dissolution of metals, including Pt and alloyed base metals, in a fuel cell operating environment could cause catalyst degradation, and still remains an issue. Another …",
            "저자": "Yonghong Bing, Hansan Liu, Lei Zhang, Dave Ghosh, Jiujun Zhang",
            "전체 인용횟수": "1184회 인용2010201120122013201420152016201720182019202020212022202342759719014314111611196100917753",
            "출처": "Chemical Society Reviews",
            "페이지": "2184-2202",
            "학술 문서": "Nanostructured Pt-alloy electrocatalysts for PEM fuel cell oxygen reduction reactionY Bing, H Liu, L Zhang, D Ghosh, J Zhang - Chemical Society Reviews, 20101184회 인용 관련 학술자료 전체 8개의 버전 ",
            "호": "6"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Nanostructured Pt-alloy electrocatalysts for PEM fuel cell oxygen reduction reaction",
        "year": null
    },
    "Progress in preparation of non-noble electrocatalysts for PEM fuel cell reactions": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006/6/1",
            "게시자": "Elsevier",
            "권": "156",
            "설명": "This paper reviews the literature on the preparation aspect of non-noble electrocatalysts for PEM fuel cell reactions, especially focusing on cathode electrocatalyst preparation methods. Various effective synthesis methods for two kinds of promising catalysts such as transition metal chalcogenides, and heat-treated nitrogen containing complexes (macrocyclic complexes) are discussed. Though some remarkable progress has been made in catalyst preparation techniques, neither of these catalysts has reached the level of a Pt based catalyst in terms of catalytic activity, durability and chemical/electrochemical stability. In order to make non-noble electrocatalysts commercially feasible, cost-effective and innovative, catalyst synthesis methods are needed for new catalyst discovery and catalyst performance optimization.",
            "저자": "Lei Zhang, Jiujun Zhang, David P Wilkinson, Haijiang Wang",
            "전체 인용횟수": "632회 인용2006200720082009201020112012201320142015201620172018201920202021202220235334043664965595545283327262212166",
            "출처": "Journal of Power Sources",
            "페이지": "171-182",
            "학술 문서": "Progress in preparation of non-noble electrocatalysts for PEM fuel cell reactionsL Zhang, J Zhang, DP Wilkinson, H Wang - Journal of Power Sources, 2006632회 인용 관련 학술자료 전체 8개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Progress in preparation of non-noble electrocatalysts for PEM fuel cell reactions",
        "year": null
    },
    "Nano-architecture and material designs for water splitting photoelectrodes": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012",
            "게시자": "Royal Society of Chemistry",
            "권": "41",
            "설명": "This review concerns the efficient conversion of sunlight into chemical fuels through the photoelectrochemical splitting of water, which has the potential to generate sustainable hydrogen fuel. In this review, we discuss various photoelectrode materials and relative design strategies with their associated fabrication for solar water splitting. Factors affecting photoelectrochemical performance of these materials and designs are also described. The most recent progress in the research and development of new materials as well as their corresponding photoelectrodes is also summarized in this review. Finally, the research strategies and future directions for water splitting are discussed with recommendations to facilitate the further exploration of new photoelectrode materials and their associated technologies.",
            "저자": "Hao Ming Chen, Chih Kai Chen, Ru-Shi Liu, Lei Zhang, Jiujun Zhang, David P Wilkinson",
            "전체 인용횟수": "533회 인용201320142015201620172018201920202021202220233855616762514855402526",
            "출처": "Chemical Society Reviews",
            "페이지": "5654-5671",
            "학술 문서": "Nano-architecture and material designs for water splitting photoelectrodesHM Chen, CK Chen, RS Liu, L Zhang, J Zhang… - Chemical Society Reviews, 2012533회 인용 관련 학술자료 전체 10개의 버전 ",
            "호": "17"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Nano-architecture and material designs for water splitting photoelectrodes",
        "year": null
    },
    "A review of heat-treatment effects on activity and stability of PEM fuel cell catalysts for oxygen reduction reaction": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2007/11/15",
            "게시자": "Elsevier",
            "권": "173",
            "설명": "This paper reviews over 120 papers regarding the effect of heat treatment on the catalytic activity and stability of proton exchange membrane (PEM) fuel cell catalysts. These catalysts include primarily unsupported and carbon-supported platinum (Pt), Pt alloys, non-Pt alloys, and transition metal macrocycles. The heat treatment can induce changes in catalyst properties such as particle size, morphology, dispersion of the metal on the support, alloying degree, active site formation, catalytic activity, and catalytic stability. The optimum heat-treatment temperature and time period are strongly dependent on the individual catalyst. With respect to Pt-based catalysts, heat treatment can induce particle-size growth, better alloying degree, and changes in the catalyst surface morphology from amorphous to more ordered states, all of which have a remarkable effect on oxygen reduction reaction (ORR) activity and stability …",
            "저자": "Cicero WB Bezerra, Lei Zhang, Hansan Liu, Kunchan Lee, Aldaléa LB Marques, Edmar P Marques, Haijiang Wang, Jiujun Zhang",
            "전체 인용횟수": "508회 인용200820092010201120122013201420152016201720182019202020212022202321294135444339353438392023252218",
            "출처": "Journal of Power Sources",
            "페이지": "891-908",
            "학술 문서": "A review of heat-treatment effects on activity and stability of PEM fuel cell catalysts for oxygen reduction reactionCWB Bezerra, L Zhang, H Liu, K Lee, ALB Marques… - Journal of Power Sources, 2007508회 인용 관련 학술자료 전체 9개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A review of heat-treatment effects on activity and stability of PEM fuel cell catalysts for oxygen reduction reaction",
        "year": null
    },
    "Nanocrystalline intermetallics on mesoporous carbon for direct formic acid fuel cell anodes": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/4",
            "게시자": "Nature Publishing Group UK",
            "권": "2",
            "설명": "Shape- and size-controlled supported metal and intermetallic nanocrystallites are of increasing interest because of their catalytic and electrocatalytic properties. In particular, intermetallics PtX (X = Bi, Pb, Pd, Ru) are very attractive because of their high activity as fuel-cell anode catalysts for formic acid or methanol oxidation. These are normally synthesized using high-temperature techniques, but rigorous size control is very challenging. Even low-temperature techniques typically produce nanoparticles with dimensions much greater than the optimum <6 nm required for fuel cell catalysis. Here, we present a simple and robust, chemically controlled process for synthesizing size-controlled noble metal or bimetallic nanocrystallites embedded within the porous structure of ordered mesoporous carbon (OMC). By using surface-modified ordered mesoporous carbon to trap the metal precursors, nanocrystallites are formed …",
            "저널": "Nature chemistry",
            "저자": "Xiulei Ji, Kyu Tae Lee, Reanne Holden, Lei Zhang, Jiujun Zhang, Gianluigi A Botton, Martin Couillard, Linda F Nazar",
            "전체 인용횟수": "504회 인용20102011201220132014201520162017201820192020202120222023725383748503551374949312620",
            "페이지": "286-293",
            "학술 문서": "Nanocrystalline intermetallics on mesoporous carbon for direct formic acid fuel cell anodesX Ji, KT Lee, R Holden, L Zhang, J Zhang, GA Botton… - Nature chemistry, 2010504회 인용 관련 학술자료 전체 13개의 버전 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Nanocrystalline intermetallics on mesoporous carbon for direct formic acid fuel cell anodes",
        "year": null
    },
    "A review of cathode materials and structures for rechargeable lithium–air batteries": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "게시자": "Royal Society of Chemistry",
            "권": "8",
            "설명": "Rechargeable lithium air (Li–air) batteries, especially the non-aqueous type, are considered the most promising energy storage and conversion device candidates for use in future electric vehicle applications due to their ultrahigh energy density. The air cathode has been identified as a key factor affecting the overall performance of Li–air batteries. The current low level performance of air cathodes is the major challenge hindering commercial applications of Li–air batteries. In the past decade, a great many cathode materials, structures and fabrication processes have been developed and investigated with the goal of enhancing cathode performance. This paper reviews, the role of the cathode in non-aqueous Li–air batteries including the cathode reaction mechanisms and the properties and morphologies of cathode materials, followed by approaches to optimize cathode performance. The most recently published …",
            "저자": "Zhong Ma, Xianxia Yuan, Lin Li, Zi-Feng Ma, David P Wilkinson, Lei Zhang, Jiujun Zhang",
            "전체 인용횟수": "479회 인용2014201520162017201820192020202120222023254571586776585044",
            "출처": "Energy & Environmental Science",
            "페이지": "2144-2198",
            "학술 문서": "A review of cathode materials and structures for rechargeable lithium–air batteriesZ Ma, X Yuan, L Li, ZF Ma, DP Wilkinson, L Zhang… - Energy & Environmental Science, 2015479회 인용 관련 학술자료 전체 3개의 버전 ",
            "호": "8"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A review of cathode materials and structures for rechargeable lithium–air batteries",
        "year": null
    },
    "Degradation mechanisms and mitigation strategies of nickel-rich NMC-based lithium-ion batteries": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020/3",
            "게시자": "Springer Singapore",
            "권": "3",
            "설명": " Abstract The demand for lithium-ion batteries (LIBs) with high mass-specific capacities, high rate capabilities and long-term cyclabilities is driving the research and development of LIBs with nickel-rich NMC (LiNixMnyCo1−x−yO2, ) cathodes and graphite (LixC6) anodes. Based on this, this review will summarize recently reported and widely recognized studies of the degradation mechanisms of Ni-rich NMC cathodes and graphite anodes. And with a broad collection of proposed mechanisms on both atomic and micrometer scales, this review can supplement previous degradation studies of Ni-rich NMC batteries. In addition, this review will categorize advanced mitigation strategies for both electrodes based on different modifications in which Ni-rich NMC cathode improvement strategies involve dopants, gradient layers, surface coatings, carbon matrixes and advanced synthesis methods …",
            "저자": "Tianyu Li, Xiao-Zi Yuan, Lei Zhang, Datong Song, Kaiyuan Shi, Christina Bock",
            "전체 인용횟수": "434회 인용20202021202220234597153137",
            "출처": "Electrochemical Energy Reviews",
            "페이지": "43-80",
            "학술 문서": "Degradation mechanisms and mitigation strategies of nickel-rich NMC-based lithium-ion batteriesT Li, XZ Yuan, L Zhang, D Song, K Shi, C Bock - Electrochemical Energy Reviews, 2020434회 인용 관련 학술자료 전체 5개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Degradation mechanisms and mitigation strategies of nickel-rich NMC-based lithium-ion batteries",
        "year": null
    },
    "Atomic cobalt as an efficient electrocatalyst in sulfur cathodes for superior room-temperature sodium-sulfur batteries": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/10/4",
            "게시자": "Nature Publishing Group UK",
            "권": "9",
            "설명": "The low-cost room-temperature sodium-sulfur battery system is arousing extensive interest owing to its promise for large-scale applications. Although significant efforts have been made, resolving low sulfur reaction activity and severe polysulfide dissolution remains challenging. Here, a sulfur host comprised of atomic cobalt-decorated hollow carbon nanospheres is synthesized to enhance sulfur reactivity and to electrocatalytically reduce polysulfide into the final product, sodium sulfide. The constructed sulfur cathode delivers an initial reversible capacity of 1081 mA h g−1 with 64.7% sulfur utilization rate; significantly, the cell retained a high reversible capacity of 508 mA h g−1 at 100 mA g−1 after 600 cycles. An excellent rate capability is achieved with an average capacity of 220.3 mA h g−1 at the high current density of 5 A g−1. Moreover, the electrocatalytic effects of atomic cobalt are clearly …",
            "저널": "Nature communications",
            "저자": "Bin-Wei Zhang, Tian Sheng, Yun-Dan Liu, Yun-Xiao Wang, Lei Zhang, Wei-Hong Lai, Li Wang, Jianping Yang, Qin-Fen Gu, Shu-Lei Chou, Hua-Kun Liu, Shi-Xue Dou",
            "전체 인용횟수": "329회 인용201920202021202220233978816858",
            "페이지": "4082",
            "학술 문서": "Atomic cobalt as an efficient electrocatalyst in sulfur cathodes for superior room-temperature sodium-sulfur batteriesBW Zhang, T Sheng, YD Liu, YX Wang, L Zhang… - Nature communications, 2018329회 인용 관련 학술자료 전체 15개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Atomic cobalt as an efficient electrocatalyst in sulfur cathodes for superior room-temperature sodium-sulfur batteries",
        "year": null
    },
    "Oxygen reduction reaction (ORR) catalyzed by carbon-supported cobalt polypyrrole (Co-PPy/C) electrocatalysts": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/8/1",
            "게시자": "Pergamon",
            "권": "54",
            "설명": "This paper reports the experimental characterization of carbon-supported cobalt polypyrrole (Co-PPy/C) catalysts synthesized using a chemical method of polymerization synthesis. Both unpyrolyzed and pyrolyzed catalysts were characterized using electrochemical techniques such as cyclic voltammetry (CV), rotating disk electrode (RDE), as well as rotating ring disk electrode (RRDE) to quantitatively obtain the oxygen reduction reaction (ORR) kinetic constants and the reaction mechanisms. The pyrolyzed catalyst showed significantly improved ORR activity as well as different ORR mechanisms, suggesting that heat-treatment is a necessary step for catalyst activity improvement. To understand the heat-treatment effect, X-ray photoelectron spectroscopy (XPS) was used to detect surface structure changes. The XPS results showed that after the sample was heat-treated, new nitrogen peaks corresponding to pyrrolic …",
            "저널": "Electrochimica Acta",
            "저자": "Kunchan Lee, Lei Zhang, Hansan Lui, Rob Hui, Zheng Shi, Jiujun Zhang",
            "전체 인용횟수": "326회 인용200920102011201220132014201520162017201820192020202120222023221212144444529231572314124",
            "페이지": "4704-4711",
            "학술 문서": "Oxygen reduction reaction (ORR) catalyzed by carbon-supported cobalt polypyrrole (Co-PPy/C) electrocatalystsK Lee, L Zhang, H Lui, R Hui, Z Shi, J Zhang - Electrochimica Acta, 2009326회 인용 관련 학술자료 전체 7개의 버전 ",
            "호": "20"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Oxygen reduction reaction (ORR) catalyzed by carbon-supported cobalt polypyrrole (Co-PPy/C) electrocatalysts",
        "year": null
    },
    "Recent progress in advanced electrode materials, separators and electrolytes for lithium batteries": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "게시자": "Royal Society of Chemistry",
            "권": "6",
            "설명": "Lithium-ion batteries (LIBs) possess several advantages over other types of viable practical batteries, including higher operating voltages, higher energy densities, longer cycle lives, lower rates of self-discharge and less environmental pollution. Therefore, LIBs have been widely and successfully applied in portable electronic devices and industrial fields. However, the rapidly increasing demands of new energy vehicles have also quickly increased the performance requirements of LIBs, including the need for higher power densities, greater capacity densities and better safety. As battery designs gradually standardize, improvements in LIB performances mainly depend on the technical progress in key electrode materials such as positive and negative electrode materials, separators and electrolytes. For LIB performances to meet the rising requirements, many studies on the structural characteristics and morphology …",
            "저자": "Hailin Zhang, Hongbin Zhao, Muhammad Arif Khan, Wenwen Zou, Jiaqiang Xu, Lei Zhang, Jiujun Zhang",
            "전체 인용횟수": "310회 인용201920202021202220233577845954",
            "출처": "Journal of Materials Chemistry A",
            "페이지": "20564-20620",
            "학술 문서": "Recent progress in advanced electrode materials, separators and electrolytes for lithium batteriesH Zhang, H Zhao, MA Khan, W Zou, J Xu, L Zhang… - Journal of Materials Chemistry A, 2018310회 인용 관련 학술자료 전체 4개의 버전 ",
            "호": "42"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Recent progress in advanced electrode materials, separators and electrolytes for lithium batteries",
        "year": null
    },
    "Recent progresses in electrocatalysts for water electrolysis": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/12",
            "게시자": "Springer Singapore",
            "권": "1",
            "설명": " Abstract The study of hydrogen evolution reaction and oxygen evolution reaction electrocatalysts for water electrolysis is a developing field in which noble metal-based materials are commonly used. However, the associated high cost and low abundance of noble metals limit their practical application. Non-noble metal catalysts, aside from being inexpensive, highly abundant and environmental friendly, can possess high electrical conductivity, good structural tunability and comparable electrocatalytic performances to state-of-the-art noble metals, particularly in alkaline media, making them desirable candidates to reduce or replace noble metals as promising electrocatalysts for water electrolysis. This article will review and provide an overview of the fundamental knowledge related to water electrolysis with a focus on the development and progress of non-noble metal-based electrocatalysts in …",
            "저자": "Muhammad Arif Khan, Hongbin Zhao, Wenwen Zou, Zhe Chen, Wenjuan Cao, Jianhui Fang, Jiaqiang Xu, Lei Zhang, Jiujun Zhang",
            "전체 인용횟수": "305회 인용201920202021202220232851568780",
            "출처": "Electrochemical Energy Reviews",
            "페이지": "483-530",
            "학술 문서": "Recent progresses in electrocatalysts for water electrolysisMA Khan, H Zhao, W Zou, Z Chen, W Cao, J Fang, J Xu… - Electrochemical Energy Reviews, 2018305회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Recent progresses in electrocatalysts for water electrolysis",
        "year": null
    },
    "Energy related CO2 conversion and utilization: advanced materials/nanomaterials, reaction mechanisms and technologies": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/10/1",
            "게시자": "Elsevier",
            "권": "40",
            "설명": "CO2 conversion to produce useful fuels/chemicals is a promising route for reducing CO2 emission as well as for exploring the promising energy storage method. To facilitate the research and development of CO2 conversion, this paper provides a comprehensive overview of CO2 conversions using advanced materials/nanomaterials and technologies for the production of useful fuels/chemicals. The molecular structure, thermodynamics and kinetics of CO2 are reviewed for the understanding of fundamentals and explaining why C=O double bonds are difficult to break. The mechanisms and features of various conversion technologies are summarized and classified into enzymatic, mineralization, photochemical/photo-electrochemical, thermochemical as well as electrochemical processes. In particular, by comparing electrochemical conversion technologies at low and high temperatures, CO2 conversion at …",
            "저자": "Yun Zheng, Wenqiang Zhang, Yifeng Li, Jing Chen, Bo Yu, Jianchen Wang, Lei Zhang, Jiujun Zhang",
            "전체 인용횟수": "248회 인용201820192020202120222023282743475844",
            "출처": "Nano Energy",
            "페이지": "512-539",
            "학술 문서": "Energy related CO2 conversion and utilization: advanced materials/nanomaterials, reaction mechanisms and technologiesY Zheng, W Zhang, Y Li, J Chen, B Yu, J Wang… - Nano Energy, 2017248회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Energy related CO2 conversion and utilization: advanced materials/nanomaterials, reaction mechanisms and technologies",
        "year": null
    },
    "Effects of electrode layer composition/thickness and electrolyte concentration on both specific capacitance and energy density of supercapacitor": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/1/15",
            "게시자": "Pergamon",
            "권": "60",
            "설명": "In this paper, the effects of several experimental conditions, such as electrode layer binder content, conducting carbon content, electrode layer thickness, as well as electrolyte concentration, on both the specific capacitance and energy density of a BP2000 carbon-based supercapacitor are investigated using both cyclic voltammetry and a galvanic charging–discharging curve. The electrode layer studied contains Super C45 carbon as the conducting additive, PTFE as the binder, and Na2SO4 as the aqueous electrolyte, respectively. With the purpose of optimizing the electrode layer structure, 15wt% of Super C45 and 5wt% of PTFE in the electrode layer with a thickness of 100μm, are found to be the best composition in terms of improving both specific capacitance and energy density. Regarding the effect of electrolyte concentration in the range of 0.1–1.0M, 0.5M of Na2SO4 gives the best performance.",
            "저널": "Electrochimica Acta",
            "저자": "Keh-Chyun Tsay, Lei Zhang, Jiujun Zhang",
            "전체 인용횟수": "238회 인용2012201320142015201620172018201920202021202220231137161923223021303025",
            "페이지": "428-436",
            "학술 문서": "Effects of electrode layer composition/thickness and electrolyte concentration on both specific capacitance and energy density of supercapacitorKC Tsay, L Zhang, J Zhang - Electrochimica Acta, 2012238회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Effects of electrode layer composition/thickness and electrolyte concentration on both specific capacitance and energy density of supercapacitor",
        "year": null
    },
    "The effect of heat treatment on nanoparticle size and ORR activity for carbon-supported Pd–Co alloy electrocatalysts": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2007/2/15",
            "게시자": "Pergamon",
            "권": "52",
            "설명": "Carbon-supported Pd–Co alloy electrocatalysts were synthesized and characterized for the purpose of the fuel cell cathode oxygen reduction reaction (ORR). An impregnation method was employed for the synthesis, in which sodium borohydride was used as a reducing agent. The synthesized catalysts were characterized in terms of structural morphology and catalytic activity by XRD, XPS and electrochemical measurements. Surface cyclic voltammetry was used to confirm the formation of the Pd–Co alloy. In order to improve activity and stability, the catalysts were heat-treated in the temperature range of 300°C to 700°C. The optimal heat-treatment temperature was found to be 300°C, where the average particle size of 8.9nm, and the highest ORR catalytic activity, were obtained. The catalyzed ORR kinetics were also studied using the rotating disk electrode (RDE) method. The kinetic parameters were then obtained …",
            "저널": "Electrochimica Acta",
            "저자": "Lei Zhang, Kunchan Lee, Jiujun Zhang",
            "전체 인용횟수": "232회 인용200720082009201020112012201320142015201620172018201920202021202220231016132319182115131218137101238",
            "페이지": "3088-3094",
            "학술 문서": "The effect of heat treatment on nanoparticle size and ORR activity for carbon-supported Pd–Co alloy electrocatalystsL Zhang, K Lee, J Zhang - Electrochimica Acta, 2007232회 인용 관련 학술자료 전체 7개의 버전 ",
            "호": "9"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The effect of heat treatment on nanoparticle size and ORR activity for carbon-supported Pd–Co alloy electrocatalysts",
        "year": null
    },
    "Non-noble metal electrocatalysts for the hydrogen evolution reaction in water electrolysis": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2021/9",
            "게시자": "Springer Singapore",
            "권": "4",
            "설명": "Water electrolysis is a sustainable approach for hydrogen production by using electricity from clean energy sources. However, both the hydrogen evolution reaction (HER) and the oxygen evolution reaction (OER) associated with water electrolysis are kinetically sluggish, leading to low efficiency in corresponding electrolysis devices. In addition, current electrocatalysts that can catalyze both HER and OER to practical rates require noble metals such as platinum that are low in abundance and high in price, severely limiting commercialization. As a result, the development of high-performance and cost-effective non-noble metal electrocatalysts to replace noble ones has intensified. Based on this, this review will comprehensively present recent research in the design, synthesis, characterization and performance validation/optimization of non-noble metal HER electrocatalysts and analyze corresponding catalytic …",
            "저자": "Huimin Wu, Chuanqi Feng, Lei Zhang, Jiujun Zhang, David P Wilkinson",
            "전체 인용횟수": "198회 인용202120222023882104",
            "출처": "Electrochemical Energy Reviews",
            "페이지": "473-507",
            "학술 문서": "Non-noble metal electrocatalysts for the hydrogen evolution reaction in water electrolysisH Wu, C Feng, L Zhang, J Zhang, DP Wilkinson - Electrochemical Energy Reviews, 2021198회 인용 관련 학술자료 전체 4개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Non-noble metal electrocatalysts for the hydrogen evolution reaction in water electrolysis",
        "year": null
    },
    "Densely connected convolutional networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections--one between each layer and its subsequent layer--our network has L (L+ 1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github. com/liuzhuang13/DenseNet.",
            "저자": "Gao Huang, Zhuang Liu, Laurens Van Der Maaten, Kilian Q Weinberger",
            "전체 인용횟수": "40071회 인용2017201820192020202120222023383224444646529838395048231",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "4700-4708",
            "학술 문서": "Densely connected convolutional networksG Huang, Z Liu, L Van Der Maaten, KQ Weinberger - Proceedings of the IEEE conference on computer …, 201740071회 인용 관련 학술자료 전체 37개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Densely connected convolutional networks",
        "year": null
    },
    "Distance metric learning for large margin nearest neighbor classification.": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/2/1",
            "권": "10",
            "설명": "The accuracy of k-nearest neighbor (kNN) classification depends significantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Mahalanobis distance metric for kNN classification from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classification using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. As in support vector machines (SVMs), the margin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach requires no modification or extension for problems in multiway (as opposed to binary) classification. In our framework, the Mahalanobis distance metric is obtained as the solution to a semidefinite program. On several data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner.",
            "저널": "Journal of machine learning research",
            "저자": "Kilian Q Weinberger, Lawrence K Saul",
            "전체 인용횟수": "4679회 인용2008200920102011201220132014201520162017201820192020202120222023132875117189198263290330411466520489484454272",
            "학술 문서": "Distance metric learning for large margin nearest neighbor classification.KQ Weinberger, LK Saul - Journal of machine learning research, 20094679회 인용 관련 학술자료 전체 19개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Distance metric learning for large margin nearest neighbor classification.",
        "year": null
    },
    "On calibration of modern neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/7/17",
            "게시자": "PMLR",
            "설명": "Confidence calibration–the problem of predicting probability estimates representative of the true correctness likelihood–is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling–a single-parameter variant of Platt Scaling–is surprisingly effective at calibrating predictions.",
            "저자": "Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q Weinberger",
            "전체 인용횟수": "4652회 인용201720182019202020212022202314116286620102612161355",
            "컨퍼런스": "International conference on machine learning",
            "페이지": "1321-1330",
            "학술 문서": "On calibration of modern neural networksC Guo, G Pleiss, Y Sun, KQ Weinberger - International conference on machine learning, 20174652회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "On calibration of modern neural networks",
        "year": null
    },
    "Bertscore: Evaluating text generation with bert": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/4/21",
            "설명": "We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTScore is more robust to challenging examples when compared to existing metrics.",
            "저널": "arXiv preprint arXiv:1904.09675",
            "저자": "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi",
            "전체 인용횟수": "2801회 인용20192020202120222023352234958821152",
            "학술 문서": "Bertscore: Evaluating text generation with bertT Zhang, V Kishore, F Wu, KQ Weinberger, Y Artzi - arXiv preprint arXiv:1904.09675, 20192801회 인용 관련 학술자료 전체 5개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Bertscore: Evaluating text generation with bert",
        "year": null
    },
    "From word embeddings to document distances": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/6/1",
            "게시자": "PMLR",
            "설명": "We present the Word Mover’s Distance (WMD), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representations for words from local co-occurrences in sentences. The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to\" travel\" to reach the embedded words of another document. We show that this distance metric can be cast as an instance of the Earth Mover’s Distance, a well studied transportation problem for which several highly efficient solvers have been developed. Our metric has no hyperparameters and is straight-forward to implement. Further, we demonstrate on eight real world document classification data sets, in comparison with seven state-of-the-art baselines, that the WMD metric leads to unprecedented low k-nearest neighbor document classification error rates.",
            "저자": "Matt Kusner, Yu Sun, Nicholas Kolkin, Kilian Weinberger",
            "전체 인용횟수": "2557회 인용201520162017201820192020202120222023981161316419434416400301",
            "컨퍼런스": "International conference on machine learning",
            "페이지": "957-966",
            "학술 문서": "From word embeddings to document distancesM Kusner, Y Sun, N Kolkin, K Weinberger - International conference on machine learning, 20152557회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "From word embeddings to document distances",
        "year": null
    },
    "Simplifying graph convolutional networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/5/24",
            "게시자": "PMLR",
            "설명": "Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complexity and redundant computation. In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers. We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications. Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN.",
            "저자": "Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, Kilian Weinberger",
            "전체 인용횟수": "2530회 인용2019202020212022202360241532802877",
            "컨퍼런스": "International conference on machine learning",
            "페이지": "6861-6871",
            "학술 문서": "Simplifying graph convolutional networksF Wu, A Souza, T Zhang, C Fifty, T Yu, K Weinberger - International conference on machine learning, 20192530회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Simplifying graph convolutional networks",
        "year": null
    },
    "Distance metric learning for large margin nearest neighbor classification": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2005",
            "권": "18",
            "설명": "We show how to learn a Mahanalobis distance metric for k-nearest neighbor (kNN) classification by semidefinite programming. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. On seven data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification--for example, achieving a test error rate of 1.3% on the MNIST handwritten digits. As in support vector machines (SVMs), the learning problem reduces to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our framework requires no modification or extension for problems in multiway (as opposed to binary) classification.",
            "저널": "Advances in neural information processing systems",
            "저자": "Kilian Q Weinberger, John Blitzer, Lawrence Saul",
            "전체 인용횟수": "2445회 인용2006200720082009201020112012201320142015201620172018201920202021202220232057801061018611512618221222120220518819413412764",
            "학술 문서": "Distance metric learning for large margin nearest neighbor classificationKQ Weinberger, J Blitzer, L Saul - Advances in neural information processing systems, 20052445회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Distance metric learning for large margin nearest neighbor classification",
        "year": null
    },
    "Deep networks with stochastic depth": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "게시자": "Springer International Publishing",
            "설명": " Very deep convolutional networks with hundreds of layers have led to significant reductions in error on competitive benchmarks. Although the unmatched expressiveness of the many layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes, and the training time can be painfully slow. To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. This simple approach complements the recent success of residual networks. It reduces training time substantially and improves the test error significantly on almost all data sets that we …",
            "저자": "Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, Kilian Q Weinberger",
            "전체 인용횟수": "2376회 인용2016201720182019202020212022202362150241286306390475433",
            "컨퍼런스": "Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14",
            "페이지": "646-661",
            "학술 문서": "Deep networks with stochastic depthG Huang, Y Sun, Z Liu, D Sedra, KQ Weinberger - Computer Vision–ECCV 2016: 14th European …, 20162376회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep networks with stochastic depth",
        "year": null
    },
    "Advances in neural information processing systems": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012",
            "저널": "(No Title)",
            "저자": "Alex Krizhevsky",
            "전체 인용횟수": "1718회 인용2014201520162017201820192020202120222023131347116191292360293239146",
            "페이지": "1097",
            "학술 문서": "Advances in neural information processing systemsA Krizhevsky - (No Title), 20121718회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Advances in neural information processing systems",
        "year": null
    },
    "Compressing neural networks with the hashing trick": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/6/1",
            "게시자": "PMLR",
            "설명": "As deep nets are increasingly used in applications suited for mobile devices, a fundamental dilemma becomes apparent: the trend in deep learning is to grow models to absorb ever-increasing data set sizes; however mobile devices are designed with very little memory and cannot store such large models. We present a novel network architecture, HashedNets, that exploits inherent redundancy in neural networks to achieve drastic reductions in model sizes. HashedNets uses a low-cost hash function to randomly group connection weights into hash buckets, and all connections within the same hash bucket share a single parameter value. These parameters are tuned to adjust to the HashedNets weight sharing architecture with standard backprop during training. Our hashing procedure introduces no additional memory overhead, and we demonstrate on several benchmark data sets that HashedNets shrink the storage requirements of neural networks substantially while mostly preserving generalization performance.",
            "저자": "Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, Yixin Chen",
            "전체 인용횟수": "1354회 인용2015201620172018201920202021202220231872124237212233158157131",
            "컨퍼런스": "International conference on machine learning",
            "페이지": "2285-2294",
            "학술 문서": "Compressing neural networks with the hashing trickW Chen, J Wilson, S Tyree, K Weinberger, Y Chen - International conference on machine learning, 20151354회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Compressing neural networks with the hashing trick",
        "year": null
    },
    "Unsupervised learning of image manifolds by semidefinite programming": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004",
            "게시자": "Kluwer Academic Publishers",
            "권": "2",
            "설명": " Can we detect low dimensional structure in high dimensional data sets of images? In this paper, we propose an algorithm for unsupervised learning of image manifolds by semidefinite programming. Given a data set of images, our algorithm computes a low dimensional representation of each image with the property that distances between nearby images are preserved. More generally, it can be used to analyze high dimensional data that lies on or near a low dimensional manifold. We illustrate the algorithm on easily visualized examples of curves and surfaces, as well as on actual images of faces, handwritten digits, and solid objects.",
            "저자": "Kilian Q Weinberger, Lawrence K Saul",
            "전체 인용횟수": "1208회 인용200520062007200820092010201120122013201420152016201720182019202020212022202344426958666980859685848360645549433028",
            "컨퍼런스": "CVPR",
            "페이지": "988-995",
            "학술 문서": "Unsupervised learning of image manifolds by semidefinite programmingKQ Weinberger, LK Saul - International journal of computer vision, 20061208회 인용 관련 학술자료 전체 34개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Unsupervised learning of image manifolds by semidefinite programming",
        "year": null
    },
    "Feature hashing for large scale multitask learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/6/14",
            "도서": "Proceedings of the 26th annual international conference on machine learning",
            "설명": "Empirical evidence suggests that hashing is an effective strategy for dimensionality reduction and practical nonparametric estimation. In this paper we provide exponential tail bounds for feature hashing and show that the interaction between random subspaces is negligible with high probability. We demonstrate the feasibility of this approach with experimental results for a new use case --- multitask learning with hundreds of thousands of tasks.",
            "저자": "Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, Josh Attenberg",
            "전체 인용횟수": "1144회 인용200920102011201220132014201520162017201820192020202120222023101732306867768910912210910211910674",
            "페이지": "1113-1120",
            "학술 문서": "Feature hashing for large scale multitask learningK Weinberger, A Dasgupta, J Langford, A Smola… - Proceedings of the 26th annual international …, 20091144회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Feature hashing for large scale multitask learning",
        "year": null
    },
    "Marginalized denoising autoencoders for domain adaptation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/6/18",
            "설명": "Stacked denoising autoencoders (SDAs) have been successfully used to learn new representations for domain adaptation. Recently, they have attained record accuracy on standard benchmark tasks of sentiment analysis across different text domains. SDAs learn robust data representations by reconstruction, recovering original features from data that are artificially corrupted with noise. In this paper, we propose marginalized SDA (mSDA) that addresses two crucial limitations of SDAs: high computational cost and lack of scalability to high-dimensional features. In contrast to SDAs, our approach of mSDA marginalizes noise and thus does not require stochastic gradient descent or other optimization algorithms to learn parameters ? in fact, they are computed in closed-form. Consequently, mSDA, which can be implemented in only 20 lines of MATLAB^{TM}, significantly speeds up SDAs by two orders of magnitude. Furthermore, the representations learnt by mSDA are as effective as the traditional SDAs, attaining almost identical accuracies in benchmark tasks.",
            "저널": "arXiv preprint arXiv:1206.4683",
            "저자": "Minmin Chen, Zhixiang Xu, Kilian Weinberger, Fei Sha",
            "전체 인용횟수": "973회 인용2012201320142015201620172018201920202021202220233222868681091301351211209660",
            "학술 문서": "Marginalized denoising autoencoders for domain adaptationM Chen, Z Xu, K Weinberger, F Sha - arXiv preprint arXiv:1206.4683, 2012973회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Marginalized denoising autoencoders for domain adaptation",
        "year": null
    },
    "Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "설명": "3D object detection is an essential task in autonomous driving. Recent techniques excel with highly accurate detection rates, provided the 3D input data is obtained from precise but expensive LiDAR technology. Approaches based on cheaper monocular or stereo imagery data have, until now, resulted in drastically lower accuracies---a gap that is commonly attributed to poor image-based depth estimation. However, in this paper we argue that it is not the quality of the data but its representation that accounts for the majority of the difference. Taking the inner workings of convolutional neural networks into consideration, we propose to convert image-based depth maps to pseudo-LiDAR representations---essentially mimicking the LiDAR signal. With this representation we can apply different existing LiDAR-based detection algorithms. On the popular KITTI benchmark, our approach achieves impressive improvements over the existing state-of-the-art in image-based performance---raising the detection accuracy of objects within the 30m range from the previous state-of-the-art of 22% to an unprecedented 74%. At the time of submission our algorithm holds the highest entry on the KITTI 3D object detection leaderboard for stereo-image-based approaches.",
            "저자": "Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark Campbell, Kilian Q Weinberger",
            "전체 인용횟수": "963회 인용2019202020212022202341152241272253",
            "컨퍼런스": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "페이지": "8445-8453",
            "학술 문서": "Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous drivingY Wang, WL Chao, D Garg, B Hariharan, M Campbell… - Proceedings of the IEEE/CVF Conference on Computer …, 2019963회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving",
        "year": null
    },
    "Snapshot ensembles: Train 1, get m for free": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/4/1",
            "설명": "Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.",
            "저널": "arXiv preprint arXiv:1704.00109",
            "저자": "Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, Kilian Q Weinberger",
            "전체 인용횟수": "933회 인용20172018201920202021202220232675111159176206176",
            "학술 문서": "Snapshot ensembles: Train 1, get m for freeG Huang, Y Li, G Pleiss, Z Liu, JE Hopcroft… - arXiv preprint arXiv:1704.00109, 2017933회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Snapshot ensembles: Train 1, get m for free",
        "year": null
    },
    "Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "권": "31",
            "설명": "Despite advances in scalable models, the inference tools used for Gaussian processes (GPs) have yet to fully capitalize on developments in computing hardware. We present an efficient and general approach to GP inference based on Blackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified batched version of the conjugate gradients algorithm to derive all terms for training and inference in a single call. BBMM reduces the asymptotic complexity of exact GP inference from O (n^ 3) to O (n^ 2). Adapting this algorithm to scalable approximations and complex GP models simply requires a routine for efficient matrix-matrix multiplication with the kernel and its derivative. In addition, BBMM uses a specialized preconditioner to substantially speed up convergence. In experiments we show that BBMM effectively uses GPU hardware to dramatically accelerate both exact GP inference and scalable approximations. Additionally, we provide GPyTorch, a software platform for scalable GP inference via BBMM, built on PyTorch.",
            "저널": "Advances in neural information processing systems",
            "저자": "Jacob Gardner, Geoff Pleiss, Kilian Q Weinberger, David Bindel, Andrew G Wilson",
            "전체 인용횟수": "927회 인용201820192020202120222023348104201282284",
            "학술 문서": "Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu accelerationJ Gardner, G Pleiss, KQ Weinberger, D Bindel… - Advances in neural information processing systems, 2018927회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration",
        "year": null
    },
    "On fairness and calibration": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "권": "30",
            "설명": "The machine learning community has become increasingly concerned with the potential for bias and discrimination in predictive models. This has motivated a growing line of work on what it means for a classification procedure to be\" fair.\" In this paper, we investigate the tension between minimizing error disparity across different population groups while maintaining calibrated probability estimates. We show that calibration is compatible only with a single error constraint (ie equal false-negatives rates across groups), and show that any algorithm that satisfies this relaxation is no better than randomizing a percentage of predictions for an existing classifier. These unsettling findings, which extend and generalize existing results, are empirically confirmed on several datasets.",
            "저널": "Advances in neural information processing systems",
            "저자": "Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, Kilian Q Weinberger",
            "전체 인용횟수": "839회 인용201720182019202020212022202353282124168213213",
            "학술 문서": "On fairness and calibrationG Pleiss, M Raghavan, F Wu, J Kleinberg… - Advances in neural information processing systems, 2017839회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "On fairness and calibration",
        "year": null
    },
    "Large scale GAN training for high fidelity natural image synthesis": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/9/28",
            "설명": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.",
            "저널": "arXiv preprint arXiv:1809.11096",
            "저자": "Andrew Brock, Jeff Donahue, Karen Simonyan",
            "전체 인용횟수": "4850회 인용20182019202020212022202321471878107712281145",
            "학술 문서": "Large scale GAN training for high fidelity natural image synthesisA Brock, J Donahue, K Simonyan - arXiv preprint arXiv:1809.11096, 20184850회 인용 관련 학술자료 전체 5개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Large scale GAN training for high fidelity natural image synthesis",
        "year": null
    },
    "Translating videos to natural language using deep recurrent neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014/12/15",
            "설명": "Solving the visual symbol grounding problem has long been a goal of artificial intelligence. The field appears to be advancing closer to this goal with recent breakthroughs in deep learning for natural language grounding in static images. In this paper, we propose to translate videos directly to sentences using a unified deep neural network with both convolutional and recurrent structure. Described video datasets are scarce, and most existing methods have been applied to toy domains with a small vocabulary of possible words. By transferring knowledge from 1.2M+ images with category labels and 100,000+ images with captions, our method is able to create sentence descriptions of open-domain videos with large vocabularies. We compare our approach with recent work using language generation metrics, subject, verb, and object prediction accuracy, and a human evaluation.",
            "저널": "arXiv preprint arXiv:1412.4729",
            "저자": "Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko",
            "전체 인용횟수": "1150회 인용2015201620172018201920202021202220234910713715416714512214292",
            "학술 문서": "Translating videos to natural language using deep recurrent neural networksS Venugopalan, H Xu, J Donahue, M Rohrbach… - arXiv preprint arXiv:1412.4729, 20141150회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Translating videos to natural language using deep recurrent neural networks",
        "year": null
    },
    "Flamingo: a visual language model for few-shot learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2022/12/6",
            "권": "35",
            "설명": "Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to:(i) bridge powerful pretrained vision-only and language-only models,(ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer, captioning tasks, which evaluate the ability to describe a scene or an event, and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.",
            "저널": "Advances in Neural Information Processing Systems",
            "저자": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikołaj Bińkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan",
            "전체 인용횟수": "1032회 인용20222023157871",
            "페이지": "23716-23736",
            "학술 문서": "Flamingo: a visual language model for few-shot learningJB Alayrac, J Donahue, P Luc, A Miech, I Barr… - Advances in Neural Information Processing Systems, 20221032회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Flamingo: a visual language model for few-shot learning",
        "year": null
    },
    "Population based training of neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/11/27",
            "설명": "Neural networks dominate the modern machine learning landscape, but their training and success still suffer from sensitivity to empirical choices of hyperparameters such as model architecture, loss function, and optimisation algorithm. In this work we present \\emph{Population Based Training (PBT)}, a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance. Importantly, PBT discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training. With just a small modification to a typical distributed hyperparameter training framework, our method allows robust and reliable training of models. We demonstrate the effectiveness of PBT on deep reinforcement learning problems, showing faster wall-clock convergence and higher final performance of agents by optimising over a suite of hyperparameters. In addition, we show the same method can be applied to supervised learning for machine translation, where PBT is used to maximise the BLEU score directly, and also to training of Generative Adversarial Networks to maximise the Inception score of generated images. In all cases PBT results in the automatic discovery of hyperparameter schedules and model selection which results in stable training and better final performance.",
            "저널": "arXiv preprint arXiv:1711.09846",
            "저자": "Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, Chrisantha Fernando, Koray Kavukcuoglu",
            "전체 인용횟수": "771회 인용2017201820192020202120222023658107139148152155",
            "학술 문서": "Population based training of neural networksM Jaderberg, V Dalibard, S Osindero, WM Czarnecki… - arXiv preprint arXiv:1711.09846, 2017771회 인용 관련 학술자료 전체 6개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Population based training of neural networks",
        "year": null
    },
    "Generating visual explanations": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "게시자": "Springer International Publishing",
            "설명": " Clearly explaining a rationale for a classification decision to an end user can be as important as the decision itself. Existing approaches for deep visual recognition are generally opaque and do not output any justification text; contemporary vision-language models can describe image content but fail to take into account class-discriminative image aspects which justify visual predictions. We propose a new model that focuses on the discriminating properties of the visible object, jointly predicts a class label, and explains why the predicted label is appropriate for the image. Through a novel loss function based on sampling and reinforcement learning, our model learns to generate sentences that realize a global sentence property, such as class specificity. Our results on the CUB dataset show that our model is able to generate explanations which are not only consistent with an image but also more discriminative …",
            "저자": "Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, Trevor Darrell",
            "전체 인용횟수": "675회 인용2016201720182019202020212022202344594961161279687",
            "컨퍼런스": "Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14",
            "페이지": "3-19",
            "학술 문서": "Generating visual explanationsLA Hendricks, Z Akata, M Rohrbach, J Donahue… - Computer Vision–ECCV 2016: 14th European …, 2016675회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Generating visual explanations",
        "year": null
    },
    "Large scale adversarial representation learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019",
            "권": "32",
            "설명": "Adversarially trained generative models (GANs) have recently achieved compelling image synthesis results. But despite early successes in using GANs for unsupervised representation learning, they have since been superseded by approaches based on self-supervision. In this work we show that progress in image generation quality translates to substantially improved representation learning performance. Our approach, BigBiGAN, builds upon the state-of-the-art BigGAN model, extending it to representation learning by adding an encoder and modifying the discriminator. We extensively evaluate the representation learning and generation capabilities of these BigBiGAN models, demonstrating that these generation-based models achieve the state of the art in unsupervised representation learning on ImageNet, as well as compelling results in unconditional image generation.",
            "저널": "Advances in neural information processing systems",
            "저자": "Jeff Donahue, Karen Simonyan",
            "전체 인용횟수": "519회 인용2019202020212022202317104154130114",
            "학술 문서": "Large scale adversarial representation learningJ Donahue, K Simonyan - Advances in neural information processing systems, 2019519회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Large scale adversarial representation learning",
        "year": null
    },
    "Efficient learning of domain-invariant image representations": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/1/15",
            "설명": "We present an algorithm that learns representations which explicitly compensate for domain mismatch and which can be efficiently realized as linear classifiers. Specifically, we form a linear transformation that maps features from the target (test) domain to the source (training) domain as part of training the classifier. We optimize both the transformation and classifier parameters jointly, and introduce an efficient cost function based on misclassification loss. Our method combines several features previously unavailable in a single algorithm: multi-class adaptation through representation learning, ability to map across heterogeneous feature spaces, and scalability to large datasets. We present experiments on several image datasets that demonstrate improved accuracy and computational advantages compared to previous approaches.",
            "저널": "arXiv preprint arXiv:1301.3224",
            "저자": "Judy Hoffman, Erik Rodner, Jeff Donahue, Trevor Darrell, Kate Saenko",
            "전체 인용횟수": "334회 인용201220132014201520162017201820192020202120222023162528434645324124229",
            "학술 문서": "Efficient learning of domain-invariant image representationsJ Hoffman, E Rodner, J Donahue, T Darrell, K Saenko - arXiv preprint arXiv:1301.3224, 2013334회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Efficient learning of domain-invariant image representations",
        "year": null
    },
    "Data-dependent initializations of convolutional neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/11/21",
            "설명": "Convolutional Neural Networks spread through computer vision like a wildfire, impacting almost all visual tasks imaginable. Despite this, few researchers dare to train their models from scratch. Most work builds on one of a handful of ImageNet pre-trained models, and fine-tunes or adapts these for specific tasks. This is in large part due to the difficulty of properly initializing these networks from scratch. A small miscalibration of the initial weights leads to vanishing or exploding gradients, as well as poor convergence properties. In this work we present a fast and simple data-dependent initialization procedure, that sets the weights of a network such that all units in the network train at roughly the same rate, avoiding vanishing or exploding gradients. Our initialization matches the current state-of-the-art unsupervised or self-supervised pre-training methods on standard computer vision tasks, such as image classification and object detection, while being roughly three orders of magnitude faster. When combined with pre-training methods, our initialization significantly outperforms prior work, narrowing the gap between supervised and unsupervised pre-training.",
            "저널": "arXiv preprint arXiv:1511.06856",
            "저자": "Philipp Krähenbühl, Carl Doersch, Jeff Donahue, Trevor Darrell",
            "전체 인용횟수": "233회 인용20152016201720182019202020212022202312033384134301812",
            "학술 문서": "Data-dependent initializations of convolutional neural networksP Krähenbühl, C Doersch, J Donahue, T Darrell - arXiv preprint arXiv:1511.06856, 2015233회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Data-dependent initializations of convolutional neural networks",
        "year": null
    },
    "Semi-supervised domain adaptation with instance constraints": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013",
            "설명": "Most successful object classification and detection methods rely on classifiers trained on large labeled datasets. However, for domains where labels are limited, simply borrowing labeled data from existing datasets can hurt performance, a phenomenon known as\" dataset bias.\" We propose a general framework for adapting classifiers from\" borrowed\" data to the target domain using a combination of available labeled and unlabeled examples. Specifically, we show that imposing smoothness constraints on the classifier scores over the unlabeled data can lead to improved adaptation results. Such constraints are often available in the form of instance correspondences, eg when the same object or individual is observed simultaneously from multiple views, or tracked between video frames. In these cases, the object labels are unknown but can be constrained to be the same or similar. We propose techniques that build on existing domain adaptation methods by explicitly modeling these relationships, and demonstrate empirically that they improve recognition accuracy in two scenarios, multicategory image classification and object detection in video.",
            "저자": "Jeff Donahue, Judy Hoffman, Erik Rodner, Kate Saenko, Trevor Darrell",
            "전체 인용횟수": "193회 인용20142015201620172018201920202021202220231613271612222424277",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "668-675",
            "학술 문서": "Semi-supervised domain adaptation with instance constraintsJ Donahue, J Hoffman, E Rodner, K Saenko, T Darrell - Proceedings of the IEEE conference on computer …, 2013193회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Semi-supervised domain adaptation with instance constraints",
        "year": null
    },
    "End-to-end adversarial text-to-speech": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020/6/5",
            "설명": "Modern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable alignment scheme based on token length prediction. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision.",
            "저널": "arXiv preprint arXiv:2006.03575",
            "저자": "Jeff Donahue, Sander Dieleman, Mikołaj Bińkowski, Erich Elsen, Karen Simonyan",
            "전체 인용횟수": "184회 인용20202021202220237456464",
            "학술 문서": "End-to-end adversarial text-to-speechJ Donahue, S Dieleman, M Bińkowski, E Elsen… - arXiv preprint arXiv:2006.03575, 2020184회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "End-to-end adversarial text-to-speech",
        "year": null
    },
    "RGB-D mapping: Using Kinect-style depth cameras for dense 3D modeling of indoor environments": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/4",
            "게시자": "SAGE Publications",
            "권": "31",
            "설명": "RGB-D cameras (such as the Microsoft Kinect) are novel sensing systems that capture RGB images along with per-pixel depth information. In this paper we investigate how such cameras can be used for building dense 3D maps of indoor environments. Such maps have applications in robot navigation, manipulation, semantic mapping, and telepresence. We present RGB-D Mapping, a full 3D mapping system that utilizes a novel joint optimization algorithm combining visual features and shape-based alignment. Visual and depth information are also combined for view-based loop-closure detection, followed by pose optimization to achieve globally consistent maps. We evaluate RGB-D Mapping on two large indoor environments, and show that it effectively combines the visual and shape information available from RGB-D cameras.",
            "저널": "The international journal of Robotics Research",
            "저자": "Peter Henry, Michael Krainin, Evan Herbst, Xiaofeng Ren, Dieter Fox",
            "전체 인용횟수": "2613회 인용20112012201320142015201620172018201920202021202220235416228025430930626026321516912410662",
            "페이지": "647-663",
            "학술 문서": "RGB-D mapping: Using Kinect-style depth cameras for dense 3D modeling of indoor environmentsP Henry, M Krainin, E Herbst, X Ren, D Fox - The international journal of Robotics Research, 20122613회 인용 관련 학술자료 전체 17개의 버전 ",
            "호": "5"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "RGB-D mapping: Using Kinect-style depth cameras for dense 3D modeling of indoor environments",
        "year": null
    },
    "RGB-D mapping: Using depth cameras for dense 3D modeling of indoor environments": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010",
            "저자": "Peter Henry, Michael Krainin, Evan Herbst, Xiaofeng Ren, Dieter Fox",
            "전체 인용횟수": "2613회 인용20112012201320142015201620172018201920202021202220235416228025430930626026321516912410662",
            "컨퍼런스": "In the 12th International Symposium on Experimental Robotics (ISER)",
            "학술 문서": "RGB-D mapping: Using Kinect-style depth cameras for dense 3D modeling of indoor environments*P Henry, M Krainin, E Herbst, X Ren, D Fox - The international journal of Robotics Research, 20122613회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "RGB-D mapping: Using depth cameras for dense 3D modeling of indoor environments",
        "year": null
    },
    "A large-scale hierarchical multi-view rgb-d object dataset": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/5/9",
            "게시자": "IEEE",
            "설명": "Over the last decade, the availability of public image repositories and recognition benchmarks has enabled rapid progress in visual object category and instance detection. Today we are witnessing the birth of a new generation of sensing technologies capable of providing high quality synchronized videos of both color and depth, the RGB-D (Kinect-style) camera. With its advanced sensing capabilities and the potential for mass adoption, this technology represents an opportunity to dramatically increase robotic object recognition, manipulation, navigation, and interaction capabilities. In this paper, we introduce a large-scale, hierarchical multi-view object dataset collected using an RGB-D camera. The dataset contains 300 objects organized into 51 categories and has been made publicly available to the research community so as to enable rapid progress based on this promising technology. This paper describes the …",
            "저자": "Kevin Lai, Liefeng Bo, Xiaofeng Ren, Dieter Fox",
            "전체 인용횟수": "1767회 인용201120122013201420152016201720182019202020212022202332821431631912311881691431271229354",
            "컨퍼런스": "2011 IEEE international conference on robotics and automation",
            "페이지": "1817-1824",
            "학술 문서": "A large-scale hierarchical multi-view rgb-d object datasetK Lai, L Bo, X Ren, D Fox - 2011 IEEE international conference on robotics and …, 20111767회 인용 관련 학술자료 전체 18개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A large-scale hierarchical multi-view rgb-d object dataset",
        "year": null
    },
    "Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/11/1",
            "설명": "Estimating the 6D pose of known objects is important for robots to interact with the real world. The problem is challenging due to the variety of objects as well as the complexity of a scene caused by clutter and occlusions between objects. In this work, we introduce PoseCNN, a new Convolutional Neural Network for 6D object pose estimation. PoseCNN estimates the 3D translation of an object by localizing its center in the image and predicting its distance from the camera. The 3D rotation of the object is estimated by regressing to a quaternion representation. We also introduce a novel loss function that enables PoseCNN to handle symmetric objects. In addition, we contribute a large scale video dataset for 6D object pose estimation named the YCB-Video dataset. Our dataset provides accurate 6D poses of 21 objects from the YCB dataset observed in 92 videos with 133,827 frames. We conduct extensive experiments on our YCB-Video dataset and the OccludedLINEMOD dataset to show that PoseCNN is highly robust to occlusions, can handle symmetric objects, and provide accurate pose estimation using only color images as input. When using depth data to further refine the poses, our approach achieves state-of-the-art results on the challenging OccludedLINEMOD dataset. Our code and dataset are available at https://rse-lab.cs.washington.edu/projects/posecnn/.",
            "저널": "arXiv preprint arXiv:1711.00199",
            "저자": "Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, Dieter Fox",
            "전체 인용횟수": "1718회 인용20182019202020212022202352185293350431396",
            "학술 문서": "Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenesY Xiang, T Schmidt, V Narayanan, D Fox - arXiv preprint arXiv:1711.00199, 20171718회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes",
        "year": null
    },
    "Inferring activities from interactions with objects": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004/10",
            "게시자": "IEEE",
            "권": "3",
            "설명": "A key aspect of pervasive computing is using computers and sensor networks to effectively and unobtrusively infer users' behavior in their environment. This includes inferring which activity users are performing, how they're performing it, and its current stage. Recognizing and recording activities of daily living is a significant problem in elder care. A new paradigm for ADL inferencing leverages radio-frequency-identification technology, data mining, and a probabilistic inference engine to recognize ADLs, based on the objects people use. We propose an approach that addresses these challenges and shows promise in automating some types of ADL monitoring. Our key observation is that the sequence of objects a person uses while performing an ADL robustly characterizes both the ADL's identity and the quality of its execution. So, we have developed Proactive Activity Toolkit (PROACT).",
            "저널": "IEEE pervasive computing",
            "저자": "Matthai Philipose, Kenneth P Fishkin, Mike Perkowitz, Donald J Patterson, Dieter Fox, Henry Kautz, Dirk Hahnel",
            "전체 인용횟수": "1205회 인용20042005200620072008200920102011201220132014201520162017201820192020202120222023347585190931161121169568716561423423181411",
            "페이지": "50-57",
            "학술 문서": "Inferring activities from interactions with objectsM Philipose, KP Fishkin, M Perkowitz, DJ Patterson… - IEEE pervasive computing, 20041205회 인용 관련 학술자료 전체 32개의 버전 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Inferring activities from interactions with objects",
        "year": null
    },
    "Learning and inferring transportation routines": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2007/4/1",
            "게시자": "Elsevier",
            "권": "171",
            "설명": "This paper introduces a hierarchical Markov model that can learn and infer a user's daily movements through an urban community. The model uses multiple levels of abstraction in order to bridge the gap between raw GPS sensor measurements and high level information such as a user's destination and mode of transportation. To achieve efficient inference, we apply Rao–Blackwellized particle filters at multiple levels of the model hierarchy. Locations such as bus stops and parking lots, where the user frequently changes mode of transportation, are learned from GPS data logs without manual labeling of training data. We experimentally demonstrate how to accurately detect novel behavior or user errors (e.g. taking a wrong bus) by explicitly modeling activities in the context of the user's historical data. Finally, we discuss an application called “Opportunity Knocks” that employs our techniques to help cognitively …",
            "저널": "Artificial intelligence",
            "저자": "Lin Liao, Donald J Patterson, Dieter Fox, Henry Kautz",
            "전체 인용횟수": "1156회 인용20062007200820092010201120122013201420152016201720182019202020212022202340457294888586698366806351573828219",
            "페이지": "311-331",
            "학술 문서": "Learning and inferring transportation routinesL Liao, DJ Patterson, D Fox, H Kautz - Artificial intelligence, 20071156회 인용 관련 학술자료 전체 36개의 버전 ",
            "호": "5-6"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning and inferring transportation routines",
        "year": null
    },
    "Bayesian Filtering for Location Estimation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2003",
            "권": "2",
            "설명": "Bayesian-filter techniques provide a powerful statistical tool to help manage measurement uncertainty and perform multisensor fusion and identity estimation. The authors survey Bayes filter implementations and show their application to real-world location-estimation tasks common in pervasive computing.",
            "저널": "IEEE Pervasive Computing Magazine",
            "저자": "D. Fox, J. Hightower, L. Liao, D. Schulz, G. Borriello",
            "전체 인용횟수": "1099회 인용20032004200520062007200820092010201120122013201420152016201720182019202020212022202343340596376718770677976594448473838273324",
            "학술 문서": "Bayesian filtering for location estimationV Fox, J Hightower, L Liao, D Schulz, G Borriello - IEEE pervasive computing, 20031099회 인용 관련 학술자료 전체 27개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Bayesian Filtering for Location Estimation",
        "year": null
    },
    "A real-time algorithm for mobile robot mapping with applications to multi-robot and 3D mapping": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2000/4/24",
            "게시자": "IEEE",
            "권": "1",
            "설명": "We present an incremental method for concurrent mapping and localization for mobile robots equipped with 2D laser range finders. The approach uses a fast implementation of scan-matching for mapping, paired with a sample-based probabilistic method for localization. Compact 3D maps are generated using a multi-resolution approach adopted from the computer graphics literature, fed by data from a dual laser system. Our approach builds 3D maps of large, cyclic environments in real-time, and it is robust. Experimental results illustrate that accurate maps of large, cyclic environments can be generated even in the absence of any odometric data.",
            "저자": "Sebastian Thrun, Wolfram Burgard, Dieter Fox",
            "전체 인용횟수": "1094회 인용200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023132952635966676469626252604440364641313132291812",
            "컨퍼런스": "Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No. 00CH37065)",
            "페이지": "321-328",
            "학술 문서": "A real-time algorithm for mobile robot mapping with applications to multi-robot and 3D mappingS Thrun, W Burgard, D Fox - Proceedings 2000 ICRA. Millennium Conference. IEEE …, 20001094회 인용 관련 학술자료 전체 20개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A real-time algorithm for mobile robot mapping with applications to multi-robot and 3D mapping",
        "year": null
    },
    "A probabilistic approach to collaborative multi-robot localization": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2000/6",
            "게시자": "Kluwer Academic Publishers",
            "권": "8",
            "설명": " This paper presents a statistical algorithm for collaborative mobile robot localization. Our approach uses a sample-based version of Markov localization, capable of localizing mobile robots in an any-time fashion. When teams of robots localize themselves in the same environment, probabilistic methods are employed to synchronize each robot's belief whenever one robot detects another. As a result, the robots localize themselves faster, maintain higher accuracy, and high-cost sensors are amortized across multiple robot platforms. The technique has been implemented and tested using two mobile robots equipped with cameras and laser range-finders for detecting other robots. The results, obtained with the real robots and in series of simulation runs, illustrate drastic improvements in localization speed and accuracy when compared to conventional single-robot localization. A further experiment demonstrates …",
            "저널": "Autonomous robots",
            "저자": "Dieter Fox, Wolfram Burgard, Hannes Kruppa, Sebastian Thrun",
            "전체 인용횟수": "1010회 인용200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023122237444143465056795041475637455242513633392320",
            "페이지": "325-344",
            "학술 문서": "A probabilistic approach to collaborative multi-robot localizationD Fox, W Burgard, H Kruppa, S Thrun - Autonomous robots, 20001010회 인용 관련 학술자료 전체 32개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A probabilistic approach to collaborative multi-robot localization",
        "year": null
    },
    "Pytorch: An imperative style, high-performance deep learning library": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/12/3",
            "설명": "Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.",
            "저널": "Advances in Neural Information Processing Systems",
            "저자": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala",
            "전체 인용횟수": "35398회 인용20192020202120222023116356982431143111851",
            "페이지": "8024--8035",
            "학술 문서": "Pytorch: An imperative style, high-performance deep learning libraryA Paszke, S Gross, F Massa, A Lerer, J Bradbury… - Advances in neural information processing systems, 201935125회 인용 관련 학술자료 전체 17개의 버전 Advances in neural information processing systems 32*A Paszke, S Gross, F Massa, A Lerer, J Bradbury… - Curran Associates, Inc, 2019906회 인용 관련 학술자료 An overview of deep learning frameworks and an introduction to pytorch*S Chintala - 20176회 인용 관련 학술자료 Understanding Natural Language With Deep Neural Networks Using Torch*S Chintala - 20164회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pytorch: An imperative style, high-performance deep learning library",
        "year": null
    },
    "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/11/19",
            "설명": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.",
            "저널": "International Conference on Learning Representations",
            "저자": "Alec Radford, Luke Metz, Soumith Chintala",
            "전체 인용횟수": "16217회 인용20162017201820192020202120222023193855188525052859297027302060",
            "학술 문서": "Unsupervised representation learning with deep convolutional generative adversarial networksA Radford, L Metz, S Chintala - arXiv preprint arXiv:1511.06434, 201516217회 인용 관련 학술자료 전체 5개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
        "year": null
    },
    "Wasserstein generative adversarial networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/7/17",
            "게시자": "PMLR",
            "설명": "We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.",
            "저널": "International conference on machine learning",
            "저자": "Martin Arjovsky, Soumith Chintala, Léon Bottou",
            "전체 인용횟수": "14532회 인용2017201820192020202120222023398125919462502288228772564",
            "페이지": "214-223",
            "학술 문서": "Wasserstein generative adversarial networksM Arjovsky, S Chintala, L Bottou - International conference on machine learning, 201714441회 인용 관련 학술자료 전체 10개의 버전 Wasserstein generative adversarial networks*A Martin, C Soumith, B Leon - International conference on machine learning, 2017166회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Wasserstein generative adversarial networks",
        "year": null
    },
    "Automatic differentiation in pytorch": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/10/28",
            "설명": "In this article, we describe an automatic differentiation module of PyTorch — a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd, and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.",
            "저널": "Neural Information Processing Systems Autodiff Workshop",
            "저자": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer",
            "전체 인용횟수": "12481회 인용20182019202020212022202371727292799251320391610",
            "학술 문서": "Automatic differentiation in pytorchA Paszke, S Gross, S Chintala, G Chanan, E Yang… - 201712481회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Automatic differentiation in pytorch",
        "year": null
    },
    "Deep generative image models using a laplacian pyramid of adversarial networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/6/18",
            "설명": "In this paper we introduce a generative model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks (convnets) within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach. Samples drawn from our model are of significantly higher quality than existing models. In a quantitive assessment by human evaluators our CIFAR10 samples were mistaken for real images around 40% of the time, compared to 10% for GAN samples. We also show samples from more diverse datasets such as STL10 and LSUN.",
            "저널": "Advances in Neural Information Processing Systems",
            "저자": "Emily Denton, Soumith Chintala, Arthur Szlam, Rob Fergus",
            "전체 인용횟수": "2802회 인용20152016201720182019202020212022202320129296417441446372357266",
            "학술 문서": "Deep generative image models using a￼ laplacian pyramid of adversarial networksEL Denton, S Chintala, R Fergus - Advances in neural information processing systems, 20152802회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep generative image models using a laplacian pyramid of adversarial networks",
        "year": null
    },
    "Pedestrian detection with unsupervised multi-stage feature learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/6/23",
            "게시자": "IEEE",
            "설명": "Pedestrian detection is a problem of considerable practical interest. Adding to the list of successful applications of deep learning methods to vision, we report state-of-theart and competitive results on all major pedestrian datasets with a convolutional network model. The model uses a few new twists, such as multi-stage features, connections that skip layers to integrate global shape information with local distinctive motif information, and an unsupervised method based on convolutional sparse coding to pre-train the filters at each stage.",
            "저널": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "저자": "Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala, Yann LeCun",
            "전체 인용횟수": "1084회 인용20132014201520162017201820192020202120222023115813116016515512897805627",
            "페이지": "3626-3633",
            "학술 문서": "Pedestrian detection with unsupervised multi-stage feature learningP Sermanet, K Kavukcuoglu, S Chintala, Y LeCun - Proceedings of the IEEE conference on computer …, 20131084회 인용 관련 학술자료 전체 21개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pedestrian detection with unsupervised multi-stage feature learning",
        "year": null
    },
    "Semantic segmentation using adversarial networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/11/25",
            "설명": "Adversarial training has been shown to produce state of the art results for generative image modeling. In this paper we propose an adversarial training approach to train semantic segmentation models. We train a convolutional semantic segmentation network along with an adversarial network that discriminates segmentation maps coming either from the ground truth or from the segmentation network. The motivation for our approach is that it can detect and correct higher-order inconsistencies between ground truth segmentation maps and the ones produced by the segmentation net. Our experiments show that our adversarial training approach leads to improved accuracy on the Stanford Background and PASCAL VOC 2012 datasets.",
            "저널": "Neural Information Processing Systems Workshop on Adversarial Training",
            "저자": "Pauline Luc, Camille Couprie, Soumith Chintala, Jakob Verbeek",
            "전체 인용횟수": "908회 인용20172018201920202021202220235510616317218513781",
            "학술 문서": "Semantic segmentation using adversarial networksP Luc, C Couprie, S Chintala, J Verbeek - arXiv preprint arXiv:1611.08408, 2016908회 인용 관련 학술자료 전체 7개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Semantic segmentation using adversarial networks",
        "year": null
    },
    "Convolutional neural networks applied to house numbers digit classification": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/11/11",
            "게시자": "IEEE",
            "설명": "We classify digits of real-world house numbers using convolutional neural networks (ConvNets). Con-vNets are hierarchical feature learning neural networks whose structure is biologically inspired. Unlike many popular vision approaches that are hand-designed, ConvNets can automatically learn a unique set of features optimized for a given task. We augmented the traditional ConvNet architecture by learning multi-stage features and by using Lp pooling and establish a new state-of-the-art of 95.10% accuracy on the SVHN dataset (48% error improvement). Furthermore, we analyze the benefits of different pooling methods and multi-stage features in ConvNets. The source code and a tutorial are available at eblearn.sf.net.",
            "저널": "International Conference on Pattern Recognition",
            "저자": "Pierre Sermanet, Soumith Chintala, Yann LeCun",
            "전체 인용횟수": "715회 인용2013201420152016201720182019202020212022202325285371741029079786239",
            "페이지": "3288-3291",
            "학술 문서": "Convolutional neural networks applied to house numbers digit classificationP Sermanet, S Chintala, Y LeCun - Proceedings of the 21st international conference on …, 2012715회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Convolutional neural networks applied to house numbers digit classification",
        "year": null
    },
    "Applied machine learning at facebook: A datacenter infrastructure perspective": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018/2/24",
            "게시자": "IEEE",
            "설명": "Machine learning sits at the core of many essential products and services at Facebook. This paper describes the hardware and software infrastructure that supports machine learning at global scale. Facebook's machine learning workloads are extremely diverse: services require many different types of models in practice. This diversity has implications at all layers in the system stack. In addition, a sizable fraction of all data stored at Facebook flows through machine learning pipelines, presenting significant challenges in delivering data to high-performance distributed training flows. Computational requirements are also intense, leveraging both GPU and CPU platforms for training and abundant CPU capacity for real-time inference. Addressing these and other emerging challenges continues to require diverse efforts that span machine learning algorithms, software, and hardware design.",
            "저널": "2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)",
            "저자": "Kim Hazelwood, Sarah Bird, David Brooks, Soumith Chintala, Utku Diril, Dmytro Dzhulgakov, Mohamed Fawzy, Bill Jia, Yangqing Jia, Aditya Kalro, James Law, Kevin Lee, Jason Lu, Pieter Noordhuis, Misha Smelyanskiy, Liang Xiong, Xiaodong Wang",
            "전체 인용횟수": "647회 인용2018201920202021202220233087136146132116",
            "페이지": "620-629",
            "학술 문서": "Applied machine learning at facebook: A datacenter infrastructure perspectiveK Hazelwood, S Bird, D Brooks, S Chintala, U Diril… - 2018 IEEE International Symposium on High …, 2018647회 인용 관련 학술자료 전체 10개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Applied machine learning at facebook: A datacenter infrastructure perspective",
        "year": null
    },
    "Automatic differentiation in PyTorch.(2017)": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/12",
            "저자": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer",
            "전체 인용횟수": "634회 인용20182019202020212022202341102112133132112",
            "페이지": "4",
            "학술 문서": "Automatic differentiation in PyTorch.(2017)A Paszke, S Gross, S Chintala, G Chanan, E Yang… - 2017622회 인용 관련 학술자료 Automatic differentiation in PyTorch*SG Adam Paszke, C Soumith, C Gregory, Y Edward… - NIPS 2017 Workshop autodiff decision program chairs, 201712회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Automatic differentiation in PyTorch.(2017)",
        "year": null
    },
    "Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv 2015": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "저널": "arXiv preprint arXiv:1511.06434",
            "저자": "Alec Radford, Luke Metz, Soumith Chintala",
            "전체 인용횟수": "502회 인용20172018201920202021202220233243360111138132",
            "학술 문서": "Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv 2015A Radford, L Metz, S Chintala - arXiv preprint arXiv:1511.06434, 2015502회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv 2015",
        "year": null
    },
    "Wasserstein gan. arXiv 2017": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "권": "30",
            "저널": "arXiv preprint arXiv:1701.07875",
            "저자": "Martin Arjovsky, Soumith Chintala, Léon Bottou",
            "전체 인용횟수": "420회 인용201720182019202020212022202341951709710769",
            "학술 문서": "Wasserstein gan. arXiv 2017M Arjovsky, S Chintala, L Bottou - arXiv preprint arXiv:1701.07875, 2017420회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Wasserstein gan. arXiv 2017",
        "year": null
    },
    "Fast convolutional nets with fbfft: A GPU performance evaluation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/5/7",
            "설명": "We examine the performance profile of Convolutional Neural Network training on the current generation of NVIDIA Graphics Processing Units. We introduce two new Fast Fourier Transform convolution implementations: one based on NVIDIA's cuFFT library, and another based on a Facebook authored FFT implementation, fbfft, that provides significant speedups over cuFFT (over 1.5x) for whole CNNs. Both of these convolution implementations are available in open source, and are faster than NVIDIA's cuDNN implementation for many common convolutional layers (up to 23.5x for some synthetic kernel configurations). We discuss different performance regimes of convolutions, comparing areas where straightforward time domain convolutions outperform Fourier frequency domain convolutions. Details on algorithmic applications of NVIDIA GPU hardware specifics in the implementation of fbfft are also provided.",
            "저널": "International Conference on Learning Representations",
            "저자": "Nicolas Vasilache, Jeff Johnson, Michael Mathieu, Soumith Chintala, Serkan Piantino, Yann LeCun",
            "전체 인용횟수": "386회 인용20142015201620172018201920202021202220231224942525844424125",
            "학술 문서": "Fast convolutional nets with fbfft: A GPU performance evaluationN Vasilache, J Johnson, M Mathieu, S Chintala… - arXiv preprint arXiv:1412.7580, 2014386회 인용 관련 학술자료 전체 8개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Fast convolutional nets with fbfft: A GPU performance evaluation",
        "year": null
    },
    "Pytorch distributed: Experiences on accelerating data parallel training": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2020/6/28",
            "설명": "This paper presents the design, implementation, and evaluation of the PyTorch distributed data parallel module. PyTorch is a widely-adopted scientific computing package used in deep learning research and applications. Recent advances in deep learning argue for the value of large datasets and large models, which necessitates the ability to scale out model training to more computational resources. Data parallelism has emerged as a popular solution for distributed training thanks to its straightforward principle and broad applicability. In general, the technique of distributed data parallelism replicates the model on every computational resource to generate gradients independently and then communicates those gradients at each iteration to keep model replicas consistent. Despite the conceptual simplicity of the technique, the subtle dependencies between computation and communication make it non-trivial to optimize the distributed training efficiency. As of v1.5, PyTorch natively provides several techniques to accelerate distributed data parallel, including bucketing gradients, overlapping computation with communication, and skipping gradient synchronization. Evaluations show that, when configured appropriately, the PyTorch distributed data parallel module attains near-linear scalability using 256 GPUs.",
            "저널": "Proceedings of the VLDB Endowment",
            "저자": "Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, Soumith Chintala",
            "전체 인용횟수": "350회 인용201920202021202220231768128143",
            "학술 문서": "Pytorch distributed: Experiences on accelerating data parallel trainingS Li, Y Zhao, R Varma, O Salpekar, P Noordhuis, T Li… - arXiv preprint arXiv:2006.15704, 2020350회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pytorch distributed: Experiences on accelerating data parallel training",
        "year": null
    },
    "Pytorch: Tensors and dynamic neural networks in python with strong gpu acceleration": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017/5",
            "권": "6",
            "저널": "PyTorch: Tensors and dynamic neural networks in Python with strong GPU acceleration",
            "저자": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan",
            "전체 인용횟수": "333회 인용20162017201820192020202120222023154611077403121",
            "페이지": "3",
            "학술 문서": "Pytorch: Tensors and dynamic neural networks in python with strong gpu accelerationA Paszke, S Gross, S Chintala, G Chanan - PyTorch: Tensors and dynamic neural networks in …, 2017333회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pytorch: Tensors and dynamic neural networks in python with strong gpu acceleration",
        "year": null
    },
    "Discovering causal signals in images": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "This paper establishes the existence of observable footprints that reveal the\" causal dispositions\" of the object categories appearing in collections of images. We achieve this goal in two steps. First, we take a learning approach to observational causal discovery, and build a classifier that achieves state-of-the-art performance on finding the causal direction between pairs of random variables, given samples from their joint distribution. Second, we use our causal direction classifier to effectively distinguish between features of objects and features of their contexts in collections of static images. Our experiments demonstrate the existence of a relation between the direction of causality and the difference between objects and their contexts, and by the same token, the existence of observable signals that reveal the causal dispositions of objects.",
            "저널": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "저자": "David Lopez-Paz, Robert Nishihara, Soumith Chintala, Bernhard Scholkopf, Léon Bottou",
            "전체 인용횟수": "218회 인용201620172018201920202021202220232891931435649",
            "페이지": "6979-6987",
            "학술 문서": "Discovering causal signals in imagesD Lopez-Paz, R Nishihara, S Chintala, B Scholkopf… - Proceedings of the IEEE conference on computer …, 2017218회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Discovering causal signals in images",
        "year": null
    },
    "Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/9/10",
            "설명": "We consider scenarios from the real-time strategy game StarCraft as new benchmarks for reinforcement learning algorithms. We propose micromanagement tasks, which present the problem of the short-term, low-level control of army members during a battle. From a reinforcement learning point of view, these scenarios are challenging because the state-action space is very large, and because there is no obvious feature representation for the state-action evaluation function. We describe our approach to tackle the micromanagement scenarios with deep neural network controllers from raw state features given by the game engine. In addition, we present a heuristic reinforcement learning algorithm which combines direct exploration in the policy space and backpropagation. This algorithm allows for the collection of traces for learning using deterministic policies, which appears much more efficient than, for example, {\\epsilon}-greedy exploration. Experiments show that with this algorithm, we successfully learn non-trivial strategies for scenarios with armies of up to 15 agents, where both Q-learning and REINFORCE struggle.",
            "저널": "arXiv preprint arXiv:1609.02993",
            "저자": "Nicolas Usunier, Gabriel Synnaeve, Zeming Lin, Soumith Chintala",
            "전체 인용횟수": "167회 인용2016201720182019202020212022202313039293116146",
            "학술 문서": "Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasksN Usunier, G Synnaeve, Z Lin, S Chintala - arXiv preprint arXiv:1609.02993, 2016167회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks",
        "year": null
    },
    "Pytorch: An imperative style, high-performance deep learning library. arXiv 2019": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1912",
            "저널": "arXiv preprint arXiv:1912.01703",
            "저자": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala",
            "전체 인용횟수": "149회 인용20202021202220236594240",
            "학술 문서": "Pytorch: An imperative style, high-performance deep learning library. arXiv 2019A Paszke, S Gross, F Massa, A Lerer, J Bradbury… - arXiv preprint arXiv:1912.01703, 1912149회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Pytorch: An imperative style, high-performance deep learning library. arXiv 2019",
        "year": null
    },
    "Generalized inner loop meta-learning": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2019/10/3",
            "설명": "Many (but not all) approaches self-qualifying as \"meta-learning\" in deep learning and reinforcement learning fit a common pattern of approximating the solution to a nested optimization problem. In this paper, we give a formalization of this shared pattern, which we call GIMLI, prove its general requirements, and derive a general-purpose algorithm for implementing similar approaches. Based on this analysis and algorithm, we describe a library of our design, higher, which we share with the community to assist and enable future research into these kinds of meta-learning approaches. We end the paper by showcasing the practical applications of this framework and library through illustrative experiments and ablation studies which they facilitate.",
            "저널": "arXiv preprint arXiv:1910.01727",
            "저자": "Edward Grefenstette, Brandon Amos, Denis Yarats, Phu Mon Htut, Artem Molchanov, Franziska Meier, Douwe Kiela, Kyunghyun Cho, Soumith Chintala",
            "전체 인용횟수": "139회 인용20192020202120222023225403239",
            "학술 문서": "Generalized inner loop meta-learningE Grefenstette, B Amos, D Yarats, PM Htut… - arXiv preprint arXiv:1910.01727, 2019139회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Generalized inner loop meta-learning",
        "year": null
    },
    "A discriminatively trained, multiscale, deformable part model": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2008/6/23",
            "게시자": "Ieee",
            "설명": "This paper describes a discriminatively trained, multiscale, deformable part model for object detection. Our system achieves a two-fold improvement in average precision over the best performance in the 2006 PASCAL person detection challenge. It also outperforms the best results in the 2007 challenge in ten out of twenty categories. The system relies heavily on deformable parts. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL challenge. Our system also relies heavily on new methods for discriminative training. We combine a margin-sensitive approach for data mining hard negative examples with a formalism we call latent SVM. A latent SVM, like a hidden CRF, leads to a non-convex training problem. However, a latent SVM is semi-convex and the training problem becomes convex once latent information is specified …",
            "저자": "Pedro Felzenszwalb, David McAllester, Deva Ramanan",
            "전체 인용횟수": "3748회 인용200820092010201120122013201420152016201720182019202020212022202320133169205207254270265282300256270264311264203",
            "컨퍼런스": "2008 IEEE conference on computer vision and pattern recognition",
            "페이지": "1-8",
            "학술 문서": "A discriminatively trained, multiscale, deformable part modelP Felzenszwalb, D McAllester, D Ramanan - 2008 IEEE conference on computer vision and pattern …, 20083748회 인용 관련 학술자료 전체 22개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A discriminatively trained, multiscale, deformable part model",
        "year": null
    },
    "Face detection, pose estimation, and landmark localization in the wild": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/6/16",
            "게시자": "IEEE",
            "설명": "We present a unified model for face detection, pose estimation, and landmark estimation in real-world, cluttered images. Our model is based on a mixtures of trees with a shared pool of parts; we model every facial landmark as a part and use global mixtures to capture topological changes due to viewpoint. We show that tree-structured models are surprisingly effective at capturing global elastic deformation, while being easy to optimize unlike dense graph structures. We present extensive results on standard face benchmarks, as well as a new “in the wild” annotated dataset, that suggests our system advances the state-of-the-art, sometimes considerably, for all three tasks. Though our model is modestly trained with hundreds of faces, it compares favorably to commercial systems trained with billions of examples (such as Google Picasa and face.com).",
            "저자": "Xiangxin Zhu, Deva Ramanan",
            "전체 인용횟수": "2956회 인용2012201320142015201620172018201920202021202220232413527633740436836130522720214786",
            "컨퍼런스": "2012 IEEE conference on computer vision and pattern recognition",
            "페이지": "2879-2886",
            "학술 문서": "Face detection, pose estimation, and landmark localization in the wildX Zhu, D Ramanan - 2012 IEEE conference on computer vision and pattern …, 20122956회 인용 관련 학술자료 전체 12개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Face detection, pose estimation, and landmark localization in the wild",
        "year": null
    },
    "Articulated Human Detection with Flexible Mixtures-of-Parts": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/12/12",
            "게시자": "IEEE",
            "권": "35",
            "설명": "We describe a method for articulated human detection and human pose estimation in static images based on a new representation of deformable part models. Rather than modeling articulation using a family of warped (rotated and foreshortened) templates, we use a mixture of small, nonoriented parts. We describe a general, flexible mixture model that jointly captures spatial relations between part locations and co-occurrence relations between part mixtures, augmenting standard pictorial structure models that encode just spatial relations. Our models have several notable properties: 1) They efficiently model articulation by sharing computation across similar warps, 2) they efficiently model an exponentially large set of global mixtures through composition of local mixtures, and 3) they capture the dependency of global geometry on local appearance (parts look different at different locations). When relations are tree …",
            "저널": "Pattern Analysis and Machine Intelligence, IEEE Transactions on",
            "저자": "Yi Yang, Deva Ramanan",
            "전체 인용횟수": "2402회 인용2010201120122013201420152016201720182019202020212022202361798167234251281239223196176191148108",
            "페이지": "2878 - 2890",
            "학술 문서": "Articulated pose estimation with flexible mixtures-of-parts*Y Yang, D Ramanan - CVPR 2011, 20111523회 인용 관련 학술자료 전체 17개의 버전 Articulated human detection with flexible mixtures of partsY Yang, D Ramanan - IEEE transactions on pattern analysis and machine …, 20121025회 인용 관련 학술자료 전체 12개의 버전 ",
            "호": "12"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Articulated Human Detection with Flexible Mixtures-of-Parts",
        "year": null
    },
    "Argoverse: 3d tracking and forecasting with rich maps": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {},
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Argoverse: 3d tracking and forecasting with rich maps",
        "year": null
    },
    "Globally-optimal greedy algorithms for tracking a variable number of objects": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {},
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Globally-optimal greedy algorithms for tracking a variable number of objects",
        "year": null
    },
    "Finding tiny faces": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {},
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Finding tiny faces",
        "year": null
    },
    "Detecting activities of daily living in first-person camera views": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {},
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Detecting activities of daily living in first-person camera views",
        "year": null
    },
    "A large-scale benchmark dataset for event recognition in surveillance video": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {},
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A large-scale benchmark dataset for event recognition in surveillance video",
        "year": null
    },
    "Efficiently scaling up crowdsourced video annotation: A set of best practices for high quality, economical video labeling": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {},
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Efficiently scaling up crowdsourced video annotation: A set of best practices for high quality, economical video labeling",
        "year": null
    },
    "Learning to parse images of articulated bodies": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {},
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning to parse images of articulated bodies",
        "year": null
    },
    "Discriminative models for multi-class object layout": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {},
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Discriminative models for multi-class object layout",
        "year": null
    },
    "3d human pose estimation= 2d pose estimation+ matching": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {},
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "3d human pose estimation= 2d pose estimation+ matching",
        "year": null
    },
    "Learning to model the tail": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {},
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Learning to model the tail",
        "year": null
    },
    "Microsoft COCO: common objects in context (2014)": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {},
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Microsoft COCO: common objects in context (2014)",
        "year": null
    },
    "Actionvlad: Learning spatio-temporal aggregation for action classification": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {},
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Actionvlad: Learning spatio-temporal aggregation for action classification",
        "year": null
    },
    "Tracking people by learning their appearance": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {},
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Tracking people by learning their appearance",
        "year": null
    },
    "Look and think twice: Capturing top-down visual attention with feedback convolutional neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {},
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Look and think twice: Capturing top-down visual attention with feedback convolutional neural networks",
        "year": null
    },
    "Strike a pose: Tracking people by finding stylized poses": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {},
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Strike a pose: Tracking people by finding stylized poses",
        "year": null
    },
    "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "설명": "This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].",
            "저널": "Proceedings of the International Conference on Learning Representations (ICLR)",
            "저자": "Karen Simonyan, Andrea Vedaldi, Andrew Zisserman",
            "전체 인용횟수": "7286회 인용201420152016201720182019202020212022202320832143436158501038135514181277",
            "학술 문서": "Deep inside convolutional networks: Visualising image classification models and saliency mapsK Simonyan, A Vedaldi, A Zisserman - arXiv preprint arXiv:1312.6034, 20137286회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps",
        "year": null
    },
    "Fully-convolutional siamese networks for object tracking": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "게시자": "Springer International Publishing",
            "설명": " The problem of arbitrary object tracking has traditionally been tackled by learning a model of the object’s appearance exclusively online, using as sole training data the video itself. Despite the success of these methods, their online-only approach inherently limits the richness of the model they can learn. Recently, several attempts have been made to exploit the expressive power of deep convolutional networks. However, when the object to track is not known beforehand, it is necessary to perform Stochastic Gradient Descent online to adapt the weights of the network, severely compromising the speed of the system. In this paper we equip a basic tracking algorithm with a novel fully-convolutional Siamese network trained end-to-end on the ILSVRC15 dataset for object detection in video. Our tracker operates at frame-rates beyond real-time and, despite its extreme simplicity, achieves state-of-the-art …",
            "저자": "Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi, Philip HS Torr",
            "전체 인용횟수": "4311회 인용2017201820192020202120222023122316608735865892738",
            "컨퍼런스": "Computer Vision–ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part II 14",
            "페이지": "850-865",
            "학술 문서": "Fully-convolutional siamese networks for object trackingL Bertinetto, J Valmadre, JF Henriques, A Vedaldi… - Computer Vision–ECCV 2016 Workshops: Amsterdam …, 20164311회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Fully-convolutional siamese networks for object tracking",
        "year": null
    },
    "Return of the Devil in the Details: Delving Deep into Convolutional Nets": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "설명": "The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source code and models to reproduce the experiments in the paper is made publicly available.",
            "저자": "Ken Chatfield, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman",
            "전체 인용횟수": "4162회 인용201420152016201720182019202020212022202329265483664678595446348327211",
            "컨퍼런스": "British Machine Vision Conference (BMVC)",
            "학술 문서": "Return of the devil in the details: Delving deep into convolutional netsK Chatfield, K Simonyan, A Vedaldi, A Zisserman - arXiv preprint arXiv:1405.3531, 20144162회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Return of the Devil in the Details: Delving Deep into Convolutional Nets",
        "year": null
    },
    "VLFeat: An open and portable library of computer vision algorithms": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2010/10/25",
            "도서": "Proceedings of the 18th ACM international conference on Multimedia",
            "설명": "VLFeat is an open and portable library of computer vision algorithms. It aims at facilitating fast prototyping and reproducible research for computer vision scientists and students. It includes rigorous implementations of common building blocks such as feature detectors, feature extractors, (hierarchical) k-means clustering, randomized kd-tree matching, and super-pixelization. The source code and interfaces are fully documented. The library integrates directly with MATLAB, a popular language for computer vision research.",
            "저자": "Andrea Vedaldi, Brian Fulkerson",
            "전체 인용횟수": "4039회 인용20062007200820092010201120122013201420152016201720182019202020212022202310111948851642193144685465804543482461701478659",
            "페이지": "1469-1472",
            "학술 문서": "VLFeat: An open and portable library of computer vision algorithmsA Vedaldi, B Fulkerson - Proceedings of the 18th ACM international conference …, 20103685회 인용 관련 학술자료 전체 10개의 버전 VLFeat: An open and portable library of computer vision algorithms (2008)*A Vedaldi, B Fulkerson - 2012336회 인용 관련 학술자료 An open implementation of the SIFT detector and descriptor*A Vedaldi - UCLA CSD, 2007126회 인용 관련 학술자료 전체 3개의 버전 An implementation of multi-dimensional maximally stable extremal regions*A Vedaldi - Feb, 200730회 인용 관련 학술자료 SIFT++ A lightweight C++ implementation of SIFT*A Vedaldi - 201122회 인용 관련 학술자료 Bag of features*A Vedaldi - Retrieved in, 20103회 인용 관련 학술자료 An Introduction to the VisionLab Features Library*A Vedaldi, B Fulkerson - 20071회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "VLFeat: An open and portable library of computer vision algorithms",
        "year": null
    },
    "Instance normalization: The missing ingredient for fast stylization": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/7/27",
            "설명": "It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will is made available on github at https://github.com/DmitryUlyanov/texture_nets. Full paper can be found at arXiv:1701.02096.",
            "저널": "arXiv preprint arXiv:1607.08022",
            "저자": "Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky",
            "전체 인용횟수": "3701회 인용201720182019202020212022202339166418652832841726",
            "학술 문서": "Instance normalization: The missing ingredient for fast stylizationD Ulyanov, A Vedaldi, V Lempitsky - arXiv preprint arXiv:1607.08022, 20163701회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Instance normalization: The missing ingredient for fast stylization",
        "year": null
    },
    "Matconvnet: Convolutional neural networks for matlab": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015/10/13",
            "도서": "Proceedings of the 23rd ACM international conference on Multimedia",
            "설명": "MatConvNet is an open source implementation of Convolutional Neural Networks (CNNs) with a deep integration in the MATLAB environment. The toolbox is designed with an emphasis on simplicity and flexibility. It exposes the building blocks of CNNs as easy-to-use MATLAB functions, providing routines for computing convolutions with filter banks, feature pooling, normalisation, and much more. MatConvNet can be easily extended, often using only MATLAB code, allowing fast prototyping of new CNN architectures. At the same time, it supports efficient computation on CPU and GPU, allowing to train complex models on large datasets such as ImageNet ILSVRC containing millions of training examples",
            "저자": "Andrea Vedaldi, Karel Lenc",
            "전체 인용횟수": "3450회 인용20152016201720182019202020212022202311741167972258935924516482",
            "페이지": "689-692",
            "학술 문서": "Matconvnet: Convolutional neural networks for matlabA Vedaldi, K Lenc - Proceedings of the 23rd ACM international conference …, 20153450회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Matconvnet: Convolutional neural networks for matlab",
        "year": null
    },
    "Deep image prior": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2018",
            "설명": "Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, super-resolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs. Apart from its diverse applications, our approach highlights the inductive bias captured by standard generator network architectures. It also bridges the gap between two very popular families of image restoration methods: learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors such as self-similarity.",
            "저자": "Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky",
            "전체 인용횟수": "2943회 인용20182019202020212022202395261398700760697",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "9446-9454",
            "학술 문서": "Deep image priorD Ulyanov, A Vedaldi, V Lempitsky - Proceedings of the IEEE conference on computer …, 20182649회 인용 관련 학술자료 전체 13개의 버전 Deep image prior*V Lempitsky, A Vedaldi, D Ulyanov - 2018 IEEE/CVF Conference on Computer Vision and …, 2018403회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep image prior",
        "year": null
    },
    "The devil is in the details: an evaluation of recent feature encoding methods": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011",
            "설명": "A large number of novel encodings for bag of visual words models have been proposed in the past two years to improve on the standard histogram of quantized local features. Examples include locality-constrained linear encoding [23], improved Fisher encoding [17], super vector encoding [27], and kernel codebook encoding [20]. While several authors have reported very good results on the challenging PASCAL VOC classification data by means of these new techniques, differences in the feature computation and learning algorithms, missing details in the description of the methods, and different tuning of the various components, make it impossible to compare directly these methods and hard to reproduce the results reported. This paper addresses these shortcomings by carrying out a rigorous evaluation of these new techniques by:(1) fixing the other elements of the pipeline (features, learning, tuning);(2) disclosing all the implementation details, and (3) identifying both those aspects of each method which are particularly important to achieve good performance, and those aspects which are less critical. This allows a consistent comparative analysis of these encoding methods. Several conclusions drawn from our analysis cannot be inferred from the original publications.",
            "저자": "Ken Chatfield, Victor Lempitsky, Andrea Vedaldi, Andrew Zisserman",
            "전체 인용횟수": "2326회 인용19881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202318343634364269558078968310585481414816102129283310016822220518711779473131177",
            "컨퍼런스": "BMVC",
            "학술 문서": "The devil is in the details: an evaluation of recent feature encoding methods.K Chatfield, VS Lempitsky, A Vedaldi, A Zisserman - BMVC, 20112326회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "The devil is in the details: an evaluation of recent feature encoding methods",
        "year": null
    },
    "Understanding deep image representations by inverting them": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "Image representations, from SIFT and Bag of Visual Words to Convolutional Neural Networks (CNNs), are a crucial component of almost any image understanding system. Nevertheless, our understanding of them remains limited. In this paper we conduct a direct analysis of the visual information contained in representations by asking the following question: given an encoding of an image, to which extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. We show that this method can invert representations such as HOG more accurately than recent alternatives while being applicable to CNNs too. We then use this technique to study the inverse of recent state-of-the-art CNN image representations for the first time. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.",
            "저자": "Aravindh Mahendran, Andrea Vedaldi",
            "전체 인용횟수": "2145회 인용2014201520162017201820192020202120222023644143189274317309338275226",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "5188-5196",
            "학술 문서": "Understanding deep image representations by inverting themA Mahendran, A Vedaldi - Proceedings of the IEEE conference on computer …, 20152145회 인용 관련 학술자료 전체 15개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Understanding deep image representations by inverting them",
        "year": null
    },
    "Describing textures in the wild": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "게시자": "IEEE",
            "설명": "Patterns and textures are key characteristics of many natural objects: a shirt can be striped, the wings of a butterfly can be veined, and the skin of an animal can be scaly. Aiming at supporting this dimension in image understanding, we address the problem of describing textures with semantic attributes. We identify a vocabulary of forty-seven texture terms and use them to describe a large dataset of patterns collected\" in the wild\". The resulting Describable Textures Dataset (DTD) is a basis to seek the best representation for recognizing describable texture attributes in images. We port from object recognition to texture recognition the Improved Fisher Vector (IFV) and Deep Convolutional-network Activation Features (DeCAF), and show that surprisingly, they both outperform specialized texture descriptors not only on our problem, but also in established material recognition datasets. We also show that our describable attributes are excellent texture descriptors, transferring between datasets and tasks; in particular, combined with IFV and DeCAF, they significantly outperform the state-of-the-art by more than 10% on both FMD and KTH-TIPS-2b benchmarks. We also demonstrate that they produce intuitive descriptions of materials and Internet images.",
            "저자": "Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, Andrea Vedaldi",
            "전체 인용횟수": "1870회 인용201420152016201720182019202020212022202353676102110134146280395563",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on,",
            "학술 문서": "Describing textures in the wildM Cimpoi, S Maji, I Kokkinos, S Mohamed, A Vedaldi - Proceedings of the IEEE conference on computer …, 20141870회 인용 관련 학술자료 전체 28개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Describing textures in the wild",
        "year": null
    },
    "End-to-end representation learning for correlation filter based tracking": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "The Correlation Filter is an algorithm that trains a linear template to discriminate between images and their translations. It is well suited to object tracking because its formulation in the Fourier domain provides a fast solution, enabling the detector to be re-trained once per frame. Previous works that use the Correlation Filter, however, have adopted features that were either manually designed or trained for a different task. This work is the first to overcome this limitation by interpreting the Correlation Filter learner, which has a closed-form solution, as a differentiable layer in a deep neural network. This enables learning deep features that are tightly coupled to the Correlation Filter. Experiments illustrate that our method has the important practical benefit of allowing lightweight architectures to achieve state-of-the-art performance at high framerates.",
            "저자": "Jack Valmadre, Luca Bertinetto, Joao Henriques, Andrea Vedaldi, Philip HS Torr",
            "전체 인용횟수": "1720회 인용201720182019202020212022202312187341323377277198",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "2805-2813",
            "학술 문서": "End-to-end representation learning for correlation filter based trackingJ Valmadre, L Bertinetto, J Henriques, A Vedaldi… - Proceedings of the IEEE conference on computer …, 20171720회 인용 관련 학술자료 전체 16개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "End-to-end representation learning for correlation filter based tracking",
        "year": null
    },
    "Speeding up Convolutional Neural Networks with Low Rank Expansions": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "설명": "The focus of this paper is speeding up the evaluation of convolutional neural networks. While delivering impressive results across a range of computer vision and machine learning tasks, these networks are computationally demanding, limiting their deployability. Convolutional layers generally consume the bulk of the processing time, and so in this work we present two simple schemes for drastically speeding up these layers. This is achieved by exploiting cross-channel or filter redundancy to construct a low rank basis of filters that are rank-1 in the spatial domain. Our methods are architecture agnostic, and can be easily applied to existing CPU and GPU convolutional frameworks for tuneable speedup performance. We demonstrate this with a real world network designed for scene text character recognition, showing a possible 2.5x speedup with no loss in accuracy, and 4.5x speedup with less than 1% drop in accuracy, still achieving state-of-the-art on standard benchmarks.",
            "저자": "Max Jaderberg, Andrea Vedaldi, Andrew Zisserman",
            "전체 인용횟수": "1687회 인용2014201520162017201820192020202120222023123497131234256270239234157",
            "컨퍼런스": "British Machine Vision Conference (BMVC)",
            "학술 문서": "Speeding up convolutional neural networks with low rank expansionsM Jaderberg, A Vedaldi, A Zisserman - arXiv preprint arXiv:1405.3866, 20141687회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Speeding up Convolutional Neural Networks with Low Rank Expansions",
        "year": null
    },
    "Fine-grained visual classification of aircraft": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/6/21",
            "설명": "This paper introduces FGVC-Aircraft, a new dataset containing 10,000 images of aircraft spanning 100 aircraft models, organised in a three-level hierarchy. At the finer level, differences between models are often subtle but always visually measurable, making visual recognition challenging but possible. A benchmark is obtained by defining corresponding classification tasks and evaluation protocols, and baseline results are presented. The construction of this dataset was made possible by the work of aircraft enthusiasts, a strategy that can extend to the study of number of other object classes. Compared to the domains usually considered in fine-grained visual classification (FGVC), for example animals, aircraft are rigid and hence less deformable. They, however, present other interesting modes of variation, including purpose, size, designation, structure, historical style, and branding.",
            "저널": "arXiv preprint arXiv:1306.5151",
            "저자": "Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, Andrea Vedaldi",
            "전체 인용횟수": "1589회 인용20142015201620172018201920202021202220231030286582109157267359471",
            "학술 문서": "Fine-grained visual classification of aircraftS Maji, E Rahtu, J Kannala, M Blaschko, A Vedaldi - arXiv preprint arXiv:1306.5151, 20131589회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Fine-grained visual classification of aircraft",
        "year": null
    },
    "Interpretable explanations of black boxes by meaningful perturbation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2017",
            "설명": "As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks\" look\" in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints. In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm. Second, we specialise the framework to find the part of an image most responsible for a classifier decision. Unlike previous works, our method is model-agnostic and testable because it is grounded in explicit and interpretable image perturbations.",
            "저자": "Ruth C Fong, Andrea Vedaldi",
            "전체 인용횟수": "1548회 인용20172018201920202021202220231695174258361340297",
            "컨퍼런스": "Proceedings of the IEEE international conference on computer vision",
            "페이지": "3429-3437",
            "학술 문서": "Interpretable explanations of black boxes by meaningful perturbationRC Fong, A Vedaldi - Proceedings of the IEEE international conference on …, 20171548회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Interpretable explanations of black boxes by meaningful perturbation",
        "year": null
    },
    "Synthetic data for text localisation in natural images": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016",
            "설명": "In this paper we introduce a new method for text detection in natural images. The method comprises two contributions: First, a fast and scalable engine to generate synthetic images of text in clutter. This engine overlays synthetic text to existing background images in a natural way, accounting for the local 3D scene geometry. Second, we use the synthetic images to train a Fully-Convolutional Regression Network (FCRN) which efficiently performs text detection and bounding-box regression at all locations and multiple scales in an image. We discuss the relation of FCRN to the recently-introduced YOLO detector, as well as other end-to-end object detection systems based on deep learning. The resulting detection network significantly out performs current methods for text detection in natural images, achieving an F-measure of 84.2% on the standard ICDAR 2013 benchmark. Furthermore, it can process 15 images per second on a GPU.",
            "저자": "Ankush Gupta, Andrea Vedaldi, Andrew Zisserman",
            "전체 인용횟수": "1526회 인용20162017201820192020202120222023787158225204286291235",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "2315-2324",
            "학술 문서": "Synthetic data for text localisation in natural imagesA Gupta, A Vedaldi, A Zisserman - Proceedings of the IEEE conference on computer …, 20161526회 인용 관련 학술자료 전체 14개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Synthetic data for text localisation in natural images",
        "year": null
    },
    "Cats and dogs": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/6/16",
            "게시자": "IEEE",
            "설명": "We investigate the fine grained object categorization problem of determining the breed of animal from an image. To this end we introduce a new annotated dataset of pets covering 37 different breeds of cats and dogs. The visual problem is very challenging as these animals, particularly cats, are very deformable and there can be quite subtle differences between the breeds. We make a number of contributions: first, we introduce a model to classify a pet breed automatically from an image. The model combines shape, captured by a deformable part model detecting the pet face, and appearance, captured by a bag-of-words model that describes the pet fur. Fitting the model involves automatically segmenting the animal in the image. Second, we compare two classification approaches: a hierarchical one, in which a pet is first assigned to the cat or dog family and then to a breed, and a flat one, in which the breed is …",
            "저자": "Omkar M Parkhi, Andrea Vedaldi, A Zisserman, CV Jawahar",
            "전체 인용횟수": "1312회 인용20122013201420152016201720182019202020212022202361735444949608398172261420",
            "컨퍼런스": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on",
            "페이지": "3498-3505",
            "학술 문서": "Cats and dogsOM Parkhi, A Vedaldi, A Zisserman, CV Jawahar - 2012 IEEE conference on computer vision and pattern …, 20121311회 인용 관련 학술자료 전체 8개의 버전 Cats and dogs*CV Jawahar, A Zisserman, A Vedaldi, OM Parkhi - 2012 IEEE Conference on Computer Vision and …, 20121회 인용 관련 학술자료 전체 2개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Cats and dogs",
        "year": null
    },
    "Reading text in the wild with convolutional neural networks": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2016/1",
            "게시자": "Springer US",
            "권": "116",
            "설명": " In this work we present an end-to-end system for text spotting—localising and recognising text in natural scene images—and text based image retrieval. This system is based on a region proposal mechanism for detection and deep convolutional neural networks for recognition. Our pipeline uses a novel combination of complementary proposal generation techniques to ensure high recall, and a fast subsequent filtering stage for improving precision. For the recognition and ranking of proposals, we train very large convolutional neural networks to perform word recognition on the whole proposal region at the same time, departing from the character classifier based systems of the past. These networks are trained solely on data produced by a synthetic text generation engine, requiring no human labelled data. Analysing the stages of our pipeline, we show state-of-the-art performance throughout. We perform …",
            "저널": "International journal of computer vision",
            "저자": "Max Jaderberg, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman",
            "전체 인용횟수": "1301회 인용2015201620172018201920202021202220232178155185209167185163109",
            "페이지": "1-20",
            "학술 문서": "Reading text in the wild with convolutional neural networksM Jaderberg, K Simonyan, A Vedaldi, A Zisserman - International journal of computer vision, 20161301회 인용 관련 학술자료 전체 17개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Reading text in the wild with convolutional neural networks",
        "year": null
    },
    "Efficient additive kernels via explicit feature maps": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2012/1/23",
            "게시자": "IEEE",
            "권": "34",
            "설명": "Large scale nonlinear support vector machines (SVMs) can be approximated by linear ones using a suitable feature map. The linear SVMs are in general much faster to learn and evaluate (test) than the original nonlinear SVMs. This work introduces explicit feature maps for the additive class of kernels, such as the intersection, Hellinger's, and χ 2  kernels, commonly used in computer vision, and enables their use in large scale problems. In particular, we: 1) provide explicit feature maps for all additive homogeneous kernels along with closed form expression for all common kernels; 2) derive corresponding approximate finite-dimensional feature maps based on a spectral analysis; and 3) quantify the error of the approximation, showing that the error is independent of the data dimension and decays exponentially fast with the approximation order for selected kernels such as χ 2 . We demonstrate that the …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Andrea Vedaldi, Andrew Zisserman",
            "전체 인용횟수": "1288회 인용201120122013201420152016201720182019202020212022202335112136168169198134858540332718",
            "페이지": "480-492",
            "학술 문서": "Efficient additive kernels via explicit feature mapsA Vedaldi, A Zisserman - IEEE transactions on pattern analysis and machine …, 20121288회 인용 관련 학술자료 전체 24개의 버전 ",
            "호": "3"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Efficient additive kernels via explicit feature maps",
        "year": null
    },
    "Deep filter banks for texture recognition and segmentation": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2015",
            "설명": "Research in texture recognition often concentrates on the problem of material recognition in uncluttered conditions, an assumption rarely met by applications. In this work we conduct a first study of material and describable texture attributes recognition in clutter, using a new dataset derived from the OpenSurface texture repository. Motivated by the challenge posed by this problem, we propose a new texture descriptor,\\dcnn, obtained by Fisher Vector pooling of a Convolutional Neural Network (CNN) filter bank.\\dcnn substantially improves the state-of-the-art in texture, material and scene recognition. Our approach achieves 79.8\\% accuracy on Flickr material dataset and 81\\% accuracy on MIT indoor scenes, providing absolute gains of more than 10\\% over existing approaches.\\dcnn easily transfers across domains without requiring feature adaptation as for methods that build on the fully-connected layers of CNNs. Furthermore,\\dcnn can seamlessly incorporate multi-scale information and describe regions of arbitrary shapes and sizes. Our approach is particularly suited at localizing``stuff''categories and obtains state-of-the-art results on MSRC segmentation dataset, as well as promising results on recognizing materials and surface attributes in clutter on the OpenSurfaces dataset.",
            "저자": "Mircea Cimpoi, Subhransu Maji, Andrea Vedaldi",
            "전체 인용횟수": "1130회 인용20132014201520162017201820192020202120222023333311118519615811611610073",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "3828-3836",
            "학술 문서": "Deep filter banks for texture recognition and segmentationM Cimpoi, S Maji, A Vedaldi - Proceedings of the IEEE conference on computer …, 20151035회 인용 관련 학술자료 전체 42개의 버전 Deep convolutional filter banks for texture recognition and segmentation*M Cimpoi, S Maji, A Vedaldi - arXiv preprint arXiv:1411.6836, 2014102회 인용 관련 학술자료 전체 4개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Deep filter banks for texture recognition and segmentation",
        "year": null
    },
    "Multiresolution gray-scale and rotation invariant texture classification with local binary patterns": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2002/7",
            "게시자": "IEEE",
            "권": "24",
            "설명": "Presents a theoretically very simple, yet efficient, multiresolution approach to gray-scale and rotation invariant texture classification based on local binary patterns and nonparametric discrimination of sample and prototype distributions. The method is based on recognizing that certain local binary patterns, termed \"uniform,\" are fundamental properties of local image texture and their occurrence histogram is proven to be a very powerful texture feature. We derive a generalized gray-scale and rotation invariant operator presentation that allows for detecting the \"uniform\" patterns for any quantization of the angular space and for any spatial resolution and presents a method for combining multiple operators for multiresolution analysis. The proposed approach is very robust in terms of gray-scale variations since the operator is, by definition, invariant against any monotonic transformation of the gray scale. Another …",
            "저널": "Pattern Analysis and Machine Intelligence, IEEE Transactions on",
            "저자": "Timo Ojala, Matti Pietikainen, Topi Maenpaa",
            "전체 인용횟수": "18775회 인용2004200520062007200820092010201120122013201420152016201720182019202020212022202351891241733063415477699621214148017031697169916051519132311951017757",
            "페이지": "971-987",
            "학술 문서": "Multiresolution gray-scale and rotation invariant texture classification with local binary patternsT Ojala, M Pietikainen, T Maenpaa - IEEE Transactions on pattern analysis and machine …, 200218775회 인용 관련 학술자료 전체 11개의 버전 ",
            "호": "7"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Multiresolution gray-scale and rotation invariant texture classification with local binary patterns",
        "year": null
    },
    "A comparative study of texture measures with classification based on featured distributions": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1996/1/1",
            "게시자": "Pergamon",
            "권": "29",
            "설명": "This paper evaluates the performance both of some texture measures which have been successfully used in various applications and of some new promising approaches proposed recently. For classification a method based on Kullback discrimination of sample and prototype distributions is used. The classification results for single features with one-dimensional feature value distributions and for pairs of complementary features with two-dimensional distributions are presented",
            "저널": "Pattern recognition",
            "저자": "Timo Ojala, Matti Pietikäinen, David Harwood",
            "전체 인용횟수": "9221회 인용20002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202325423934427574106153171256395480603726763842747771714605580497368",
            "페이지": "51-59",
            "학술 문서": "A comparative study of texture measures with classification based on featured distributionsT Ojala, M Pietikäinen, D Harwood - Pattern recognition, 19969221회 인용 관련 학술자료 전체 11개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A comparative study of texture measures with classification based on featured distributions",
        "year": null
    },
    "Face description with local binary patterns: Application to face recognition": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006/10/30",
            "게시자": "IEEE",
            "권": "28",
            "설명": "This paper presents a novel and efficient facial image representation based on local binary pattern (LBP) texture features. The face image is divided into several regions from which the LBP feature distributions are extracted and concatenated into an enhanced feature vector to be used as a face descriptor. The performance of the proposed method is assessed in the face recognition problem under different challenges. Other applications and several extensions are also discussed",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Timo Ahonen, Abdenour Hadid, Matti Pietikainen",
            "전체 인용횟수": "7372회 인용200720082009201020112012201320142015201620172018201920202021202220234883131203295437555685773705730669592472400289203",
            "페이지": "2037-2041",
            "학술 문서": "Face description with local binary patterns: Application to face recognitionT Ahonen, A Hadid, M Pietikainen - IEEE transactions on pattern analysis and machine …, 20067273회 인용 관련 학술자료 전체 18개의 버전 Face description with local binary patterns: Application to face recognitionA Timo, H Abdenour, P Matti - IEEE transactions on pattern analysis and machine …, 2006112회 인용 관련 학술자료 Face description with local binary patterns: Application to face recognitionA Hadid - IEEE Trans. Pattern Analysis and Machine Intelligence, 200651회 인용 관련 학술자료 ",
            "호": "12"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Face description with local binary patterns: Application to face recognition",
        "year": null
    },
    "Face recognition with local binary patterns": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2004",
            "게시자": "Springer Berlin Heidelberg",
            "설명": " In this work, we present a novel approach to face recognition which considers both shape and texture information to represent face images. The face area is first divided into small regions from which Local Binary Pattern (LBP) histograms are extracted and concatenated into a single, spatially enhanced feature histogram efficiently representing the face image. The recognition is performed using a nearest neighbour classifier in the computed feature space with Chi square as a dissimilarity measure. Extensive experiments clearly show the superiority of the proposed scheme over all considered methods (PCA, Bayesian Intra/extrapersonal Classifier and Elastic Bunch Graph Matching) on FERET tests which include testing the robustness of the method against different facial expressions, lighting and aging of the subjects. In addition to its efficiency, the simplicity of the proposed method allows for very fast …",
            "저자": "Timo Ahonen, Abdenour Hadid, Matti Pietikäinen",
            "전체 인용횟수": "3360회 인용2004200520062007200820092010201120122013201420152016201720182019202020212022202311293253809514418423426232231229324424521017516914682",
            "컨퍼런스": "Computer Vision-ECCV 2004: 8th European Conference on Computer Vision, Prague, Czech Republic, May 11-14, 2004. Proceedings, Part I 8",
            "페이지": "469-481",
            "학술 문서": "Face recognition with local binary patternsT Ahonen, A Hadid, M Pietikäinen - Computer Vision-ECCV 2004: 8th European …, 20043344회 인용 관련 학술자료 전체 12개의 버전 Face recognition with local binary patterns. Computer Vision, ECCV 2004 Proceedings*T Ahonen, A Hadid, M Pietikäinen - Lecture Notes in Computer Science, 200431회 인용 관련 학술자료 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Face recognition with local binary patterns",
        "year": null
    },
    "Dynamic texture recognition using local binary patterns with an application to facial expressions": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2007/4/23",
            "게시자": "IEEE",
            "권": "29",
            "설명": "Dynamic texture (DT) is an extension of texture to the temporal domain. Description and recognition of DTs have attracted growing attention. In this paper, a novel approach for recognizing DTs is proposed and its simplifications and extensions to facial image analysis are also considered. First, the textures are modeled with volume local binary patterns (VLBP), which are an extension of the LBP operator widely used in ordinary texture analysis, combining motion and appearance. To make the approach computationally simple and easy to extend, only the co-occurrences of the local binary patterns on three orthogonal planes (LBP-TOP) are then considered. A block-based method is also proposed to deal with specific dynamic events such as facial expressions in which local information and its spatial locations should also be taken into account. In experiments with two DT databases, DynTex and Massachusetts …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Guoying Zhao, Matti Pietikainen",
            "전체 인용횟수": "3288회 인용2007200820092010201120122013201420152016201720182019202020212022202316516480102147213248315299269291263267230217175",
            "페이지": "915-928",
            "학술 문서": "Dynamic texture recognition using local binary patterns with an application to facial expressionsG Zhao, M Pietikainen - IEEE transactions on pattern analysis and machine …, 20073288회 인용 관련 학술자료 전체 17개의 버전 ",
            "호": "6"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Dynamic texture recognition using local binary patterns with an application to facial expressions",
        "year": null
    },
    "Adaptive document image binarization": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2000/2/1",
            "게시자": "Pergamon",
            "권": "33",
            "설명": "A new method is presented for adaptive document image binarization, where the page is considered as a collection of subcomponents such as text, background and picture. The problems caused by noise, illumination and many source type-related degradations are addressed. Two new algorithms are applied to determine a local threshold for each pixel. The performance evaluation of the algorithm utilizes test images with ground-truth, evaluation metrics for binarization of textual and synthetic images, and a weight-based ranking procedure for the final result presentation. The proposed algorithms were tested with images including different types of document components and degradations. The results were compared with a number of known techniques in the literature. The benchmarking results show that the method adapts and performs well in each case qualitatively and quantitatively.",
            "저널": "Pattern recognition",
            "저자": "Jaakko Sauvola, Matti Pietikäinen",
            "전체 인용횟수": "3009회 인용200320042005200620072008200920102011201220132014201520162017201820192020202120222023131732394554101118145150157222220187223227236238206161167",
            "페이지": "225-236",
            "학술 문서": "Adaptive document image binarizationJ Sauvola, M Pietikäinen - Pattern recognition, 20003009회 인용 관련 학술자료 전체 10개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Adaptive document image binarization",
        "year": null
    },
    "A texture-based method for modeling the background and detecting moving objects": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2006/2/21",
            "게시자": "IEEE",
            "권": "28",
            "설명": "This paper presents a novel and efficient texture-based method for modeling the background and detecting moving objects from a video sequence. Each pixel is modeled as a group of adaptive local binary pattern histograms that are calculated over a circular region around the pixel. The approach provides us with many advantages compared to the state-of-the-art. Experimental results clearly justify our model.",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Marko Heikkila, Matti Pietikainen",
            "전체 인용횟수": "1779회 인용2006200720082009201020112012201320142015201620172018201920202021202220231826537710512214914716914814914312310781664825",
            "페이지": "657-662",
            "학술 문서": "A texture-based method for modeling the background and detecting moving objectsM Heikkila, M Pietikainen - IEEE transactions on pattern analysis and machine …, 20061779회 인용 관련 학술자료 전체 16개의 버전 ",
            "호": "4"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A texture-based method for modeling the background and detecting moving objects",
        "year": null
    },
    "Performance evaluation of texture measures with classification based on Kullback discrimination of distributions": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1994/10/9",
            "게시자": "IEEE",
            "권": "1",
            "설명": "This paper evaluates the performance both of some texture measures which have been successfully used in various applications and of some new promising approaches. For classification a method based on Kullback discrimination of sample and prototype distributions is used. The classification results for single features with one-dimensional feature value distributions and for pairs of complementary features with two-dimensional distributions are presented.",
            "저자": "Timo Ojala, Matti Pietikainen, David Harwood",
            "전체 인용횟수": "1779회 인용20112012201320142015201620172018201920202021202220232466103139177171154172150165169155101",
            "컨퍼런스": "Proceedings of 12th international conference on pattern recognition",
            "페이지": "582-585",
            "학술 문서": "Performance evaluation of texture measures with classification based on Kullback discrimination of distributionsT Ojala, M Pietikainen, D Harwood - Proceedings of 12th international conference on …, 19941779회 인용 관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Performance evaluation of texture measures with classification based on Kullback discrimination of distributions",
        "year": null
    },
    "WLD: A robust local image descriptor": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2009/8/18",
            "게시자": "IEEE",
            "권": "32",
            "설명": "Inspired by Weber's Law, this paper proposes a simple, yet very powerful and robust local descriptor, called the Weber Local Descriptor (WLD). It is based on the fact that human perception of a pattern depends not only on the change of a stimulus (such as sound, lighting) but also on the original intensity of the stimulus. Specifically, WLD consists of two components: differential excitation and orientation. The differential excitation component is a function of the ratio between two terms: One is the relative intensity differences of a current pixel against its neighbors, the other is the intensity of the current pixel. The orientation component is the gradient orientation of the current pixel. For a given image, we use the two components to construct a concatenated WLD histogram. Experimental results on the Brodatz and KTH-TIPS2-a texture databases show that WLD impressively outperforms the other widely used descriptors …",
            "저널": "IEEE transactions on pattern analysis and machine intelligence",
            "저자": "Jie Chen, Shiguang Shan, Chu He, Guoying Zhao, Matti Pietikäinen, Xilin Chen, Wen Gao",
            "전체 인용횟수": "1310회 인용201020112012201320142015201620172018201920202021202220231236699013014116914112310581916640",
            "페이지": "1705-1720",
            "학술 문서": "WLD: A robust local image descriptorJ Chen, S Shan, C He, G Zhao, M Pietikäinen, X Chen… - IEEE transactions on pattern analysis and machine …, 20091253회 인용 관련 학술자료 전체 15개의 버전 A robust descriptor based on weber’s law*J Chen, S Shan, G Zhao, X Chen, W Gao… - 2008 IEEE Conference on Computer Vision and …, 200880회 인용 관련 학술자료 전체 9개의 버전 ",
            "호": "9"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "WLD: A robust local image descriptor",
        "year": null
    },
    "Computer vision using local binary patterns": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/6/21",
            "게시자": "Springer Science & Business Media",
            "권": "40",
            "설명": "The recent emergence of Local Binary Patterns (LBP) has led to significant progress in applying texture methods to various computer vision problems and applications. The focus of this research has broadened from 2D textures to 3D textures and spatiotemporal (dynamic) textures. Also, where texture was once utilized for applications such as remote sensing, industrial inspection and biomedical image analysis, the introduction of LBP-based approaches have provided outstanding results in problems relating to face and activity analysis, with future scope for face and facial expression recognition, biometrics, visual surveillance and video analysis. Computer Vision Using Local Binary Patterns provides a detailed description of the LBP methods and their variants both in spatial and spatiotemporal domains. This comprehensive reference also provides an excellent overview as to how texture methods can be utilized for solving different kinds of computer vision and image analysis problems. Source codes of the basic LBP algorithms, demonstrations, some databases and a comprehensive LBP bibliography can be found from an accompanying web site. Topics include: local binary patterns and their variants in spatial and spatiotemporal domains, texture classification and segmentation, description of interest regions, applications in image retrieval and 3D recognition-Recognition and segmentation of dynamic textures, background subtraction, recognition of actions, face analysis using still images and image sequences, visual speech recognition and LBP in various applications. Written by pioneers of LBP, this book is an essential resource for …",
            "저자": "Matti Pietikäinen, Abdenour Hadid, Guoying Zhao, Timo Ahonen",
            "전체 인용횟수": "1270회 인용201220132014201520162017201820192020202120222023328412616016513815413194715341",
            "학술 문서": "Computer vision using local binary patternsM Pietikäinen, A Hadid, G Zhao, T Ahonen - 20111029회 인용 관련 학술자료 전체 12개의 버전 Local binary patterns for still images*M Pietikäinen, A Hadid, G Zhao, T Ahonen… - Computer vision using local binary patterns, 2011282회 인용 관련 학술자료 전체 7개의 버전 Computer vision using local binary patterns*A Hadid, MK Pietikainen, G Zhao, T Ahonen - 201127회 인용 관련 학술자료 Lbp in different applications*M Pietikäinen, A Hadid, G Zhao, T Ahonen… - Computer Vision Using Local Binary Patterns, 20115회 인용 관련 학술자료 전체 3개의 버전 Texture classification and segmentation*M Pietikäinen, A Hadid, G Zhao, T Ahonen… - Computer Vision Using Local Binary Patterns, 20114회 인용 관련 학술자료 전체 3개의 버전 Face Analysis Using Image Sequences*M Pietikäinen, A Hadid, G Zhao, T Ahonen… - Computer Vision Using Local Binary Patterns, 2011관련 학술자료 전체 3개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Computer vision using local binary patterns",
        "year": null
    },
    "Outex-new framework for empirical evaluation of texture analysis algorithms": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2002/8/11",
            "게시자": "IEEE",
            "권": "1",
            "설명": "This paper presents the current status of a new initiative aimed at developing a versatile framework and image database for empirical evaluation of texture analysis algorithms. The proposed Outex framework contains a large collection of surface textures captured under different conditions, which facilitates construction of a wide range of texture analysis problems. The problems are encapsulated into test suites, for which baseline results obtained with algorithms from literature are provided. The rich functionality of the framework is demonstrated with examples in texture classification, segmentation and retrieval. The framework has a web site for public dissemination of the database and comparative results obtained by research groups world wide.",
            "저자": "Timo Ojala, Topi Maenpaa, Matti Pietikainen, Jaakko Viertola, Juha Kyllonen, Sami Huovinen",
            "전체 인용횟수": "863회 인용200220032004200520062007200820092010201120122013201420152016201720182019202020212022202349825172419183944556758676470625846413224",
            "컨퍼런스": "2002 international conference on pattern recognition",
            "페이지": "701-706",
            "학술 문서": "Outex-new framework for empirical evaluation of texture analysis algorithmsT Ojala, T Maenpaa, M Pietikainen, J Viertola… - 2002 international conference on pattern recognition, 2002863회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Outex-new framework for empirical evaluation of texture analysis algorithms",
        "year": null
    },
    "Gray scale and rotation invariant texture classification with local binary patterns": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2000",
            "게시자": "Springer Berlin Heidelberg",
            "설명": " This paper presents a theoretically very simple yet efficient approach for gray scale and rotation invariant texture classification based on local binary patterns and nonparametric discrimination of sample and prototype distributions. The proposed approach is very robust in terms of gray scale variations, since the operators are by definition invariant against any monotonic transformation of the gray scale. Another advantage is computational simplicity, as the operators can be realized with a few operations in a small neighborhood and a lookup table. Excellent experimental results obtained in two true problems of rotation invariance, where the classifier is trained at one particular rotation angle and tested with samples from other rotation angles, demonstrate that good discrimination can be achieved with the statistics of simple rotation invariant local binary patterns. These operators characterize the spatial …",
            "저자": "Timo Ojala, Matti Pietikäinen, Topi Mäenpää",
            "전체 인용횟수": "803회 인용20002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202324216712139223327313342325373668074786733",
            "컨퍼런스": "Computer Vision-ECCV 2000: 6th European Conference on Computer Vision Dublin, Ireland, June 26–July 1, 2000 Proceedings, Part I 6",
            "페이지": "404-420",
            "학술 문서": "Gray scale and rotation invariant texture classification with local binary patternsT Ojala, M Pietikäinen, T Mäenpää - Computer Vision-ECCV 2000: 6th European …, 2000803회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Gray scale and rotation invariant texture classification with local binary patterns",
        "year": null
    },
    "Face spoofing detection from single images using micro-texture analysis": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/10/11",
            "게시자": "IEEE",
            "설명": "Current face biometric systems are vulnerable to spoo ing attacks. A spoofing attack occurs when a person tries to masquerade as someone else by falsifying data and thereby gaining illegitimate access. Inspired by image quality assessment, characterization of printing artifacts, and differences in light reflection, we propose to approach the problem of spoofing detection from texture analysis point of view. Indeed, face prints usually contain printing quality defects that can be well detected using texture features. Hence, we present a novel approach based on analyzing facial image textures for detecting whether there is a live person in front of the camera or a face print. The proposed approach analyzes the texture of the facial images using multi-scale local binary patterns (LBP). Compared to many previous works, our proposed approach is robust, computationally fast and does not require user-cooperation. In addition …",
            "저자": "Jukka Määttä, Abdenour Hadid, Matti Pietikäinen",
            "전체 인용횟수": "785회 인용201220132014201520162017201820192020202120222023625314783787110492889561",
            "컨퍼런스": "2011 international joint conference on Biometrics (IJCB)",
            "페이지": "1-7",
            "학술 문서": "Face spoofing detection from single images using micro-texture analysisJ Määttä, A Hadid, M Pietikäinen - 2011 international joint conference on Biometrics (IJCB …, 2011785회 인용 관련 학술자료 전체 13개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Face spoofing detection from single images using micro-texture analysis",
        "year": null
    },
    "Facial expression recognition from near-infrared videos": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2011/8/1",
            "게시자": "Elsevier",
            "권": "29",
            "설명": "Facial expression recognition is to determine the emotional state of the face regardless of its identity. Most of the existing datasets for facial expressions are captured in a visible light spectrum. However, the visible light (VIS) can change with time and location, causing significant variations in appearance and texture. In this paper, we present a novel research on a dynamic facial expression recognition, using near-infrared (NIR) video sequences and LBP-TOP (Local binary patterns from three orthogonal planes) feature descriptors. NIR imaging combined with LBP-TOP features provide an illumination invariant description of face video sequences. Appearance and motion features in slices are used for expression classification, and for this, discriminative weights are learned from training examples. Furthermore, component-based facial features are presented to combine geometric and appearance information …",
            "저널": "Image and vision computing",
            "저자": "Guoying Zhao, Xiaohua Huang, Matti Taini, Stan Z Li, Matti PietikäInen",
            "전체 인용횟수": "700회 인용20122013201420152016201720182019202020212022202341011817275889101131129109",
            "페이지": "607-619",
            "학술 문서": "Facial expression recognition from near-infrared videosG Zhao, X Huang, M Taini, SZ Li, M PietikäInen - Image and vision computing, 2011700회 인용 관련 학술자료 전체 6개의 버전 ",
            "호": "9"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Facial expression recognition from near-infrared videos",
        "year": null
    },
    "Rotation-invariant texture classification using feature distributions": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2000/1/1",
            "게시자": "Pergamon",
            "권": "33",
            "설명": "A distribution-based classification approach and a set of recently developed texture measures are applied to rotation-invariant texture classification. The performance is compared to that obtained with the well-known circular-symmetric autoregressive random field (CSAR) model approach. A difficult classification problem of 15 different Brodatz textures and seven rotation angles is used in experiments. The results show much better performance for our approach than for the CSAR features. A detailed analysis of the confusion matrices and the rotation angles of misclassified samples produces several interesting observations about the classification problem and the features used in this study.",
            "저널": "Pattern recognition",
            "저자": "Matti Pietikäinen, Timo Ojala, Zelin Xu",
            "전체 인용횟수": "625회 인용20002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202366991014101015152128315541425354453639272518",
            "페이지": "43-52",
            "학술 문서": "Rotation-invariant texture classification using feature distributionsM Pietikäinen, T Ojala, Z Xu - Pattern recognition, 2000625회 인용 관련 학술자료 전체 11개의 버전 ",
            "호": "1"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Rotation-invariant texture classification using feature distributions",
        "year": null
    },
    "An experimental comparison of autoregressive and Fourier-based descriptors in 2D shape classification": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "1995/2",
            "게시자": "IEEE",
            "권": "17",
            "설명": "An experimental comparison of shape classification methods based on autoregressive modeling and Fourier descriptors of closed contours is carried out. The performance is evaluated using two independent sets of data: images of letters and airplanes. Silhouette contours are extracted from non-occluded 2D objects rotated, scaled, and translated in 3D space. Several versions of both types of methods are implemented and tested systematically. The comparison clearly shows better performance of Fourier-based methods, especially for images containing noise.< >",
            "저널": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "저자": "Hannu Kauppinen, Tapio Seppanen, Matti Pietikainen",
            "전체 인용횟수": "619회 인용1996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023847111618132234334534483732282432243016202217121075",
            "페이지": "201-207",
            "학술 문서": "An experimental comparison of autoregressive and Fourier-based descriptors in 2D shape classificationH Kauppinen, T Seppanen, M Pietikainen - IEEE Transactions on Pattern Analysis and Machine …, 1995619회 인용 관련 학술자료 전체 12개의 버전 ",
            "호": "2"
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "An experimental comparison of autoregressive and Fourier-based descriptors in 2D shape classification",
        "year": null
    },
    "Remote heart rate measurement from face videos under realistic situations": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2014",
            "설명": "Heart rate is an important indicator of people's physiological state. Recently, several papers reported methods to measure heart rate remotely from face videos. Those methods work well on stationary subjects under well controlled conditions, but their performance significantly degrades if the videos are recorded under more challenging conditions, specifically when subjects' motions and illumination variations are involved. We propose a framework which utilizes face tracking and Normalized Least Mean Square adaptive filtering methods to counter their influences. We test our framework on a large difficult and public database MAHNOB-HCI and demonstrate that our method substantially outperforms all previous methods. We also use our method for long term heart rate monitoring in a game evaluation scenario and achieve promising results.",
            "저자": "Xiaobai Li, Jie Chen, Guoying Zhao, Matti Pietikainen",
            "전체 인용횟수": "592회 인용201520162017201820192020202120222023224050686389869767",
            "컨퍼런스": "Proceedings of the IEEE conference on computer vision and pattern recognition",
            "페이지": "4264-4271",
            "학술 문서": "Remote heart rate measurement from face videos under realistic situationsX Li, J Chen, G Zhao, M Pietikainen - Proceedings of the IEEE conference on computer …, 2014592회 인용 관련 학술자료 전체 11개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "Remote heart rate measurement from face videos under realistic situations",
        "year": null
    },
    "A spontaneous micro-expression database: Inducement, collection and baseline": {
        "DOI": null,
        "abstract": null,
        "authors": null,
        "cite_bibtex": null,
        "conference": null,
        "crossref_json": null,
        "google_schorlar_metadata": {
            "게시 날짜": "2013/4/22",
            "게시자": "IEEE",
            "설명": "Micro-expressions are short, involuntary facial expressions which reveal hidden emotions. Micro-expressions are important for understanding humans' deceitful behavior. Psychologists have been studying them since the 1960's. Currently the attention is elevated in both academic fields and in media. However, while general facial expression recognition (FER) has been intensively studied for years in computer vision, little research has been done in automatically analyzing micro-expressions. The biggest obstacle to date has been the lack of a suitable database. In this paper we present a novel Spontaneous Micro-expression Database SMIC, which includes 164 micro-expression video clips elicited from 16 participants. Micro-expression detection and recognition performance are provided as baselines. SMIC provides sufficient source material for comprehensive testing of automatic systems for analyzing micro …",
            "저자": "Xiaobai Li, Tomas Pfister, Xiaohua Huang, Guoying Zhao, Matti Pietikäinen",
            "전체 인용횟수": "571회 인용201320142015201620172018201920202021202220233122224424664629011882",
            "컨퍼런스": "2013 10th IEEE International Conference and Workshops on Automatic face and gesture recognition (fg)",
            "페이지": "1-6",
            "학술 문서": "A spontaneous micro-expression database: Inducement, collection and baselineX Li, T Pfister, X Huang, G Zhao, M Pietikäinen - 2013 10th IEEE International Conference and …, 2013571회 인용 관련 학술자료 전체 9개의 버전 "
        },
        "journal": null,
        "reference_list": null,
        "referenced_list": null,
        "title": "A spontaneous micro-expression database: Inducement, collection and baseline",
        "year": null
    }
}