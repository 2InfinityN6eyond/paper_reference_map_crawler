{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from containers import Paper, Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "def fillCrossRefMetaData(paper):\n",
    "    # Fill in the paper metadata from the CrossRef API\n",
    "\n",
    "    title = paper.title\n",
    "    #author = paper.author\n",
    "\n",
    "    encoded_title = urllib.parse.quote(title)\n",
    "    api_url = f\"https://api.crossref.org/works?query.title={encoded_title}\"\n",
    "    response = requests.get(api_url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        items = data['message'].get('items', [])\n",
    "        if items:\n",
    "            # Assuming the first result is the correct one\n",
    "            #return items[0].get('DOI')\n",
    "            paper.doi = items[0].get('DOI')\n",
    "    \n",
    "            doi_api_url = f\"https://api.crossref.org/works/{doi}\"\n",
    "            metadata_response = requests.get(doi_api_url)\n",
    "            if metadata_response.status_code == 200:\n",
    "                data = metadata_response.json()\n",
    "                reference_list = data['message'].get('reference', [])\n",
    "                paper.reference_list = reference_list\n",
    "            else :\n",
    "                paper.reference_list = False\n",
    "        else :\n",
    "            paper.doi = False\n",
    "    else :\n",
    "        paper.doi = False\n",
    "\n",
    "\n",
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "class CrossRefFetcher :\n",
    "    def __init__(self) :\n",
    "        pass\n",
    "\n",
    "    def fetchMetaDatafromTitle(self, paper) :\n",
    "        '''\n",
    "        args :\n",
    "            paper : Paper\n",
    "                expect paper.title\n",
    "        '''\n",
    "        title = urllib.parse.quote(paper.title)\n",
    "        url = f'https://api.crossref.org/works?query.bibliographic={title}&rows=1'\n",
    "\n",
    "        try :\n",
    "            r = requests.get(url)\n",
    "            metadata = r.json()['message']['items'][0]\n",
    "            if len(metadata) == 0 :\n",
    "                paper.DOI = False\n",
    "                paper.reference_list = False\n",
    "                return None\n",
    "            reference_list = []\n",
    "            try :\n",
    "                paper.DOI = metadata['DOI']\n",
    "                for reference in metadata['reference'] :\n",
    "                    if 'DOI' in reference :\n",
    "                        reference_list.append(reference['DOI'])\n",
    "            except :\n",
    "                pass\n",
    "            paper.crossref_json = metadata\n",
    "            paper.reference_list = reference_list\n",
    "        \n",
    "        except Exception as e :\n",
    "            paper.DOI = False\n",
    "            paper.crossref_json = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole paper dict size : 3245, paper to process : 2605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 2497/2605 [51:51<02:05,  1.16s/it] "
     ]
    }
   ],
   "source": [
    "whole_author_list = []\n",
    "whole_paper_dict = {}\n",
    "\n",
    "\n",
    "AUTHOR_FILE_PATH = \"./author_list.json\"\n",
    "if os.path.exists(AUTHOR_FILE_PATH) :\n",
    "    with open(AUTHOR_FILE_PATH, \"r\") as f :\n",
    "        author_list_raw = json.load(f)\n",
    "    for author in author_list_raw :\n",
    "        whole_author_list.append(Author(**author))\n",
    "\n",
    "\n",
    "\n",
    "WHOLE_PAPER_FILE_PATH = \"./whole_paper_dict.json\"\n",
    "if os.path.exists(WHOLE_PAPER_FILE_PATH) :\n",
    "    with open(WHOLE_PAPER_FILE_PATH, \"r\") as f :\n",
    "        whole_paper_dict = json.load(f)\n",
    "    for k, v in whole_paper_dict.items() :\n",
    "        whole_paper_dict[k] = Paper(**v)\n",
    "\n",
    "PROCESSED_PAPER_FILE_PATH = \"./processed_paper_dict.json\"\n",
    "if os.path.exists(PROCESSED_PAPER_FILE_PATH) :\n",
    "    with open(PROCESSED_PAPER_FILE_PATH, \"r\") as f :\n",
    "        processed_paper_dict = json.load(f)\n",
    "    for k, v in processed_paper_dict.items() :\n",
    "        processed_paper_dict[k] = Paper(**v)\n",
    "\n",
    "def checkAlreadyProcessed(key) :\n",
    "    return (\n",
    "        key in processed_paper_dict\n",
    "    ) and (\n",
    "        processed_paper_dict[key].DOI is not False\n",
    "    ) and (\n",
    "        processed_paper_dict[key].DOI is not None\n",
    "    )\n",
    "        \n",
    "paper_to_process_keys_list = list(filter(\n",
    "    lambda key : not checkAlreadyProcessed(key),\n",
    "    whole_paper_dict.keys()\n",
    "))\n",
    "\n",
    "print(f\"whole paper dict size : {len(whole_paper_dict)}, paper to process : {len(paper_to_process_keys_list)}\")\n",
    "\n",
    "crossref_fetcher = CrossRefFetcher()\n",
    "for key in tqdm(paper_to_process_keys_list) :\n",
    "    paper = whole_paper_dict[key]\n",
    "    crossref_fetcher.fetchMetaDatafromTitle(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Deep residual learning for image recognition', 'Faster r-cnn: Towards real-time object detection with region proposal networks', 'Delving deep into rectifiers: Surpassing human-level performance on imagenet classification', 'Spatial pyramid pooling in deep convolutional networks for visual recognition', 'Identity mappings in deep residual networks', 'Single image haze removal using dark channel prior', 'Guided image filtering', 'Shufflenet: An extremely efficient convolutional neural network for mobile devices', 'R-fcn: Object detection via region-based fully convolutional networks', 'Shufflenet v2: Practical guidelines for efficient cnn architecture design', 'Learning to detect a salient object', 'Convolutional neural networks at constrained time cost', 'Channel pruning for accelerating very deep neural networks', 'Yolox: Exceeding yolo series in 2021', 'Stereo matching using belief propagation', 'Lazy snapping', 'Large kernel matters--improve semantic segmentation by global convolutional network', 'Instance-aware semantic segmentation via multi-task network cascades', 'Saliency optimization from robust background detection', 'Image super-resolution using gradient profile prior', 'Deep ADMM-Net for Compressive Sensing MRI', 'Learning a convolutional neural network for non-uniform motion blur removal', 'Image inpainting by patch propagation using patch sparsity', 'ADMM-CSNet: A Deep Learning Approach for Image Compressive Sensing', 'Gradient profile prior and its applications in image super-resolution and enhancement', 'Proximal Dehaze-Net: A Prior Learning-Based Deep Network for Single Image Dehazing', 'Multimodal 2D+3D Facial Expression Recognition with Deep Fusion Convolutional Neural Network', 'View-GCN: View-Based Graph Convolutional Network for 3D Shape Analysis', 'Learning discriminative part detectors for image classification and cosegmentation', 'Unpaired Brain MR-to-CT Synthesis Using a Structure-Constrained CycleGAN', 'Total Variation Regularized RPCA for Background Subtraction from Compressive Measurements', 'A Novel Tensor Robust PCA Approach for Background Subtraction from Compressive Measurements', 'BM3D-Net: A Convolutional Neural Network for Transform-Domain Collaborative Filtering', 'Fast image deconvolution using closed-form thresholding formulas of Lq (q= 12, 23) regularization', 'Finding matches in a haystack: A max-pooling strategy for graph matching in the presence of outliers', 'Context-constrained hallucination for image super-resolution', 'Optimizing a Parameterized Plug-and-Play ADMM for Iterative Low-Dose CT Reconstruction', 'Spherical Space Domain Adaptation With Robust Pseudo-Label Loss', 'Learning to estimate and remove non-uniform image blur', 'Multi-Scale Context Aggregation by Dilated Convolutions', 'An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling', 'CARLA: An Open Urban Driving Simulator', 'Efficient inference in fully connected CRFs with Gaussian edge potentials', 'Direct Sparse Odometry', 'Playing for Data: Ground Truth from Computer Games', 'Dilated Residual Networks', 'Open3D: A Modern Library for 3D Data Processing', 'Guided Policy Search', 'Point Transformer', 'Learning to See in the Dark', 'Habitat: A Platform for Embodied AI Research', 'End-to-end Driving via Conditional Imitation Learning', 'Photographic Image Synthesis with Cascaded Refinement Networks', 'Learning Agile and Dynamic Motor Skills for Legged Robots', 'Vision Transformers for Dense Prediction', 'Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer', 'Multi-Task Learning as Multi-Objective Optimization', 'Tracking Objects as Points', 'Tanks and Temples: Benchmarking Large-Scale Scene Reconstruction', 'Rich feature hierarchies for accurate object detection and semantic segmentation', 'Normalized cuts and image segmentation', 'Scale-space and edge detection using anisotropic diffusion', 'A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics', 'Shape matching and object recognition using shape contexts', 'Contour detection and hierarchical image segmentation', 'Recovering high dynamic range radiance maps from photographs', 'Modeling and rendering architecture from photographs: A hybrid geometry-and image-based approach', 'Learning to detect natural image boundaries using local brightness, color, and texture cues', 'Region-based convolutional networks for accurate object detection and segmentation', 'Slowfast networks for video recognition', 'Learning a classification model for segmentation', 'Representing and recognizing the visual appearance of materials using three-dimensional textons', 'Blobworld: Image segmentation using expectation-maximization and its application to image querying', 'Multiscale vision transformers', 'Hypercolumns for object segmentation and fine-grained localization', 'Learning rich features from RGB-D images for object detection and segmentation', 'Recognizing action at a distance', 'Spectral grouping using the nystrom method', 'Large displacement optical flow: descriptor matching in variational motion estimation', 'Pointnet: Deep learning on point sets for 3d classification and segmentation', 'Pointnet++: Deep hierarchical feature learning on point sets in a metric space', \"The earth mover's distance as a metric for image retrieval\", 'Shapenet: An information-rich 3d model repository', 'Frustum pointnets for 3d object detection from rgb-d data', 'A metric for distributions with applications to image databases', 'Primitives for the manipulation of general subdivisions and the computation of Voronoi', 'Wireless sensor networks: an information processing approach', 'Kpconv: Flexible and deformable convolution for point clouds', 'A point set generation network for 3d object reconstruction from a single image', 'A concise and provably informative multi‐scale signature based on heat diffusion', 'Handbook of discrete and computational geometry', 'Volumetric and multi-view cnns for object classification on 3d data', 'Deep knowledge tracing', 'Learning representations and generative models for 3d point clouds', 'Taskonomy: Disentangling task transfer learning', 'A dichromatic framework for balanced trees', 'Deep hough voting for 3d object detection in point clouds', 'A scalable active framework for region annotation in 3d shape collections', 'Locating and bypassing holes in sensor networks', 'Image-to-image translation with conditional adversarial networks', 'Unpaired image-to-image translation using cycle-consistent adversarial networks', 'The unreasonable effectiveness of deep features as a perceptual metric', 'Context Encoders: Feature Learning by Inpainting', 'Texture synthesis by non-parametric sampling', 'Colorful Image Colorization', 'Image quilting for texture synthesis and transfer', 'Cycada: Cycle-consistent adversarial domain adaptation', 'Unsupervised visual representation learning by context prediction', 'Unbiased look at dataset bias', 'Curiosity-driven exploration by self-supervised prediction', 'Discovering objects and their location in images', 'Toward Multimodal Image-to-Image Translation', 'Generative visual manipulation on the natural image manifold', 'Scene completion using millions of photographs', 'Ensemble of Exemplar-SVMs for Object Detection and Beyond', 'IM2GPS: estimating geographic information from a single image', 'Automatic photo pop-up', 'Geometric context from a single image', 'A taxonomy and evaluation of dense two-frame stereo correspondence algorithms', 'Computer vision: algorithms and applications', 'Photo tourism: exploring photo collections in 3D', 'The lumigraph', 'A comparison and evaluation of multi-view stereo reconstruction algorithms', 'A database and evaluation methodology for optical flow', 'Modeling the world from internet photo collections', 'Image alignment and stitching: A tutorial', 'Building rome in a day', 'High-accuracy stereo depth maps using structured light', 'High-quality video view interpolation using a layered representation', 'Layered depth images', 'Edge-preserving decompositions for multi-scale tone and detail manipulation', 'Fast surface interpolation using hierarchical basis functions', 'Digital photography with flash and no-flash image pairs', 'Creating full view panoramic image mosaics and environment maps', 'Video mosaics for virtual environments', 'Locally adapted hierarchical basis preconditioning', 'A comparative study of energy minimization methods for markov random fields with smoothness-based priors', 'Synthesizing realistic facial expressions from photographs', 'Microsoft coco: Common objects in context', 'Vqa: Visual question answering', 'Cider: Consensus-based image description evaluation', 'Edge boxes: Locating object proposals from edges', 'Clevr: A diagnostic dataset for compositional language and elementary visual reasoning', 'Microsoft coco captions: Data collection and evaluation server', 'From captions to visual concepts and back', 'Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks', 'Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences', 'Structured forests for fast edge detection', 'Fast edge detection using structured forests', 'Shuffle and learn: unsupervised learning using temporal order verification', 'A cooperative algorithm for stereo matching and occlusion detection', 'fastMRI: An open dataset and benchmarks for accelerated MRI', 'Automatic estimation and removal of noise from a single image', \"Mind's eye: A recurrent visual representation for image caption generation\", 'Inferring and executing programs for visual reasoning', 'Sketch tokens: A learned mid-level representation for contour and object detection', 'Human attention in visual question answering: Do humans and deep networks look at the same regions?', 'SMPL: A skinned multi-person linear model', 'The robust estimation of multiple motions: Parametric and piecewise-smooth flow fields', 'A naturalistic open source movie for optical flow evaluation', 'Secrets of optical flow estimation and their principles', 'Robust anisotropic diffusion', 'HumanEva: Synchronized Video and Motion Capture Dataset and Baseline Algorithm for Evaluation of Articulated Human\\xa0Motion', 'End-to-end recovery of human shape and pose', 'Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image', 'Eigentracking: Robust matching and tracking of articulated objects using a view-based representation', 'Fields of experts: A framework for learning image priors', 'Optical flow estimation using a spatial pyramid network', 'Expressive body capture: 3d hands, face, and body from a single image', 'On the unification of line processes, outlier rejection, and robust statistics with applications in early vision', 'Stochastic tracking of 3D human figures using 2D image motion', 'Learning from synthetic humans', 'Fields of experts', 'On human motion prediction using recurrent neural networks', 'Towards understanding action recognition', 'Learning to reconstruct 3D human pose and shape via model-fitting in the loop', 'Snakes: Active contour models', 'Elastically deformable models', 'Deformable models in medical image analysis: a survey', 'Image segmentation using deep learning: A survey', 'Realistic modeling for facial animation', 'Dynamic 3D models with local and global deformations: Deformable superquadrics', 'Regularization of inverse visual problems involving discontinuities', 'Multilinear analysis of image ensembles: Tensorfaces', 'Constraints on deformable models: Recovering 3D shape and nonrigid motion', \"Artificial fishes: physics, locomotion, perception, behaviour. SIGGRAPH'94\", 'Deformable models', 'Modeling inelastic deformation: viscolelasticity, plasticity, fracture', 'Physically‐based facial modelling, analysis, and animation', 'Analysis and synthesis of facial image sequences using physical and anatomical models', 'Animating Autonomous Pedestrians in Urban Environments', 'The computation of visible-surface representations', 'Cognitive modeling: Knowledge, reasoning and planning for intelligent characters', 'T-snakes: Topology adaptive snakes', 'A dynamic finite element surface model for segmentation and tracking in multidimensional medical images with application to cardiac 4D image analysis', 'Visual learning and recognition of 3-D objects from appearance', 'Columbia object image library (coil-20)', 'Contrast restoration of weather degraded images', 'Reflectance and texture of real-world surfaces', 'Attribute and simile classifiers for face verification', 'Vision and the atmosphere', 'Shape from focus', 'Instant dehazing of images using polarization', 'Vision in bad weather', 'Radiometric self calibration', 'Chromatic framework for vision in bad weather', 'Catadioptric omnidirectional camera', 'Surface reflection: physical and geometrical perspectives', 'Generalized assorted pixel camera: postcapture control of resolution, dynamic range, and spectrum', 'A theory of single-viewpoint catadioptric image formation', 'High dynamic range imaging: Spatially varying pixel exposures', 'Polarization-based vision through haze', \"Generalization of Lambert's reflectance model\", 'Detection and removal of rain from videos', 'Fast separation of direct and global components of a scene using high frequency illumination', 'High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs', 'CyCADA: Cycle-Consistent Adversarial Domain Adaptation', 'Semantic Image Synthesis with Spatially-Adaptive Normalization', 'Generative Visual Manipulation on the Natural Image Manifold', 'Video-to-Video Synthesis', 'Contrastive Learning for Unpaired Image-to-Image Translation', 'Generating adversarial examples with adversarial networks', 'Learning the signatures of the human grasp using a scalable tactile glove', 'Real-time User-guided Image Colorization with Learned Deep Priors', 'SDEdit: Guided Image synthesis and editing with stochastic differential equations', 'GAN Dissection: Visualizing and Understanding Generative Adversarial Networks', 'Spatially transformed adversarial examples', 'Differentiable augmentation for data-efficient gan training', 'Weakly supervised histopathology cancer image segmentation and classification', 'Dataset Distillation', 'Depth-supervised NeRF: Fewer Views and Faster Training for Free', 'State of the Art on Neural Rendering', 'Light field rendering', 'Efficient variants of the ICP algorithm', 'Display of surfaces from volume data', 'A volumetric method for building complex models from range images', 'Light field photography with a hand-held plenoptic camera', 'The digital Michelangelo project: 3D scanning of large statues', 'Fast texture synthesis using tree-structured vector quantization', 'Zippered polygon meshes from range images', 'Fast volume rendering using a shear-warp factorization of the viewing transformation', 'Efficient ray tracing of volume data', 'QSplat: A multiresolution point rendering system for large meshes', 'High performance imaging using large camera arrays', 'A practical model for subsurface light transport', 'Light field microscopy', 'Real-time 3D model acquisition', 'Fitting smooth surfaces to dense polygon meshes', 'The use of points as a display primitive', 'Filling holes in complex surfaces using volumetric diffusion', 'Light fields and computational imaging', 'Wave optics theory and 3-D deconvolution for the light field microscope', 'OBBTree: A hierarchical structure for rapid interference detection', 'Reciprocal n-Body Collision Avoidance', 'Reciprocal velocity obstacles for real-time multi-agent navigation', 'I-collide: An interactive and exact collision detection system for large-scale environments', 'Simplification envelopes', 'Fast computation of generalized Voronoi diagrams using graphics hardware', 'Appearance-preserving simplification', 'GPUTeraSort: high performance graphics co-processor sorting for large database management', 'Visibility culling using hierarchical occlusion maps', 'FCL: A general purpose library for collision and proximity queries', 'Fast BVH construction on GPUs', 'Data structures and applications', 'Fast proximity queries with swept sphere volumes', 'Fast computation of database operations using graphics processors', 'Efficient inverse kinematics for general 6R manipulators', 'The hybrid reciprocal velocity obstacle', 'Trafficpredict: Trajectory prediction for heterogeneous traffic-agents', 'CULLIDE: Interactive collision detection between complex models in large environments using graphics hardware', 'Accurate and fast proximity queries between polyhedra using convex surface decomposition', 'Laplacian surface editing', 'Multi-level partition of unity implicits', 'Vnect: Real-time 3d human pose estimation with a single rgb camera', 'Interactive multi-resolution modeling on arbitrary meshes', 'Free-viewpoint video of human actors', 'Feature sensitive surface extraction from volume data', 'Performance capture from sparse multi-view video', 'Ridge-valley lines on meshes via implicit surface fitting', 'Motion capture using joint skeleton tracking and surface estimation', 'A statistical model of human pose and body shape', 'Image-based reconstruction of spatial appearance and geometric detail', 'A perceptual framework for contrast processing of high dynamic range images', 'Differential coordinates for interactive mesh editing', 'Realistic, hardware-accelerated shading and lighting', 'Real-time generation of continuous levels of detail for height fields', 'A data-driven approach for real-time full body pose reconstruction from a depth camera', 'Stackless KD‐tree traversal for high performance GPU ray tracing', 'Dynamic range independent image quality assessment', 'A general framework for mesh decimation', 'Stylerig: Rigging stylegan for 3d control over portrait images', 'TensorFlow: Large-scale machine learning on heterogeneous systems, 2015', 'Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints', 'TensorFlow: Large-scale machine learning on heterogeneous systems, software available from tensorflow. org (2015)', 'Tfx: A tensorflow-based production-scale machine learning platform', 'TensorFlow: large-scale machine learning on heterogeneous distributed systems. 2015', 'Non‐Rigid Registration Under Isometric Deformations', 'TensorFlow: Large-scale machine learning on heterogeneous systems (2015), software available from tensorflow. org', '12th USENIX symposium on operating systems design and implementation (OSDI 16)', 'Dynamic local remeshing for elastoplastic simulation', 'Adaptive space deformations based on rigid cells', 'Polyhedral finite elements using harmonic basis functions', 'Tensorflow: A system for large-scale machine learning', 'A finite element method on convex polyhedra', 'Simulating liquids and solid-liquid interactions with lagrangian meshes', 'Modular bases for fluid dynamics', 'Shape decomposition using modal analysis', 'Predictive QoS routing to mobile sinks in wireless sensor networks', '& Zheng, X.(2016). Tensorflow: A system for large-scale machine learning', 'Data stashing: energy-efficient information delivery to mobile sinks through trajectory prediction', 'Interactive 3D Painting on Point-Sampled Objects.', 'Volume rendering', 'Brook for GPUs: stream computing on graphics hardware', 'Radiosity and realistic image synthesis', 'Polaris: A system for query, analysis, and visualization of multidimensional relational databases', 'Ray tracing on programmable graphics hardware', 'An efficient representation for irradiance environment maps', 'A rapid hierarchical radiosity algorithm', 'Larrabee: a many-core x86 architecture for visual computing', 'Reflection from layered surfaces due to subsurface scattering', 'A signal-processing framework for inverse rendering', 'Hierarchical splatting: A progressive refinement algorithm for volume rendering', 'Sequoia: Programming the memory hierarchy', 'Show me: Automatic presentation for visual analysis', 'Realistic modeling and rendering of plant ecosystems', 'Beam tracing polygonal objects', 'Photon mapping on programmable graphics hardware', 'Pixelwise view selection for unstructured multi-view stereo', 'Self-calibration and metric reconstruction inspite of varying and unknown intrinsic camera parameters', 'Visual modeling with a hand-held camera', 'Detailed real-time urban 3d reconstruction from video', 'D2-net: A trainable cnn for joint description and detection of local features', 'Building rome on a cloudless day', 'Semantic3d. net: A new large-scale point cloud classification benchmark', 'PX4: A node-based multithreaded open source robotics framework for deeply embedded platforms', 'A comparative analysis of RANSAC techniques leading to adaptive real-time random sample consensus', 'Convolutional occupancy networks', 'A general framework for motion segmentation: Independent, articulated, rigid, non-rigid, degenerate and non-degenerate', 'A multi-view stereo benchmark with high-resolution images and multi-camera videos', 'USAC: A universal framework for random sample consensus', 'Benchmarking 6dof outdoor visual localization in changing conditions', 'Pulling things out of perspective', 'Pixhawk: A system for autonomous flight using onboard computer vision', 'City-scale landmark identification on mobile devices', 'GPU-based video feature tracking and matching', 'PIXHAWK: A micro aerial vehicle design for autonomous flight using onboard computer vision', 'Real-time visibility-based fusion of depth maps', 'Real-time texture synthesis by patch-based sampling', 'Plenoptic sampling', 'Poisson matting', 'Systems and experiment paper: Construction of panoramic image mosaics with global and local alignment', 'Image completion with structure propagation', 'Image deblurring with blurred/noisy image pairs', 'Rendering with concentric mosaics', 'Mesh editing with poisson-based gradient field manipulation', 'Full-frame video stabilization with motion inpainting', 'The design and implementation of xiaoice, an empathetic social chatbot', 'Motion texture: a two-level statistical model for character motion synthesis', 'Symmetric stereo matching for occlusion handling', 'From Eliza to XiaoIce: challenges and opportunities with social chatbots', 'Fundamental limits of reconstruction-based superresolution algorithms under local translation', 'Large mesh deformation using the volumetric graph laplacian', 'Joint bilateral upsampling', 'Interactive digital photomontage', 'The hemi-cube: A radiosity solution for complex environments', 'Unstructured lumigraph rendering', 'A progressive refinement approach to fast radiosity image generation', 'Deep photo: Model-based photograph enhancement and viewing', 'Verbs and adverbs: Multidimensional motion interpolation', 'Wang tiles for image and texture generation', 'Entrepreneurial self-efficacy: A systematic review of the literature on its theoretical foundations, measurement, antecedents, and outcomes, and an agenda for future research', 'The virtual cinematographer: A paradigm for automatic real-time camera control and directing', 'A radiosity method for non-diffuse environments', 'Optimized color sampling for robust matting', 'Efficient generation of motion transitions using spacetime constraints', 'Interactive spacetime control for animation', 'Interactive video cutout', 'Image and video matting: a survey', 'Initial public health response and interim clinical guidance for the 2019 novel coronavirus outbreak—United States, December 31, 2019–February 4, 2020', 'Particle-based fluid simulation for interactive applications', 'A benchmark dataset and evaluation methodology for video object segmentation', 'Efficient simplification of point-sampled surfaces', 'Surfels: Surface elements as rendering primitives', 'Towards better understanding of gradient-based attribution methods for deep neural networks', 'Surface splatting', 'Meshless deformations based on shape matching', 'Multi‐scale feature extraction on point‐sampled surfaces', 'Interactive Virtual Materials.', 'Optimized spatial hashing for collision detection of deformable objects.', 'Shape modeling with point-sampled geometry', 'Algebraic point set surfaces', 'Nonlinear disparity mapping for stereoscopic 3D', 'Simulating facial surgery using finite element models', 'Point based animation of elastic, plastic and melting objects', 'Pointshop 3D: An interactive system for point-based surface editing', 'Feature preserving point set surfaces based on non‐linear kernel regression', 'Scene reconstruction from high spatio-angular resolution light fields.', 'blue-c: a spatially immersive display and 3d video portal for telepresence', 'Practical motion capture in everyday surroundings', 'Fast bilateral filtering for the display of high-dynamic-range images', 'Learning to predict where humans look', 'Image and depth from a conventional camera with a coded aperture', 'Eulerian video magnification for revealing subtle changes in the world', 'Understanding and evaluating blind deconvolution algorithms', 'Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines', 'Flash photography enhancement via intrinsic relighting', 'Phase-based video motion processing', 'Learning photographic global tonal adjustment with a database of input/output image pairs', 'What do different evaluation metrics tell us about saliency models?', 'Non-iterative, feature-preserving mesh smoothing', 'Bilateral filtering: Theory and applications', 'Detecting pulse from head motions in video', 'A fast approximation of the bilateral filter using a signal processing approach', 'Efficient marginal likelihood optimization in blind deconvolution', 'Real-time edge-aware image processing with the bilateral grid', 'Deep bilateral learning for real-time image enhancement', 'Image-based modeling and photo editing', 'A survey of visibility for walkthrough applications', 'Shape distributions', 'Scannet: Richly-annotated 3d reconstructions of indoor scenes', 'The princeton shape benchmark', 'Rotation invariant spherical harmonic representation of 3 d shape descriptors', 'Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop', 'Dilated residual networks', 'Matterport3d: Learning from rgb-d data in indoor environments', 'A search engine for 3D models', 'Semantic scene completion from a single depth image', 'Adaptive display algorithm for interactive frame rates during visualization of complex virtual environments', 'A benchmark for 3D mesh segmentation', '3dmatch: Learning local geometric descriptors from rgb-d reconstructions', 'Modeling by example', 'Robotic pick-and-place of novel objects in clutter with multi-affordance grasping and cross-domain image matching', 'Shape-based recognition of 3D point clouds in urban environments', 'Blended intrinsic maps', 'Learning synergies between pushing and grasping with self-supervised deep reinforcement learning', 'Ibrnet: Learning multi-view image-based rendering', 'RING: A client-server system for multi-user virtual environments', 'Progressive meshes', 'Surface reconstruction from unorganized points', 'Poisson surface reconstruction', 'Mesh optimization', 'Multiresolution analysis of arbitrary meshes', 'Screened poisson surface reconstruction', 'View-dependent refinement of progressive meshes', 'Geometry images', 'Piecewise smooth surface reconstruction', 'Smooth view-dependent level-of-detail control and its application to terrain rendering', 'Texture mapping progressive meshes', 'Multi-view stereo for community photo collections', 'Geometry clipmaps: terrain rendering using nested regular grids', 'Automatic reconstruction of B-spline surfaces of arbitrary topological type', 'Fast exact and approximate geodesics on meshes', 'New quadric metric for simplifiying meshes with appearance attributes', 'Spherical parametrization and remeshing', 'Progressive simplicial complexes', 'High-quality streamable free-viewpoint video', 'Eigenfaces vs. fisherfaces: Recognition using class specific linear projection', 'From few to many: Illumination cone models for face recognition under variable lighting and pose', 'Efficient region tracking with parametric models of geometry and illumination', 'What is the set of images of an object under all possible illumination conditions?', 'Localizing parts of faces using a consensus of exemplars', 'Leafsnap: A computer vision system for automatic plant species identification', 'The bas-relief ambiguity', 'A theory of network localization', 'Rigidity, computation, and randomization in network localization', 'Describable visual attributes for face verification and image search', 'Facetracer: A search engine for large collections of images with faces', 'Face swapping: automatically replacing faces in photographs', 'Poof: Part-based one-vs.-one features for fine-grained categorization, face verification, and attribute estimation', 'In search of illumination invariants', 'From few to many: Generative models for recognition under variable pose and illumination', 'A Bayesian approach to binocular steropsis', 'Birdsnap: Large-scale fine-grained visual categorization of birds', 'Illumination cones for recognition under variable lighting: Faces', 'Real-time tracking of image regions with changes in geometry and illumination', 'Global contrast based salient region detection', 'Struck: Structured Output Tracking with Kernels', 'The visual object tracking vot2014 challenge results', 'Res2Net: A New Multi-scale Backbone Architecture', 'Salient Object Detection: A Benchmark', 'Deeply supervised salient object detection with short connections', 'Salient object detection: A discriminative regional feature integration approach', 'BING: Binarized normed gradients for objectness estimation at 300fps', 'Structure-measure: A New Way to Evaluate Foreground Maps', 'Richer Convolutional Features for Edge Detection', 'PoolNet+: Exploring the Potential of Pooling for Salient Object Detection', 'EGNet: Edge Guidance Network for Salient Object Detection', 'Enhanced-alignment Measure for Binary Foreground Map Evaluation', 'Salient object detection: A survey', 'Object Region Mining with Adversarial Erasing: A Simple Classification to Semantic Segmentation Approach', 'Sketch2Photo: internet image montage', 'Attention mechanisms in computer vision: A survey', 'GMS: Grid-based Motion Statistics for Fast, Ultra-robust Feature Correspondence', 'Efficient Salient Region Detection with Soft Image Abstraction', 'STC: A Simple to Complex Framework for Weakly-supervised Semantic Segmentation', 'A theory of shape by space carving', 'Photorealistic scene reconstruction by voxel coloring', 'View Morphing', 'Synthesizing obama: learning lip sync from audio', 'Multicore bundle adjustment', 'Towards internet-scale multi-view stereo', 'Dynamicfusion: Reconstruction and tracking of non-rigid scenes in real-time', 'The megaface benchmark: 1 million faces for recognition at scale', 'Spacetime faces: high resolution capture for modeling and animation', 'Rapid shape acquisition using color structured light and multi-pass dynamic programming', 'Nerfies: Deformable neural radiance fields', 'Bundle adjustment in the large', 'Shape and spatially-varying brdfs from photometric stereo', 'Manhattan-world stereo', 'Structure from motion without correspondence', 'Computer vision: a modern approach', 'Describing objects by their attributes', 'Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary', 'Matching words and pictures', 'Object recognition with gradient-based learning', 'Every picture tells a story: Generating sentences from images', 'A novel algorithm for color constancy', 'Interactive motion generation from examples', 'Utility data annotation with amazon mechanical turk', 'Learning the semantics of words and pictures', 'Finding naked people', 'Recovering the spatial layout of cluttered rooms', 'Invariant descriptors for 3 d object recognition and pose', 'Motion synthesis from annotations', 'Names and faces in the news', 'Tracking people by learning their appearance', 'Strike a pose: Tracking people by finding stylized poses', 'Safetynet: Detecting and rejecting adversarial examples robustly', 'Finding and tracking people from the bottom up', 'Rendering synthetic objects into legacy photographs', 'The handbook of New Zealand mammals', 'High predictability in introduction outcomes and the geographical range size of introduced Australian birds: a role for climate', 'Using presence‐only and presence–absence data to estimate the current and potential distributions of established invasive species', 'The hare, the tortoise and the crocodile: the ecology of angiosperm dominance, conifer persistence and fern filtering', 'Enumerating a continental-scale threat: how many feral cats are in Australia?', 'Factors preventing the recovery of New Zealand forests following control of invasive deer', 'Climatic suitability, life‐history traits, introduction effort, and the establishment and spread of introduced mammals in Australia', 'Propagule size and the relative success of exotic ungulate and bird introductions to New Zealand', 'Testing the metabolic theory of ecology: allometric scaling exponents in mammals', 'A systematic review of the impacts and management of introduced deer (family Cervidae) in Australia', 'Diet and diet preferences of introduced ungulates (Order: Artiodactyla) in New Zealand', 'Species distributions, surrogacy, and important conservation regions in Canada', 'Modeling the relationship between fecal pellet indices and deer density', 'Testing the irruptive paradigm of large‐herbivore dynamics', 'Estimates of maximum annual population growth rates (rm) of mammals and their application in wildlife management', 'Control of pest mammals for biodiversity protection in Australia. I. Patterns of control and monitoring', 'Towards a global terrestrial species monitoring program', 'Interspecific and geographic variation in the diets of sympatric carnivores: dingoes/wild dogs and red foxes in south-eastern Australia', 'Impacts of introduced deer and extinct moa on New Zealand ecosystems', 'A substantial energetic cost to male reproduction in a sexually dimorphic ungulate', 'Collision and Proximity Queries', 'Collision detection between geometric models: A survey', 'A fast algorithm for incremental distance calculation.', 'Continuum Modeling of Crowd Turbulence', 'Aggregate dynamics for dense crowd simulation', 'Clearpath: highly parallel collision avoidance for multi-agent simulation', 'Efficient Collision Detection for Animation and Robotics', 'DAB: Interactive haptic painting with 3D virtual brushes', 'Collision Detection', 'A survey on hair modeling: Styling, simulation, and rendering', 'PLEdestrians: A Least-Effort Approach to Crowd Simulation.', 'A stable solution-processed polymer semiconductor with record high-mobility for printed transistors', 'Dynamical observation of bamboo-like carbon nanotube growth', 'Direct observation of single-walled carbon nanotube growth at the atomistic scale', 'Site-specific growth of Au–Pd alloy horns on Au nanorods: a platform for highly sensitive monitoring of catalytic reactions by surface enhancement raman spectroscopy', 'Interface confined hydrogen evolution reaction in zero valent metal nanoparticles-intercalated molybdenum disulfide', 'Nitrogen and phosphorus co-doped graphene quantum dots: synthesis from adenosine triphosphate, optical properties, and cellular imaging', 'Hydrothermal Synthesis of CeO2 Nanocrystals: Ostwald Ripening or Oriented Attachment?', 'Facile growth of homogeneous Ni(OH)2 coating on carbon nanosheets for high-performance asymmetric supercapacitor applications', 'High‐performance broadband photodetector using solution‐processible PbSe–TiO2–graphene hybrids', '2H-MoS2 on Mo2CTx MXene Nanohybrid for Efficient and Durable Electrocatalytic Hydrogen Evolution', 'In-Situ-Grown Mg(OH)2-Derived Hybrid α-Ni(OH)2 for Highly Stable Supercapacitor', 'Plasmonic gold nanocrosses with multidirectional excitation and strong photothermal effect', 'Platinum nanoparticles encapsulated in MFI zeolite crystals by a two-step dry gel conversion method as a highly selective hydrogenation catalyst', 'Toward high throughput interconvertible graphane-to-graphene growth and patterning', 'In Situ Observation and Electrochemical Study of Encapsulated Sulfur Nanoparticles by MoS2 Flakes', 'Ultrafast electrochemical expansion of black phosphorus toward high-yield synthesis of few-layer phosphorene', 'Heterogeneous photo-Fenton reaction on hematite (α-Fe 2 O 3){104},{113} and {001} surface facets', 'High-performance NaFePO 4 formed by aqueous ion-exchange and its mechanism for advanced sodium ion batteries', 'CO2 hydrogenation to ethanol over Cu@ Na-Beta', 'An experimental and theoretical investigation of the anisotropic branching in gold nanocrosses', 'The Design and Analysis of Spatial Data Structures', 'Pruning filters for efficient convnets', 'The quadtree and related hierarchical data structures', 'Applications of spatial data structures: computer graphics, image processing, and GIS', 'Foundations of multidimensional and metric data structures', 'Distance browsing in spatial databases', 'Twitterstand: news in tweets', 'A general approach to connected-component labeling for arbitrary image representations', 'Index-driven similarity search in metric spaces (survey article)', 'Ranking in spatial databases', 'Efficient component labeling of images of arbitrary dimension represented by linear bintrees', 'Scalable network distance browsing in spatial databases', 'Incremental distance join algorithms for spatial databases', 'Neighbor finding techniques for images represented by quadtrees', 'A hierarchical strategy for path planning among moving obstacles (mobile robot)', 'Properties of embedding methods for similarity searching in metric spaces', 'Region representation: Quadtrees from boundary codes', 'Connected component labeling using quadtrees', 'Spatial join techniques', 'SoftPOSIT: Simultaneous pose and correspondence determination', 'Level set methods and dynamic implicit surfaces', 'A non-oscillatory Eulerian approach to interfaces in multimaterial flows (the ghost fluid method)', 'Level set methods: an overview and some recent results', 'A hybrid particle level set method for improved interface capturing', 'Visual simulation of smoke', 'Practical animation of liquids', 'Robust treatment of collisions, contact and friction for cloth animation', 'Animation and rendering of complex water surfaces', 'Simulating water and smoke with an octree data structure', 'A boundary condition capturing method for multiphase incompressible flow', \"A boundary condition capturing method for Poisson's equation on irregular domains\", 'Simulation of clothing with folds and wrinkles', 'A second-order-accurate symmetric discretization of the Poisson equation on irregular domains', 'Fast surface reconstruction using the level set method', 'Physically based modeling and animation of fire', 'Automatic determination of facial muscle activations from sparse motion capture marker data', 'Invertible finite elements for robust simulation of large deformation', 'Nonconvex rigid bodies with stacking', 'A vortex particle method for smoke, water and explosions', 'A review of level-set methods and some recent applications', 'PatchMatch: A randomized correspondence algorithm for structural image editing', 'Actions as space-time shapes', 'Toward multimodal image-to-image translation', 'In defense of nearest-neighbor based image classification', 'Matching local self-similarities across images and videos', 'High-resolution image inpainting using multi-scale neural patch synthesis', 'Space-time completion of video', 'Styleclip: Text-driven manipulation of stylegan imagery', 'Summarizing visual data using bidirectional similarity', 'Deep photo style transfer', 'Localizing moments in video with natural language', 'The generalized PatchMatch correspondence algorithm', 'Image melding: Combining inconsistent images using patch-based synthesis', 'Space-time video completion', 'Controlling perceptual factors in neural style transfer', 'Space-time behavior based correlation', 'Robust patch-based HDR reconstruction of dynamic scenes', 'Spacetime constraints', 'Analyzing oriented patterns', 'Subdivision surfaces in character animation', 'Hierarchical Z-buffer visibility', 'Rapid, stable fluid dynamics for computer graphics', 'Efficient, fair interpolation using Catmull-Clark surfaces', 'Active contour modes', 'Symmetry-seeking models and 3D object reconstruction', 'Reaction-diffusion textures', 'Signal matching through scale space', 'Untangling cloth', 'Smoothed local histogram filters', 'Terzopoulos', 'Interactive depth of field using simulated diffusion on a GPU', 'Rendering of 3D scenes on a display using hierarchical z-buffer visibility', 'Texture mapping and other uses of scalar fields on subdivision surfaces in computer graphics and animation', 'Stylizing animation by example', 'Physically based modeling', 'A style-based generator architecture for generative adversarial networks', 'Progressive growing of gans for improved quality, stability, and variation', 'Analyzing and improving the image quality of stylegan', 'Temporal ensembling for semi-supervised learning', 'Pruning convolutional neural networks for resource efficient inference', 'Noise2Noise: Learning image restoration without clean data', 'Training generative adversarial networks with limited data', 'Alias-free generative adversarial networks', 'Few-shot unsupervised image-to-image translation', 'Understanding the efficiency of ray traversal on GPUs', 'Improved precision and recall metric for assessing generative models', 'Elucidating the design space of diffusion-based generative models', 'Audio-driven facial animation by joint end-to-end learning of pose and emotion', 'Interactive reconstruction of Monte Carlo image sequences using a recurrent denoising autoencoder', 'Semi-supervised semantic segmentation needs strong, varied perturbations', 'High-quality self-supervised deep image denoising', 'ediffi: Text-to-image diffusion models with an ensemble of expert denoisers', 'Modular primitives for high-performance differentiable rendering', 'Incremental Instant Radiosity for Real-Time Indirect Illumination.', 'Fast parallel construction of high-quality bounding volume hierarchies', 'Image analogies', 'The space of human body shapes: reconstruction and parameterization from range scans', 'A bayesian approach to digital matting', 'Surface light fields for 3D photography', 'Spatio-Angular Resolution Tradeoffs in Integral Photography.', 'Articulated body deformation from range scan data', 'Multi-view stereo revisited', 'Video matting of complex scenes', 'Face2face: Real-time face capture and reenactment of rgb videos', 'VNect: Real-time 3D Human Pose Estimation with a Single RGB Camera', 'Monocular 3D Human Pose Estimation Using Transfer Learning and Improved CNN Supervision', 'Monocular 3d human pose estimation in the wild using improved cnn supervision', 'BundleFusion: Real-time Globally Consistent 3D Reconstruction using Online Surface Re-integration', 'BundleFusion: Real-time Globally Consistent 3D Reconstruction using On-the-fly Surface Re-integration', 'Neural Sparse Voxel Fields', 'Deep Video Portraits', 'NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction', 'MoFA: Model-based Deep Convolutional Face Autoencoder for Unsupervised Monocular Reconstruction', 'GANerated Hands for Real-time 3D Hand Tracking from Monocular RGB', 'Motion capture using simultaneous skeleton tracking and surface estimation', 'Real-time Non-rigid Reconstruction using an RGB-D Camera', 'UpSet: visualization of intersecting sets', 'A data-driven reflectance model', 'Saturated reconstruction of a volume of neocortex', 'HiGlass: web-based visual exploration and analysis of genome interaction maps', 'Face transfer with multilinear models', 'Design galleries: A general approach to setting parameters for computer graphics and animation', 'Blind image deblurring using dark channel prior', '3D TV: a scalable system for real-time acquisition, transmission, and autostereoscopic display of dynamic scenes', 'What makes a visualization memorable?', 'The volumepro real-time ray-casting system', 'Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks', 'Sliced and radon wasserstein barycenters of measures', 'The transfer function bake-off', 'Point-based graphics', 'Analysis of human faces using a measurement-based skin reflectance model', 'Lineup: Visual analysis of multi-attribute rankings', 'Beyond memorability: Visualization recognition and recall', 'Design and fabrication of materials with desired deformation behavior', 'A global human walking model with real-time kinematic personification', 'Joint-dependent local deformations for hand animation and object grasping', 'Hierarchical model for real time simulation of virtual human crowds', 'Stepping into virtual reality', 'Crowd Simulation', 'Computer Animation. Computer Science Workbench', 'Stable real-time 3D tracking using online and offline information', 'Simulation of facial muscle actions based on rational free form deformations', 'Dressing Animated Synthetic Actors with Complex Deformable Clothes', 'A model of human crowd behavior: Group inter-relationship and collision detection analysis', 'Simulation of object and human skin formations in a grasping task', 'Biomechanical models for soft tissue simulation', 'Abstract muscle action procedures for human face animation', 'Robust 3d hand pose estimation in single depth images: from single-view cnn to multi-view cnns', 'Merging trust in collaborative filtering to alleviate data sparsity and cold start', 'Real time muscle deformations using mass-spring systems', '3d convolutional neural networks for efficient and robust hand pose estimation from single depth images', 'A vision-based approach to behavioral animation', 'Magnenat-Thalmann Nadia (1990)“A Vision-Based Approach to Behavioral Animation”', 'Mixing virtual and real scenes in the site of ancient Pompeii', 'Swin transformer: Hierarchical vision transformer using shifted windows', 'Real-time kd-tree construction on graphics hardware', 'Swin transformer v2: Scaling up capacity and resolution', 'Learning texture transformer network for image super-resolution', 'Face x-ray for more general face forgery detection', 'Cswin transformer: A general vision transformer backbone with cross-shaped windows', 'Dynamic player groups for interest management in multi-character virtual environments', 'Synthesis of bidirectional texture functions on arbitrary surfaces', 'Learning pyramid-context encoder network for high-quality image inpainting', 'View-dependent displacement mapping', 'Subspace gradient domain mesh deformation', 'Vector quantized diffusion model for text-to-image synthesis', 'Synthesis of progressively-variant textures on arbitrary surfaces', 'Geometry-driven photorealistic facial expression synthesis', 'Data-parallel octrees for surface reconstruction', 'Chaos mosaic: Fast and memory efficient texture synthesis', 'An interactive approach to semantic modeling of indoor scenes with an rgbd camera', 'Noise2noise: Learning image restoration without clean data', 'Efficient sparse voxel octrees', 'eDiffi: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers', 'Ambient occlusion fields', 'Incremental instant radiosity for real-time indirect illumination', 'Alias-free shadow maps', 'Megakernels considered harmful: wavefront path tracing on GPUs', 'Deep Learning Face Attributes in the Wild', 'Dynamic Graph CNN for Learning on Point Clouds', 'MMDetection: Open MMLab Detection Toolbox and Benchmark', 'DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations', 'Hybrid Task Cascade for Instance Segmentation', 'Large-Scale Long-Tailed Recognition in an Open World', 'MaskGAN: Towards Diverse and Interactive Facial Image Manipulation', 'Semantic Image Segmentation via Deep Parsing Network', 'Learning to Prompt for Vision-Language Models', 'Video Frame Synthesis using Deep Voxel Flow', 'Domain Generalization: A Survey', 'Conditional Prompt Learning for Vision-Language Models', 'Large-scale Celebfaces Attributes (CelebA) Dataset', 'Generalized Out-of-Distribution Detection: A Survey', 'Incorporating Convolution Designs into Visual Transformers', 'Talking Face Generation by Adversarially Disentangled Audio-Visual Representation', 'CARAFE: Content-Aware ReAssembly of FEatures', 'Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade', 'Long-tailed Recognition by Routing Diverse Distribution-Aware Experts', 'Knowledge Distillation Meets Self-Supervision', 'Unsupervised learning of depth and ego-motion from video', 'Megadepth: Learning single-view depth prediction from internet photos', 'Stereo magnification: Learning view synthesis using multiplane images', \"Deepstereo: Learning to predict new views from the world's imagery\", 'Material recognition in the wild with the materials in context database', 'Nerf++: Analyzing and improving neural radiance fields', 'Location recognition using prioritized feature matching', 'Worldwide Pose Estimation using 3D Point Clouds', 'Scene summarization for online image collections', 'Neural scene flow fields for space-time view synthesis of dynamic scenes', 'Robust global translations with 1dsfm', 'Discrete-continuous optimization for large-scale structure from motion', 'Intrinsic images in the wild', 'The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression', 'Lucas-kanade 20 years on: A unifying framework', 'Multi-pie', 'Active appearance models revisited', 'Hand keypoint detection in single images using multiview bootstrapping', 'The template update problem', 'Panoptic studio: A massively multiview system for social motion capture', 'Real-time combined 2D+ 3D active appearance models', 'Extraction of visual features for lipreading', 'Equivalence and efficiency of image alignment algorithms', 'Painful data: The UNBC-McMaster shoulder pain expression archive database', 'Detecting depression from facial actions and vocal prosody', 'Audiovisual automatic speech recognition', 'Generic vs. person specific active appearance models', 'Lucas-Kanade 20 years on: A unifying framework: Part 3', 'Appearance-based face recognition and light-fields', 'Audio visual speech recognition', 'Automatically detecting pain in video through facial action units', 'BioTIME: A database of biodiversity time series for the Anthropocene', 'A deep learning approach for generalized speech animation', 'Context encoders: Feature learning by inpainting', 'Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials', 'Objects as points', 'Adversarial feature learning', 'Saliency filters: Contrast based filtering for salient region detection', 'Center-based 3d object detection and tracking', 'Bottom-up object detection by grouping extreme and center points', 'Sampling matters in deep embedding learning', 'Constrained convolutional neural networks for weakly supervised segmentation', 'Long-term feature banks for detailed video understanding', 'Geodesic object proposals', 'Learning dense correspondence via 3d-guided cycle consistency', 'Compressed video action recognition', 'Learning by cheating', 'Video compression through image interpolation', 'A system for retargeting of streaming video', 'Objects as points. arXiv 2019', 'Parameter learning and convergent inference for dense random fields', 'Wavelets for computer graphics: theory and applications', 'Fast multiresolution image querying', 'Wavelets for computer graphics: a primer. 1', 'Computer-generated watercolor', 'Video textures', 'Rendering antialiased shadows with depth maps', 'Computer-generated pen-and-ink illustration', 'Multiresolution curves', 'Orientable textures for image-based pen-and-ink illustration', 'Gaze-based interaction for semi-automatic photo cropping', 'Interactive pen-and-ink illustration', 'Hierarchical image caching for accelerated walkthroughs of complex environments', 'Numerical shape from shading and occluding boundaries', 'Traffic monitoring and accident detection at intersections', 'Object shape and reflectance modeling from observation', 'Determining surface orientations of specular surfaces by using the photometric stereo method', 'Separating reflection components of textured surfaces using a single image', 'Shape from interreflections', 'Determining shape and reflectance of hybrid surfaces by photometric sampling', 'Toward an assembly plan from observation. I. Task recognition with polyhedral objects', 'Acquiring a radiance distribution to superimpose virtual objects onto a real scene', 'High-resolution hyperspectral imaging via matrix factorization', 'Principal component analysis with missing data and its application to polyhedral object modeling', 'Temporal-color space analysis of reflection', 'Generating whole body motions for a biped humanoid robot from captured human dances', 'Automatic generation of object recognition programs', 'Generating an interpretation tree from a CAD model for 3D-object recognition in bin-picking tasks', 'Detectability, uniqueness, and reliability of eigen windows for stable verification of partially occluded objects', 'Illumination from shadows', 'Consensus surfaces for modeling 3D objects from multiple range images', 'Transparent surface modeling from a pair of polarization images', 'Realtime multi-person 2d pose estimation using part affinity fields', 'Convolutional pose machines', 'Bayesian modeling of dynamic scenes for object detection', 'Neural volumes: Learning dynamic renderable volumes from images', 'Total capture: A 3d deformation model for tracking faces, hands, and bodies', 'Background subtraction for freely moving cameras', 'Reconstructing 3d human pose from 2d image landmarks', 'Pose machines: Articulated pose estimation via inference machines', 'Recycle-gan: Unsupervised video retargeting', 'Openpose: realtime multi-person 2d pose estimation using part affinity fields (2018)', 'Monocular total capture: Posing face, body, and hands in the wild', 'Trajectory space: A dual representation for nonrigid structure from motion', 'Nonrigid structure from motion in trajectory space', 'Exploring the space of a human action', 'On the use of computable features for film classification', 'Deep appearance models for face rendering', 'Mode-seeking by medoidshifts', 'Supervision-by-registration: An unsupervised approach to improve the precision of facial landmark detectors', 'Nerf: Representing scenes as neural radiance fields for view synthesis', 'Fourier features let networks learn high frequency functions in low dimensional domains', 'Local light field fusion: Practical view synthesis with prescriptive sampling guidelines', 'Depth from combining defocus and correspondence using light-field cameras', 'Image to image translation for domain adaptation', 'Learning-based view synthesis for light field cameras', 'All-frequency shadows using non-linear wavelet lighting approximation', 'Spacetime stereo: A unifying framework for depth from triangulation', 'Efficiently combining positions and normals for precise 3D geometry', 'On the relationship between radiance and irradiance: determining the illumination from images of a convex Lambertian object', 'Deep high dynamic range imaging of dynamic scenes.', 'Occlusion-aware depth estimation using light-field cameras', 'Triple product wavelet integrals for all-frequency relighting', 'Analytic PCA construction for theoretical analysis of lighting variability in images of a Lambertian object', 'Frequency space environment map rendering', 'Data-driven elastic models for cloth: modeling and measurement', 'Pushing the boundaries of view extrapolation with multiplane images', 'Structured importance sampling of environment maps', 'Faceforensics++: Learning to detect manipulated facial images', 'Real-time 3D reconstruction at scale using voxel hashing', 'Deferred neural rendering: Image synthesis using neural textures', 'Bundlefusion: Real-time globally consistent 3d reconstruction using on-the-fly surface reintegration', 'Deep video portraits', 'Shape completion using 3d-encoder-predictor cnns and shape synthesis', 'Real-time non-rigid reconstruction using an RGB-D camera', 'Deepvoxels: Learning persistent 3d feature embeddings', '3d-sis: 3d semantic instance segmentation of rgb-d scans', 'Local implicit grid representations for 3d scenes', 'Real-time expression transfer for facial reenactment.', 'Faceforensics: A large-scale video dataset for forgery detection in human faces', 'State of the art on neural rendering', 'Volumedeform: Real-time volumetric non-rigid reconstruction', '3dmv: Joint 3d-multi-view prediction for 3d semantic scene segmentation', 'Recovering High Dynamic Range Radiance Maps from Photographs', 'High dynamic range imaging: acquisition, display, and image-based lighting', 'Rendering synthetic objects into real scenes: Bridging traditional and image-based graphics with global illumination and high dynamic range photography', 'Acquiring the reflectance field of a human face', 'Efficient view-dependent image-based rendering with projective texture-mapping', 'Rendering for an interactive 360 light field display', 'Inverse global illumination: Recovering reflectance models of real scenes from photographs', 'A tone mapping algorithm for high contrast images', 'Rapid Acquisition of Specular and Diffuse Normal Maps from Polarized Spherical Gradient Illumination.', 'Deepview: View synthesis with learned gradient descent', 'Image-based lighting', 'Performance relighting and reflectance transformation with time-multiplexed illumination', 'Dynamic shape capture using multi-view photometric stereo', 'Baking neural radiance fields for real-time view synthesis', 'Nerfactor: Neural factorization of shape and reflectance under an unknown illumination', 'Multiview face capture using polarized spherical gradient illumination', 'Linear light source reflectometry', 'A lighting reproduction approach to live-action compositing', 'Ada and Grace: Toward realistic and engaging virtual museum guides', 'Efficient Variants of the ICP Algorithm', 'The Digital Michelangelo Project: 3D Scanning of Large Statues', 'Rotation invariant spherical harmonic representation of 3D shape descriptors', 'Suggestive contours for conveying shape', 'Estimating curvatures and their derivatives on triangle meshes', 'Geometrically stable sampling for the ICP algorithm', 'Multiscale shape and detail enhancement from multi-light image collections', 'A planar-reflective symmetry transform for 3D shapes', 'Stripe boundary codes for real-time structured-light range scanning of moving objects', 'A new change of variables for efficient BRDF representation', 'Chopper: Partitioning Models into 3D-Printable Parts', 'A reflective symmetry descriptor for 3D models', 'Global non-rigid alignment of 3-D scans', 'Interactive control of avatars animated with human motion data', 'Animating human athletics', 'Graphical modeling and animation of brittle fracture', 'Synthesizing physically realistic human motion in low-dimensional, behavior-specific spaces', 'Segmenting motion capture data into distinct behaviors', 'Footstep planning for the honda asimo humanoid', 'Hierarchical aligned cluster analysis for temporal clustering of human motion', 'Animation of dynamic legged locomotion', 'Performance animation from low-dimensional control signals', 'Social interactions: A first-person perspective', 'Adapting human motion for the control of a humanoid robot', 'Graphical modeling and animation of ductile fracture', 'Motion capture-driven simulations that hit and react', 'A storytelling robot: Modeling and evaluation of human-like gaze behavior', 'Adjusting step length for rough terrain locomotion', 'Adapting simulated behaviors for new characters', 'Construction and optimal search of interpolated motion graphs', 'Two methods for display of high contrast images', 'Group behaviors for systems with significant dynamics', 'A closed-form solution to natural image matting', 'Colorization using optimization', 'Gradient domain high dynamic range compression', 'Crowds by example', 'Spectral matting', 'Interactive local adjustment of tonal values', 'Colorization by Example', 'Non-rigid dense correspondence with applications for image enhancement', 'Stylespace analysis: Disentangled controls for stylegan image generation', 'Texture mixing and texture movie synthesis using statistical learning', 'Solid texture synthesis from 2d exemplars', 'Bounded-distortion piecewise mesh parameterization', 'Synthesizing training images for boosting human 3D pose estimation', 'Mathematics and geometry education with collaborative augmented reality', 'ARToolkitPlus for pose tracking on mobile devices', 'Augmented Reality - Principles and Practice', 'Pose tracking from natural features on mobile phones', 'The Studierstube Augmented Reality Project', 'First steps towards handheld augmented reality', 'Real-time detection and tracking for augmented reality on mobile phones', 'Construct3D: a virtual reality application for mathematics and geometry education', 'Towards massively multi-user augmented reality on handheld devices', 'Experiences with handheld augmented reality', 'Indoor positioning and navigation with camera phones', 'Handheld augmented reality for underground infrastructure visualization', 'Tools for interacting with virtual environments', 'Global localization from monocular slam on a mobile phone', 'Using transparent props for interaction with the virtual table', '“Studierstube”: An environment for collaboration in augmented reality', 'The virtual showcase', 'Location based applications for mobile augmented reality', 'Collaborative augmented reality for outdoor navigation and information browsing', 'Robust and unobtrusive marker tracking on mobile phones', 'Removing camera shake from a single photograph', 'Gaussian Process Dynamical Models for Human Motion', 'Gaussian process dynamical models for human motion', 'Recovering non-rigid 3D shape from image streams', 'Style-based inverse kinematics', 'Painterly rendering with curved brush strokes of multiple sizes', 'Style machines', 'Illustrating smooth surfaces', 'Ganspace: Discovering interpretable gan controls', 'Learning 3D mesh segmentation and labeling', 'Gaussian process dynamical models', 'Nonrigid structure-from-motion: Estimating shape and motion with hierarchical priors', 'Recognizing image style', 'Discrete differential-geometry operators for triangulated 2-manifolds', 'Implicit fairing of irregular meshes using diffusion and curvature flow', 'Sparse matrix solvers on the GPU: conjugate gradients and multigrid', 'Spherical wavelets: Efficiently representing functions on the sphere', 'MAPS: Multiresolution adaptive parameterization of surfaces', 'Interpolating subdivision for meshes with arbitrary topology', 'Progressive geometry compression', 'Subdivision for modeling and animation. 2000', 'Subdivision surfaces: a new paradigm for thin‐shell finite‐element analysis', 'Multiresolution signal processing for meshes', 'Interactive multiresolution mesh editing', 'Building your own wavelets at home', 'Discrete shells', 'Normal meshes', 'Interactive animation of structured deformable objects', 'Surface drawing: creating organic 3D shapes with the hand and tangible tools', 'CHARMS: a simple framework for adaptive simulation', 'Wavelet radiosity', 'Consistent mesh parameterizations', 'Multiresolution mesh morphing', 'Image-based visual hulls', 'Eye tracking for everyone', 'Articulated mesh animation from multi-view silhouettes', 'Experimental Analysis of BRDF Models.', 'Polyhedral visual hulls for real-time rendering', 'Chopper: Partitioning models into 3D-printable parts', 'Image-based 3D photography using opacity hulls', 'Computational design of mechanical characters', 'Towards real-time photorealistic 3D holography with deep neural networks', '3D television system and method', 'Video face replacement', 'MultiFab: a machine vision assisted platform for multi-material 3D printing', 'Multi-scale capture of facial geometry and motion', 'Volume graphics', 'GPU cluster for high performance computing', 'Volume visualization', 'Virtual voyage: Interactive navigation in the human colon', 'Apparatus and method for volume processing and rendering', 'System and method for performing a three-dimensional virtual examination of objects, such as internal organs', 'Penalized-distance volumetric skeleton algorithm', 'Generation of transfer functions with stochastic search techniques', '3D scan-conversion algorithms for voxel-based graphics', 'Efficient algorithms for 3D scan-conversion of parametric curves, surfaces, and volumes', 'Computer aided treatment planning and visualization with image registration and fusion', '3D virtual colonoscopy', 'Volume sculpting', 'Fundamentals of surface voxelization', 'Implementing lattice Boltzmann computation on graphics hardware', 'Discrete ray tracing', 'Template‐based volume viewing', 'Cube-4-a scalable architecture for real-time volume rendering', 'Automatic centerline extraction for virtual colonoscopy', 'System and method for performing a three-dimensional virtual examination', 'Randomized kinodynamic planning', 'RRT-Connect: An efficient approach to single-query path planning', 'Rapidly-exploring random trees: Progress and prospects', 'Openrave: A planning architecture for autonomous robotics', 'Task space regions: A framework for pose-constrained manipulation planning', 'Planning motions with intentions', 'HERB: a home exploring robotic butler', 'Dex-net 1.0: A cloud-based network of 3d objects for robust grasp planning using a multi-armed bandit model with correlated rewards', 'Dynamically-stable motion planning for humanoid robots', 'Cloud-based robot grasping with the google object recognition engine', 'Manipulation planning on constraint manifolds', 'Multipartite RRTs for rapid replanning in dynamic environments', 'Effective sampling and distance metrics for 3D rigid body path planning', 'Anytime path planning and replanning in dynamic environments', 'Motion planning for humanoid robots', 'Synthesizing animations of human manipulation tasks', 'Cloud-enabled humanoid robotics', 'Manipulation planning among movable obstacles', 'Planning biped navigation strategies in complex environments', 'Polygon mesh processing', '√ 3-subdivision', 'Mixed-integer quadrangulation', 'Fast image-based localization using direct 2d-to-3d matching', 'Efficient & effective prioritized matching for large-scale image-based localization', 'A survey of point-based techniques in computer graphics', 'Interpolatory subdivision on open quadrilateral nets with arbitrary topology', 'Geometric algebra with applications in engineering', 'An intuitive framework for real-time freeform modeling', 'Efficient high quality rendering of point sampled geometry.', 'A shrink wrapping approach to remeshing polygonal surfaces', 'A remeshing approach to multiresolution modeling', 'Image Retrieval for Image-Based Localization Revisited.', 'Improving image-based localization by active correspondence search', 'OpenMesh–a generic and efficient polygon mesh data structure', 'Computer Graphics: Principles and Practice', 'Fundamentals of interactive computer graphics', 'Introduction to computer graphics', 'The human factors of computer graphics interaction techniques', 'Evaluating a web lecture intervention in a human–computer interaction course', 'The art of natural man-machine conversation', 'Interfaces for advanced computing', 'Visualizing the world-wide web with the navigational view builder', 'A second generation user interface design environment: The model and the runtime architecture', 'Visualizing complex hypermedia networks through multiple hierarchical views', 'Coupling application design and user interface design', 'UIDE-An intelligent user interface design environment', 'Coupling a UI framework with automatic generation of context-sensitive animated help', 'Providing high-level control and expert assistance in the user interface presentation design', 'A knowledge-based user interface management system', 'Automating interface evaluation', 'Defining interfaces at a high level of abstraction', 'DON: user interface presentation design assistant', 'Resultmaps: Visualization for search interfaces', 'Dynamic process visualization', 'Insulin resistance and insulin secretory dysfunction as precursors of non-insulin-dependent diabetes mellitus: prospective studies of Pima Indians', 'Enlarged subcutaneous abdominal adipocyte size, but not obesity itself, predicts type II diabetes independent of insulin resistance', 'A high concentration of fasting plasma non-esterified fatty acids is a risk factor for the development of NIDDM', 'Improved meal-related β-cell function and insulin sensitivity by the dipeptidyl peptidase-IV inhibitor vildagliptin in metformin-treated patients with type 2 diabetes over 1year', 'Vildagliptin, a dipeptidyl peptidase-IV inhibitor, improves model-assessed β-cell function in patients with type 2 diabetes', 'Vildagliptin therapy reduces postprandial intestinal triglyceride-rich lipoprotein particles in patients with type 2 diabetes', 'Comparison between vildagliptin and metformin to sustain reductions in HbA1c over 1\\xa0year in drug‐naïve patients with Type\\xa02 diabetes', 'Alpha cell function in health and disease: influence of glucagon-like peptide-1', 'Effects of vildagliptin on glucose control in patients with type 2 diabetes inadequately controlled with a sulphonylurea', 'In vitro insulin resistance of human adipocytes isolated from subjects with noninsulin-dependent diabetes mellitus.', 'Nateglinide alone and in combination with metformin improves glycemic control by reducing mealtime glucose levels in type 2 diabetes.', 'Kinetics of glucose disposal in whole body and across the forearm in man.', 'Improved glycaemic control with dipeptidyl peptidase‐4 inhibition in patients with type 2 diabetes: vildagliptin (LAF237) dose response', 'The dipeptidyl peptidase IV inhibitor vildagliptin suppresses endogenous glucose production and enhances islet function after single-dose administration in type 2 diabetic patients', 'Vildagliptin add‐on to metformin produces similar efficacy and reduced hypoglycaemic risk compared with glimepiride, with no weight gain: results from a 2‐year study', 'Vildagliptin in drug-naive patients with type 2 diabetes: a 24-week, double-blind, randomized, placebo-controlled, multiple-dose study', 'Rationale and application of fatty acid oxidation inhibitors in treatment of diabetes mellitus', 'Insulin therapy in obese, non-insulin-dependent diabetes induces improvements in insulin action and secretion that are maintained for two weeks after insulin withdrawal', 'Vildagliptin enhances islet responsiveness to both hyper-and hypoglycemia in patients with type 2 diabetes', 'Assessing the cardio–cerebrovascular safety of vildagliptin: meta‐analysis of adjudicated events from a large Phase III type 2 diabetes population', 'International Marketing', 'Global Entrepreneur', 'A morphable model for the synthesis of 3D faces', 'Face recognition based on fitting a 3D morphable model', 'A 3D face model for pose and illumination invariant face recognition', 'Prototype-referenced shape encoding revealed by high-level aftereffects', 'Optimal step nonrigid ICP algorithms for surface registration', 'A neuron-silicon junction: a Retzius cell of the leech on an insulated-gate field-effect transistor', 'Linear object classes and image synthesis from a single example image', 'Reanimating faces in images and video', 'Face identification across different poses and illuminations with a 3d morphable model', 'Face identification by fitting a 3d morphable model using linear shape and texture error functions', 'Estimating 3D shape and texture using pixel intensity, edges, specular highlights, texture constraints and a prior', 'Comparison of view-based object recognition algorithms using realistic 3D models', '3d morphable face models—past, present, and future', 'Efficient, robust and accurate fitting of a 3D morphable model', 'Face recognition based on frontal views generated from non-frontal images', 'Recognition and structure from one 2D model view: Observations on prototypes, object classes and symmetries', 'Exchanging faces in images', 'A statistical method for robust 3D surface reconstruction from sparse data', 'Method and apparatus for the processing of images', 'Expression invariant 3D face recognition with a morphable model', 'Rendering effective route maps: improving usability through generalization', 'Adding conditional control to text-to-image diffusion models', 'Multiscale shape and detail enhancement from multi-light image collections.', 'Interactive furniture layout using interior design guidelines', 'Graphical histories for visualization: Supporting analysis, communication, and evaluation', 'Design considerations for collaborative visual analytics', 'Saliency in VR: How do people explore virtual environments?', 'The two-user responsive workbench: support for collaboration through individual views of a shared space', 'Sizing the horizon: the effects of chart size and layering on the graphical perception of time series visualizations', 'Designing effective step-by-step assembly instructions', 'Interactive 3D architectural modeling from unordered photo collections', 'Scented widgets: Improving navigation cues with embedded visualizations', 'Hover widgets: using the tracking state to extend the capabilities of pen-operated devices', 'Revision: Automated classification, analysis and redesign of chart images', 'Software design patterns for information visualization', 'Terascale direct numerical simulations of turbulent combustion using S3D', 'Parallel volume rendering using binary-swap compositing', 'Data, information, and knowledge in visualization', 'Visual analytics', 'Collaborative visualization: Definition, challenges, and research agenda', 'Visual analysis of large heterogeneous social networks by semantic and structural abstraction', 'In situ visualization for large-scale combustion simulations', 'Portvis: a tool for port-based detection of security events', 'A fast volume rendering algorithm for time-varying fields using a time-space partitioning (TSP) tree', 'From mesh generation to scientific visualization: An end-to-end approach to parallel supercomputing', 'Breaking news on twitter', 'Size-based transfer functions: A new volume exploration technique', 'Big-data visualization', 'Importance-driven time-varying data visualization', 'Design considerations for optimizing storyline visualizations', 'Scientific storytelling using visualization', 'A model and framework for visualization exploration', 'An intelligent system approach to higher-dimensional classification of volume data', 'In situ visualization at extreme scale: Challenges and opportunities', 'Opening the black box-data driven visualization of neural networks', 'The algorithmic beauty of plants', 'Visual models of plants interacting with their environment', 'A plausible model of phyllotaxis', 'Lindenmayer systems, fractals, and plants', 'Graphical applications of L-systems', 'Synthetic topiary', 'Modeling and visualization of leaf venation patterns', 'Using L‐systems for modeling source–sink interactions, architecture and physiology of growing trees: the L‐PEACH model', 'Model for the regulation of Arabidopsis thaliana leaf margin development', 'The use of positional information in the modeling of plants', 'Evolution and development of inflorescence architectures', 'Control of bud activation by an auxin transport switch', 'Integration of transport-based models for phyllotaxis and midvein formation', 'Modeling plant growth and development', 'Variance‐based color image quantization for frame buffer display', 'Animation of plant development', 'The genetics of geometry', 'Modeling trees with a space colonization algorithm.', 'Self-organizing tree models for image synthesis', 'Graphcut textures: Image and video synthesis using graph cuts', 'The aware home: A living laboratory for ubiquitous computing research', 'Coding, analysis, interpretation, and recognition of facial expressions', 'Efficient hierarchical graph-based video segmentation', 'Texture optimization for example-based synthesis', 'One-shot learning for semantic segmentation', 'Auto-directed video stabilization with robust l1 optimal camera paths', 'Facial expression recognition using a dynamic model and motion energy', 'Exploiting human actions and object context for recognition tasks', 'Image and video based painterly animation', 'Increasing the opportunities for aging in place', 'A practical approach for recognizing eating moments with wrist-mounted inertial sensing', 'Motion regularization for model-based head tracking', 'Detecting and tracking eyes by using their physiological properties, dynamics, and appearance', 'Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames', 'Ubiquitous sensing for smart and aware environments', 'A vision system for observing and extracting facial action parameters', 'Gaussian process regression flow for analysis of motion trajectories', 'Recognizing multitasked activities from video using stochastic context-free grammar', 'Simulating humans: computer graphics animation and control', 'Real-time inverse kinematics techniques for anthropomorphic limbs', 'Animated conversation: rule-based generation of facial expression, gesture & spoken intonation for multiple conversational agents', 'Animating facial expressions', 'Controlling individual agents in high-density crowd simulation', 'Model-based image analysis of human motion using constraint propagation', 'The EMOTE model for effort and shape', 'Inverse kinematics positioning using nonlinear programming for highly articulated figures', 'Creating interactive virtual humans: Some assembly required', 'Modeling crowd and trained leader behavior during building evacuation', 'Eyes alive', 'Generating facial expressions for speech', 'Virtual crowds: Methods, simulation, and control', 'Digital representations of human movement', 'Crowd simulation incorporating agent psychological models, roles and communication', 'Articulated figure positioning by multiple constraints', 'Animating human locomotion with inverse dynamics', 'Techniques for generating the goal-directed motion of articulated structures', 'Parameterized action representation for virtual human agents', 'Real-time inverse kinematics of the human arm', 'Predicting protein structures with a multiplayer online game', 'ROSETTA3: an object-oriented software suite for the simulation and design of macromolecules', 'Continuum crowds', 'Motion warping', 'Physically based motion transformation', 'Crystal structure of a monomeric retroviral protease solved by protein folding game players', 'Algorithm discovery by protein folding game players', 'Discovery of complex behaviors through contact-invariant optimization', 'Learning physics-based motion style with nonlinear inverse optimization', 'Nonlinear inverse reinforcement learning with gaussian processes', 'Fluid control using the adjoint method', 'Interactive skeleton-driven dynamic deformations', 'Keyframe control of smoke simulations', 'Increased Diels-Alderase activity through backbone remodeling guided by Foldit players', 'Synthesis of complex dynamic character motion from simple animations', 'Near-optimal character animation with continuous control', 'Dance reveals symmetry especially in young men', 'On visible surface generation by a priori tree structures', 'Optimal surface reconstruction from planar contours', 'The office of the future: A unified approach to image-based modeling and spatially immersive displays', 'Merging virtual objects with the real world: Seeing ultrasound imagery within the patient', 'A sorting classification of parallel rendering', 'Pixel-planes 5: A heterogeneous multiprocessor graphics system using processor-enhanced memories', 'Methods and systems for real-time structured light depth extraction and endoscope using real-time structured light depth extraction', 'Multi-projector displays using camera-based registration', 'Augmented reality visualization for laparoscopic surgery', 'Optical versus video see-through head-mounted displays in medical visualization', 'Spatially augmented reality', 'Dynamic shader lamps: Painting on movable objects', 'Near real-time shaded display of rigid objects', 'Encumbrance-free telepresence system with real-time 3D capture and display using commodity depth cameras', 'Comparison of optical and video see-through, head-mounted displays', 'Augmented reality guidance for needle biopsies: an initial randomized, controlled trial in phantoms', 'Conveying the 3D shape of smoothly curving transparent surfaces via texture', 'Evaluation of reorientation techniques and distractors for walking in large virtual environments', 'Pinlight displays: wide field of view augmented reality eyeglasses using defocused point light sources', 'Image rendering by adaptive refinement', 'A comprehensive set of sequence analysis programs for the VAX', 'Smithies 0 (1984) A comprehensive set of sequence analysis programs for the VAX', 'Paint by numbers: Abstract image representations', 'Fast shadows and lighting effects using texture mapping', 'The accumulation buffer: Hardware support for high-quality rendering', 'Smithies, 0.(1984)', 'Direct WYSIWYG painting and texturing on 3D shapes: (An error occurred during the printing of this article that reversed the print order of pages 118 and 119. While we have\\xa0…', 'ConMan: A visual programming language for interactive graphics', 'Texture mapping as a fundamental drawing primitive', 'Method and apparatus for painting on a computer', 'System and method for manipulating digital images', 'System and method of cropping an image', 'System and method of changing attributes of an image-based product', 'A multifocus method for controlling depth of field', 'Fabrication of free form structures from planar materials', 'Image processing by linear interpolation and extrapolation', 'Digital filtering for lenticular printing', 'Large-area synthesis of high-quality and uniform graphene films on copper foils', 'Observation of a Spin-Flip 1 Transition in ', 'The KSTAR project: An advanced steady state superconducting tokamak experiment', 'Fundamental physics at the intensity frontier', 'High transparency La2O3-CaO-B2O3-SiO2 glass for diagnosis x-rays shielding material application', 'Gamma radiation shielding and optical properties measurements of zinc bismuth borate glasses', 'Newton-type algorithms for dynamics-based robot movement optimization', 'Measurements of branching fractions and direct  asymmetries for ,  and  decays', 'Optimal robot motions for physical criteria', 'Luminescence characteristics of Dy3+ doped Gd2O3-CaO-SiO2-B2O3 scintillating glasses', 'Initial performance of the COSINE-100 experiment', 'Fast simulation of skeleton-driven deformable body characters', 'Influence of Er3+ ion concentration on optical and photoluminescence properties of Er3+-doped gadolinium-calcium silica borate glasses', 'New gadolinium based glasses for gamma-rays shielding materials', 'Observation of  and  decay into ', 'Development of lithium yttrium borate glass doped with Dy3+ for laser medium, W-LEDs and scintillation materials applications', 'Energy transfer from Gd3+ to Sm3+ and luminescence characteristics of CaO–Gd2O3–SiO2–B2O3 scintillating glasses', 'White light emission of dysprosium doped lanthanum calcium phosphate oxide and oxyfluoride glasses', 'Physically based grasp quality evaluation under pose uncertainty', 'Production and optical properties of Gd-loaded liquid scintillator for the RENO neutrino detector', 'Re-tiling polygonal surfaces', 'Generating textures on arbitrary surfaces using reaction-diffusion', 'Shape transformation using variational implicit functions', 'Simplification and repair of polygonal models using volumetric techniques', 'LCIS: A boundary hierarchy for detail-preserving contrast reduction', 'Modelling with implicit surfaces that interpolate', 'Fast and memory efficient polygonal simplification', 'State of the art in example-based texture synthesis', 'Texture synthesis on surfaces', 'Image-guided streamline placement', 'Rigid fluid: animating the interplay between rigid bodies and fluid', 'Feature-based surface parameterization and texture mapping', 'Image-driven simplification', 'Terrain synthesis from digital elevation models', 'Preparing for the unknown: Learning a universal policy with online system identification', 'Progressive growing of GANs for improved quality, stability, and variation', 'Analyzing and improving the image quality of StyleGAN', 'GANSpace: Discovering Interpretable GAN Controls', 'Differentiable Monte Carlo Ray Tracing through Edge Sampling', 'Learning to predict 3D objects with an interpolation-based differentiable renderer', 'Recent advances in adaptive sampling and reconstruction for Monte Carlo rendering', 'Two-shot SVBRDF capture for stationary materials', 'Reflectance modeling by neural texture synthesis', 'Practical SVBRDF capture in the frequency domain.', 'Temporal light field reconstruction for rendering distribution effects', 'Matrix radiance transfer', 'PCT: Point Cloud Transformer', 'Traffic-sign detection and classification in the wild', 'Attention Mechanisms in Computer Vision: A Survey', 'Salientshape: Group saliency in image collections', 'Beyond self-attention: External attention using two linear layers for visual tasks', 'A shape‐preserving approach to image resizing', 'Visual attention network', '3-sweep: Extracting editable objects from a single photo', 'Geometry and convergence analysis of algorithms for registration of 3D shapes', 'Morphing and sampling network for dense point cloud completion', 'Sketch2Scene: Sketch-based Co-retrieval and Co-placement of 3D Models', 'Repfinder: finding approximately repeated scene elements for image editing', 'Modifying the shape of NURBS surfaces with geometric constraints', 'Fast mesh segmentation using random walks', 'Efficient affinity-based edit propagation using kd tree', 'Properties of two types of generalized Ball curves', 'A second order algorithm for orthogonal projection onto curves and surfaces', 'Supervised contrastive learning', 'Sift flow: Dense correspondence across scenes and its applications', 'Deep convolutional neural network for image deconvolution', 'Beyond pixels: exploring new representations and applications for motion analysis', 'Sift flow: Dense correspondence across different scenes', 'DepthTransfer: Depth Extraction from Video Using Non-parametric Sampling', 'Noise estimation from a single image', 'Face hallucination: Theory and practice', 'Florence: A new foundation model for computer vision', 'A two-step approach to hallucinating faces: global parametric model and local nonparametric model', 'Motion magnification', 'On Bayesian adaptive video super resolution', 'Nonparametric scene parsing via label transfer', 'Unsupervised joint object discovery and segmentation in internet images', 'Nonparametric scene parsing: Label transfer via dense scene alignment', 'Depth extraction from video using non-parametric sampling', 'Exploring features in a bayesian framework for material recognition', 'Nerd: Neural reflectance decomposition from image collections', 'Color transfer between images', 'Photographic tone reproduction for digital images', 'Fundamentals of computer graphics', 'A polygonal approximation to direct scalar volume rendering', 'A non-photorealistic lighting model for automatic technical illustration', 'A model of visual adaptation for realistic image synthesis', 'An anisotropic phong brdf model', 'A practical analytic model for daylight', 'Interactive ray tracing for isosurface rendering', 'Realistic ray tracing', 'A microfacet-based BRDF generator', 'Ray tracing deformable scenes using dynamic bounding volume hierarchies', 'Visual navigation of large environments using textured clusters', 'Interactive technical illustration', 'Interactive ray tracing', 'Spatially nonuniform scaling functions for high contrast images', 'State of the art in ray tracing animated scenes', 'Monte carlo techniques for direct lighting calculations', 'Interactive ray tracing for volume visualization', 'A model of visual masking for computer graphics', 'High-quality pre-integrated volume rendering using hardware-accelerated pixel shading', 'Efficiently using graphics hardware in volume rendering applications', 'Word cloud explorer: Text analytics based on word clouds', 'Interactive volume on standard PC graphics hardware using multi-textures and multi-stage rasterization', 'Smart hardware-accelerated volume rendering', 'A simple and flexible volume rendering framework for graphics-hardware-based raycasting', 'State-of-the-Art of Visualization for Eye Tracking Data', 'Spatiotemporal social media analytics for abnormal event detection and examination using seasonal-trend decomposition', 'Visualization of eye tracking data: A taxonomy and survey', 'Design and development of an indoor navigation and object identification system for the blind', 'Visualizing ontologies with VOWL', 'Hardware-based ray casting for tetrahedral meshes', 'Interactive clipping techniques for texture-based volume visualization and volume shading', 'Hardware-accelerated volume and isosurface rendering based on cell-projection', 'Level-of-detail volume rendering via 3D textures', 'Spatiotemporal anomaly detection through visual analysis of geolocated twitter messages', 'Real-time exploration of regular volume data by adaptive reconstruction of isosurfaces', 'Public behavior response analysis in disaster events utilizing visual analytics of microblog data', 'Visual classifier training for text document retrieval', 'Scatterblogs2: Real-time monitoring of microblog messages through user-guided filtering', 'A survey of general‐purpose computation on graphics hardware', 'GPU computing', 'Level of detail for 3D graphics', 'Optix: a general purpose ray tracing engine', 'View-dependent simplification of arbitrary polygonal environments', \"A Developer's Survey of Polygonal Simplification Algorithms\", 'Near-eye light field displays', 'General-purpose computation on graphics hardware', 'Towards foveated rendering for gaze-tracked virtual reality', 'The light field stereoscope.', 'Portals and mirrors: Simple, fast evaluation of potentially visible sets', 'A multigrid solver for boundary value problems using programmable graphics hardware', 'CUDA: Scalable parallel programming for high-performance scientific computing', 'How gpus work', 'Near-eye microlens array displays', 'Perceptually driven simplification for interactive rendering', 'A high-accuracy, low-cost localization system for wireless sensor networks', 'Towards virtual reality infinite walking: dynamic saccadic redirection', 'QBIC project: querying images by content, using color, texture, and shape', 'A signal processing approach to fair surface design', 'The ball-pivoting algorithm for surface reconstruction', 'Estimation of planar curves, surfaces, and nonplanar space curves defined by implicit equations with applications to edge and range image segmentation', 'Geometric compression through topological surgery', 'Estimating the tensor of curvature of a surface from a polyhedral approximation', 'Curve and surface smoothing without shrinkage', 'Simple, accurate, and robust projector-camera calibration', 'Progressive forest split compression', \"Building a digital model of Michelangelo's Florentine Pieta\", 'Produce recognition system', 'Geometric signal processing on polygonal meshes', 'Optimal surface smoothing as filter design', 'SSD: Smooth signed distance surface reconstruction', 'Geometry coding and VRML', 'Parameterized families of polynomials for bounded algebraic curve and surface fitting', 'A benchmark for surface reconstruction', 'Linear anisotropic mesh filtering', 'Object imaging system', 'Applying shape from lighting variation to bump map capture', 'Spatial-depth super resolution for range images', 'Stereo matching with color-weighted correlation, hierarchical belief propagation, and occlusion handling', 'Ga-net: Guided aggregation net for end-to-end stereo matching', 'Salient object detection in the deep learning era: An in-depth survey', 'The apolloscape dataset for autonomous driving', 'The apolloscape open dataset for autonomous driving and its application', 'Saliency-aware video object segmentation', 'Multi-resolution real-time stereo on commodity graphics hardware', 'High-quality real-time stereo using adaptive cost aggregation and dynamic programming', 'Fusion of time-of-flight depth and stereo for high accuracy depth maps', 'Real-time Global Stereo Matching Using Hierarchical Belief Propagation.', 'Camera-based calibration techniques for seamless multiprojector displays', 'Iou loss for 2d/3d object detection', 'Accurate 3d pose estimation from a single depth image', 'A survey on human motion analysis from depth data', 'Depth estimation via affinity learned with convolutional spatial propagation network', 'Attention u-net: Learning where to look for the pancreas', 'Attention u-net: Learning where to look for the pancreas. arXiv 2018', 'Real-time use of artificial intelligence in identification of diminutive polyps during colonoscopy: a prospective study', 'Self-supervised learning for medical image analysis using image context restoration', 'Automated abdominal multi-organ segmentation with subject-specific atlas generation', 'Artificial intelligence-assisted polyp detection for colonoscopy: initial experience', 'An application of cascaded 3D fully convolutional networks for medical image segmentation', \"Extraction of airways from CT (EXACT'09)\", 'DRINet for medical image segmentation', 'Automated anatomical labeling of the bronchial branch and its application to the virtual bronchoscopy system', 'Fast generation of digitally reconstructed radiographs using attenuation fields with application to 2D-3D image registration', 'Fully automated diagnostic system with artificial intelligence using endocytoscopy to identify the presence of histologic inflammation associated with ulcerative colitis (with\\xa0…', 'Hierarchical 3D fully convolutional networks for multi-organ segmentation', 'Characterization of colorectal lesions using a computer-aided diagnostic system for narrow-band imaging endocytoscopy', 'Artificial intelligence-assisted system improves endoscopic identification of colorectal neoplasms', 'Expression levels of thymidine phosphorylase and dihydropyrimidine dehydrogenase in various human tumor tissues.', 'Exploring duplicated regions in natural images', 'Tracking of a bronchoscope using epipolar geometry analysis and intensity-based image registration of real and virtual endoscopic images', 'Discriminative dictionary learning for abdominal multi-organ segmentation', 'Recognition of bronchus in three-dimensional X-ray CT images with applications to virtualized bronchoscopy system', 'Teddy: a sketching interface for 3D freeform design', 'As-rigid-as-possible shape manipulation', 'Fibermesh: designing freeform surfaces with 3d curves', 'Speed-dependent automatic zooming for browsing large documents', 'Flatland: New dimensions in office whiteboards', 'Laplacian mesh optimization', 'Interactive beautification: A technique for rapid geometric design', 'Toward acceptable domestic robots: Applying insights from social psychology', 'A suggestive interface for 3D drawing', 'Plushie: an interactive design system for plush toys', 'Evaluating human-robot interaction: Focusing on the holistic interaction experience', 'System for applying application behaviors to freeform data', 'Sensitive couture for interactive garment modeling and editing.', 'Voice as sound: using non-verbal voice input for interactive control', 'Overlay presentation of textual and graphical annotations', 'Interactive design of botanical trees using freehand sketches and example-based editing', 'Eyes on a Car: an Interface Design for Communication between an Autonomous Car and a Pedestrian', 'SketchChair: an all-in-one chair design system for end users', 'Guided exploration of physically valid shapes for furniture design.', 'Structured annotations for 2D-to-3D modeling', 'As-rigid-as-possible surface modeling', 'Optimized scale-and-stretch for image resizing', 'On linear variational surface deformation methods', 'Shell structures for architecture: form finding and optimization', 'Color harmonization', 'A comparative study of image retargeting', 'Bounded biharmonic weights for real-time deformation.', 'Linear rotation-invariant coordinates for meshes', 'Make it stand: balancing shapes for 3D fabrication', 'Least-squares rigid motion using svd', 'Depth synthesis and local warps for plausible image-based navigation', 'Differential representations for mesh processing', 'A sketch-based interface for detail-preserving mesh editing', 'Laplacian mesh processing', 'Instant field-aligned meshes.', 'Reverse engineering of geometric models—an introduction', 'An overview of genetic algorithms: Part 1, fundamentals', 'Pct: Point cloud transformer', 'An overview of genetic algorithms: Part 2, research topics', 'A sequential niche technique for multimodal function optimization', 'Registration of 3D point clouds and meshes: A survey from rigid to nonrigid', 'Faithful least-squares fitting of spheres, cylinders, cones and tori for reliable segmentation', 'Fast and effective feature-preserving mesh denoising', 'Merging and splitting eigenspace models', 'Incremental eigenanalysis for classification.', 'Algorithms for reverse engineering boundary representation models', 'Differential geometry applied to curve and surface design', 'Constrained fitting in reverse engineering', 'Robust segmentation of primitives from range data in the presence of geometric degeneracy', 'A survey of blending methods that use parametric surfaces', 'Sweeping of three-dimensional objects', 'Mesh saliency via spectral processing', 'Computing and rendering point set surfaces', 'Point set surfaces', 'How do humans sketch objects?', 'As-rigid-as-possible shape interpolation', 'Visualizing time-series on spirals.', 'Sketch-based image retrieval: Benchmark and bag-of-features descriptors', 'Sketch-based shape retrieval', 'Representing animations by principal components', 'Abc: A big cad model dataset for geometric deep learning', 'Differential coordinates for local mesh morphing and deformation', 'Recent advances in mesh morphing', 'Context-based surface completion', 'Linear combination of transformations', 'Depth of presence in virtual environments', 'Walking> walking-in-place> flying, in virtual environments', 'Taking steps: the influence of a walking technique on presence in virtual reality', 'A virtual presence counter', 'The drift table: designing for ludic engagement', 'The influence of body movement on subjective presence in virtual environments', 'The impact of avatar realism and eye gaze control on perceived quality of communication in a shared immersive virtual environment', 'Next-generation big data analytics: State of the art, challenges, and future research topics', 'Public speaking in virtual reality: Facing an audience of avatars', '3D-printing of non-assembly, articulated models', 'Is the rubber hand illusion induced by immersive virtual reality?', 'Lessons from the lighthouse: collaboration in a shared mixed reality system', 'Computer graphics and virtual environments: from realism to real-time', 'Redirected walking in place', 'Expected, sensed, and desired: A framework for designing sensing-based interaction', 'Human tails: ownership and control of extended humanoid avatars', 'Automatic recognition of non-acted affective postures', 'Collaborating in networked immersive spaces: as good as being there together?', 'Spatial social behavior in second life', 'The impact of a self-avatar on cognitive load in immersive virtual reality', 'The Unreasonable Effectiveness of Deep Features as a Perceptual Metric', 'Making Convolutional Networks Shift-Invariant Again', 'Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction', 'Real-time user-guided image colorization with learned deep priors', 'CNN-generated images are surprisingly easy to spot... for now', 'Stochastic adversarial video prediction', 'Swapping Autoencoder for Deep Image Manipulation', 'On Aliased Resizing Libraries and Surprising Subtleties in FID Calculation', 'Few-shot Image Generation via Cross-domain Correspondence', 'Sensor Fusion for Semantic Segmentation of Urban Scenes', 'Editing Conditional Radiance Fields', 'Few-shot Image Generation with Elastic Weight Consolidation', 'Interactive Sketch & Fill: Multiclass Sketch-to-Image Translation', 'Multi-Concept Customization of Text-to-Image Diffusion', 'Detecting Photoshopped Faces by Scripting Photoshop', 'Scaling up GANs for Text-to-Image Synthesis', 'Transforming and Projecting Images into Class-conditional Generative Networks', 'Motion graphs', 'Retargetting motion to new characters', 'Content-preserving warps for 3D video stabilization', 'Visual comparison for information visualization', 'Automated extraction and parameterization of motions in large data sets', 'Through-the-lens camera control', 'Motion editing with spacetime constraints', 'Building efficient, accurate character skins from examples', 'Flexible automatic motion blending with registration curves.', 'Computer puppetry: An importance-based approach', 'Subspace video stabilization', 'Automatic image retargeting', 'Scalable behaviors for crowd simulation', 'Footskate cleanup for motion capture editing', 'Interactive dynamics', 'Parametric motion graphs', 'Automatic image retargeting with fisheye-view warping', 'Video retargeting: automating pan and scan', 'Error bars considered harmful: Exploring alternate encodings for mean and error', 'Image retargeting using mesh parametrization', 'An image synthesizer', 'Texturing & modeling: a procedural approach', 'Improv: A system for scripting interactive actors in virtual worlds', 'Real-time continuous pose recovery of human hands using convolutional networks', 'Improving noise', 'Pad: an alternative approach to the computer interface', 'Hypertexture', 'Accelerating eulerian fluid simulation with convolutional networks', 'Method and apparatus for providing input to a processor, and a sensor pad', 'Fractal computer user centerface with zooming capability', 'Real time responsive animation with personality', 'An autostereoscopic display', 'Pad++: A zoomable graphical sketchpad for exploring alternate interface physics', 'Quikwriting: continuous stylus-based text entry', 'The impact of individual, competitive, and collaborative mathematics game play on learning, performance, and motivation.', 'Painterly rendering for video and interaction', 'A survey of procedural noise functions', 'Measuring bidirectional texture reflectance with a kaleidoscope', 'Gestures and touches on force-sensitive input devices', 'Human-guided simple search: combining information visualization and heuristic search', 'StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks', 'AttnGAN: Fine-grained text to image generation with attentional generative adversarial networks', 'StackGAN++: Realistic image synthesis with stacked generative adversarial networks', 'Traffic-Sign Detection and Classification in the Wild', 'SegAN: Adversarial Network with Multi-scale L1 Loss for Medical Image Segmentation', 'Spda-cnn: Unifying semantic part detection and abstraction for fine-grained recognition', 'Salientshape: group saliency in image collections', 'Shape registration in implicit spaces using information theory and free form deformations', 'RepFinder: finding approximately repeated scene elements for image editing', 'High Resolution Acquisition, Learning and Transfer of Dynamic 3‐D Facial Expressions', 'Segmentation and tracking of cytoskeletal filaments using open active contours', 'Phase-field modeling and machine learning of electric-thermal-mechanical breakdown of polymer-based dielectrics', 'Multimodal deep learning for cervical dysplasia diagnosis', 'Learning with dynamic group sparsity', 'Multimodal Recurrent Model with Attention for Automated Radiology Report Generation', 'System and method for whole body landmark detection, segmentation and change quantification in digital images', 'Simultaneous image transformation and sparse representation recovery', 'Metamorphs: Deformable shape and texture models', 'Medical Image Segmentation', 'Flocks, herds and schools: A distributed behavioral model', 'Steering behaviors for autonomous characters', 'Computer animation with scripts and actors', 'Competition, coevolution and the game of tag', 'Interaction with groups of autonomous characters', 'An evolved, vision-based behavioral model of coordinated group motion', 'Big fast crowds on ps3', 'Boids background and update', 'Boids (flocks, herds, and schools: a distributed behavioral model)', 'Evolution of corridor following behavior in a noisy world', '10 Evolution of Obstacle Avoidance Behavior: Using Noise to Promote Robust Solutions', 'An Evolved, Vision-Based Model of Obstacle Avoidance Behavior', 'Not bumping into things', 'Individual-based models', \"Stylized Depiction in Computer Graphics: Non-Photorealistic, Painterly and'Toon Rendering\", 'Interactive evolution of camouflage', 'Stylized depiction in computer graphics', 'The difficulty of roving eyes [vehicle control by genetic programming]', 'Computer animation in the world of actors and scripts', 'Composing Social Interactions via Social Games', 'Recent advances in augmented reality', 'A touring machine: Prototyping 3D mobile augmented reality systems for exploring the urban environment', 'Knowledge-based augmented reality', 'A psychological perspective on augmented reality in the mathematics classroom', 'Windows on the world: 2D windows for 3D augmented reality', 'DART: a toolkit for rapid design exploration of augmented reality experiences', 'Augmented reality in architectural construction, inspection and renovation', 'Roomalive: Magical experiences enabled by scalable, adaptive projector-camera units', 'Support for multitasking and background awareness using interactive peripheral displays', 'Wizard of Oz support throughout an iterative design process', 'Enveloping users and computers in a collaborative 3D augmented reality', 'Presence and engagement in an interactive drama', 'New media and the permanent crisis of aura', 'Exploring spatial narratives and mixed reality experiences in Oakland Cemetery', 'Living laboratories: the future computing environments group at the Georgia Institute of Technology', 'Integrating virtual and physical context to support knowledge workers', 'A distributed 3D graphics library', 'Game jams: Community, motivations, and learning among jammers', 'Augmented reality as a new media experience', 'Chromium: a stream-processing framework for interactive rendering on clusters', 'Digital light field photography', 'Fourier slice photography', 'Plenoctrees for real-time rendering of neural radiance fields', 'DiffuserCam: lensless single-exposure 3D imaging', 'Burst denoising with kernel prediction networks', 'Single image reflection separation with perceptual losses', 'Learning to synthesize a 4D RGBD light field from a single image', 'Stegastamp: Invisible hyperlinks in physical photographs', 'Zoom to learn, learn to zoom', 'Learned initializations for optimizing coordinate-based neural representations', 'Partnet: A large-scale benchmark for fine-grained and hierarchical part-level 3d object understanding', 'Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation', 'Gspn: Generative shape proposal network for 3d instance segmentation in point cloud', 'Sapien: A simulated part-based interactive environment', 'Structurenet: Hierarchical graph networks for 3d shape generation', 'Supervised fitting of geometric primitives to 3d point clouds', 'Category-level articulated object pose estimation', 'Complete & label: A domain adaptation approach to semantic segmentation of lidar point clouds', 'Pointnet++: Deep hierarchical feature learning on point sets in a metric space. arXiv 2017', 'Shapenet: An information-rich 3d model repository. arXiv 2015', 'Texturenet: Consistent local parametrizations for learning from high-resolution signals on meshes', 'Curriculum deepsdf', 'Deep part induction from articulated object pairs', 'Large-scale 3d shape reconstruction and segmentation from shapenet core55', 'Learning hierarchical shape segmentation and labeling from online repositories', 'P4contrast: Contrastive learning with pairs of point-pixel pairs for rgb-d scene understanding', 'Contrastive multimodal fusion with tupleinfonce', 'Analysis of emotion recognition using facial expressions, speech and multimodal information', 'Expression cloning', 'Sgpn: Similarity group proposal network for 3d point cloud instance segmentation', 'The virtual classroom: a virtual reality environment for the assessment and rehabilitation of attention deficits', 'Recurrent slice networks for 3d segmentation of point clouds', 'Modeling and video projection for augmented virtual environments', 'Disn: Deep implicit surface network for high-quality single-view 3d reconstruction', 'Hybrid inertial and vision tracking for augmented reality registration', 'Cognitive, performance, and systems issues for augmented reality applications in manufacturing and maintenance', 'Fusion of vision and gyro tracking for robust augmented reality registration', 'Accelerating volume reconstruction with 3D texture hardware', 'Dynamic registration correction in video-based augmented reality systems', 'Depth-aware cnn for rgb-d segmentation', 'Natural feature tracking for augmented reality', 'Augmented-reality tool employing scene-feature autocalibration during camera motion', 'Approaches to large-scale urban modeling', 'Grid-gcn for fast and scalable point cloud learning', 'Rigid head motion in expressive speech animation: Analysis and synthesis', 'Virtual environment applications in clinical neuropsychology', 'Virtual reality and cognitive rehabilitation: a brief review of the future', 'Interaction of antimicrobial dermaseptin and its fluorescently labeled analogs with phospholipid membranes', 'Condition-Dependent Transcriptome Reveals High-Level Regulatory Architecture in Bacillus subtilis', 'Peptides as weapons against microorganisms in the chemical defense system of vertebrates', 'The complete genome sequence of Lactobacillus bulgaricus reveals extensive and ongoing reductive evolution', 'Multifunctional host defense peptides: intracellular‐targeting antimicrobial peptides', 'Isolation, amino acid sequence and synthesis of dermaseptin, a novel antimicrobial peptide of amphibian skin', 'The vertebrate peptide antibiotics dermaseptins have overlapping structural features but target specific microorganisms.', 'Global Network Reorganization During Dynamic Adaptations of Bacillus subtilis Metabolism', 'Meningococcal meningitis: unprecedented incidence of serogroup X—related cases in 2006 in Niger', 'Antimicrobial peptides from hylid and ranin frogs originated from a 150‐million‐year‐old ancestral precursor with a conserved signal peptide but a hypermutable antimicrobial domain', 'Complete genome sequence of the fish pathogen Flavobacterium psychrophilum', 'Isolation and structure of novel defensive peptides from frog skin', 'Precursors for peptide hormones share common secondary structures forming features at the proteolytic processing sites', 'The NH2-terminal alpha-helical domain 1-18 of dermaseptin is responsible for antimicrobial activity.', 'Staphylococcus aureus Transcriptome Architecture: From Laboratory to Infection-Mimicking Conditions', 'A global metagenomic map of urban microbiomes and antimicrobial resistance', 'Roles of diversifying selection and coordinated evolution in the evolution of amphibian antimicrobial peptides', 'The WalKR system controls major staphylococcal virulence genes and is involved in triggering the host inflammatory response', 'Fit genotypes and escape variants of subgroup III  Neisseria meningitidis during three pandemics of  epidemic meningitis', 'Covalent structure, synthesis, and structure-function studies of mesentericin Y 10537, a defensive peptide from Gram-positive bacteria Leuconostoc mesenteroides', 'MeshLab: an Open-Source 3D Mesh Processing System', 'Meshlab: an open-source mesh processing tool.', 'Metro: Measuring Error on Simplified Surfaces', 'A comparison of mesh simplification algorithms', 'Ambient occlusion and edge cueing for enhancing real time molecular visualization', 'A low cost 3D scanner based on structured light', 'DeWall: A fast divide and conquer Delaunay triangulation algorithm in Ed', 'Polycube-maps', 'Efficient and flexible sampling with blue noise properties of triangular meshes', 'BDAM—Batched Dynamic Adaptive Meshes for high performance terrain visualization', 'Multiresolution decimation based on global error', 'Speeding up isosurface extraction using interval trees', 'Elastic textures for additive fabrication', 'External memory management and simplification of huge meshes', 'Adaptive tetrapuzzles: efficient out-of-core construction and visualization of gigantic multiresolution polygonal models', 'Planet-sized batched dynamic adaptive meshes (P-BDAM)', 'Protected interactive 3D graphics via remote rendering', 'Representation and visualization of terrain surfaces at variable resolution', 'Multiple textures stitching and blending on 3D objects', 'A general method for preserving attribute values on simplified meshes', 'Habitat: A platform for embodied ai research', 'On evaluation of embodied navigation agents', 'The Replica dataset: A digital replica of indoor spaces', 'Example-based synthesis of 3D object arrangements', 'Back-action-evading measurements of nanomechanical motion', 'Physically-based rendering for indoor scene understanding using convolutional neural networks', 'Habitat 2.0: Training home assistants to rearrange their habitat', 'MINOS: Multimodal indoor simulator for navigation in complex environments', 'Characterizing structural relationships in scenes using graph kernels', 'Scan2cad: Learning cad model alignment in rgb-d scans', 'Deep Convolutional Priors for Indoor Scene Synthesis', 'Learning spatial knowledge for text to 3D scene generation', 'Text2shape: Generating shapes from natural language by learning joint embeddings', 'Shrec16 track: largescale 3d shape retrieval from shapenet core55', 'Component-based face recognition with 3D morphable models', 'What object attributes determine canonical views?', 'Perceptual evaluation of tone mapping operators with real-world scenes', 'Evaluation of a shape-based model of human face discrimination using FMRI and behavioral techniques', '3D shape and 2D surface textures of human faces: The role of “averages” in attractiveness and age', 'To what extent do unique parts influence recognition across changes in viewpoint?', 'Estimating coloured 3D face models from single images: An example based approach', 'Free-form deformation of solid geometric models', 'T-splines and T-NURCCs', 'Isogeometric analysis using T-splines', 'T-spline simplification and local refinement', 'Conversion of complex contour line definitions into polygonal element mosaics', 'A physically based approach to 2–D shape blending', 'Local refinement of analysis-suitable T-splines', 'Implicit representation of parametric curves and surfaces', 'Isogeometric boundary element analysis using unstructured T-splines', '2-D shape blending: an intrinsic solution to the vertex path problem', 'Isogeometric finite element data structures based on Bézier extraction of T‐splines', 'Non-uniform recursive subdivision surfaces', 'Ray tracing trimmed rational surface patches', 'On linear independence of T-spline blending functions', 'Implicitization using moving curves and surfaces', 'Loop detection in surface patch intersections', 'Piecewise algebraic surface patches', 'Curve intersection using Bézier clipping', 'Implicit and parametric curves and surfaces for computer aided geometric design', 'Watertight trimmed NURBS', 'Heart disease and stroke statistics 2005 update', 'Learning and building together in an immersive virtual world', 'Reliable blast UDP: Predictable high performance bulk data transfer', 'CAVE2: a hybrid reality environment for immersive simulation and information analysis', 'The NICE project: Learning together in a virtual world', 'Scientists in wonderland: A report on visualization applications in the CAVE virtual reality environment', 'The optiputer', 'High-performance dynamic graphics streaming for scalable adaptive graphics environment', 'Sage: the scalable adaptive graphics environment', 'Multi-perspective collaborative design in persistent networked virtual environments', 'Advances in the dynallax solid-state dynamic parallax barrier autostereoscopic visualization display system', 'CAVERN: A distributed architecture for supporting scalable persistence and interoperability in collaborative virtual environments', 'Ethnic minorities and coronary heart disease: an update and future directions', 'SAGE2: A new approach for data intensive collaboration using Scalable Resolution Shared Displays', 'A review of tele-immersive applications in the CAVE research network', 'The future of the CAVE', 'NICE: combining constructionism, narrative and collaboration in a virtual learning environment', 'Articulate: A Semi-automated Model for Translating Natural Language Queries into Meaningful Visualizations', 'Optical network infrastructure for grid', 'CAVERNsoft G2: a toolkit for high performance tele-immersive collaboration', 'DualGAN: Unsupervised dual learning for image-to-image translation', 'Learning implicit fields for generative shape modeling', 'A survey on shape correspondence', 'Consolidation of unorganized point clouds for surface reconstruction', 'GRASS: Generative recursive autoencoders for shape structures', 'Curve skeleton extraction from incomplete point cloud', 'Edge-aware point set resampling', 'Automatic reconstruction of tree skeletal structures from point clouds', 'Segmentation of 3D meshes through spectral clustering', 'Point cloud skeletons via laplacian based contraction', 'L1-medial skeleton of point cloud.', 'Unsupervised co-segmentation of a set of shapes via descriptor-space spectral clustering', 'Active co-analysis of a set of shapes', 'Mean curvature skeletons', 'BSP-Net: Generating compact meshes via binary space partitioning', 'A spectral approach to shape-based retrieval of articulated 3D models', 'Spectral mesh processing', 'Deformation‐driven shape correspondence', 'Smartboxes for interactive urban reconstruction', 'Robust 3D shape correspondence in the spectral domain', 'A complete electron microscopy volume of the brain of adult Drosophila melanogaster', 'Patient geometry‐driven information retrieval for IMRT treatment plan quality control', 'Data-driven approach to generating achievable dose–volume histogram objectives in intensity-modulated radiotherapy planning', 'Reconstruction of solid models from oriented point sets', 'Symmetry descriptors and 3D shape matching', 'A reflective symmetry descriptor', 'Non-photorealistic virtual environments', 'Streaming multigrid for gradient-domain operations on large images', 'Shape-based retrieval and analysis of 3D models', 'Multilevel streaming for out-of-core surface reconstruction', 'Shape matching and anisotropy', 'Increased organ sparing using shape-based treatment plan optimization for intensity modulated radiation therapy of pancreatic adenocarcinoma', 'Early experiences with a 3D model search engine', 'Scene representation networks: Continuous 3d-structure-aware neural scene representations', 'Mofa: Model-based deep convolutional face autoencoder for unsupervised monocular reconstruction', 'State of the art on monocular 3D face reconstruction, tracking, and applications', 'Non-rigid neural radiance fields: Reconstruction and novel view synthesis of a dynamic scene from monocular video', 'Dynamic neural radiance fields for monocular 4d facial avatar reconstruction', 'Real-time joint tracking of a hand manipulating an object from rgb-d input', 'State of the art on 3D reconstruction with RGB‐D cameras', 'Self-supervised multi-level face model learning for monocular reconstruction at over 250 hz', 'The generalized patchmatch correspondence algorithm', 'Robust mesh watermarking', 'Lapped textures', 'Real-time hatching', 'WYSIWYG NPR: Drawing strokes directly on 3D models', 'Turkergaze: Crowdsourcing saliency with webcam based eye tracking', 'Pairedcyclegan: Asymmetric style transfer for applying and removing makeup', 'Multiperspective panoramas for cel animation', 'Where do people draw lines?', 'How well do line drawings depict shape?', 'Palette-based photo recoloring.', 'Real-time fur over arbitrary surfaces', 'Text-based editing of talking-head video', 'Digital bas-relief from 3D scenes', 'Coherent stylized silhouettes', 'Building and using a scalable display wall system', 'A modified look-up table for implicit disambiguation of marching cubes', 'Discretized marching cubes', 'Multiresolution modeling and visualization of volume data based on simplicial complexes', '3DHOP: 3D heritage online presenter', 'Multiresolution volume visualization with a texture-based octree', 'Simplification, LOD and multiresolution: Principles and applications', 'Multiple Textures Stitching and Blending on 3D Objects', 'Interactive (de) weathering of an image using physical models', 'Analysis of rain and snow in frequency space', 'Estimating the natural illumination conditions from a single outdoor image', 'Removing weather effects from monochrome images', 'Yield estimation in vineyards by visual grape detection', 'Shedding light on the weather', 'Detecting ground shadows in outdoor consumer photographs', 'Structured light in scattering media', 'What do the sun and the sky tell us about the camera?', 'Automated visual yield estimation in vineyards', 'A practical analytic single scattering model for real time rendering', 'Acquiring scattering properties of participating media by dilution', 'Structured light 3D scanning in the presence of global illumination', 'Exploring the spatial hierarchy of mixture models for human pose estimation', 'Real-time volume graphics', 'The state of the art in flow visualization: Dense and texture‐based techniques', 'A taxonomy and survey of dynamic graph visualization', 'State-of-the-art of visualization for eye tracking data.', 'State of the Art of Parallel Coordinates.', 'The State of the Art in Visualizing Dynamic Graphs.', 'Visual interaction with dimensionality reduction: A structured literature analysis', 'Visual analytics methodology for eye movement studies', 'Parallel edge splatting for scalable dynamic graph visualization', 'Continuous scatterplots', 'Analysis and visualisation of movement: an interdisciplinary review', 'Interactive cutaway illustrations', 'Evaluation of traditional, orthogonal, and radial tree diagrams by an eye tracking study', 'What you see is what you can change: Human-centered machine learning by interactive visualization', 'A parallel preconditioned conjugate gradient solver for the Poisson problem on a multi-GPU platform', 'Quality metrics for information visualization', 'Continuous parallel coordinates', 'Scale-recurrent Network for Deep Image Deblurring', 'Investigating haze-relevant features in a learning framework for image dehazing', 'Deep Video Deblurring for Hand-held Cameras', 'Video snapcut: robust video object cutout using localized classifiers', 'Detail-revealing deep video super-resolution', 'Edge-based blur kernel estimation using patch priors', 'An iterative optimization approach for unified image segmentation and matting', 'A perceptually motivated online benchmark for image matting', 'Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training', 'Text localization in natural images using stroke feature transform and text covariance descriptors', 'Video tooning', 'Image and video segmentation by anisotropic kernel mean shift', 'Handling Outliers in Non-Blind Image Deconvolution', 'The cartoon animation filter', 'Soft scissors: an interactive tool for realtime high quality matting', 'Handling noise in single image deblurring using directional filters', 'Deep Photo Style Transfer', 'Local Laplacian filters: edge-aware image processing with a Laplacian pyramid', 'Two-scale tone management for photographic look', 'Deep joint demosaicking and denoising', 'Displacement interpolation using Lagrangian mass transport', 'Decoupling algorithms from schedules for easy optimization of image processing pipelines', '6D hands: markerless hand-tracking for computer aided design', 'Automatic portrait segmentation for image stylization', 'Automatic photo adjustment using deep neural networks', 'Fast local Laplacian filters: theory and applications', 'User-assisted intrinsic images', 'A topological approach to hierarchical segmentation using mean shift', 'Blur kernel estimation using the Radon transform', 'Recent progress in layered transition metal carbides and/or nitrides (MXenes) and their composites: synthesis and applications', 'In‐situ formation of hollow hybrids composed of cobalt sulfides embedded within porous carbon polyhedra/carbon nanotubes for high‐performance lithium‐ion batteries', 'Recent progress in synthesis, properties and potential applications of SiC nanomaterials', 'Metal–Organic‐Framework‐Based Catalysts for Photoreduction of CO2', 'Zeolitic Imidazolate Framework 67‐Derived High Symmetric Porous Co3O4 Hollow Dodecahedra with Highly Enhanced Lithium Storage Capability', 'Recent progress on polymer materials for additive manufacturing', 'Polymeric composites for powder-based additive manufacturing: Materials and applications', 'Porous Spinel ZnxCo3–xO4 Hollow Polyhedra Templated for High-Rate Lithium-Ion Batteries', 'MOF-templated formation of porous CuO hollow octahedra for lithium-ion battery anode materials', 'Carbon nanomaterials in tribology', 'Emerging 3D‐printed electrochemical energy storage devices: a critical review', 'Scalable synthesis of Ca-doped α-Fe2O3 with abundant oxygen vacancies for enhanced degradation of organic pollutants through peroxymonosulfate activation', 'Recent advances on high‐entropy alloys for 3D printing', 'Modeling temperature and residual stress fields in selective laser melting', 'Self-adjusting activity induced by intrinsic reaction intermediate in Fe–N–C single-atom catalysts', 'Recent progress on graphene-analogous 2D nanomaterials: Properties, modeling and applications', 'Materials development and potential applications of transparent ceramics: A review', 'A review of recent works on inclusions', 'Isogeometric analysis of large-deformation thin shells using RHT-splines for multiple-patch coupling', 'Dependence of elastic and optical properties on surface terminated groups in two-dimensional MXene monolayers: a first-principles study', 'Realistic image synthesis using photon mapping', 'Global illumination using photon maps', 'Efficient simulation of light transport in scenes with participating media using photon maps', 'Modeling and rendering of weathered stone', 'Light diffusion in multi-layered translucent materials', 'Light scattering from human hair fibers', 'A rapid hierarchical rendering technique for translucent materials', 'Progressive photon mapping', 'Photon maps in bidirectional Monte Carlo ray tracing of complex objects', 'Stochastic progressive photon mapping', 'Importance driven path tracing using the photon map', 'Multidimensional adaptive sampling and reconstruction for ray tracing', 'Wavelet importance sampling: efficiently evaluating products of complex functions', 'Convolutional neural networks for large-scale remote-sensing image classification', 'Variational shape approximation', 'Intrinsic parameterizations of surface meshes', 'Anisotropic polygonal remeshing', 'Can semantic labeling methods generalize to any city? the inria aerial image labeling benchmark', 'A survey of surface reconstruction from point clouds', 'Variational tetrahedral meshing', 'Periodic global parameterization', 'State of the art in surface reconstruction from point clouds', 'Interactive geometry remeshing', 'Recent advances in compression of 3D meshes', 'Recent advances in remeshing of surfaces', 'Valence‐driven connectivity encoding for 3D meshes', 'Voronoi-based variational reconstruction of unoriented point sets', 'Designing quadrangulations with discrete harmonic forms', 'Progressive compression for lossless transmission of triangle meshes', 'Isotropic surface remeshing', 'High-resolution aerial image labeling with convolutional neural networks', 'Integer-grid maps for reliable quad meshing', 'Recovering accurate 3d human pose in the wild using imus and a moving camera', 'AMASS: Archive of motion capture as surface shapes', 'D-nerf: Neural radiance fields for dynamic scenes', 'Neural body fitting: Unifying deep learning and model based human pose and shape estimation', 'Implicit functions in feature space for 3d shape reconstruction and completion', 'Single-shot multi-person 3d pose estimation from monocular rgb', 'Video based reconstruction of 3d people models', 'ClothCap: Seamless 4D clothing capture and retargeting', 'Dyna: A model of dynamic human shape in motion', 'Dynamic FAUST: Registering human bodies in motion', 'XNect: Real-time multi-person 3D motion capture with a single RGB camera', 'Multi-garment net: Learning to dress 3d people from images', 'Doublefusion: Real-time capture of human performances with inner body shapes from a single depth sensor', 'Tex2shape: Detailed full human body geometry from a single image', 'Learning to reconstruct people in clothing from a single RGB camera', 'Detailed, accurate, human shape estimation from clothed 3D scan sequences', 'A generative model of people in clothing', 'Learning to dress 3d people in generative clothing', 'Everybody needs somebody: Modeling social and grouping behavior on a linear programming multiple people tracker', 'Pvnet: Pixel-wise voting network for 6dof pose estimation', 'LoFTR: Detector-free local feature matching with transformers', 'Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans', 'Consistent depth maps recovery from a video sequence', 'Robust monocular SLAM in dynamic environments', 'Deep snake for real-time instance segmentation', 'Laplacian regularized gaussian mixture model for data clustering', 'Nice-slam: Neural implicit scalable encoding for slam', 'Locality preserving nonnegative matrix factorization', 'Animatable neural radiance fields for modeling dynamic human bodies', 'Depth completion from sparse lidar data with depth-normal constraints', 'Poisson shape interpolation', 'Ad-nerf: Audio driven neural radiance fields for talking head synthesis', 'Real-time voxelization for complex polygonal models', 'Learning object-compositional neural radiance field for editable scene rendering', 'Fast and robust multi-person 3d pose estimation from multiple views', 'Real-time soft shadows in dynamic scenes using spherical harmonic exponentiation', 'Towards 3d human pose estimation in the wild: a weakly-supervised approach', 'Reassembling fractured objects by geometric matching', 'Integral invariants for robust geometry processing', 'Consistent shape maps via semidefinite programming', 'Dense human body correspondences using convolutional networks', 'Joint shape segmentation with linear programming', 'Hybridpose: 6d object pose estimation under hybrid representations', 'Structure-aware shape processing', 'Surfnet: Generating 3d shape surfaces using deep residual networks', 'Single-view reconstruction via joint analysis of image and shape collections.', 'Functional map networks for analyzing and exploring large shape collections', 'Connected fermat spirals for layered fabrication', 'Near-optimal joint object matching via convex relaxation', 'Genus zero surface conformal mapping and its application to brain surface mapping', 'Global conformal surface parameterization', 'Fundamentals of spherical parameterization for 3D meshes', 'Silhouette clipping', 'Discrete surface Ricci flow', 'Computational conformal geometry', 'Computing conformal structure of surfaces', 'Dense non-rigid surface registration using high-order graph matching', 'Conformal geometry and its applications on 3D shape matching, recognition, and stitching', 'Polycube splines', 'Manifold splines', 'Variational principles for Minkowski type problems, discrete optimal transport, and discrete Monge-Ampere equations', 'Ricci flow for 3D shape analysis', 'A discrete uniformization theorem for polyhedral surfaces', 'Kernel estimation from salient structure for robust motion deblurring', 'Harmonic volumetric mapping for solid modeling applications', 'Fingerprint recognition system', 'Optimal global conformal surface parameterization', 'Interpolating and approximating implicit surfaces from polygon soup', 'Adaptive anisotropic remeshing for cloth simulation', 'Interactively deformable models for surgery simulation', 'Spectral surface reconstruction from noisy point clouds', 'A method for animating viscoelastic fluids', 'Fast simulation of mass-spring systems', 'Skeletal parameter estimation from optical motion capture data', 'Fluid animation with dynamic meshes', 'Computational studies of human motion: Part 1, tracking and motion synthesis', 'Dynamic simulation of splashing fluids', 'Automatic joint parameter estimation from magnetic motion capture data', 'Animating explosions', 'Synthesizing sounds from rigid-body simulations', 'Where2Act: From Pixels to Actions for Articulated 3D Objects', 'Pointnet: Deep learning on point sets for 3d classification and segmentation. arXiv 2016', 'Generative 3D Part Assembly via Dynamic Graph Learning', 'Learning 3D Part Assembly from a Single Image', 'VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D ARTiculated Objects', 'Learning to Group: A Bottom-Up Framework for 3D Part Discovery in Unseen Categories', 'StructEdit: Learning structural shape variations', 'PT2PC: Learning to Generate 3D Point Cloud Shapes from Part Tree Conditions', 'O2O-Afford: Annotation-Free Large-Scale Object-Object Affordance Learning', 'Dsm-net: Disentangled structured mesh net for controllable generation of fine geometry', 'The adobeindoornav dataset: Towards deep reinforcement learning based real-world indoor robot visual navigation', 'GIMO: Gaze-Informed Human Motion Prediction in Context', 'AdaAfford: Learning to Adapt Manipulation Affordance for 3D Articulated Objects via Few-shot Interactions', 'DSG-Net: Learning Disentangled Structure and Geometry for 3D Shape Generation', 'Rethinking sampling in 3d point cloud generative adversarial networks', 'Learning to Regrasp by Learning to Place', 'Pointcnn: Convolution on x-transformed points', 'Build-to-last: Strength to weight 3D printed objects', 'Knowledge and heuristic-based modeling of laser-scanned trees', 'Synthesizing training images for boosting human 3d pose estimation', 'A generic deep architecture for single image reflection removal and image smoothing', 'Visual clustering in parallel coordinates', 'Attentive moment retrieval in videos', 'Fit and diverse: Set evolution for inspiring 3d shape galleries', 'Cross-modal moment localization in videos', 'Multiresolution tetrahedral framework for visualizing regular volume data', 'Grains: Generative recursive autoencoders for indoor scenes', 'Inverse procedural modelling of trees', 'POP: A hybrid point and polygon rendering system for large data', 'Non-local scan consolidation for 3D urban scenes', 'An integrative model of organizational trust', 'Toward a stewardship theory of management', 'Stewardship theory or agency theory: CEO governance and shareholder returns', 'An integrative model of organizational trust: Past, present, and future', 'The effect of the performance appraisal system on trust for management: A field quasi-experiment.', 'The trusted general manager and business unit performance: Empirical evidence of a competitive advantage', 'Group decision and social interaction: A theory of social decision schemes.', 'Group performance.', 'Boards and company performance-research challenges the conventional wisdom', 'Organizational behavior and human decision processes', 'Is Blood Thicker Than Water? A Study of Stewardship Perceptions in Family Business', 'Perceptions of country corruption: Antecedents and outcomes', 'Some compelling intuitions about group consensus decisions, theoretical and empirical research, and interpersonal aggregation phenomena: Selected examples 1950–1990', 'The decision processes of 6-and 12-person mock juries assigned unanimous and two-thirds majority rules.', 'Davis, Schoorman, and Donaldson reply: The distinctiveness of agency theory and stewardship theory', 'Guilt beyond a reasonable doubt: Effects of concept definition and assigned decision rule on the judgments of mock jurors.', 'CEO governance and shareholder returns: Agency theory or stewardship theory', 'Empowerment in veterinary clinics: The role of trust in delegation', 'Group decision making and social influence: A social interaction sequence model.', 'The empirical study of decision processes in juries: A critical review', 'Face recognition in unconstrained videos with matched background similarity', 'Age and Gender Classification using Convolutional Neural Networks', 'Age and Gender Estimation of Unfiltered Faces', 'Effective Face Frontalization in Unconstrained Images', 'Descriptor based methods in the wild', 'Violent Flows: Real-Time Detection of Violent Crowd Behavior', 'Regressing Robust and Discriminative 3D Morphable Models with a very Deep Neural Network', 'FSGAN: Subject Agnostic Face Swapping and Reenactment', 'Deep Face Recognition: a Survey', 'Do We Really Need to Collect Millions of Faces for Effective Face Recognition?', 'Do we really need to collect millions of faces for effective face recognition', 'Effective Unconstrained Face Recognition by Combining Multiple Descriptors and Learned Background Statistics', 'Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns', 'Similarity scores based on background samples', 'On face segmentation, face swapping, and face perception', 'Motion Interchange Patterns for Action Recognition in Unconstrained Videos', 'Multiple one-shots for utilizing class label information', 'Facial landmark detection with tweaked convolutional neural networks', 'LATCH: Learned Arrangements of Three Patch Codes', 'Viewing Real-World Faces in 3D', 'Volume illustration: Nonphotorealistic rendering of volume models', 'The use of mobile devices in aiding dietary assessment and evaluation', 'Rendering and animation of gaseous phenomena by combining fast volume and scanline A-buffer techniques', 'Use of technology in children’s dietary assessment', 'Mass media and the contagion of fear: the case of Ebola in America', 'An augmented fast marching method for computing skeletons and centerlines', 'Evidence-based development of a mobile telephone food record', 'Manifold: A model-agnostic framework for interpretation and diagnosis of machine learning models', 'Visual analytics law enforcement tools', 'A visual analytics approach to understanding spatiotemporal hotspots', 'Non-photorealistic volume rendering using stippling techniques', 'Interactive translucent volume rendering and procedural modeling', 'Novel technologies for assessing dietary intake: evaluating the usability of a mobile telephone food record among adults and adolescents', 'A real-time cloud modeling, rendering, and animation system', 'Structuring feature space: A non-parametric method for volumetric transfer function generation', 'Seam carving for content-aware image resizing', 'Improved seam carving for video retargeting', 'Sketch2photo: Internet image montage', 'A survey on mesh segmentation techniques', 'Consistent mesh partitioning and skeletonisation using the shape diameter function', 'Multi-operator media retargeting', 'Pose-oblivious shape signature', 'Inverse kinematics techniques in computer graphics: A survey', 'Visual media retargeting', 'Mesh scissoring with minima rule and part salience', 'Contextual part analogies in 3D objects', 'Automatic editing of footage from multiple social cameras', 'Intelligent mesh scissoring using 3d snakes', 'Symmetry hierarchy of man‐made objects', 'On‐the‐fly curve‐skeleton computation for 3d shapes', 'A formulation of boundary mesh segmentation', 'Style and abstraction in portrait sketching', 'Efficient RANSAC for point‐cloud shape detection', 'Octree-based Point-Cloud Compression.', '3D Zernike descriptors for content based shape retrieval', '3D shape matching with 3D shape contexts', 'Mesh reduction with error control', 'Shape retrieval using 3D Zernike descriptors', 'Automatic reconstruction of parametric building models from indoor point clouds', 'A two-streamed network for estimating fine-scaled depth maps from single rgb images', 'Acquisition, synthesis, and rendering of bidirectional texture functions', 'Simple and efficient compression of animation sequences', 'Efficient and realistic visualization of cloth', 'An Adaptable Surface Parameterization Method.', '\" Last-Mile\" preparation for a potential disaster–Interdisciplinary approach towards tsunami early warning and an evacuation information system for the coastal city of Padang\\xa0…', 'Shape recognition in 3d point-clouds', 'A geometric approach to 3D object comparison', 'Completion and reconstruction with primitive shapes', 'Robust normal estimation for point clouds with sharp features', 'GPU-based trimming and tessellation of NURBS and T-Spline surfaces', 'Automatic reconstruction of fully volumetric 3D building models from oriented point clouds', 'Accurate, dense, and robust multiview stereopsis', 'Multi-view stereo: A tutorial', 'Reconstructing building interiors from images', 'Accurate camera calibration from multi-view stereo and bundle adjustment', 'Reconstructing the world’s museums', 'Planenet: Piece-wise planar reconstruction from a single rgb image', 'Raster-to-vector: Revisiting floorplan transformation', 'Carved visual hulls for image-based modeling', 'Planercnn: 3d plane detection and reconstruction from a single image', 'House-gan: Relational generative adversarial networks for graph-constrained house layout generation', 'RIDI: Robust IMU double integration', 'Dense 3d motion capture from synchronized video streams', 'Structured indoor modeling', 'Ronin: Robust neural inertial navigation in the wild: Benchmark, evaluations, & new methods', 'Reconstructing rome', 'Artificial intelligence in healthcare: past, present and future', 'Clopidogrel with aspirin in acute minor stroke or transient ischemic attack', 'Blue luminescent graphene quantum dots and graphene oxide prepared by tuning the carbonization degree of citric acid', 'Efficient visible light nitrogen fixation with BiOBr nanosheets of oxygen vacancies on the exposed {001} facets', 'Recent advances in gas storage and separation using metal–organic frameworks', 'An in vivo model of functional and vascularized human brain organoids', 'Oxygen vacancy‐mediated photocatalysis of BiOCl: reactivity, selectivity, and perspectives', 'Fusobacterium nucleatum increases proliferation of colorectal cancer cells and tumor development in mice by activating toll-like receptor 4 signaling to nuclear factor− κB, and\\xa0…', 'Prevalence, risk factors, and management of dementia and mild cognitive impairment in adults aged 60 years or older in China: a cross-sectional study', 'Polyamine-functionalized carbon quantum dots for chemical sensing', 'Candidate single-nucleotide polymorphisms from a genomewide association study of Alzheimer disease', 'Solar water splitting and nitrogen fixation with layered bismuth oxyhalides', 'Advances in designs and mechanisms of semiconducting metal oxide nanostructures for high-precision gas sensors operated at room temperature', 'Near-Infrared Photodetector Based on MoS2/Black Phosphorus Heterojunction', 'Dual Single‐Atomic Ni‐N4 and Fe‐N4 Sites Constructing Janus Hollow Graphene for Selective Oxygen Electrocatalysis', 'Coupling N2 and CO2 in H2O to synthesize urea under ambient conditions', 'Recent advances in carbon dioxide utilization', 'Formation of uniform colloidal spheres from lignin, a renewable resource recovered from pulping spent liquor', 'Pathogenesis of human systemic lupus erythematosus: a cellular perspective', 'Na 3 V 2 (PO 4) 3@ C core–shell nanocomposites for rechargeable sodium-ion batteries', 'Introduction to implicit surfaces', 'Collision detection for deformable objects', 'Smoothed particles: A new paradigm for animating highly deformable bodies', 'Dynamic real-time deformations using space & time adaptive sampling', 'Super-helices for predicting the dynamics of natural hair', 'Animating Soft Substances with Implicit Surfaces', 'Virtual garments: A fully geometric approach for clothing design', 'Interactive animation of ocean waves', 'An implicit formulation for precise contact modeling between flexible solids', 'Computer animation of human walking: a survey', 'Real-time collision detection for virtual surgery', 'Animation of deformable models using implicit surfaces', 'Controlling anisotropy in mass-spring systems', 'Hierarchical pattern mapping', 'A sketch-based interface for clothing virtual characters', 'Animating lava flows', 'Pattern-based texturing revisited', 'Automatic Reconstruction of Unstructured 3D data : Combining Medial Axis and Implicit Surfaces', 'Implicit skinning: Real-time skin deformation with contact modeling', 'Valley-selective circular dichroism of monolayer molybdenum disulphide', 'Evolution of Electronic Structure in Atomically Thin Sheets of WS2 and WSe2', 'Raman spectroscopy of graphene-based materials and its applications in related devices', 'Phonon and Raman scattering of two-dimensional transition metal dichalcogenides from monolayer, multilayer to bulk material', 'Strong Photoluminescence Enhancement of MoS2 through Defect Engineering and Oxygen Bonding', 'Nanotube–polymer composites for ultrafast photonics', 'Synthesis of few-layer GaSe nanosheets for high performance photodetectors', 'Lattice dynamics in mono-and few-layer sheets of WS 2 and WSe 2', 'The shear mode of multilayer graphene', 'Epitaxial Monolayer MoS2 on Mica with Novel Photoluminescence', 'Raman spectroscopy of shear and layer breathing modes in multilayer MoS', 'Robust optical emission polarization in MoS monolayers through selective valley excitation', 'Strain tuning of optical emission energy and polarization in monolayer and bilayer MoS', 'Carrier and Polarization Dynamics in Monolayer ', 'Review on the Raman spectroscopy of different types of layered materials', 'Black phosphorus ink formulation for inkjet printing of optoelectronics and photonics', 'Intercalation of Few-Layer Graphite Flakes with FeCl3: Raman Determination of Fermi Level, Layer by Layer Decoupling, and Stability', 'Raman scattering of non–planar graphite: arched edges, polyhedral crystals, whiskers and cones', 'Photoluminescence spectroscopy of carbon nanotube bundles: evidence for exciton energy transfer', 'Temperature-dependent Raman spectra and anomalous Raman phenomenon of highly oriented pyrolytic graphite', 'The asymptotic decider: resolving the ambiguity in marching cubes.', 'A data reduction scheme for triangulated surfaces', 'Evolutionary morphing', 'Multiresolution techniques for interactive texture-based volume visualization', 'A quantitative spatiotemporal atlas of gene expression in the Drosophila blastoderm', 'A topological hierarchy for functions on triangulated surfaces', 'Curvature approximation for triangulated surfaces', 'A practical approach to Morse-Smale complex computation: Scalability and generality', 'FastBit: interactively searching massive data', 'Topology-controlled volume rendering', 'Iso‐geometric Finite Element Analysis Based on Catmull‐Clark: ubdivision Solids', 'Construction of vector field hierarchies', 'Three-dimensional morphology and gene expression in the Drosophilablastoderm at cellular resolution I: data acquisition pipeline', 'Visualizing and modeling scattered multivariate data', 'Discrete sibson interpolation', 'Topologically clean distance fields', 'Efficient computation of Morse-Smale complexes for three-dimensional scalar functions', 'Constructing hierarchies for triangle meshes', 'State of the Art of Performance Visualization.', 'Data point selection for piecewise linear curve approximation', 'Efficient prediction structures for multiview video coding', 'Multi-view video plus depth representation and coding', 'Depth map creation and image-based rendering for advanced 3DTV services providing interoperability and scalability', '3D video and free viewpoint video-technologies, applications and MPEG standards', 'Coding algorithms for 3DTV—a survey', 'Multiview imaging and 3DTV', 'The effects of multiview depth video compression on multiview rendering', 'Interactive 3-D video representation and coding technologies', 'View synthesis for advanced 3D video systems', 'Intermediate view interpolation based on multiview video plus depth for advanced 3D video systems', 'Long-term global motion estimation and its application for sprite coding, content description, and segmentation', 'SIFT implementation and optimization for general-purpose GPU', '3DAV exploration of video-based rendering technology in MPEG', '3D video and free viewpoint video—From capture to display', 'Three-dimensional video postproduction and processing', 'An overview of available and emerging 3D video formats and depth enhanced stereo as efficient generic solution', 'Efficient compression of multi-view video exploiting inter-view dependencies based on H. 264/MPEG4-AVC', 'Intermediate view synthesis and multi-view data signal extraction', 'Salnet360: Saliency maps for omni-directional images with cnn', 'Transverse momentum spectra and nuclear modification factors of charged particles in pp, p-Pb and Pb-Pb collisions at the LHC', 'Production of charged pions, kaons, and (anti-)protons in Pb-Pb and inelastic  collisions at  TeV', 'Measurement of D0, D+, D*+ and D s + production in Pb-Pb collisions at s N N = 5.02  TeV', 'Multiplicity dependence of light-flavor hadron production in  collisions at ', 'Production of deuterons, tritons,  nuclei, and their antinuclei in  collisions at , 2.76, and 7 TeV', 'J/ψ suppression at forward rapidity in Pb–Pb collisions at sNN= 5.02 TeV', 'Measurement of D-meson production at mid-rapidity in pp collisions at  s = 7 \\xa0TeV', 'Multiplicity dependence of (multi-)strange hadron production in proton-proton collisions at s\\xa0=\\xa013 TeV', 'Energy dependence of forward-rapidity  J / ψ and  ψ ( 2 S ) production in pp collisions at the LHC', '-Meson Azimuthal Anisotropy in Midcentral Pb-Pb Collisions at ', 'Anisotropic flow of identified particles in Pb-Pb collisions at s N N = 5.02  TeV', ' and  meson production at high transverse momentum in  and Pb-Pb collisions at  TeV', 'Predicting popularity of online videos using support vector regression', 'Inclusive J/ψ production at forward and backward rapidity in p-Pb collisions at s N N = 8.16  TeV', 'Investigations of Anisotropic Flow Using Multiparticle Azimuthal Correlations in , , Xe-Xe, and Pb-Pb Collisions at the LHC', 'Energy dependence and fluctuations of anisotropic flow in Pb-Pb collisions at s N N = 5.02  and 2.76 TeV', 'Anisotropic flow in Xe–Xe collisions at sNN= 5.44 TeV', 'Λ c + production in pp collisions at s = 7  TeV and in p-Pb collisions at s N N = 5.02  TeV', 'Measurements of inclusive jet spectra in  and central Pb-Pb collisions at ', 'Transverse momentum spectra and nuclear modification factors of charged particles in Xe–Xe collisions at sNN= 5.44 TeV', 'Eye tracking: methodology theory and practice', 'A breadth-first survey of eye-tracking applications', 'Using virtual reality technology for aircraft visual inspection training: presence and comparison studies', 'Eye tracking cognitive load using pupil diameter and microsaccades with fixed gaze', 'The Index of Pupillary Activity: Measuring Cognitive Load vis-à-vis Task Difficulty with Pupil Oscillation', 'Eye tracking for spatial research: Cognition, computation, challenges', 'Longitudinal evaluation of discrete consecutive gaze gestures for text entry', 'Efficient eye pointing with a fisheye lens', 'Gaze-contingent displays: A review', 'Foveated gaze-contingent displays for peripheral LOD management, 3D visualization, and stereo imaging', 'Use of eye movements as feedforward training for a synthetic aircraft inspection task', 'Binocular eye tracking in virtual reality for inspection training', 'Focusing on the essential: considering attention in display design', 'Gaze transition entropy', 'Scanpath comparison revisited', 'Adapting starburst for elliptical iris segmentation', 'Gaze-based interaction: A 30 year retrospective', '3-D eye movement analysis', 'Entropy-based statistical analysis of eye movement transitions', 'Gaze-vs. hand-based pointing in virtual environments', 'Context-aware saliency detection', 'Hierarchical mesh decomposition using fuzzy clustering and cuts', 'How to evaluate foreground maps?', 'What makes a patch distinct?', 'Mesh segmentation using feature point and core extraction', 'The bloomier filter: an efficient data structure for static support lookup tables', 'Direct visibility of point sets', 'Mesh segmentation-a comparative study', 'Metamorphosis of polyhedral surfaces using decomposition', 'Content based retrieval of VRML objects—an iterative and interactive approach', 'Online dynamic graph drawing', 'Polyhedral surface decomposition with applications', 'BOXTREE: A hierarchical representation for surfaces in 3D', 'Strategies for polyhedral surface decomposition: An experimental study', 'Dynamic drawing of clustered graphs', 'Multi-level graph layout on the GPU', 'Demarcating curves for shape illustration', 'Surface regions of interest for viewpoint selection', 'Paper craft models from meshes', 'High-quality motion deblurring from a single image', 'Video frame synthesis using deep voxel flow', 'A comparative study of energy minimization methods for markov random fields', 'Keyframe-based tracking for rotoscoping and animation', 'Color compatibility from large datasets', 'Photographing long scenes with multi-viewpoint panoramas', 'Panoramic video textures', 'Tangible interaction+ graphical interpretation: a new approach to 3D modeling', 'Efficient gradient-domain compositing using quadtrees', 'Exploratory font selection using crowdsourced attributes', 'Methods and apparatus for video completion', 'Optimizing content-preserving projections for wide-angle images', 'Designscape: Design with interactive layout suggestions', 'Learning layouts for single-pagegraphic designs', 'Patch-based convolutional neural network for whole slide tissue image classification', 'Spatial organization and molecular correlation of tumor-infiltrating lymphocytes using deep learning on pathology images', 'Two-person interaction detection using body-pose features and multiple instance learning', 'Deriving reproducible biomarkers from multi-site resting-state data: An Autism-based example', 'Face recognition from a single training image under arbitrary unknown lighting using spherical harmonics', 'Neural Face Editing with Intrinsic Image Disentangling', 'Is decreased prefrontal cortical sensitivity to monetary reward associated with impaired motivation and self-control in cocaine addiction?', 'Distribution matching for crowd counting', 'Face recognition under variable lighting using harmonic image exemplars', 'Large-scale training of shadow detectors with noisily-annotated shadow examples', 'Using multiple cues for hand tracking and model refinement', 'High resolution acquisition, learning and transfer of dynamic 3‐D facial expressions', 'Topology-preserving deep image segmentation', 'Deforming autoencoders: Unsupervised disentangling of shape and appearance', 'Shadow detection with conditional generative adversarial networks', 'Robust Histopathology Image Analysis: to Label or to Synthesize?', 'Real-time accurate object detection using multiple resolutions', 'Sparse autoencoder for unsupervised nucleus detection and representation in histopathology images', 'Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction', 'On centroidal Voronoi tessellation—energy smoothness and fast computation', 'Geometric modeling with conical meshes and developable surfaces', 'Fitting B-spline curves to point clouds by curvature-based squared distance minimization', 'Computation of rotation minimizing frames', 'Geometry of multi-layer freeform structures for architecture', 'Freeform surfaces from single curved panels', 'Isotropic remeshing with fast and exact computation of restricted Voronoi diagram', 'An algebraic condition for the separation of two ellipsoids', 'Control point adjustment for B-spline curve approximation', 'All-hex meshing using singularity-restricted field', 'Efficient collision detection using a dual OBB-sphere bounding volume hierarchy', 'ToothNet: automatic tooth instance segmentation and identification from cone beam CT images', 'Denoising point sets via L0 minimization', 'Neural rendering and reenactment of human actor videos', 'Variational mesh segmentation via quadric surface fitting', 'Robust computation of the rotation minimizing frame for sweep surface modeling', 'Continuous collision detection for two moving elliptic disks', 'Industrial geometry: recent advances and applications in CAD', 'Image inpainting for irregular holes using partial convolutions', 'Occlusion-aware Depth Estimation Using Light-field Cameras', 'Few-Shot Video-to-Video Synthesis', 'One-shot free-view neural talking-head synthesis for video conferencing', 'A 4D light-field dataset and CNN architectures for material recognition', 'Dancing to music', 'Depth estimation with occlusion modeling using light-field cameras', 'Gaugan: semantic image synthesis with spatially adaptive normalization', 'Light Field Video Capture Using a Learning-Based Hybrid Imaging System', 'Generative adversarial networks for image and video synthesis: Algorithms and applications', 'A taxonomy and evaluation of dense light field depth estimation algorithms', 'Partial convolution based padding', 'World-consistent video-to-video synthesis', 'Depth estimation and specular removal for glossy surfaces using point and line consistency with light-field cameras', 'Domain stylization: A strong, simple baseline for synthetic to real image domain adaptation', 'Depth from semi-calibrated stereo and defocus', 'O-cnn: Octree-based convolutional neural networks for 3d shape analysis', 'Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set', 'TextFlow: Towards Better Understanding of Evolving Topics in Text', 'Cost-effective printing of 3D objects with skin-frame structures', 'Disentangled and Controllable Face Image Generation via 3D Imitative-Contrastive Learning', 'Diffuse-specular separation and depth recovery from image sequences', 'Group-free 3d object detection via transformers', 'DeepToF: Off-the-shelf Real-time Correction of Multipath Interference in Time-of-flight Imaging', 'Adaptive O-CNN: a patch-based deep representation of 3D shapes', 'Generalized displacement maps', 'High resolution multispectral video capture with a hybrid camera system', 'A prism-mask system for multispectral video acquisition', 'Automatic acquisition of high-fidelity facial performances using monocular videos', 'A closer look at local aggregation operators in point cloud analysis', 'Fabricating spatially-varying subsurface scattering', 'Modeling surface appearance from a single photograph using self-augmented convolutional neural networks', 'Leveraging motion capture and 3D scanning for high-fidelity facial performance acquisition', 'Robust text detection in natural images with edge-enhanced maximally stable extremal regions', 'System and method for gesture recognition in three dimensions using stereo imaging and color vision', 'Artificial fishes: Autonomous locomotion, perception, behavior, and learning in a simulated physical world', 'Method, Device, Mobile Terminal, and Computer Program Product for a Point of Interest Based Scheme for Improving Mobile Visual Searching Functionalities', 'Outdoors augmented reality on mobile phone using loxel-based visual feature organization', 'Neuroanimator: Fast neural network emulation and control of physics-based models', 'Mobile visual search', 'CHoG: Compressed histogram of gradients a low bit-rate feature descriptor', 'Light field mapping: Efficient representation and hardware rendering of surface light fields', 'Automated learning of muscle-actuated locomotion through control abstraction', 'Dynamic gesture recognition from stereo sequences', 'Method and system for digital plenoptic imaging', 'Unified real-time tracking and recognition with rotation-invariant fast features', 'Compressed histogram of gradients: A low-bitrate descriptor', 'The stanford mobile visual search data set', 'Landmark-based pedestrian navigation from collections of geotagged photos', 'Method and system for tracking vantage points from which pictures of an object have been taken', 'Method, Apparatus and Computer Program Product for Performing a Visual Search Using Grid-Based Feature Organization', 'Interactive modeling of plants', 'Floating points: A method for computing stipple drawings', 'Image enhancement by unsharp masking the depth buffer', 'Voronoi treemaps for the visualization of software metrics', 'Approximate image-based tree-modeling using particle flows', 'Voronoi treemaps', 'Interactive visualization of complex plant ecosystems', 'Recursive Wang tiles for real-time blue noise', 'Capturing and viewing gigapixel images', 'Computer-generated pen-and-ink illustration of trees', 'Sketch-based tree modeling using markov random field', 'Digital design of nature: computer generated plants and organics', 'Beyond stippling—Methods for distributing objects on the plane', \"Capacity-constrained point distributions: A variant of Lloyd's method\", 'Plastic trees: interactive self-adapting botanical tree models', 'Microfacet models for refraction through rough surfaces', 'Image-based BRDF measurement including human skin', 'An evaluation of reconstruction filters for volume rendering', 'Dual photography', 'Microstructures to control elasticity in 3D printing', 'Simulating knitted cloth at the yarn level', 'Inverse rendering for computer graphics', 'Image-based bidirectional reflectance distribution function measurement', 'Inverse lighting for photography', 'Manifold exploration: A markov chain monte carlo technique for rendering scenes with difficult specular transport', 'Stitch meshes for modeling knitted clothing with yarn-level detail', 'Data‐driven estimation of cloth simulation models', 'Modeling and rendering for realistic facial animation', 'A radiative transfer framework for rendering materials with anisotropic structure', 'Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields', 'Mip-nerf 360: Unbounded anti-aliased neural radiance fields', 'Fully automated detection of diabetic macular edema and dry age-related macular degeneration from optical coherence tomography images', 'Nerv: Neural reflectance and visibility fields for relighting and view synthesis', 'Block-nerf: Scalable large scene neural view synthesis', 'Advances in neural rendering', 'Ref-nerf: Structured view-dependent appearance for neural radiance fields', 'Depth from shading, defocus, and correspondence using light-field angular coherence', 'Humannerf: Free-viewpoint rendering of moving people from monocular video', 'Nerf in the dark: High dynamic range view synthesis from noisy raw images', 'Neural reflectance fields for appearance acquisition', 'CPR-curved planar reformation', 'Importance-driven volume rendering', 'Volumeshop: An interactive system for direct volume illustration', 'Importance-driven focus of attention', 'Importance-driven feature enhancement in volume visualization', 'Two-level volume rendering', 'Style transfer functions for illustrative volume rendering', 'Exploded views for volume data', 'Uncertainty‐aware exploration of continuous parameter spaces using multivariate prediction', 'Fast visualization of object contours by non‐photorealistic volume rendering', 'Curvature-based transfer functions for direct volume rendering', 'Animating flow fields: rendering of oriented line integral convolution', 'Illustrative context-preserving exploration of volume data', 'Strategies for interactive exploration of 3D flow using evenly-spaced illuminated streamlines', 'World lines', 'Optimal regular volume sampling', 'Mastering transfer function specification by using VolumePro technology', 'Real-time techniques for 3D flow visualization', 'Enhancing depth-perception with flexible volumetric halos', 'State of the art in transfer functions for direct volume rendering', 'On power-law relationships of the internet topology', 'Power laws and the AS-level Internet topology', 'Composable controllers for physics-based character animation', 'Dynamic free-form deformations for animation synthesis', 'Behavior planning for character animation', 'Expressive speech-driven facial animation', 'Egocentric affordance fields in pedestrian steering', 'Steerbench: a benchmark suite for evaluating steering behaviors', 'Style components.', 'The virtual stuntman: dynamic characters with a repertoire of autonomous motor skills', 'Hybrid control for interactive character animation', 'A modular framework for adaptive agent-based steering', 'Unsupervised learning for speech motion editing', 'Footstep navigation for dynamic crowds', 'Real-time speech motion synthesis from recorded motions', 'The art of deception: Adaptive precision reduction for area efficient physics acceleration', 'A behavior-authoring framework for multiactor simulations', 'Fool me twice: Exploring and exploiting error tolerance in physics-based animation', 'Dynamic animation and control environment', 'Scenario space: characterizing coverage, quality, and failure of steering algorithms', 'Mesh parameterization methods and their applications', 'Mesh parameterization: Theory and practice', 'Cross-parameterization and compatible remeshing of 3D models', 'Parameterization of faceted surfaces for meshing using angle-based flattening', 'ABF++: fast and robust angle based flattening', 'High resolution passive facial performance capture', 'Curved folding', 'D-charts: Quasi-developable mesh segmentation', 'Markerless garment capture', 'Upright orientation of man-made objects', 'True2Form: 3D Curve Networks from 2D Sketches via Selective Regularization', 'Seamster: inconspicuous low-distortion texture seam layout', 'Matchmaker: constructing constrained texture maps', 'All‐hex mesh generation via volumetric polycube deformation', 'Pyramid coordinates for morphing and deformation', 'NeRF: Representing scenes as neural radiance fields for view synthesis', 'Dreamfusion: Text-to-3d using 2d diffusion', 'Unprocessing images for learned raw denoising', 'Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs', 'Zero-shot text-guided object generation with dream fields', 'Triangle mesh compression', 'Spectral compression of mesh geometry', 'A local/global approach to mesh parameterization', 'High-quality passive facial performance capture using anchor frames.', 'Mesh-based inverse kinematics', 'On the metric properties of discrete space-filling curves', 'Compression of soft-body animation sequences', 'Explicit surface remeshing', 'Conformal flattening by curvature prescription and metric scaling', 'Context‐aware skeletal shape deformation', 'Isotropic remeshing of surfaces: a local parameterization approach', 'Simplification and compression of 3D meshes', 'Guaranteed intersection-free polygon morphing', 'Discrete one-forms on meshes and applications to 3D mesh parameterization', 'Characterizing shape using conformal factors.', 'Method and system for detecting and classifying objects in an image', 'Deformation transfer for triangle meshes', 'Real-time hand-tracking with a color glove', 'Automatic rigging and animation of 3d characters', 'Style translation for human motion', 'Interactive multiresolution surface viewing', 'Multiobjective control with frictional contacts', 'Interactive manipulation of rigid body simulations', 'Contact-aware nonlinear control of dynamic characters', 'Motion fields for interactive character locomotion', 'Interactive simulation of stylized human locomotion', 'Real-time enveloping with rotational regression', 'Topologically-constrained latent variable models', 'A visibility matching tone reproduction operator for high dynamic range scenes', 'The 3D model acquisition pipeline', 'Tone reproduction for realistic images', 'High-quality texture reconstruction from multiple scans', 'The zonal method for calculating light intensities in the presence of a participating medium', 'Two-dimensional liquid phase and the px. sqroot. 3 phase of alkanethiol self-assembled monolayers on au (111)', 'Improving radiosity solutions through the use of analytically determined form-factors', 'A progressive multi-pass method for global illumination', 'Digital modeling of material appearance', 'An experimental evaluation of computer graphics imagery', 'Comparing real and synthetic images: Some ideas about metrics', 'A scalable parallel algorithm for self-organizing maps with applications to sparse data mining problems', '3D imaging spectroscopy for measuring hyperspectral patterns on solid objects', 'Understanding and improving the realism of image composites', 'Geometric simplicationfor indirect illuminationcalculations', 'Extending the radiosity method to include specularly reflecting and translucent materials'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_paper_dict.keys()\n",
    "processed_paper_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Deep residual learning for image recognition\" in processed_paper_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_paper_dict[\"Deep residual learning for image recognition\"].DOI is not False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10.1007/3-540-49430-8_11', '10.1017/CBO9780511812651', '10.1109/CVPR.2007.383266', '10.1109/CVPR.2015.7298965', '10.1109/72.279181', '10.1162/neco.1989.1.4.541', '10.1007/3-540-49430-8_2', '10.1109/CVPR.2015.7299173', '10.1109/ICCV.2015.123', '10.1162/neco.1997.9.8.1735', '10.1109/TPAMI.2010.57', '10.1109/TPAMI.2011.235', '10.5244/C.25.76', '10.1137/1.9780898719505', '10.1109/ICCV.2015.169', '10.1007/s11263-009-0275-4', '10.1109/CVPR.2014.81', '10.1007/978-3-642-42054-2_55', '10.1145/1179352.1142005', '10.1007/978-1-4757-3121-7', '10.1109/34.56188', '10.1109/CVPR.2015.7298594']\n"
     ]
    }
   ],
   "source": [
    "IDX = 0\n",
    "paper_to_process_keys_list[IDX]\n",
    "\n",
    "paper = whole_paper_dict[paper_to_process_keys_list[IDX]]\n",
    "print(paper.reference_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, paper in whole_paper_dict.items() :\n",
    "    whole_paper_dict[key] = paper.toDict()\n",
    "with open(PROCESSED_PAPER_FILE_PATH, \"w\") as f :\n",
    "    json.dump(whole_paper_dict, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Deep Residual Learning for Image Recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': 'Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.', '저자': 'Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun', '전체 인용횟수': '193462회 인용2016201720182019202020212022202312005069119632076428660385664519640487', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2016', '학술 문서': 'Deep residual learning for image recognitionK He, X Zhang, S Ren, J Sun\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2016189280회 인용 관련 학술자료 전체 76개의 버전 Deep residual learning for image recognition*S Jian, H Kaiming, R Shaoqing, Z Xiangyu\\xa0- IEEE Conference on Computer Vision & Pattern\\xa0…, 20165375회 인용 관련 학술자료 전체 2개의 버전 Deep residual learning for image recognition. CoRR abs/1512.03385 (2015)*K He, X Zhang, S Ren, J Sun - 2015470회 인용 관련 학술자료 Deep residual learning for image recognition. arXiv e-prints*K He, X Zhang, S Ren, J Sun\\xa0- arXiv preprint arXiv:1512.03385, 2015157회 인용 관련 학술자료 Zhang, X.—Ren, S.—Sun, J.: Deep Residual Learning for Image Recognition*K He\\xa0- Proceedings of the 2016 IEEE Conference on\\xa0…, 2016131회 인용 관련 학술자료 Deep Learning and Image Recognition*C Li, X Li, M Chen, X Sun\\xa0- 2023 IEEE 6th International Conference on Electronic\\xa0…, 2023106회 인용 관련 학술자료 Deep residual learning for image recognition In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 770–778*K He, X Zhang, S Ren, J Sun\\xa0- IEEE. https://doi. org/10.1109/cvpr, 201698회 인용 관련 학술자료 Deep residual learning for image recognition. CoRR (2015)*K He, X Zhang, S Ren, J Sun\\xa0- arXiv preprint arXiv:1512.03385, 201680회 인용 관련 학술자료 Deep residual learning for image recognition. 2016 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV*K He, X Zhang, S Ren, J Sun - 201664회 인용 관련 학술자료 Deep Residual Learning for Image Recognition. 2015. doi: 10.48550*K He, X Zhang, S Ren, J Sun\\xa0- arXiv preprint ARXIV.1512.0338563회 인용 관련 학술자료 Shaoqing Ren and Jian Sun. Deep residual learning for image recognition*K He, X Zhang\\xa0- 2016 IEEE Confer-ence on Computer Vision and\\xa0…, 201656회 인용 관련 학술자료 Deep residual learning for image recognition [J](2015)*K He, X Zhang, S Ren, J Sun\\xa0- arXiv preprint arXiv:1512.03385, 201654회 인용 관련 학술자료 Deep residual learning for image recognition, December 2015*K He, X Zhang, S Ren, J Sun\\xa0- arXiv preprint arXiv:1512.03385, 201535회 인용 관련 학술자료 Deep Residual Learning for Image Recognition. arXiv e-prints, art*K He, X Zhang, S Ren, J Sun\\xa0- arXiv preprint arXiv:1512.03385, 201532회 인용 관련 학술자료 Deep residual learning for image recognition. arXiv preprint (2015)*K He, X Zhang, S Ren, J Sun\\xa0- arXiv preprint arXiv:1512.0338530회 인용 관련 학술자료 S. & Sun, J.(2016) Deep residual learning for image recognition*K He, X Zhang, S Ren\\xa0- Proceedings of the IEEE international conference on\\xa0…, 201618회 인용 관련 학술자료 Deep Residual Learning for Image Recognition. dec 2015*K He, X Zhang, S Ren, J Sun\\xa0- URL http://arxiv. org/abs/1512.0338518회 인용 관련 학술자료 '}, title='Deep Residual Learning for Image Recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/6/4', '설명': 'State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP) using 300 proposals per image. Code is available at https://github. com/ShaoqingRen/faster_rcnn.', '저자': 'Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun', '전체 인용횟수': '67246회 인용2016201720182019202020212022202369123324973809610158129101421812056', '컨퍼런스': 'Neural Information Processing Systems (NIPS), 2015', '학술 문서': 'Faster r-cnn: Towards real-time object detection with region proposal networksS Ren, K He, R Girshick, J Sun\\xa0- Advances in neural information processing systems, 201567038회 인용 관련 학술자료 전체 42개의 버전 Faster r-cnn: Towards real-time object detection with region proposal networks. arXiv 2015*S Ren, K He, R Girshick, J Sun\\xa0- arXiv preprint arXiv:1506.01497, 2015381회 인용 관련 학술자료 Faster R-CNN: towards real-time object detection with region proposal networks. İn: International Conference on Neural Information Processing Systems*SQ Ren, KM He, R Girshick, J Sun - 2018207회 인용 관련 학술자료 Jjitopa Sun 2017 Intelligence M*S Ren, K He, R Girshick\\xa0- Faster r-cnn: Towards real-time object detection with\\xa0…59회 인용 관련 학술자료 Faster r-cnn: Towards real-time object detection with region proposal networks [J]S Tren, K He, R Girshick\\xa0- Advances in neural information processing systems, 20159회 인용 관련 학술자료 '}, title='Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Mask R-CNN': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/3/20', '설명': 'We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, eg, allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.', '저널': 'arXiv preprint arXiv:1703.06870', '저자': 'Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick', '전체 인용횟수': '31086회 인용2017201820192020202120222023165135434765034662775156723', '학술 문서': 'Mask r-cnnK He, G Gkioxari, P Dollár, R Girshick\\xa0- Proceedings of the IEEE international conference on\\xa0…, 201730662회 인용 관련 학술자료 전체 23개의 버전 2017 IEEE International Conference on Computer Vision (ICCV)*K He, G Gkioxari, P Dollár, R Girshick, M R-CNN\\xa0- IEEE, Venice, Italy, 2017359회 인용 관련 학술자료 Mask r-cnn. arXiv 2017*K He, G Gkioxari, P Dollár, R Girshick\\xa0- arXiv preprint arXiv:1703.06870, 2020135회 인용 관련 학술자료 Mask r-cnn. arXiv*K He, G Gkioxari, P Dollár, R Girshick\\xa0- arXiv preprint arXiv:1703.06870, 201772회 인용 관련 학술자료 Mask r-cnn*G Gkioxari, P Dollár, R Girshick\\xa0- IEEE International conference on Computer Vision\\xa0…, 201754회 인용 관련 학술자료 Mask r-cnn*K Dollár, R Girshick\\xa0- Proceedings of the IEEE international conference on\\xa0…, 201752회 인용 관련 학술자료 전체 7개의 버전 Mask r-cnn*R Mask, K He, G Gkioxari, P Dollár, R Girshick\\xa0- Proceedings of the IEEE International Conference on\\xa0…, 20176회 인용 관련 학술자료 전체 2개의 버전 Mask R-CNN. Proc*K He, G Gkioxari, P Dollár, R Girshick\\xa0- 2017 IEEE International Conference on Computer\\xa0…, 20184회 인용 관련 학술자료 '}, title='Mask R-CNN', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Focal Loss for Dense Object Detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/8/7', '설명': 'The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors.', '저자': 'Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollár', '전체 인용횟수': '25726회 인용20182019202020212022202353718743457553469997122', '컨퍼런스': 'International Conference on Computer Vision (ICCV), 2017', '학술 문서': 'Focal loss for dense object detectionTY Lin, P Goyal, R Girshick, K He, P Dollár\\xa0- Proceedings of the IEEE international conference on\\xa0…, 201725633회 인용 관련 학술자료 전체 23개의 버전 Focal loss for dense object detection. arXiv 2017*TY Lin, P Goyal, R Girshick, K He, P Dollár\\xa0- arXiv preprint arXiv:1708.02002, 2002115회 인용 관련 학술자료 Focal loss for dense object detection. arXiv*TY Lin, P Goyal, R Girshick, K He, P Dollár\\xa0- arXiv preprint arXiv:1708.02002, 201780회 인용 관련 학술자료 Piotr Doll ar. Focal loss for dense object detection*TY Lin, P Goyal, R Girshick, K He\\xa0- Proceedings of the IEEE International Conference on\\xa0…, 201761회 인용 관련 학술자료 Focal loss for dense object detection. CoRR abs/1708.02002 (2017)*T Lin, P Goyal, RB Girshick, K He, P Dollár\\xa0- arXiv preprint arXiv:1708.02002, 201755회 인용 관련 학술자료 Focal Loss for Dense Object Detection (RetinaNet)*TY Lin, P Goyal, R Girshick, K He, P Dollar\\xa0- Proceedings of the IEEE International Conference on\\xa0…, 201711회 인용 관련 학술자료 '}, title='Focal Loss for Dense Object Detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Feature Pyramid Networks for Object Detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.', '저자': 'Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie', '전체 인용횟수': '22599회 인용201720182019202020212022202311263720043160483259665735', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2017', '학술 문서': 'Feature pyramid networks for object detectionTY Lin, P Dollár, R Girshick, K He, B Hariharan…\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 201722526회 인용 관련 학술자료 전체 22개의 버전 Feature pyramid networks for object detection. arXiv 2016*TY Lin, P Dollár, R Girshick, K He, B Hariharan…\\xa0- arXiv preprint arXiv:1612.03144121회 인용 관련 학술자료 Feature pyramid networks for object detection. 2016*TY Lin, P Dollár, R Girshick, K He, B Hariharan…\\xa0- arXiv preprint arXiv:1612.03144, 201759회 인용 관련 학술자료 Feature pyramid networks for object detection*P Dollar, R Girshick, K He, B Hariharan, S Belongie\\xa0- CVPR, 201729회 인용 관련 학술자료 '}, title='Feature Pyramid Networks for Object Detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%) on this dataset.', '저자': 'Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun', '전체 인용횟수': '22598회 인용2015201620172018201920202021202220232196521289214529423483379736702990', '컨퍼런스': 'International Conference on Computer Vision (ICCV), 2015', '학술 문서': 'Delving deep into rectifiers: Surpassing human-level performance on imagenet classificationK He, X Zhang, S Ren, J Sun\\xa0- Proceedings of the IEEE international conference on\\xa0…, 201522483회 인용 관련 학술자료 전체 21개의 버전 IEEE International Conference on Computer Vision*K He, X Zhang, S Ren, J Sun\\xa0- Delving Deep into Rectifiers: Surpassing Human-Level\\xa0…, 201565회 인용 관련 학술자료 Delving deep into rectifiers: surpassing human-level performance on ImageNet classification. CoRR abs/1502.01852 (2015)*K He, X Zhang, S Ren, J Sun\\xa0- arXiv preprint arXiv:1502.01852, 201550회 인용 관련 학술자료 Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. arXiv e-prints*K He, X Zhang, S Ren, J Sun\\xa0- arXiv preprint arXiv:1502.01852, 201530회 인용 관련 학술자료 '}, title='Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/6/18', '설명': 'Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224   224) input image. This requirement is “artificial” and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, “spatial pyramid pooling”, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image\\xa0…', '저자': 'Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun', '전체 인용횟수': '13825회 인용201420152016201720182019202020212022202357214462772103814431634210725662200', '컨퍼런스': 'European Conference on Computer Vision (ECCV), 2014', '학술 문서': 'Spatial pyramid pooling in deep convolutional networks for visual recognitionK He, X Zhang, S Ren, J Sun\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 201513825회 인용 관련 학술자료 전체 26개의 버전 '}, title='Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning a Deep Convolutional Network for Image Super-Resolution': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/9/6', '게시자': 'Springer International Publishing', '설명': ' We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) [15] that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage.', '저자': 'Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang', '전체 인용횟수': '12913회 인용201520162017201820192020202120222023102348727126617442044225223031999', '컨퍼런스': 'European Conference on Computer Vision (ECCV), 2014', '학술 문서': 'Image super-resolution using deep convolutional networks*C Dong, CC Loy, K He, X Tang\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20158622회 인용 관련 학술자료 전체 13개의 버전 Learning a deep convolutional network for image super-resolutionC Dong, CC Loy, K He, X Tang\\xa0- Computer Vision–ECCV 2014: 13th European\\xa0…, 20145583회 인용 관련 학술자료 전체 8개의 버전 '}, title='Learning a Deep Convolutional Network for Image Super-Resolution', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Aggregated Residual Transformations for Deep Neural Networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/7/21', '설명': 'We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call\" cardinality\"(the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.', '저자': 'Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He', '전체 인용횟수': '11037회 인용201720182019202020212022202313556610881691244427352292', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2017', '학술 문서': 'Aggregated residual transformations for deep neural networksS Xie, R Girshick, P Dollár, Z Tu, K He\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 201711018회 인용 관련 학술자료 전체 13개의 버전 Aggregated residual transformations for deep neural networks. CoRR*S Xie, RB Girshick, P Dollár, Z Tu, K He\\xa0- arXiv preprint arxiv:1611.05431, 20167회 인용 관련 학술자료 Aggregated residual transformations for deep neural networks, CoRR abs/1611.05431*S Xie, RB Girshick, P Dollár, Z Tu, K He\\xa0- arXiv preprint arXiv:1611.05431, 20165회 인용 관련 학술자료 Aggregated residual transformations for deep neural networks. arXiv e-prints*S Xie, R Girshick, P Dollár, Z Tu, K He\\xa0- arXiv preprint arXiv:1611.05431, 20164회 인용 관련 학술자료 Aggregated residual transformations for deep neural networks. arXiv preprint 2016*S Xie, R Girshick, P Dollar, Z Tu, K He\\xa0- Source:< https://arxiv. org/abs/1611.05431 v14회 인용 관련 학술자료 Aggregated residual transformations for deep neural networks (pp. 1492–1500)*S Xie, R Girshick, P Dollar, Z Tu, K He\\xa0- Proceedings of the IEEE Conference on Computer\\xa0…, 20173회 인용 관련 학술자료 Aggregated residual transformations for deep neural networks. arXiv e-prints, page*S Xie, R Girshick, P Dollár, Z Tu, K He\\xa0- arXiv preprint arXiv:1611.05431, 20162회 인용 관련 학술자료 col.(2017).«Aggregated residual transformations for deep neural networks»*S Xie\\xa0- Computer Vision and Pattern Recognition (CVPR)\\xa0…2회 인용 관련 학술자료 Aggregated residual transformations for deep neural networks. arXiv 2017, arXiv*S Xie, R Girshick, P Dollár, Z Tu, K He\\xa0- arXiv preprint arXiv:1611.054312회 인용 관련 학술자료 ar, Zhuowen Tu, and Kaiming He. 2017. Aggregated Residual Transformations for Deep Neural Networks*S Xie, R Girshick, P Doll\\xa0- Proceedings of the IEEE Conference on Computer\\xa0…2회 인용 관련 학술자료 others, Dollár P, Tu Z, He K (2017) Aggregated residual transformations for deep neural networks*S Xie, R Girshick\\xa0- arXiv preprint arXiv:1611.054312회 인용 관련 학술자료 '}, title='Aggregated Residual Transformations for Deep Neural Networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Identity Mappings in Deep Residual Networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': ' Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62\\xa0% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at:                      https://github.com/KaimingHe/resnet-1k-layers                                        .', '저자': 'Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun', '전체 인용횟수': '11015회 인용20162017201820192020202120222023120498100314381793212121871739', '컨퍼런스': 'European Conference on Computer Vision (ECCV), 2016', '학술 문서': 'Identity mappings in deep residual networksK He, X Zhang, S Ren, J Sun\\xa0- Computer Vision–ECCV 2016: 14th European\\xa0…, 201610833회 인용 관련 학술자료 전체 8개의 버전 Identity mappings in deep residual networks*H Kaiming, Z Xiangyu, R Shaoqing, S Jian\\xa0- European conference on computer vision, 2016240회 인용 관련 학술자료 Identity mappings in deep residual networks (2016)*K He, X Zhang, S Ren, J Sun\\xa0- arXiv preprint arXiv:1603.05027, 201630회 인용 관련 학술자료 Identity mappings in deep residual networks. CoRR abs/1603.05027 (2016)*K He, X Zhang, S Ren, J Sun\\xa0- arXiv preprint arXiv:1603.05027, 201621회 인용 관련 학술자료 '}, title='Identity Mappings in Deep Residual Networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Non-local Neural Networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our non-local models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks. Code will be made available.', '저자': 'Xiaolong Wang, Ross Girshick, Abhinav Gupta, Kaiming He', '전체 인용횟수': '9430회 인용2018201920202021202220231106491373227426552315', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2018', '학술 문서': 'Non-local neural networksX Wang, R Girshick, A Gupta, K He\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20189430회 인용 관련 학술자료 전체 15개의 버전 '}, title='Non-local Neural Networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Momentum Contrast for Unsupervised Visual Representation Learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020', '설명': 'We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.', '저자': 'Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick', '전체 인용횟수': '9318회 인용2020202120222023455176332463797', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2020', '학술 문서': 'Momentum contrast for unsupervised visual representation learningK He, H Fan, Y Wu, S Xie, R Girshick\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 20209318회 인용 관련 학술자료 전체 19개의 버전 Girshick Ross B.. 2020*H Kaiming, F Haoqi, W Yuxin, X Saining\\xa0- …\\xa0for unsupervised visual representation learning. In\\xa0…, 202011회 인용 관련 학술자료 '}, title='Momentum Contrast for Unsupervised Visual Representation Learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Single Image Haze Removal using Dark Channel Prior': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009/6', '게시자': 'IEEE', '설명': 'In this paper, we propose a simple but effective image prior-dark channel prior to remove haze from a single input image. The dark channel prior is a kind of statistics of outdoor haze-free images. It is based on a key observation-most local patches in outdoor haze-free images contain some pixels whose intensity is very low in at least one color channel. Using this prior with the haze imaging model, we can directly estimate the thickness of the haze and recover a high-quality haze-free image. Results on a variety of hazy images demonstrate the power of the proposed prior. Moreover, a high-quality depth map can also be obtained as a byproduct of haze removal.', '저자': 'Kaiming He, Jian Sun, Xiaoou Tang', '전체 인용횟수': '8317회 인용2010201120122013201420152016201720182019202020212022202376111159241344493585582704724872102112001137', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2009', '학술 문서': 'Single image haze removal using dark channel priorK He, J Sun, X Tang\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20108272회 인용 관련 학술자료 전체 44개의 버전 Single image haze removal using dark channel priors*J Sun, K He, X Tang\\xa0- US Patent 8,340,461, 201259회 인용 관련 학술자료 전체 4개의 버전 '}, title='Single Image Haze Removal using Dark Channel Prior', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Guided Image Filtering': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/9/5', '게시자': 'Springer Berlin Heidelberg', '도서': 'European Conference on Computer Vision (ECCV), 2010', '설명': 'In this paper, we propose a novel explicit image filter called guided filter. Derived from a local linear model, the guided filter computes the filtering output by considering the content of a guidance image, which can be the input image itself or another different image. The guided filter can be used as an edge-preserving smoothing operator like the popular bilateral filter [1], but it has better behaviors near edges. The guided filter is also a more generic concept beyond smoothing: It can transfer the structures of the guidance image to the filtering output, enabling new filtering applications like dehazing and guided feathering. Moreover, the guided filter naturally has a fast and nonapproximate linear time algorithm, regardless of the kernel size and the intensity range. Currently, it is one of the fastest edge-preserving filters. Experiments show that the guided filter is both effective and efficient in a great variety of computer\\xa0…', '저자': 'Kaiming He, Jian Sun, Xiaoou Tang', '전체 인용횟수': '7752회 인용201120122013201420152016201720182019202020212022202343112210355581811798861829830824790644', '학술 문서': 'Guided image filteringK He, J Sun, X Tang\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20127752회 인용 관련 학술자료 전체 27개의 버전 '}, title='Guided Image Filtering', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'R-FCN: Object Detection via Region-based Fully Convolutional Networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': 'We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets), for object detection. We show competitive results on the PASCAL VOC datasets (eg, 83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20 times faster than the Faster R-CNN counterpart. Code is made publicly available at: https://github. com/daijifeng001/r-fcn.', '저자': 'Jifeng Dai, Yi Li, Kaiming He, Jian Sun', '전체 인용횟수': '6977회 인용20162017201820192020202120222023192917361200128713131208851', '컨퍼런스': 'Neural Information Processing Systems (NIPS), 2016', '학술 문서': 'R-fcn: Object detection via region-based fully convolutional networksJ Dai, Y Li, K He, J Sun\\xa0- Advances in neural information processing systems, 20166959회 인용 관련 학술자료 전체 10개의 버전 R-fcn: Object detection via region-based fully convolutional networks. arXiv 2016*J Dai, Y Li, K He, J Sun\\xa0- arXiv preprint arXiv:1605.06409, 2016114회 인용 관련 학술자료 '}, title='R-FCN: Object Detection via Region-based Fully Convolutional Networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/6/8', '설명': 'Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves ~90% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.', '저널': 'arXiv preprint arXiv:1706.02677', '저자': 'Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, Kaiming He', '전체 인용횟수': '3569회 인용201720182019202020212022202355289488603739727647', '학술 문서': 'Accurate, large minibatch sgd: Training imagenet in 1 hourP Goyal, P Dollár, R Girshick, P Noordhuis…\\xa0- arXiv preprint arXiv:1706.02677, 20173548회 인용 관련 학술자료 전체 9개의 버전 Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv 2017P Goyal, P Dollár, R Girshick, P Noordhuis…\\xa0- arXiv preprint arXiv:1706.02677, 201986회 인용 관련 학술자료 Accurate, large minibatch sgd: Training imagenet in 1 hourG Priya, D Piotr, N Pieter, W Lukasz, K Aapo, T Andrew…\\xa0- arXiv preprint arXiv:1706.02677, 201730회 인용 관련 학술자료 '}, title='Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Masked Autoencoders Are Scalable Vision Learners': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2022', '설명': 'This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, eg, 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: eg, a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.', '저자': 'Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick', '전체 인용횟수': '3525회 인용202120222023249932478', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2022', '학술 문서': 'Masked autoencoders are scalable vision learnersK He, X Chen, S Xie, Y Li, P Dollár, R Girshick\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 20223525회 인용 관련 학술자료 전체 12개의 버전 '}, title='Masked Autoencoders Are Scalable Vision Learners', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Group Normalization': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/3/22', '설명': \"Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems---BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code.\", '저자': 'Yuxin Wu, Kaiming He', '전체 인용횟수': '3421회 인용20182019202020212022202357317599791901734', '컨퍼런스': 'European Conference on Computer Vision (ECCV), 2018', '학술 문서': 'Group normalizationY Wu, K He\\xa0- Proceedings of the European conference on computer\\xa0…, 20183421회 인용 관련 학술자료 전체 16개의 버전 '}, title='Group Normalization', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Exploring Simple Siamese Representation Learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021', '설명': 'Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These models maximize the similarity between two augmentations of one image, subject to certain conditions for avoiding collapsing solutions. In this paper, we report surprising empirical results that simple Siamese networks can learn meaningful representations even using none of the following:(i) negative sample pairs,(ii) large batches,(iii) momentum encoders. Our experiments show that collapsing solutions do exist for the loss and structure, but a stop-gradient operation plays an essential role in preventing collapsing. We provide a hypothesis on the implication of stop-gradient, and further show proof-of-concept experiments verifying it. Our\" SimSiam\" method achieves competitive results on ImageNet and downstream tasks. We hope this simple baseline will motivate people to rethink the roles of Siamese architectures for unsupervised representation learning. Code is made available.(https://github. com/facebookresearch/simsiam)', '저자': 'Xinlei Chen, Kaiming He', '전체 인용횟수': '2931회 인용20212022202337111591383', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2021', '학술 문서': 'Exploring simple siamese representation learningX Chen, K He\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 20212931회 인용 관련 학술자료 전체 6개의 버전 '}, title='Exploring Simple Siamese Representation Learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Convolutional Neural Networks at Constrained Time Cost': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': \"Though recent advanced convolutional neural networks (CNNs) have been improving the image recognition accuracy, the models are getting more complex and time-consuming. For real-world applications in industrial and commercial scenarios, engineers and developers are often faced with the requirement of constrained time budget. In this paper, we investigate the accuracy of CNNs under constrained time cost. Under this constraint, the designs of the network architectures should exhibit as trade-offs among the factors like depth, numbers of filters, filter sizes, etc. With a series of controlled comparisons, we progressively modify a baseline model while preserving its time complexity. This is also helpful for understanding the importance of the factors in network designs. We present an architecture that achieves very competitive accuracy in the ImageNet dataset (11.8% top-5 error, 10-view test), yet is 20% faster than``AlexNet''(16.0% top-5 error, 10-view test).\", '저자': 'Kaiming He, Jian Sun', '전체 인용횟수': '2892회 인용19881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202318343533354269548077968010485481412714920252628473733295580159245294310295211', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2015', '학술 문서': 'Convolutional neural networks at constrained time costK He, J Sun\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20152892회 인용 관련 학술자료 전체 13개의 버전 '}, title='Convolutional Neural Networks at Constrained Time Cost', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'SlowFast Networks for Video Recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': 'We present SlowFast networks for video recognition. Our model involves (i) a Slow pathway, operating at low frame rate, to capture spatial semantics, and (ii) a Fast pathway, operating at high frame rate, to capture motion at fine temporal resolution. The Fast pathway can be made very lightweight by reducing its channel capacity, yet can learn useful temporal information for video recognition. Our models achieve strong performance for both action classification and detection in video, and large improvements are pin-pointed as contributions by our SlowFast concept. We report state-of-the-art accuracy on major video recognition benchmarks, Kinetics, Charades and AVA. Code has been made available at: https://github. com/facebookresearch/SlowFast.', '저자': 'Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, Kaiming He', '전체 인용횟수': '2761회 인용2019202020212022202366295593851942', '컨퍼런스': 'International Conference on Computer Vision (ICCV), 2019', '학술 문서': 'Slowfast networks for video recognitionC Feichtenhofer, H Fan, J Malik, K He\\xa0- Proceedings of the IEEE/CVF international conference\\xa0…, 20192755회 인용 관련 학술자료 전체 10개의 버전 Slowfast networks for video recognition*F Christoph, F Haoqi, H Kaiming\\xa0- Proceedings of the IEEE international conference on\\xa0…, 201926회 인용 관련 학술자료 '}, title='SlowFast Networks for Video Recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Improved Baselines with Momentum Contrastive Learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020/3/9', '설명': \"Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.\", '저널': 'arXiv preprint arXiv:2003.04297', '저자': 'Xinlei Chen, Haoqi Fan, Ross Girshick, Kaiming He', '전체 인용횟수': '2696회 인용202020212022202311151310051055', '학술 문서': 'Improved baselines with momentum contrastive learningX Chen, H Fan, R Girshick, K He\\xa0- arXiv preprint arXiv:2003.04297, 20202678회 인용 관련 학술자료 전체 3개의 버전 Improved baselines with momentum contrastive learning. arXiv 2020X Chen, H Fan, R Girshick, K He\\xa0- arXiv preprint arXiv:2003.04297, 200383회 인용 관련 학술자료 '}, title='Improved Baselines with Momentum Contrastive Learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'An Empirical Study of Training Self-Supervised Vision Transformers': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021', '저자': 'Xinlei Chen, Saining Xie, Kaiming He', '전체 인용횟수': '1811회 인용202120222023107738954', '컨퍼런스': 'International Conference on Computer Vision (ICCV), 2021', '학술 문서': 'Multiscale vision transformers*H Fan, B Xiong, K Mangalam, Y Li, Z Yan, J Malik…\\xa0- Proceedings of the IEEE/CVF international conference\\xa0…, 20211811회 인용 관련 학술자료 전체 11개의 버전 '}, title='An Empirical Study of Training Self-Supervised Vision Transformers', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Instance-aware Semantic Segmentation via Multi-task Network Cascades': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': 'Semantic segmentation research has recently witnessed rapid progress, but many leading methods are unable to identify object instances. In this paper, we present Multi-task Network Cascades for instance-aware semantic segmentation. Our model consists of three networks, respectively differentiating instances, estimating masks, and categorizing objects. These networks form a cascaded structure, and are designed to share their convolutional features. We develop an algorithm for the nontrivial end-to-end training of this causal, cascaded structure. Our solution is a clean, single-step training framework and can be generalized to cascades that have more stages. We demonstrate state-of-the-art instance-aware semantic segmentation accuracy on PASCAL VOC. Meanwhile, our method takes only 360ms testing an image using VGG-16, which is two orders of magnitude faster than previous systems for this challenging problem. As a by product, our method also achieves compelling object detection results which surpass the competitive Fast/Faster R-CNN systems. The method described in this paper is the foundation of our submissions to the MS COCO 2015 segmentation competition, where we won the 1st place.', '저자': 'Jifeng Dai, Kaiming He, Jian Sun', '전체 인용횟수': '1577회 인용201520162017201820192020202120222023554170185267286269183135', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2016', '학술 문서': 'Instance-aware semantic segmentation via multi-task network cascadesJ Dai, K He, J Sun\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20161577회 인용 관련 학술자료 전체 16개의 버전 '}, title='Instance-aware Semantic Segmentation via Multi-task Network Cascades', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Exploring the Limits of Weakly Supervised Pretraining': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/5/2', '설명': 'State-of-the-art visual perception models for a wide range of tasks rely on supervised pretraining. ImageNet classification is the de facto pretraining task for these models. Yet, ImageNet is now nearly ten years old and is by modern standards\" small\". Even so, relatively little is known about the behavior of pretraining with datasets that are multiple orders of magnitude larger. The reasons are obvious: such datasets are difficult to collect and annotate. In this paper, we present a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images. Our experiments demonstrate that training for large-scale hashtag prediction leads to excellent results. We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4%(97.6% top-5). We also perform extensive experiments that provide novel empirical data on the relationship between large-scale pretraining and transfer learning performance.', '저자': 'Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, Laurens van der Maaten', '전체 인용횟수': '1379회 인용20182019202020212022202330162258313339270', '컨퍼런스': 'European Conference on Computer Vision (ECCV), 2018', '학술 문서': 'Exploring the limits of weakly supervised pretrainingD Mahajan, R Girshick, V Ramanathan, K He, M Paluri…\\xa0- Proceedings of the European conference on computer\\xa0…, 20181379회 인용 관련 학술자료 전체 8개의 버전 '}, title='Exploring the Limits of Weakly Supervised Pretraining', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Designing Network Design Spaces': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020', '설명': 'In this work, we present a new network design paradigm. Our goal is to help advance the understanding of network design and discover design principles that generalize across settings. Instead of focusing on designing individual network instances, we design network design spaces that parametrize populations of networks. The overall process is analogous to classic manual design of networks, but elevated to the design space level. Using our methodology we explore the structure aspect of network design and arrive at a low-dimensional design space consisting of simple, regular networks that we call RegNet. The core insight of the RegNet parametrization is surprisingly simple: widths and depths of good networks can be explained by a quantized linear function. We analyze the RegNet design space and arrive at interesting findings that do not match the current practice of network design. The RegNet design space provides simple and fast networks that work well across a wide range of flop regimes. Under comparable training settings and flops, the RegNet models outperform the popular EfficientNet models while being up to 5x faster on GPUs.', '저자': 'Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollár', '전체 인용횟수': '1346회 인용202020212022202352292486501', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2020', '학술 문서': 'Designing network design spacesI Radosavovic, RP Kosaraju, R Girshick, K He, P Dollár\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 20201336회 인용 관련 학술자료 전체 11개의 버전 Designing network design spaces*R Ilija, RP Kosaraju, R Girshick, K He, P Dollar\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 202021회 인용 관련 학술자료 '}, title='Designing Network Design Spaces', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Panoptic Segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': 'We propose and study a task we name panoptic segmentation (PS). Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more unified view of image segmentation. For more analysis and up-to-date results, please check the arXiv version of the paper:\\\\smallhttps://arxiv. org/abs/1801.00868.', '저자': 'Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, Piotr Dollár', '전체 인용횟수': '1305회 인용2018201920202021202220231881177282345393', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2019', '학술 문서': 'Panoptic segmentationA Kirillov, K He, R Girshick, C Rother, P Dollár\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 20191303회 인용 관련 학술자료 전체 8개의 버전 Panoptic segmentation*K Alexander, H Kaiming, R Carsten, D Piotr\\xa0- Proceedings of the IEEE Conference on Computer\\xa0…, 201914회 인용 관련 학술자료 '}, title='Panoptic Segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Rethinking ImageNet Pre-training': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': \"We report competitive results on object detection and instance segmentation on the COCO dataset using standard models trained from random initialization. The results are no worse than their ImageNet pre-training counterparts even when using the hyper-parameters of the baseline system (Mask R-CNN) that were optimized for fine-tuning pre-trained models, with the sole exception of increasing the number of training iterations so the randomly initialized models may converge. Training from random initialization is surprisingly robust; our results hold even when:(i) using only 10% of the training data,(ii) for deeper and wider models, and (iii) for multiple tasks and metrics. Experiments show that ImageNet pre-training speeds up convergence early in training, but does not necessarily provide regularization or improve final target task accuracy. To push the envelope we demonstrate 50.9 AP on COCO object detection without using any external data---a result on par with the top COCO 2017 competition results that used ImageNet pre-training. These observations challenge the conventional wisdom of ImageNet pre-training for dependent tasks and we expect these discoveries will encourage people to rethink the current de facto paradigm ofpre-training and fine-tuning'in computer vision.\", '저자': 'Kaiming He, Ross Girshick, Piotr Dollár', '전체 인용횟수': '1153회 인용2018201920202021202220236108217280285250', '컨퍼런스': 'International Conference on Computer Vision (ICCV), 2019', '학술 문서': 'Rethinking imagenet pre-trainingK He, R Girshick, P Dollár\\xa0- Proceedings of the IEEE/CVF International Conference\\xa0…, 20191153회 인용 관련 학술자료 전체 7개의 버전 '}, title='Rethinking ImageNet Pre-training', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/3/5', '설명': 'Recent leading approaches to semantic segmentation rely on deep convolutional networks trained with human-annotated, pixel-level segmentation masks. Such pixel-accurate supervision demands expensive labeling effort and limits the performance of deep networks that usually benefit from more training data. In this paper, we propose a method that achieves competitive accuracy but only requires easily obtained bounding box annotations. The basic idea is to iterate between automatically generating region proposals and training convolutional networks. These two steps gradually recover segmentation masks for improving the networks, and vise versa. Our method, called\" BoxSup\", produces competitive results (eg, 62.0% mAP for validation) supervised by boxes only, on par with strong baselines (eg, 63.8% mAP) fully supervised by masks under the same setting. By leveraging a large amount of bounding boxes, BoxSup further yields state-of-the-art results on PASCAL VOC 2012 and PASCAL-CONTEXT.', '저자': 'Jifeng Dai, Kaiming He, Jian Sun', '전체 인용횟수': '1153회 인용20142015201620172018201920202021202220233216890120149161177197144', '컨퍼런스': 'International Conference on Computer Vision (ICCV), 2015', '학술 문서': 'Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentationJ Dai, K He, J Sun\\xa0- Proceedings of the IEEE international conference on\\xa0…, 20151153회 인용 관련 학술자료 전체 11개의 버전 '}, title='BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Panoptic Feature Pyramid Networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019/1/8', '설명': \"The recently introduced panoptic segmentation task has renewed our community's interest in unifying the tasks of instance segmentation (for thing classes) and semantic segmentation (for stuff classes). However, current state-of-the-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation, without performing any shared computation. In this work, we aim to unify these methods at the architectural level, designing a single network for both tasks. Our approach is to endow Mask R-CNN, a popular instance segmentation method, with a semantic segmentation branch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for instance segmentation, but also yields a lightweight, top-performing method for semantic segmentation. In this work, we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and accurate baseline for both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and aid future research in panoptic segmentation.\", '저자': 'Alexander Kirillov, Ross Girshick, Kaiming He, Piotr Dollár', '전체 인용횟수': '1116회 인용2019202020212022202341139230337364', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2019', '학술 문서': 'Panoptic feature pyramid networksA Kirillov, R Girshick, K He, P Dollár\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 20191116회 인용 관련 학술자료 전체 7개의 버전 '}, title='Panoptic Feature Pyramid Networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep Hough Voting for 3D Object Detection in Point Clouds': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019/4/21', '설명': \"Current 3D object detection methods are heavily influenced by 2D detectors. In order to leverage architectures in 2D detectors, they often convert 3D point clouds to regular grids (ie, to voxel grids or to bird's eye view images), or rely on detection in 2D images to propose 3D boxes. Few works have attempted to directly detect objects in point clouds. In this work, we return to first principles to construct a 3D detection pipeline for point cloud data and as generic as possible. However, due to the sparse nature of the data--samples from 2D manifolds in 3D space--we face a major challenge when directly predicting bounding box parameters from scene points: a 3D object centroid can be far from any surface point thus hard to regress accurately in one step. To address the challenge, we propose VoteNet, an end-to-end 3D object detection network based on a synergy of deep point set networks and Hough voting. Our model achieves state-of-the-art 3D detection on two large datasets of real 3D scans, ScanNet and SUN RGB-D with a simple design, compact model size and high efficiency. Remarkably, VoteNet outperforms previous methods by using purely geometric information without relying on color images.\", '저자': 'Charles R Qi, Or Litany, Kaiming He, Leonidas J Guibas', '전체 인용횟수': '1040회 인용2019202020212022202311113257344311', '컨퍼런스': 'International Conference on Computer Vision (ICCV), 2019', '학술 문서': 'Deep hough voting for 3d object detection in point cloudsCR Qi, O Litany, K He, LJ Guibas\\xa0- proceedings of the IEEE/CVF International Conference\\xa0…, 20191040회 인용 관련 학술자료 전체 12개의 버전 '}, title='Deep Hough Voting for 3D Object Detection in Point Clouds', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': 'Large-scale data are of crucial importance for learning semantic segmentation models, but annotating per-pixel masks is a tedious and inefficient procedure. We note that for the topic of interactive image segmentation, scribbles are very widely used in academic research and commercial software, and are recognized as one of the most user-friendly ways of interacting. In this paper, we propose to use scribbles to annotate images, and develop an algorithm to train convolutional networks for semantic segmentation supervised by scribbles. Our algorithm is based on a graphical model that jointly propagates information from scribbles to unmarked pixels and learns network parameters. We present competitive object semantic segmentation results on the PASCAL VOC dataset by using scribbles as annotations. Scribbles are also favored for annotating stuff (eg, water, sky, grass) that has no well-defined shape, and our method shows excellent results on the PASCAL-CONTEXT dataset thanks to extra inexpensive scribble annotations.', '저자': 'Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, Jian Sun', '전체 인용횟수': '1036회 인용2016201720182019202020212022202364874119135203224212', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2016', '학술 문서': 'Scribblesup: Scribble-supervised convolutional networks for semantic segmentationD Lin, J Dai, J Jia, K He, J Sun\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20161036회 인용 관련 학술자료 전체 17개의 버전 '}, title='ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Is Faster R-CNN Doing Well for Pedestrian Detection?': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': ' Detecting pedestrian has been arguably addressed as a special topic beyond general object detection. Although recent deep learning object detectors such as Fast/Faster R-CNN have shown excellent performance for general object detection, they have limited success for detecting pedestrian, and previous leading pedestrian detectors were in general hybrid methods combining hand-crafted and deep convolutional features. In this paper, we investigate issues involving Faster R-CNN for pedestrian detection. We discover that the Region Proposal Network (RPN) in Faster R-CNN indeed performs well as a stand-alone pedestrian detector, but surprisingly, the downstream classifier degrades the results. We argue that two reasons account for the unsatisfactory accuracy: (i) insufficient resolution of feature maps for handling small instances, and (ii) lack of any bootstrapping strategy for mining hard negative\\xa0…', '저자': 'Liliang Zhang, Liang Lin, Xiaodan Liang, Kaiming He', '전체 인용횟수': '1000회 인용20162017201820192020202120222023510416219714916712178', '컨퍼런스': 'European Conference on Computer Vision (ECCV), 2016', '학술 문서': 'Is faster R-CNN doing well for pedestrian detection?L Zhang, L Lin, X Liang, K He\\xa0- Computer Vision–ECCV 2016: 14th European\\xa0…, 20161000회 인용 관련 학술자료 전체 7개의 버전 '}, title='Is Faster R-CNN Doing Well for Pedestrian Detection?', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Accelerating Very Deep Convolutional Networks for Classification and Detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'This paper aims to accelerate the test-time computation of convolutional neural networks (CNNs), especially very deep CNNs    that have substantially impacted the computer vision community. Unlike previous methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We develop an effective solution to the resulting nonlinear optimization problem without the need of stochastic gradient descent (SGD). More importantly, while previous methods mainly focus on optimizing one or two layers, our nonlinear method enables an asymmetric reconstruction that reduces the rapidly accumulated error when multiple (e.g.,   10) layers are approximated. For the widely used very deep VGG-16 model    , our method achieves a whole-model speedup of 4   with merely a 0.3 percent increase of top-5 error in ImageNet classification. Our 4    accelerated VGG\\xa0…', '저널': 'IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)', '저자': 'Xiangyu Zhang, Jianhua Zou, Kaiming He, Jian Sun', '전체 인용횟수': '942회 인용2015201620172018201920202021202220237214288132140188193117', '학술 문서': 'Accelerating very deep convolutional networks for classification and detectionX Zhang, J Zou, K He, J Sun\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 2015942회 인용 관련 학술자료 전체 10개의 버전 '}, title='Accelerating Very Deep Convolutional Networks for Classification and Detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Feature Denoising for Improving Adversarial Robustness': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': 'Adversarial attacks to image classification systems present challenges to convolutional networks and opportunities for understanding them. This study suggests that adversarial perturbations on images lead to noise in the features constructed by these networks. Motivated by this observation, we develop new network architectures that increase adversarial robustness by performing feature denoising. Specifically, our networks contain blocks that denoise the features using non-local means or other filters; the entire networks are trained end-to-end. When combined with adversarial training, our feature denoising networks substantially improve the state-of-the-art in adversarial robustness in both white-box and black-box attack settings. On ImageNet, under 10-iteration PGD white-box attacks where prior art has 27.9% accuracy, our method achieves 55.7%; even under extreme 2000-iteration PGD white-box attacks, our method secures 42.6% accuracy. Our method was ranked first in Competition on Adversarial Attacks and Defenses (CAAD) 2018---it achieved 50.6% classification accuracy on a secret, ImageNet-like test dataset against 48 unknown attackers, surpassing the runner-up approach by 10%. Code is available at https://github. com/facebookresearch/ImageNet-Adversarial-Training.', '저자': 'Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan Yuille, Kaiming He', '전체 인용횟수': '882회 인용201820192020202120222023473196213199193', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2019', '학술 문서': 'Feature denoising for improving adversarial robustnessC Xie, Y Wu, L Maaten, AL Yuille, K He\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 2019882회 인용 관련 학술자료 전체 12개의 버전 '}, title='Feature Denoising for Improving Adversarial Robustness', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'PointRend: Image Segmentation as Rendering': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020', '설명': \"We present a new method for efficient high-quality image segmentation of objects and scenes. By analogizing classical computer graphics methods for efficient rendering with over-and undersampling challenges faced in pixel labeling tasks, we develop a unique perspective of image segmentation as a rendering problem. From this vantage, we present the PointRend (Point-based Rendering) neural network module: a module that performs point-based segmentation predictions at adaptively selected locations based on an iterative subdivision algorithm. PointRend can be flexibly applied to both instance and semantic segmentation tasks by building on top of existing state-of-the-art models. While many concrete implementations of the general idea are possible, we show that a simple design already achieves excellent results. Qualitatively, PointRend outputs crisp object boundaries in regions that are over-smoothed by previous methods. Quantitatively, PointRend yields significant gains on COCO and Cityscapes, for both instance and semantic segmentation. PointRend's efficiency enables output resolutions that are otherwise impractical in terms of memory or computation compared to existing approaches. Code has been made available at https://github. com/facebookresearch/detectron2/tree/master/projects/PointRend.\", '저자': 'Alexander Kirillov, Yuxin Wu, Kaiming He, Ross Girshick', '전체 인용횟수': '767회 인용202020212022202340176247301', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2020', '학술 문서': 'Pointrend: Image segmentation as renderingA Kirillov, Y Wu, K He, R Girshick\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 2020767회 인용 관련 학술자료 전체 10개의 버전 '}, title='PointRend: Image Segmentation as Rendering', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Optimized Product Quantization for Approximate Nearest Neighbor Search': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013', '설명': 'Product quantization is an effective vector quantization approach to compactly encode high-dimensional vectors for fast approximate nearest neighbor (ANN) search. The essence of product quantization is to decompose the original high-dimensional space into the Cartesian product of a finite number of low-dimensional subspaces that are then quantized separately. Optimal space decomposition is important for the performance of ANN search, but still remains unaddressed. In this paper, we optimize product quantization by minimizing quantization distortions wrt the space decomposition and the quantization codebooks. We present two novel methods for optimization: a nonparametric method that alternatively solves two smaller sub-problems, and a parametric method that is guaranteed to achieve the optimal solution if the input data follows some Gaussian distribution. We show by experiments that our optimized approach substantially improves the accuracy of product quantization for ANN search.', '저자': 'Tiezheng Ge, Kaiming He, Qifa Ke, Jian Sun', '전체 인용횟수': '756회 인용20122013201420152016201720182019202020212022202328294975667677768695100', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2013', '학술 문서': 'Optimized product quantization for approximate nearest neighbor searchT Ge, K He, Q Ke, J Sun\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2013439회 인용 관련 학술자료 전체 14개의 버전 Optimized product quantization*T Ge, K He, Q Ke, J Sun\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 2013354회 인용 관련 학술자료 전체 17개의 버전 '}, title='Optimized Product Quantization for Approximate Nearest Neighbor Search', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Detecting and Recognizing Human-Object Interactions': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'To understand the visual world, a machine must not only recognize individual object instances but also how they interact. Humans are often at the center of such interactions and detecting human-object interactions is an important practical and scientific problem. In this paper, we address the task of detecting (human, verb, object) triplets in challenging everyday photos. We propose a novel model that is driven by a human-centric approach. Our hypothesis is that the appearance of a person--their pose, clothing, action--is a powerful cue for localizing the objects they are interacting with. To exploit this cue, our model learns to predict an action-specific density over target object locations based on the appearance of a detected person. Our model also jointly learns to detect people and objects, and by fusing these predictions it efficiently infers interaction triplets in a clean, jointly trained end-to-end system we call InteractNet. We validate our approach on the recently introduced Verbs in COCO (V-COCO) and HICO-DET datasets, where we show quantitatively compelling results.', '저자': 'Georgia Gkioxari, Ross Girshick, Piotr Dollár, Kaiming He', '전체 인용횟수': '609회 인용201720182019202020212022202363267121128133117', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2018', '학술 문서': 'Detecting and recognizing human-object interactionsG Gkioxari, R Girshick, P Dollár, K He\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2018606회 인용 관련 학술자료 전체 12개의 버전 Detecting and recognizing human-object interactions*GGRGP Dollár, K He\\xa0- CVPR, 20184회 인용 관련 학술자료 전체 2개의 버전 '}, title='Detecting and Recognizing Human-Object Interactions', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Convolutional Feature Masking for Joint Object and Stuff Segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'The topic of semantic segmentation has witnessed considerable progress due to the powerful features learned by convolutional neural networks (CNNs). The current leading approaches for semantic segmentation exploit shape information by extracting CNN features from masked image regions. This strategy introduces artificial boundaries on the images and may impact the quality of the extracted features. Besides, the operations on the raw image domain require to compute thousands of networks on a single image, which is time-consuming. In this paper, we propose to exploit shape information via masking convolutional features. The proposal segments (eg, super-pixels) are treated as masks on the convolutional feature maps. The CNN features of segments are directly masked out from these maps and used to train classifiers for recognition. We further propose a joint method to handle objects and\" stuff\"(eg, grass, sky, water) in the same framework. State-of-the-art results are demonstrated on the challenging PASCAL VOC benchmarks, with a compelling computational speed.', '저자': 'Jifeng Dai, Kaiming He, Jian Sun', '전체 인용횟수': '523회 인용20142015201620172018201920202021202220234276590766371443631', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2015', '학술 문서': 'Convolutional feature masking for joint object and stuff segmentationJ Dai, K He, J Sun\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2015523회 인용 관련 학술자료 전체 12개의 버전 '}, title='Convolutional Feature Masking for Joint Object and Stuff Segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Detectron': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '권': '6', '저자': 'Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Dollár, Kaiming He', '전체 인용횟수': '520회 인용201820192020202120222023501341171146635', '학술 문서': 'DetectronR Girshick, I Radosavovic, G Gkioxari, P Dollár, K He - 2018499회 인용 관련 학술자료 Detectron (2018)*R Girshick, I Radosavovic, G Gkioxari, P Dollár, K He - 201151회 인용 관련 학술자료 '}, title='Detectron', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Instance-sensitive Fully Convolutional Networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': ' Fully convolutional networks (FCNs) have been proven very successful for semantic segmentation, but the FCN outputs are unaware of object instances. In this paper, we develop FCNs that are capable of proposing instance-level segment candidates. In contrast to the previous FCN that generates one score map, our FCN is designed to compute a small set of instance-sensitive score maps, each of which is the outcome of a pixel-wise classifier of a relative position to instances. On top of these instance-sensitive score maps, a simple assembling module is able to output instance candidate at each position. In contrast to the recent DeepMask method for segmenting instances, our method does not have any high-dimensional layer related to the mask resolution, but instead exploits image local coherence for estimating instances. We present competitive results of instance segment proposal on both PASCAL\\xa0…', '저자': 'Jifeng Dai, Kaiming He, Yi Li, Shaoqing Ren, Jian Sun', '전체 인용횟수': '499회 인용20152016201720182019202020212022202329397110375756749', '컨퍼런스': 'European Conference on Computer Vision (ECCV), 2016', '학술 문서': 'Instance-sensitive fully convolutional networksJ Dai, K He, Y Li, S Ren, J Sun\\xa0- Computer Vision–ECCV 2016: 14th European\\xa0…, 2016499회 인용 관련 학술자료 전체 4개의 버전 '}, title='Instance-sensitive Fully Convolutional Networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Long-Term Feature Banks for Detailed Video Understanding': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': 'To understand the world, we humans constantly need to relate the present to the past, and put events in context. In this paper, we enable existing video models to do the same. We propose a long-term feature bank--supportive information extracted over the entire span of a video--to augment state-of-the-art video models that otherwise would only view short clips of 2-5 seconds. Our experiments demonstrate that augmenting 3D convolutional networks with a long-term feature bank yields state-of-the-art results on three challenging video datasets: AVA, EPIC-Kitchens, and Charades. Code is available online.', '저자': 'Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming He, Philipp Krähenbühl, Ross Girshick', '전체 인용횟수': '494회 인용201920202021202220233183140127110', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2019', '학술 문서': 'Long-term feature banks for detailed video understandingCY Wu, C Feichtenhofer, H Fan, K He, P Krahenbuhl…\\xa0- Proceedings of the IEEE/CVF Conference on Computer\\xa0…, 2019494회 인용 관련 학술자료 전체 12개의 버전 '}, title='Long-Term Feature Banks for Detailed Video Understanding', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Object Detection Networks on Convolutional Feature Maps': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': 'Most object detectors contain two important components: a feature extractor and an object classifier. The feature extractor has rapidly evolved with significant research efforts leading to better deep convolutional architectures. The object classifier, however, has not received much attention and many recent systems (like SPPnet and Fast/Faster R-CNN) use simple multi-layer perceptrons. This paper demonstrates that carefully designing deep networks for object classification is just as important. We experiment with region-wise classifier networks that use shared, region-independent convolutional features. We call them “Networks on Convolutional feature maps” (NoCs). We discover that aside from deep feature maps, a  deep  and  convolutional  per-region classifier is of particular importance for object detection, whereas latest superior image classification models (such as ResNets and GoogLeNets) do not directly\\xa0…', '저널': 'IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)', '저자': 'Shaoqing Ren, Kaiming He, Ross Girshick, Xiangyu Zhang, Jian Sun', '전체 인용횟수': '491회 인용20152016201720182019202020212022202362335708583795647', '학술 문서': 'Object detection networks on convolutional feature mapsS Ren, K He, R Girshick, X Zhang, J Sun\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 2016491회 인용 관련 학술자료 전체 11개의 버전 '}, title='Object Detection Networks on Convolutional Feature Maps', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'K-means hashing: An affinity-preserving quantization method for learning binary compact codes': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013', '설명': 'In computer vision there has been increasing interest in learning hashing codes whose Hamming distance approximates the data similarity. The hashing functions play roles in both quantizing the vector space and generating similarity-preserving codes. Most existing hashing methods use hyper-planes (or kernelized hyper-planes) to quantize and encode. In this paper, we present a hashing method adopting the k-means quantization. We propose a novel Affinity-Preserving K-means algorithm which simultaneously performs k-means clustering and learns the binary indices of the quantized cells. The distance between the cells is approximated by the Hamming distance of the cell indices. We further generalize our algorithm to a product space for learning longer codes. Experiments show our method, named as K-means Hashing (KMH), outperforms various state-of-the-art hashing encoding methods.', '저자': 'Kaiming He, Fang Wen, Jian Sun', '전체 인용횟수': '471회 인용20132014201520162017201820192020202120222023436545173605158381816', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2013', '학술 문서': 'K-means hashing: An affinity-preserving quantization method for learning binary compact codesK He, F Wen, J Sun\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2013471회 인용 관련 학술자료 전체 13개의 버전 '}, title='K-means hashing: An affinity-preserving quantization method for learning binary compact codes', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Data Distillation: Towards Omni-Supervised Learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'We investigate omni-supervised learning, a special regime of semi-supervised learning in which the learner exploits all available labeled data plus internet-scale sources of unlabeled data. Omni-supervised learning is lower-bounded by performance on existing labeled datasets, offering the potential to surpass state-of-the-art fully supervised methods. To exploit the omni-supervised setting, we propose data distillation, a method that ensembles predictions from multiple transformations of unlabeled data, using a single model, to automatically generate new training annotations. We argue that visual recognition models have recently become accurate enough that it is now possible to apply classic ideas about self-training to challenging real-world data. Our experimental results show that in the cases of human keypoint detection and general object detection, state-of-the-art models trained with data distillation surpass the performance of using labeled data from the COCO dataset alone.', '저자': 'Ilija Radosavovic, Piotr Dollár, Ross Girshick, Georgia Gkioxari, Kaiming He', '전체 인용횟수': '438회 인용201720182019202020212022202322766881187561', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2018', '학술 문서': 'Data distillation: Towards omni-supervised learningI Radosavovic, P Dollár, R Girshick, G Gkioxari, K He\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2018438회 인용 관련 학술자료 전체 16개의 버전 '}, title='Data Distillation: Towards Omni-Supervised Learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Statistics of Patch Offsets for Image Completion': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012', '게시자': 'Springer Berlin/Heidelberg', '설명': ' Image completion involves filling missing parts in images. In this paper we address this problem through the statistics of patch offsets. We observe that if we match similar patches in the image and obtain their offsets (relative positions), the statistics of these offsets are sparsely distributed. We further observe that a few dominant offsets provide reliable information for completing the image. With these offsets we fill the missing region by combining a stack of shifted images via optimization. A variety of experiments show that our method yields generally better results and is faster than existing state-of-the-art methods.', '저널': 'European Conference on Computer Vision (ECCV), 2012', '저자': 'Kaiming He, Jian Sun', '전체 인용횟수': '424회 인용20122013201420152016201720182019202020212022202321436443640465046434020', '학술 문서': 'Statistics of patch offsets for image completionK He, J Sun\\xa0- Computer Vision–ECCV 2012: 12th European\\xa0…, 2012255회 인용 관련 학술자료 전체 8개의 버전 Image completion approaches using the statistics of similar patches*K He, J Sun\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 2014177회 인용 관련 학술자료 전체 7개의 버전 Image completion based on patch offset statistics*K He, J Sun\\xa0- US Patent App. 14/297,530, 20142회 인용 관련 학술자료 전체 2개의 버전 '}, title='Statistics of Patch Offsets for Image Completion', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Exploring Randomly Wired Neural Networks for Image Recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': 'Neural networks for image recognition have evolved through extensive manual design from simple chain-like models to structures with multiple wiring paths. The success of ResNets and DenseNets is due in large part to their innovative wiring plans. Now, neural architecture search (NAS) studies are exploring the joint optimization of wiring and operation types, however, the space of possible wirings is constrained and still driven by manual design despite being searched. In this paper, we explore a more diverse set of connectivity patterns through the lens of randomly wired neural networks. To do this, we first define the concept of a stochastic network generator that encapsulates the entire network generation process. Encapsulation provides a unified view of NAS and randomly wired networks. Then, we use three classical random graph models to generate randomly wired graphs for networks. The results are surprising: several variants of these random generators yield network instances that have competitive accuracy on the ImageNet benchmark. These results suggest that new efforts focusing on designing better network generators may lead to new breakthroughs by exploring less constrained search spaces with more room for novel design.', '저자': 'Saining Xie, Alexander Kirillov, Ross Girshick, Kaiming He', '전체 인용횟수': '400회 인용20192020202120222023581031117058', '컨퍼런스': 'Proceedings of the IEEE/CVF International Conference on Computer Vision', '페이지': '1284-1293', '학술 문서': 'Exploring randomly wired neural networks for image recognitionS Xie, A Kirillov, R Girshick, K He\\xa0- Proceedings of the IEEE/CVF International Conference\\xa0…, 2019398회 인용 관련 학술자료 전체 8개의 버전 Exploring randomly wired neural networks for image recognition. arXiv 2019*S Xie, A Kirillov, R Girshick, K He\\xa0- arXiv preprint arXiv:1904.01569, 20192회 인용 관련 학술자료 Exploring Randomly Wired Neural Networks for Image Recognition. arXiv e-prints, page*S Xie, A Kirillov, R Girshick, K He\\xa0- arXiv preprint arXiv:1904.01569, 20192회 인용 관련 학술자료 '}, title='Exploring Randomly Wired Neural Networks for Image Recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A Global Sampling Method for Alpha Matting': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011/6/20', '게시자': 'IEEE', '설명': 'Alpha matting refers to the problem of softly extracting the foreground from an image. Given a trimap (specifying known foreground/background and unknown pixels), a straightforward way to compute the alpha value is to sample some known foreground and background colors for each unknown pixel. Existing sampling-based matting methods often collect samples near the unknown pixels only. They fail if good samples cannot be found nearby. In this paper, we propose a global sampling method that uses all samples available in the image. Our global sample set avoids missing good samples. A simple but effective cost function is defined to tackle the ambiguity in the sample selection process. To handle the computational complexity introduced by the large number of samples, we pose the sampling task as a correspondence problem. The correspondence search is efficiently achieved by generalizing a randomized\\xa0…', '저자': 'Kaiming He, Christoph Rhemann, Carsten Rother, Xiaoou Tang, Jian Sun', '전체 인용횟수': '389회 인용2010201120122013201420152016201720182019202020212022202313162119324326184032404743', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2011', '학술 문서': 'A global sampling method for alpha mattingK He, C Rhemann, C Rother, X Tang, J Sun\\xa0- CVPR 2011, 2011382회 인용 관련 학술자료 전체 16개의 버전 Opacity measurement using a global pixel set*K He, J Sun, CCE Rother, X Tang\\xa0- US Patent 8,855,411, 20149회 인용 관련 학술자료 전체 4개의 버전 \" A Global Sampling Method for Alpha Matting\"; Poster: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2011, Colorado Springs; 21.06. 2011-23.06. 2011; in:\" IEEE Computer Vision and Pattern Recognition\",(2011), 8 S.*K He, C Rhemann, C Rother, X Tang, J Sun'}, title='A Global Sampling Method for Alpha Matting', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'TensorMask: A Foundation for Dense Object Segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019/3/28', '설명': 'Sliding-window object detectors that generate bounding-box object predictions over a dense, regular grid have advanced rapidly and proven popular. In contrast, modern instance segmentation approaches are dominated by methods that first detect object bounding boxes, and then crop and segment these regions, as popularized by Mask R-CNN. In this work, we investigate the paradigm of dense sliding-window instance segmentation, which is surprisingly under-explored. Our core observation is that this task is fundamentally different than other dense prediction tasks such as semantic segmentation or bounding-box object detection, as the output at every spatial location is itself a geometric structure with its own spatial dimensions. To formalize this, we treat dense instance segmentation as a prediction task over 4D tensors and present a general framework called TensorMask that explicitly captures this geometry and enables novel operators on 4D tensors. We demonstrate that the tensor view leads to large gains over baselines that ignore this structure, and leads to results comparable to Mask R-CNN. These promising results suggest that TensorMask can serve as a foundation for novel advances in dense mask prediction and a more complete understanding of the task. Code will be made available.', '저자': 'Xinlei Chen, Ross Girshick, Kaiming He, Piotr Dollár', '전체 인용횟수': '361회 인용2019202020212022202365911310081', '컨퍼런스': 'International Conference on Computer Vision (ICCV), 2019', '학술 문서': 'Tensormask: A foundation for dense object segmentationX Chen, R Girshick, K He, P Dollár\\xa0- Proceedings of the IEEE/CVF international conference\\xa0…, 2019361회 인용 관련 학술자료 전체 9개의 버전 '}, title='TensorMask: A Foundation for Dense Object Segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Constant Time Weighted Median Filtering for Stereo Matching and Beyond': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013', '설명': 'Despite the continuous advances in local stereo matching for years, most efforts are on developing robust cost computation and aggregation methods. Little attention has been seriously paid to the disparity refinement. In this work, we study weighted median filtering for disparity refinement. We discover that with this refinement, even the simple box filter aggregation achieves comparable accuracy with various sophisticated aggregation methods (with the same refinement). This is due to the nice weighted median filtering properties of removing outlier error while respecting edges/structures. This reveals that the previously overlooked refinement can be at least as crucial as aggregation. We also develop the first constant time algorithm for the previously time-consuming weighted median filter. This makes the simple combination\" box aggregation+ weighted median\" an attractive solution in practice for both speed and accuracy. As a byproduct, the fast weighted median filtering unleashes its potential in other applications that were hampered by high complexities. We show its superiority in various applications such as depth upsampling, clip-art JPEG artifact removal, and image stylization.', '저자': 'Ziyang Ma, Kaiming He, Yichen Wei, Jian Sun, Enhua Wu', '전체 인용횟수': '351회 인용201420152016201720182019202020212022202310466053403434301622', '컨퍼런스': 'International Conference on Computer Vision (ICCV), 2013', '학술 문서': 'Constant time weighted median filtering for stereo matching and beyondZ Ma, K He, Y Wei, J Sun, E Wu\\xa0- Proceedings of the IEEE International Conference on\\xa0…, 2013351회 인용 관련 학술자료 전체 18개의 버전 '}, title='Constant Time Weighted Median Filtering for Stereo Matching and Beyond', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Exploring Plain Vision Transformer Backbones for Object Detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2022', '설명': 'We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection. This design enables the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for fine-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is sufficient to build a simple feature pyramid from a single-scale feature map (without the common FPN design) and (ii) it is sufficient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain ViT backbones pre-trained as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the previous leading methods that were all based on hierarchical backbones, reaching up to 61.3 AP on the COCO dataset using only ImageNet-1K pre-training. We hope our\\xa0…', '저자': 'Yanghao Li, Hanzi Mao, Ross Girshick, Kaiming He', '전체 인용횟수': '342회 인용202120222023174263', '컨퍼런스': 'European Conference on Computer Vision (ECCV), 2022', '학술 문서': 'Exploring plain vision transformer backbones for object detectionY Li, H Mao, R Girshick, K He\\xa0- European Conference on Computer Vision, 2022342회 인용 관련 학술자료 전체 5개의 버전 '}, title='Exploring Plain Vision Transformer Backbones for Object Detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning to Segment Every Thing': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'Most methods for object instance segmentation require all training examples to be labeled with segmentation masks. This requirement makes it expensive to annotate new categories and has restricted instance segmentation models to~ 100 well-annotated classes. The goal of this paper is to propose a new partially supervised training paradigm, together with a novel weight transfer function, that enables training instance segmentation models on a large set of categories all of which have box annotations, but only a small fraction of which have mask annotations. These contributions allow us to train Mask R-CNN to detect and segment 3000 visual concepts using box annotations from the Visual Genome dataset and mask annotations from the 80 classes in the COCO dataset. We evaluate our approach in a controlled study on the COCO dataset. This work is a first step towards instance segmentation models that have broad comprehension of the visual world.', '저자': 'Ronghang Hu, Piotr Dollár, Kaiming He, Trevor Darrell, Ross Girshick', '전체 인용횟수': '324회 인용201820192020202120222023256880705128', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2018', '학술 문서': 'Learning to segment every thingR Hu, P Dollár, K He, T Darrell, R Girshick\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2018324회 인용 관련 학술자료 전체 11개의 버전 '}, title='Learning to Segment Every Thing', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Efficient and Accurate Approximations of Nonlinear Convolutional Networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'This paper aims to accelerate the test-time computation of deep convolutional neural networks (CNNs). Unlike existing methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We minimize the reconstruction error of the nonlinear responses, subject to a low-rank constraint which helps to reduce the complexity of filters. We develop an effective solution to this constrained nonlinear optimization problem. An algorithm is also presented for reducing the accumulated error when multiple layers are approximated. A whole-model speedup ratio of 4x is demonstrated on a large network trained for ImageNet, while the top-5 error rate is only increased by 0.9%. Our accelerated model has a comparably fast speed as the\" AlexNet\", but is 4.7% more accurate.', '저자': 'Xiangyu Zhang, Jianhua Zou, Xiang Ming, Kaiming He, Jian Sun', '전체 인용횟수': '318회 인용2014201520162017201820192020202120222023172329464556443131', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2015', '학술 문서': 'Efficient and accurate approximations of nonlinear convolutional networksX Zhang, J Zou, X Ming, K He, J Sun\\xa0- Proceedings of the IEEE Conference on Computer\\xa0…, 2015318회 인용 관련 학술자료 전체 13개의 버전 '}, title='Efficient and Accurate Approximations of Nonlinear Convolutional Networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Fast Guided Filter': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/5/5', '설명': 'The guided filter is a technique for edge-aware image filtering. Because of its nice visual quality, fast speed, and ease of implementation, the guided filter has witnessed various applications in real products, such as image editing apps in phones and stereo reconstruction, and has been included in official MATLAB and OpenCV. In this note, we remind that the guided filter can be simply sped up from O(N) time to O(N/s^2) time for a subsampling ratio s. In a variety of applications, this leads to a speedup of >10x with almost no visible degradation. We hope this acceleration will improve performance of current applications and further popularize this filter. Code is released.', '저널': 'arXiv preprint arXiv:1505.00996', '저자': 'Kaiming He, Jian Sun', '전체 인용횟수': '261회 인용20152016201720182019202020212022202341930244742293232', '학술 문서': 'Fast guided filterK He, J Sun\\xa0- arXiv preprint arXiv:1505.00996, 2015261회 인용 관련 학술자료 전체 2개의 버전 '}, title='Fast Guided Filter', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Masked Autoencoders As Spatiotemporal Learners': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2022/5/18', '설명': 'This paper studies a conceptually simple extension of Masked Autoencoders (MAE) to spatiotemporal representation learning from videos. We randomly mask out spacetime patches in videos and learn an autoencoder to reconstruct them in pixels. Interestingly, we show that our MAE method can learn strong representations with almost no inductive bias on spacetime (only except for patch and positional embeddings), and spacetime-agnostic random masking performs the best. We observe that the optimal masking ratio is as high as 90%(vs. 75% on images), supporting the hypothesis that this ratio is related to information redundancy of the data. A high masking ratio leads to a large speedup, eg,> 4x in wall-clock time or even more. We report competitive results on several challenging video datasets using vanilla Vision Transformers. We observe that MAE can outperform supervised pre-training by large margins. We further report encouraging results of training on real-world, uncurated Instagram data. Our study suggests that the general framework of masked autoencoding (BERT, MAE, etc.) can be a unified methodology for representation learning with minimal domain knowledge.', '저자': 'Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, Kaiming He', '전체 인용횟수': '232회 인용202120222023135195', '컨퍼런스': 'Neural Information Processing Systems (NeurIPS), 2022', '학술 문서': 'Masked autoencoders as spatiotemporal learnersC Feichtenhofer, Y Li, K He\\xa0- Advances in neural information processing systems, 2022232회 인용 관련 학술자료 전체 4개의 버전 '}, title='Masked Autoencoders As Spatiotemporal Learners', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Fast Matting using Large Kernel Matting Laplacian Matrices': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/6/13', '게시자': 'IEEE', '설명': 'Image matting is of great importance in both computer vision and graphics applications. Most existing state-of-the-art techniques rely on large sparse matrices such as the matting Laplacian. However, solving these linear systems is often time-consuming, which is unfavored for the user interaction. In this paper, we propose a fast method for high quality matting. We first derive an efficient algorithm to solve a large kernel matting Laplacian. A large kernel propagates information more quickly and may improve the matte quality. To further reduce running time, we also use adaptive kernel sizes by a KD-tree trimap segmentation technique. A variety of experiments show that our algorithm provides high quality results and is 5 to 20 times faster than previous methods.', '저자': 'Kaiming He, Jian Sun, Xiaoou Tang', '전체 인용횟수': '221회 인용2010201120122013201420152016201720182019202020212022202331223202213201581629161311', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2010', '학술 문서': 'Fast matting using large kernel matting laplacian matricesK He, J Sun, X Tang\\xa0- 2010 IEEE Computer Society Conference on Computer\\xa0…, 2010210회 인용 관련 학술자료 전체 11개의 버전 Variable kernel size image matting*J Sun, K He\\xa0- US Patent 8,625,888, 201412회 인용 관련 학술자료 전체 4개의 버전 '}, title='Fast Matting using Large Kernel Matting Laplacian Matrices', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021', '설명': 'We present a large-scale study on unsupervised spatiotemporal representation learning from videos. With a unified perspective on four recent image-based frameworks, we study a simple objective that can easily generalize all these methods to space-time. Our objective encourages temporally-persistent features in the same video, and in spite of its simplicity, it works surprisingly well across:(i) different unsupervised frameworks,(ii) pre-training datasets,(iii) downstream datasets, and (iv) backbone architectures. We draw a series of intriguing observations from this study, eg, we discover that encouraging long-spanned persistency can be effective even if the timespan is 60 seconds. In addition to state-of-the-art results in multiple benchmarks, we report a few promising cases in which unsupervised pre-training can outperform its supervised counterpart. Code will be made available at https://github. com/facebookresearch/SlowFast.', '저자': 'Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, Kaiming He', '전체 인용횟수': '196회 인용20202021202220232208886', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2021', '학술 문서': 'A large-scale study on unsupervised spatiotemporal representation learningC Feichtenhofer, H Fan, B Xiong, R Girshick, K He\\xa0- Proceedings of the IEEE/CVF Conference on Computer\\xa0…, 2021196회 인용 관련 학술자료 전체 6개의 버전 '}, title='A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Transitive Invariance for Self-supervised Visual Representation Learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/8/9', '설명': 'Learning visual representations with self-supervised learning has become popular in computer vision. The idea is to design auxiliary tasks where labels are free to obtain. Most of these tasks end up providing data to learn specific kinds of invariance useful for recognition. In this paper, we propose to exploit different self-supervised approaches to learn representations invariant to (i) inter-instance variations (two objects in the same class should have similar features) and (ii) intra-instance variations (viewpoint, pose, deformations, illumination, etc). Instead of combining two approaches with multi-task learning, we argue to organize and reason the data with multiple variations. Specifically, we propose to generate a graph with millions of objects mined from hundreds of thousands of videos. The objects are connected by two types of edges which correspond to two types of invariance:\" different instances but a similar viewpoint and category\" and\" different viewpoints of the same instance\". By applying simple transitivity on the graph with these edges, we can obtain pairs of images exhibiting richer visual invariance. We use this data to train a Triplet-Siamese network with VGG16 as the base architecture and apply the learned representations to different recognition tasks. For object detection, we achieve 63.2% mAP on PASCAL VOC 2007 using Fast R-CNN (compare to 67.3% with ImageNet pre-training). For the challenging COCO dataset, our method is surprisingly close (23.5%) to the ImageNet-supervised counterpart (24.4%) using the Faster R-CNN framework. We also show that our network can perform significantly better than the ImageNet network\\xa0…', '저자': 'Xiaolong Wang, Kaiming He, Abhinav Gupta', '전체 인용횟수': '195회 인용20172018201920202021202220231283051293125', '컨퍼런스': 'International Conference on Computer Vision (ICCV), 2017', '학술 문서': 'Transitive invariance for self-supervised visual representation learningX Wang, K He, A Gupta\\xa0- Proceedings of the IEEE international conference on\\xa0…, 2017195회 인용 관련 학술자료 전체 6개의 버전 '}, title='Transitive Invariance for Self-supervised Visual Representation Learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Computing Nearest-Neighbor Fields via Propagation-Assisted KD-Trees': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012', '설명': 'Matching patches between two images, also known as computing nearest-neighbor fields, has been proven a useful technique in various computer vision/graphics algorithms. But this is a computationally challenging nearest-neighbor search task, because both the query set and the candidate set are of image size. In this paper, we propose Propagation-Assisted KD-Trees to quickly compute an approximate solution. We develop a novel propagation search method for kd-trees. In this method the tree nodes checked by each query are propagated from the nearby queries. This method not only avoids the time-consuming backtracking in traditional tree methods, but is more accurate. Experiments on public data show that our method is 10-20 times faster than the PatchMatch method [4] at the same accuracy, or reduces its error by 70% at the same running time. Our method is also 2-5 times faster and is more accurate\\xa0…', '저자': 'Kaiming He, Jian Sun', '전체 인용횟수': '174회 인용20122013201420152016201720182019202020212022202311119241919211592261', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2012', '학술 문서': 'Computing nearest-neighbor fields via propagation-assisted kd-treesK He, J Sun\\xa0- 2012 IEEE Conference on Computer Vision and\\xa0…, 2012174회 인용 관련 학술자료 전체 11개의 버전 '}, title='Computing Nearest-Neighbor Fields via Propagation-Assisted KD-Trees', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Rectangling Panoramic Images via Warping': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/7/1', '게시자': 'ACM', '권': '32', '설명': 'Stitched panoramic images mostly have irregular boundaries. Artists and common users generally prefer rectangular boundaries, which can be obtained through cropping or image completion techniques. In this paper, we present a content-aware warping algorithm that generates rectangular images from stitched panoramic images. Our algorithm consists of two steps. The first local step is mesh-free and preliminarily warps the image into a rectangle. With a grid mesh placed on this rectangle, the second global step optimizes the mesh to preserve shapes and straight lines. In various experiments we demonstrate that the results of our approach are often visually plausible, and the introduced distortion is often unnoticeable.', '저널': 'ACM Transactions on Graphics (TOG)', '저자': 'Kaiming He, Huiwen Chang, Jian Sun', '전체 인용횟수': '132회 인용201320142015201620172018201920202021202220231213138141720141614', '페이지': '79', '학술 문서': 'Rectangling panoramic images via warpingK He, H Chang, J Sun\\xa0- ACM Transactions on Graphics (TOG), 201391회 인용 관련 학술자료 전체 5개의 버전 Creation of Rectangular Images from Input Images*K He, H Chang, J Sun\\xa0- US Patent App. 14/079,310, 201541회 인용 관련 학술자료 전체 2개의 버전 ', '호': '4'}, title='Rectangling Panoramic Images via Warping', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Graph Structure of Neural Networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020/11/21', '게시자': 'PMLR', '설명': 'Neural networks are often represented as graphs of connections between neurons. However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance. Here we systematically investigate how does the graph structure of neural networks affect their predictive performance. To this end, we develop a novel graph-based representation of neural networks called relational graph, where layers of neural network computation correspond to rounds of message exchange along the graph structure. Using this representation we show that:(1) a “sweet spot” of relational graphs leads to neural networks with significantly improved predictive performance;(2) neural network’s performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph;(3) our findings are consistent across many different tasks and datasets;(4) the sweet spot can be identified efficiently;(5) top-performing neural networks have graph structure surprisingly similar to those of real biological neural networks. Our work opens new directions for the design of neural architectures and the understanding on neural networks in general.', '저자': 'Jiaxuan You, Jure Leskovec, Kaiming He, Saining Xie', '전체 인용횟수': '131회 인용2019202020212022202319403842', '컨퍼런스': 'International Conference on Machine Learning', '페이지': '10881-10891', '학술 문서': 'Graph structure of neural networksJ You, J Leskovec, K He, S Xie\\xa0- International Conference on Machine Learning, 2020131회 인용 관련 학술자료 전체 11개의 버전 '}, title='Graph Structure of Neural Networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Benchmarking Detection Transfer Learning with Vision Transformers': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021/11/22', '설명': 'Object detection is a central downstream task used to test if pre-trained network parameters confer benefits, such as improved accuracy or training speed. The complexity of object detection methods can make this benchmarking non-trivial when new architectures, such as Vision Transformer (ViT) models, arrive. These difficulties (e.g., architectural incompatibility, slow training, high memory consumption, unknown training formulae, etc.) have prevented recent studies from benchmarking detection transfer learning with standard ViT models. In this paper, we present training techniques that overcome these challenges, enabling the use of standard ViT models as the backbone of Mask R-CNN. These tools facilitate the primary goal of our study: we compare five ViT initializations, including recent state-of-the-art self-supervised learning methods, supervised initialization, and a strong random initialization baseline. Our results show that recent masking-based unsupervised learning methods may, for the first time, provide convincing transfer learning improvements on COCO, increasing box AP up to 4% (absolute) over supervised and prior self-supervised pre-training methods. Moreover, these masking-based initializations scale better, with the improvement growing as model size increases.', '저널': 'arXiv preprint arXiv:2111.11429', '저자': 'Yanghao Li, Saining Xie, Xinlei Chen, Piotr Dollar, Kaiming He, Ross Girshick', '전체 인용횟수': '105회 인용202220234461', '학술 문서': 'Benchmarking detection transfer learning with vision transformersY Li, S Xie, X Chen, P Dollar, K He, R Girshick\\xa0- arXiv preprint arXiv:2111.11429, 2021105회 인용 관련 학술자료 전체 2개의 버전 '}, title='Benchmarking Detection Transfer Learning with Vision Transformers', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A Multigrid Method for Efficiently Training Video Models': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020', '설명': 'Training competitive deep video models is an order of magnitude slower than training their counterpart image models. Slow training causes long research cycles, which hinders progress in video understanding research. Following standard practice for training image models, video model training has used a fixed mini-batch shape: a specific number of clips, frames, and spatial size. However, what is the optimal shape? High resolution models perform well, but train slowly. Low resolution models train faster, but are less accurate. Inspired by multigrid methods in numerical optimization, we propose to use variable mini-batch shapes with different spatial-temporal resolutions that are varied according to a schedule. The different shapes arise from resampling the training data on multiple sampling grids. Training is accelerated by scaling up the mini-batch size and learning rate when shrinking the other dimensions. We empirically demonstrate a general and robust grid schedule that yields a significant out-of-the-box training speedup without a loss in accuracy for different models (I3D, non-local, SlowFast), datasets (Kinetics, Something-Something, Charades), and training settings (with and without pre-training, 128 GPUs or 1 GPU). As an illustrative example, the proposed multigrid method trains a ResNet-50 SlowFast network 4.5 x faster (wall-clock time, same hardware) while also improving accuracy (+ 0.8% absolute) on Kinetics-400 compared to baseline training. Code is available online.', '저자': 'Chao-Yuan Wu, Ross Girshick, Kaiming He, Christoph Feichtenhofer, Philipp Krähenbühl', '전체 인용횟수': '104회 인용20192020202120222023113413019', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2020', '학술 문서': 'A multigrid method for efficiently training video modelsCY Wu, R Girshick, K He, C Feichtenhofer…\\xa0- Proceedings of the IEEE/CVF Conference on Computer\\xa0…, 2020104회 인용 관련 학술자료 전체 12개의 버전 '}, title='A Multigrid Method for Efficiently Training Video Models', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Sparse Projections for High-Dimensional Binary Codes': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'This paper addresses the problem of learning long binary codes from high-dimensional data. We observe that two key challenges arise while learning and using long binary codes:(1) lack of an effective regularizer for the learned high-dimensional mapping and (2) high computational cost for computing long codes. In this paper, we overcome both these problems by introducing a sparsity encouraging regularizer that reduces the effective number of parameters involved in the learned projection operator. This regularizer not only reduces overfitting but, due to the sparse nature of the projection matrix, also leads to a dramatic reduction in the computational cost. To evaluate the effectiveness of our method, we analyze its performance on the problems of nearest neighbour search, image retrieval and image classification. Experiments on a number of challenging datasets show that our method leads to better accuracy than dense projections (ITQ and LSH) with the same code lengths, and meanwhile is over an order of magnitude faster. Furthermore, our method is also more accurate and faster than other recently proposed methods for speeding up high-dimensional binary encoding.', '저자': 'Yan Xia, Kaiming He, Pushmeet Kohli, Jian Sun', '전체 인용횟수': '102회 인용2015201620172018201920202021202220235132811519684', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2015', '학술 문서': 'Sparse projections for high-dimensional binary codesY Xia, K He, P Kohli, J Sun\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2015102회 인용 관련 학술자료 전체 13개의 버전 '}, title='Sparse Projections for High-Dimensional Binary Codes', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Joint Inverted Indexing': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013', '설명': 'Inverted indexing is a popular non-exhaustive solution to large scale search. An inverted file is built by a quantizer such as k-means or a tree structure. It has been found that multiple inverted files, obtained by multiple independent random quantizers, are able to achieve practically good recall and speed. Instead of computing the multiple quantizers independently, we present a method that creates them jointly. Our method jointly optimizes all codewords in all quantizers. Then it assigns these codewords to the quantizers. In experiments this method shows significant improvement over various existing methods that use multiple independent quantizers. On the one-billion set of SIFT vectors, our method is faster and more accurate than a recent state-of-the-art inverted indexing method.', '저자': 'Yan Xia, Kaiming He, Fang Wen, Jian Sun', '전체 인용횟수': '87회 인용2014201520162017201820192020202120222023711101411109224', '컨퍼런스': 'International Conference on Computer Vision (ICCV), 2013', '학술 문서': 'Joint inverted indexingY Xia, K He, F Wen, J Sun\\xa0- Proceedings of the IEEE International Conference on\\xa0…, 201387회 인용 관련 학술자료 전체 15개의 버전 '}, title='Joint Inverted Indexing', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Are Labels Necessary for Neural Architecture Search?': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020/3/26', '설명': ' Existing neural network architectures in computer vision—whether designed by humans or by machines—were typically found using both images and their associated labels. In this paper, we ask the question: can we find high-quality neural architectures using only images, but no human-annotated labels? To answer this question, we first define a new setup called Unsupervised Neural Architecture Search (UnNAS). We then conduct two sets of experiments. In sample-based experiments, we train a large number (500) of diverse architectures with either supervised or unsupervised objectives, and find that the architecture rankings produced with and without labels are highly correlated. In search-based experiments, we run a well-established NAS algorithm (DARTS) using various unsupervised objectives, and report that the architectures searched without labels can be competitive to their counterparts\\xa0…', '저자': 'Chenxi Liu, Piotr Dollár, Kaiming He, Ross Girshick, Alan Yuille, Saining Xie', '전체 인용횟수': '79회 인용20202021202220235272918', '컨퍼런스': 'European Conference on Computer Vision (ECCV), 2020', '학술 문서': 'Are labels necessary for neural architecture search?C Liu, P Dollár, K He, R Girshick, A Yuille, S Xie\\xa0- Computer Vision–ECCV 2020: 16th European\\xa0…, 202079회 인용 관련 학술자료 전체 10개의 버전 '}, title='Are Labels Necessary for Neural Architecture Search?', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Graph Cuts for Supervised Binary Coding': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '설명': ' Learning short binary codes is challenged by the inherent discrete nature of the problem. The graph cuts algorithm is a well-studied discrete label assignment solution in computer vision, but has not yet been applied to solve the binary coding problems. This is partially because it was unclear how to use it to learn the encoding (hashing) functions for out-of-sample generalization. In this paper, we formulate supervised binary coding as a single optimization problem that involves both the encoding functions and the binary label assignment. Then we apply the graph cuts algorithm to address the discrete optimization problem involved, with no continuous relaxation. This method, named as Graph Cuts Coding (GCC), shows competitive results in various datasets.', '저자': 'Tiezheng Ge, Kaiming He, Jian Sun', '전체 인용횟수': '75회 인용201520162017201820192020202120222023515161389531', '컨퍼런스': 'European Conference on Computer Vision (ECCV), 2014', '학술 문서': 'Graph cuts for supervised binary codingT Ge, K He, J Sun\\xa0- Computer Vision–ECCV 2014: 13th European\\xa0…, 201475회 인용 관련 학술자료 전체 5개의 버전 '}, title='Graph Cuts for Supervised Binary Coding', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Scaling Language-Image Pre-training via Masking': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2023', '설명': 'We present Fast Language-Image Pre-training (FLIP), a simple and more efficient method for training CLIP. Our method randomly masks out and removes a large portion of image patches during training. Masking allows us to learn from more image-text pairs given the same wall-clock time and contrast more samples per iteration with similar memory footprint. It leads to a favorable trade-off between accuracy and training time. In our experiments on 400 million image-text pairs, FLIP improves both accuracy and speed over the no-masking baseline. On a large diversity of downstream tasks, FLIP dominantly outperforms the CLIP counterparts trained on the same data. Facilitated by the speedup, we explore the scaling behavior of increasing the model size, data size, or training length, and report encouraging results and comparisons. We hope that our work will foster future research on scaling vision-language learning.', '저자': 'Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, Kaiming He', '전체 인용횟수': '70회 인용20222023169', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2023', '학술 문서': 'Scaling language-image pre-training via maskingY Li, H Fan, R Hu, C Feichtenhofer, K He\\xa0- Proceedings of the IEEE/CVF Conference on Computer\\xa0…, 202370회 인용 관련 학술자료 전체 3개의 버전 '}, title='Scaling Language-Image Pre-training via Masking', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'GLoMo: unsupervised learning of transferable relational graphs': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'Modern deep transfer learning approaches have mainly focused on learning generic feature vectors from one task that are transferable to other tasks, such as word embeddings in language and pretrained convolutional features in vision. However, these approaches usually transfer unary features and largely ignore more structured graphical representations. This work explores the possibility of learning generic latent relational graphs that capture dependencies between pairs of data units (eg, words or pixels) from large-scale unlabeled data and transferring the graphs to downstream tasks. Our proposed transfer learning framework improves performance on various tasks including question answering, natural language inference, sentiment analysis, and image classification. We also show that the learned graphs are generic enough to be transferred to different embeddings on which the graphs have not been trained (including GloVe embeddings, ELMo embeddings, and task-specific RNN hidden units), or embedding-free units such as image pixels.', '저자': 'Zhilin Yang, Jake Zhao, Bhuwan Dhingra, Kaiming He, William W Cohen, Ruslan R Salakhutdinov, Yann LeCun', '전체 인용횟수': '66회 인용201820192020202120222023216161596', '컨퍼런스': 'Neural Information Processing Systems (NIPS), 2018', '학술 문서': 'Glomo: Unsupervisedly learned relational graphs as transferable representations*Z Yang, J Zhao, B Dhingra, K He, WW Cohen…\\xa0- arXiv preprint arXiv:1806.05662, 201840회 인용 관련 학술자료 전체 4개의 버전 Glomo: Unsupervised learning of transferable relational graphsZ Yang, J Zhao, B Dhingra, K He, WW Cohen…\\xa0- Advances in Neural Information Processing Systems, 201826회 인용 관련 학술자료 전체 6개의 버전 '}, title='GLoMo: unsupervised learning of transferable relational graphs', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Product Sparse Coding': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '설명': 'Sparse coding is a widely involved technique in computer vision. However, the expensive computational cost can hamper its applications, typically when the codebook size must be limited due to concerns on running time. In this paper, we study a special case of sparse coding in which the codebook is a Cartesian product of two subcodebooks. We present algorithms to decompose this sparse coding problem into smaller subproblems, which can be separately solved. Our solution, named as Product Sparse Coding (PSC), reduces the time complexity from O (K) to O (sqrt (K)) in the codebook size K. In practice, this can be 20-100x faster than standard sparse coding. In experiments we demonstrate the efficiency and quality of this method on the applications of image classification and image retrieval.', '저자': 'Tiezheng Ge, Kaiming He, Jian Sun', '전체 인용횟수': '29회 인용2014201520162017201820192020202120222023259261112', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2014', '학술 문서': 'Product sparse codingT Ge, K He, J Sun\\xa0- Proceedings of the IEEE Conference on Computer\\xa0…, 201429회 인용 관련 학술자료 전체 15개의 버전 '}, title='Product Sparse Coding', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Content-Aware Rotation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013', '설명': 'We present an image editing tool called Content-Aware Rotation. Casually shot photos can appear tilted, and are often corrected by rotation and cropping. This trivial solution may remove desired content and hurt image integrity. Instead of doing rigid rotation, we propose a warping method that creates the perception of rotation and avoids cropping. Human vision studies suggest that the perception of rotation is mainly due to horizontal/vertical lines. We design an optimization-based method that preserves the rotation of horizontal/vertical lines, maintains the completeness of the image content, and reduces the warping distortion. An efficient algorithm is developed to address the challenging optimization. We demonstrate our content-aware rotation method on a variety of practical cases.', '저자': 'Kaiming He, Huiwen Chang, and Jian Sun', '전체 인용횟수': '29회 인용20142015201620172018201920202021202220231312832513', '컨퍼런스': 'International Conference on Computer Vision (ICCV), 2013', '학술 문서': 'Content-aware rotationK He, H Chang, J Sun\\xa0- Proceedings of the IEEE International Conference on\\xa0…, 201326회 인용 관련 학술자료 전체 11개의 버전 Content-aware image rotation*K He, H Chang, J Sun\\xa0- US Patent 9,466,092, 20163회 인용 관련 학술자료 전체 4개의 버전 '}, title='Content-Aware Rotation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A Geodesic-Preserving Method for Image Warping': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'The manipulation of panoramic/wide-angle images is usually achieved via image warping. Though various techniques have been developed for preserving shapes and straight lines for warping, these are not sufficient for panoramic/wide-angle images. The image projections will turn the straight lines into curved\" geodesic lines\", and it is fundamentally impossible to keep all these lines straight. In this work, we propose a geodesic-preserving method for content-aware image warping. An energy term is introduced to preserve the geodesic appearance of the geodesic lines, and can be used with shape-preserving terms. Our method is demonstrated in various applications, including rectangling panoramas, resizing panoramic/wide-angle images, and wide-angle image manipulation. An extension to ellipse preservation for general images is also presented.', '저자': 'Dongping Li, Kaiming He, Jian Sun, Kun Zhou', '전체 인용횟수': '28회 인용20152016201720182019202020212022202313244356', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2015', '학술 문서': 'A geodesic-preserving method for image warpingD Li, K He, J Sun, K Zhou\\xa0- Proceedings of the IEEE Conference on Computer\\xa0…, 201528회 인용 관련 학술자료 전체 13개의 버전 '}, title='A Geodesic-Preserving Method for Image Warping', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Faster R-CNN: Towards real-time object detection with region proposal networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP) using 300 proposals per image. Code is available at https://github. com/ShaoqingRen/faster_rcnn.', '저자': 'Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun', '전체 인용횟수': '67089회 인용2016201720182019202020212022202369123324971809410151128971420311938', '컨퍼런스': 'Advances in neural information processing systems', '페이지': '91-99', '학술 문서': 'Faster r-cnn: Towards real-time object detection with region proposal networksS Ren, K He, R Girshick, J Sun\\xa0- Advances in neural information processing systems, 201567038회 인용 관련 학술자료 전체 42개의 버전 Faster r-cnn: Towards real-time object detection with region proposal networks. arXiv 2015*S Ren, K He, R Girshick, J Sun\\xa0- arXiv preprint arXiv:1506.01497, 2015381회 인용 관련 학술자료 Faster r-cnn: Towards real-time object detection with region proposal networks [J]S Tren, K He, R Girshick\\xa0- Advances in neural information processing systems, 20159회 인용 관련 학술자료 Faster R 鄄 CNN: towards real 鄄 time object detection with region proposal networks*S REN, K HE, R GIRSHICK\\xa0- Proc of the 28th International Conference on Neural\\xa0…, 20157회 인용 관련 학술자료 '}, title='Faster R-CNN: Towards real-time object detection with region proposal networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Microsoft coco: Common objects in context': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '게시자': 'Springer International Publishing', '설명': ' We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and\\xa0…', '저자': 'Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, C Lawrence Zitnick', '전체 인용횟수': '41213회 인용2015201620172018201920202021202220232495341150225340025444745190719439', '컨퍼런스': 'Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13', '페이지': '740-755', '학술 문서': 'Microsoft coco: Common objects in contextTY Lin, M Maire, S Belongie, J Hays, P Perona…\\xa0- Computer Vision–ECCV 2014: 13th European\\xa0…, 201441213회 인용 관련 학술자료 전체 22개의 버전 '}, title='Microsoft coco: Common objects in context', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'You only look once: Unified, real-time object detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': 'We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.', '저자': 'Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi', '전체 인용횟수': '41140회 인용20162017201820192020202120222023162982243245865986815198088688', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '779-788', '학술 문서': 'You only look once: Unified, real-time object detectionJ Redmon, S Divvala, R Girshick, A Farhadi\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 201641086회 인용 관련 학술자료 전체 48개의 버전 You only look once: Unified, real-time object detection. arXiv 2015*J Redmon, S Divvala, R Girshick, A Farhadi\\xa0- arXiv preprint arXiv:1506.02640, 2020231회 인용 관련 학술자료 andAli Farhadi,“*J Redmon, S Divvala, R Girshick\\xa0- You only look once: Unified, real-timeobject detection,”\\xa0…, 201642회 인용 관련 학술자료 '}, title='You only look once: Unified, real-time object detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Rich feature hierarchies for accurate object detection and semantic segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '설명': 'Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012---achieving a mAP of 53.3%. Our approach combines two key insights:(1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www. cs. berkeley. edu/~ rbg/rcnn.', '저자': 'Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik', '전체 인용횟수': '33858회 인용201420152016201720182019202020212022202323989614782212317142564629537754874466', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '580-587', '학술 문서': 'Rich feature hierarchies for accurate object detection and semantic segmentationR Girshick, J Donahue, T Darrell, J Malik\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 201433858회 인용 관련 학술자료 전체 43개의 버전 '}, title='Rich feature hierarchies for accurate object detection and semantic segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Fast R-CNN': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++(using Caffe) and is available under the open-source MIT License at https://github. com/rbgirshick/fast-rcnn.', '저자': 'Ross Girshick', '전체 인용횟수': '30819회 인용2015201620172018201920202021202220231046251460264039774619539356204882', '컨퍼런스': 'Proceedings of the IEEE International Conference on Computer Vision', '페이지': '1440-1448', '학술 문서': 'Fast r-cnnR Girshick\\xa0- Proceedings of the IEEE international conference on\\xa0…, 201530789회 인용 관련 학술자료 전체 39개의 버전 Fast r-cnn. arXiv*R Girshick\\xa0- arXiv preprint arXiv:1504.08083, 2015102회 인용 관련 학술자료 Fast R-CNN. computer science*R Girshick\\xa0- Proceedings of the 2015 IEEE International\\xa0…, 201511회 인용 관련 학술자료 '}, title='Fast R-CNN', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Focal loss for dense object detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors.', '저자': 'Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollár', '전체 인용횟수': '25726회 인용20182019202020212022202353718743457553469997122', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '2980-2988', '학술 문서': 'Focal loss for dense object detectionTY Lin, P Goyal, R Girshick, K He, P Dollár\\xa0- Proceedings of the IEEE international conference on\\xa0…, 201725633회 인용 관련 학술자료 전체 23개의 버전 Focal loss for dense object detection. arXiv 2017*TY Lin, P Goyal, R Girshick, K He, P Dollár\\xa0- arXiv preprint arXiv:1708.02002, 2002115회 인용 관련 학술자료 Focal loss for dense object detection. arXiv*TY Lin, P Goyal, R Girshick, K He, P Dollár\\xa0- arXiv preprint arXiv:1708.02002, 201780회 인용 관련 학술자료 Piotr Doll ar. Focal loss for dense object detection*TY Lin, P Goyal, R Girshick, K He\\xa0- Proceedings of the IEEE International Conference on\\xa0…, 201761회 인용 관련 학술자료 Focal loss for dense object detection. CoRR abs/1708.02002 (2017)*T Lin, P Goyal, RB Girshick, K He, P Dollár\\xa0- arXiv preprint arXiv:1708.02002, 201755회 인용 관련 학술자료 Focal Loss for Dense Object Detection (RetinaNet)*TY Lin, P Goyal, R Girshick, K He, P Dollar\\xa0- Proceedings of the IEEE International Conference on\\xa0…, 201711회 인용 관련 학술자료 '}, title='Focal loss for dense object detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Feature pyramid networks for object detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.', '저자': 'Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie', '전체 인용횟수': '22542회 인용201720182019202020212022202311363419923145482359465737', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '2117-2125', '학술 문서': 'Feature pyramid networks for object detectionTY Lin, P Dollár, R Girshick, K He, B Hariharan…\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 201722542회 인용 관련 학술자료 전체 22개의 버전 '}, title='Feature pyramid networks for object detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Caffe: Convolutional architecture for fast feature embedding': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/11/3', '도서': 'Proceedings of the 22nd ACM international conference on Multimedia', '설명': 'Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community\\xa0…', '저자': 'Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, Trevor Darrell', '전체 인용횟수': '17904회 인용20142015201620172018201920202021202220231821117242134143359265817361207858490', '페이지': '675-678', '학술 문서': 'Caffe: Convolutional architecture for fast feature embeddingY Jia, E Shelhamer, J Donahue, S Karayev, J Long…\\xa0- Proceedings of the 22nd ACM international conference\\xa0…, 201417847회 인용 관련 학술자료 전체 22개의 버전 Caffe: An open source convolutional architecture for fast feature embedding (2013)*Y Jia, E Shelhamer - 201374회 인용 관련 학술자료 Proceedings of the 22nd ACM international conference on Multimedia*Y Jia, E Shelhamer, J Donahue, S Karayev, J Long… - 201464회 인용 관련 학술자료 Caffe: convolutional architecture for fast feature embedding (2014). arXiv preprint*Y Jia, E Shelhamer, J Donahue, S Karayev, J Long…\\xa0- arXiv preprint arXiv:1408.509310회 인용 관련 학술자료 Diy deep learning for vision: a hands-on tutorial with caffe*E Shelhamer, J Donahue, J Long, Y Jia, R Girshick\\xa0- Proceedings of the 13th European Conference on\\xa0…, 20144회 인용 관련 학술자료 The Caffe Framework: DIY Deep Learning*E Shelhamer, J Donahue, J Long\\xa0- Berkeley Vision and Learning Center (BVLC), 20161회 인용 관련 학술자료 '}, title='Caffe: Convolutional architecture for fast feature embedding', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Object detection with discriminatively trained part-based models': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009/9/22', '게시자': 'IEEE', '권': '32', '설명': 'We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI--SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and\\xa0…', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Pedro F Felzenszwalb, Ross B Girshick, David McAllester, Deva Ramanan', '전체 인용횟수': '12659회 인용20102011201220132014201520162017201820192020202120222023105332500829112113031384132712051093978903763508', '페이지': '1627-1645', '학술 문서': 'Object detection with discriminatively trained part-based modelsPF Felzenszwalb, RB Girshick, D McAllester…\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 200912659회 인용 관련 학술자료 전체 49개의 버전 ', '호': '9'}, title='Object detection with discriminatively trained part-based models', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Aggregated residual transformations for deep neural networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/7/22', '게시자': 'IEEE', '설명': 'We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call\" cardinality\"(the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.', '저널': 'Computer Vision and Pattern Recognition, 2017. CVPR 2017. IEEE Conference on', '저자': 'Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He', '전체 인용횟수': '11058회 인용201720182019202020212022202313556710891691244527392305', '학술 문서': 'Aggregated residual transformations for deep neural networksS Xie, R Girshick, P Dollár, Z Tu, K He\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 201711018회 인용 관련 학술자료 전체 13개의 버전 Aggregated residual transformations for deep neural networks. arXiv 2016*S Xie, R Girshick, P Dollár, Z Tu, K He\\xa0- arXiv preprint arXiv:1611.0543181회 인용 관련 학술자료 Aggregated residual transformations for deep neural networks. CoRR*S Xie, RB Girshick, P Dollár, Z Tu, K He\\xa0- arXiv preprint arxiv:1611.05431, 20167회 인용 관련 학술자료 Aggregated residual transformations for deep neural networks, CoRR abs/1611.05431*S Xie, RB Girshick, P Dollár, Z Tu, K He\\xa0- arXiv preprint arXiv:1611.05431, 20165회 인용 관련 학술자료 Aggregated residual transformations for deep neural networks. arXiv e-prints*S Xie, R Girshick, P Dollár, Z Tu, K He\\xa0- arXiv preprint arXiv:1611.05431, 20164회 인용 관련 학술자료 Aggregated residual transformations for deep neural networks. arXiv preprint 2016*S Xie, R Girshick, P Dollar, Z Tu, K He\\xa0- Source:< https://arxiv. org/abs/1611.05431 v14회 인용 관련 학술자료 Aggregated residual transformations for deep neural networks (pp. 1492–1500)*S Xie, R Girshick, P Dollar, Z Tu, K He\\xa0- Proceedings of the IEEE Conference on Computer\\xa0…, 20173회 인용 관련 학술자료 Aggregated residual transformations for deep neural networks. arXiv e-prints, page*S Xie, R Girshick, P Dollár, Z Tu, K He\\xa0- arXiv preprint arXiv:1611.05431, 20162회 인용 관련 학술자료 col.(2017).«Aggregated residual transformations for deep neural networks»*S Xie\\xa0- Computer Vision and Pattern Recognition (CVPR)\\xa0…2회 인용 관련 학술자료 Aggregated residual transformations for deep neural networks. arXiv 2017, arXiv*S Xie, R Girshick, P Dollár, Z Tu, K He\\xa0- arXiv preprint arXiv:1611.054312회 인용 관련 학술자료 ar, Zhuowen Tu, and Kaiming He. 2017. Aggregated Residual Transformations for Deep Neural Networks*S Xie, R Girshick, P Doll\\xa0- Proceedings of the IEEE Conference on Computer\\xa0…2회 인용 관련 학술자료 others, Dollár P, Tu Z, He K (2017) Aggregated residual transformations for deep neural networks*S Xie, R Girshick\\xa0- arXiv preprint arXiv:1611.054312회 인용 관련 학술자료 '}, title='Aggregated residual transformations for deep neural networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Non-local neural networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our non-local models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks. Code will be made available.', '저자': 'Xiaolong Wang, Ross Girshick, Abhinav Gupta, Kaiming He', '전체 인용횟수': '9448회 인용2018201920202021202220231106491373227426552332', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '7794-7803', '학술 문서': 'Non-local neural networksX Wang, R Girshick, A Gupta, K He\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20189448회 인용 관련 학술자료 전체 15개의 버전 '}, title='Non-local neural networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Momentum contrast for unsupervised visual representation learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020', '설명': 'We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.', '저자': 'Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick', '전체 인용횟수': '9338회 인용2020202120222023455176332453817', '컨퍼런스': 'Proceedings of the IEEE/CVF conference on computer vision and pattern recognition', '페이지': '9729-9738', '학술 문서': 'Momentum contrast for unsupervised visual representation learningK He, H Fan, Y Wu, S Xie, R Girshick\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 20209338회 인용 관련 학술자료 전체 19개의 버전 '}, title='Momentum contrast for unsupervised visual representation learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Accurate, large minibatch sgd: Training imagenet in 1 hour': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/6/8', '설명': 'Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves ~90% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.', '저널': 'arXiv preprint arXiv:1706.02677', '저자': 'Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, Kaiming He', '전체 인용횟수': '3558회 인용201720182019202020212022202355288488600736727643', '학술 문서': 'Accurate, large minibatch sgd: Training imagenet in 1 hourP Goyal, P Dollár, R Girshick, P Noordhuis…\\xa0- arXiv preprint arXiv:1706.02677, 20173548회 인용 관련 학술자료 전체 9개의 버전 Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv 2017P Goyal, P Dollár, R Girshick, P Noordhuis…\\xa0- arXiv preprint arXiv:1706.02677, 201986회 인용 관련 학술자료 '}, title='Accurate, large minibatch sgd: Training imagenet in 1 hour', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Masked autoencoders are scalable vision learners': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2022', '설명': 'This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, eg, 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: eg, a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.', '저자': 'Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick', '전체 인용횟수': '3550회 인용202120222023249932504', '컨퍼런스': 'Proceedings of the IEEE/CVF conference on computer vision and pattern recognition', '페이지': '16000-16009', '학술 문서': 'Masked autoencoders are scalable vision learnersK He, X Chen, S Xie, Y Li, P Dollár, R Girshick\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 20223550회 인용 관련 학술자료 전체 12개의 버전 '}, title='Masked autoencoders are scalable vision learners', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Unsupervised deep embedding for clustering analysis': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/6/11', '게시자': 'PMLR', '설명': 'Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.', '저자': 'Junyuan Xie, Ross Girshick, Ali Farhadi', '전체 인용횟수': '2864회 인용20162017201820192020202120222023961150289476586654616', '컨퍼런스': 'International conference on machine learning', '페이지': '478-487', '학술 문서': 'Unsupervised deep embedding for clustering analysisJ Xie, R Girshick, A Farhadi\\xa0- International conference on machine learning, 20162864회 인용 관련 학술자료 전체 15개의 버전 '}, title='Unsupervised deep embedding for clustering analysis', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Improved baselines with momentum contrastive learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020/3/9', '설명': \"Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.\", '저널': 'arXiv preprint arXiv:2003.04297', '저자': 'Xinlei Chen, Haoqi Fan, Ross Girshick, Kaiming He', '전체 인용횟수': '2696회 인용202020212022202311151310051055', '학술 문서': 'Improved baselines with momentum contrastive learningX Chen, H Fan, R Girshick, K He\\xa0- arXiv preprint arXiv:2003.04297, 20202678회 인용 관련 학술자료 전체 3개의 버전 Improved baselines with momentum contrastive learning. arXiv 2020X Chen, H Fan, R Girshick, K He\\xa0- arXiv preprint arXiv:2003.04297, 200383회 인용 관련 학술자료 '}, title='Improved baselines with momentum contrastive learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Training region-based object detectors with online hard example mining': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': 'The field of object detection has made significant advances riding on the wave of region-based ConvNets, but their training procedure still includes many heuristics and hyperparameters that are costly to tune. We present a simple yet surprisingly effective online hard example mining (OHEM) algorithm for training region-based ConvNet detectors. Our motivation is the same as it has always been--detection datasets contain an overwhelming number of easy examples and a small number of hard examples. Automatic selection of these hard examples can make training more effective and efficient. OHEM is a simple and intuitive algorithm that eliminates several heuristics and hyperparameters in common use. But more importantly, it yields consistent and significant boosts in detection performance on benchmarks like PASCAL VOC 2007 and 2012. Its effectiveness increases as datasets become larger and more difficult, as demonstrated by the results on the MS COCO dataset. Moreover, combined with complementary advances in the field, OHEM leads to state-of-the-art results of 78.9% and 76.3% mAP on PASCAL VOC 2007 and 2012 respectively.', '저자': 'Abhinav Shrivastava, Abhinav Gupta, Ross Girshick', '전체 인용횟수': '2565회 인용2016201720182019202020212022202316121266402461448456366', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '761-769', '학술 문서': 'Training region-based object detectors with online hard example miningA Shrivastava, A Gupta, R Girshick\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20162565회 인용 관련 학술자료 전체 10개의 버전 '}, title='Training region-based object detectors with online hard example mining', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Detectron2': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019/10', '권': '2', '저자': 'Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, Ross Girshick', '전체 인용횟수': '2395회 인용2020202120222023234616800727', '페이지': '2019', '학술 문서': 'Detectron2Y Wu, A Kirillov, F Massa, WY Lo, R Girshick - 20192221회 인용 관련 학술자료 Detectron2. 2019*Y Wu, A Kirillov, F Massa, WY Lo, R Girshick - 2019300회 인용 관련 학술자료 Detectron2*W Yuxin, A Kirillov, F Massa, L Wan-Yen, R Girshick\\xa0- Citado na, 201922회 인용 관련 학술자료 Detectron2: A PyTorch-based modular object detection library*Y Wu, A Kirillov, F Massa, WY Lo, R Girshick\\xa0- Meta AI, 201917회 인용 관련 학술자료 facebookresearch/detectron2*Y Wu, A Kirillov, F Massa, WY Lo, R Girshick - 20207회 인용 관련 학술자료 and Ross Girshick*AK Yuxin Wu, F Massa, WY Lo - 20195회 인용 관련 학술자료 Wan-YenLo, and Ross Girshick. 2019. Detectron2U Wu, A Kirillov, F Massa - 20192회 인용 관련 학술자료 Alexander Kirillov, Francisco Massa, Wan-Yen Lo and Ross Girshick, Detectron2W Yuxin - 20192회 인용 관련 학술자료 ', '호': '3'}, title='Detectron2', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Clevr: A diagnostic dataset for compositional language and elementary visual reasoning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'When building artificial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress and discover short-comings. Existing benchmarks for visual question answer-ing can help, but have strong biases that models can exploit to correctly answer questions without reasoning. They also conflate multiple sources of error, making it hard to pinpoint model weaknesses. We present a diagnostic dataset that tests a range of visual reasoning abilities. It contains minimal biases and has detailed annotations describing the kind of reasoning each question requires. We use this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations.', '저자': 'Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, Ross Girshick', '전체 인용횟수': '2068회 인용201720182019202020212022202351179242318384458425', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '2901-2910', '학술 문서': 'Clevr: A diagnostic dataset for compositional language and elementary visual reasoningJ Johnson, B Hariharan, L Van Der Maaten, L Fei-Fei…\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20172068회 인용 관련 학술자료 전체 18개의 버전 '}, title='Clevr: A diagnostic dataset for compositional language and elementary visual reasoning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Hypercolumns for object segmentation and fine-grained localization': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '게시자': 'IEEE', '설명': 'Recognition algorithms based on convolutional networks (CNNs) typically use the output of the last layer as feature representation. However, the information in this layer may be too coarse to allow precise localization. On the contrary, earlier layers may be precise in localization but will not capture semantics. To get the best of both worlds, we define the hypercolumn at a pixel as the vector of activations of all CNN units above that pixel. Using hypercolumns as pixel descriptors, we show results on three fine-grained localization tasks: simultaneous detection and segmentation [20], where we improve state-of-the-art from 49.7 mean AP^ r [20] to 59.0, keypoint localization, where we get a 3.3 point boost over [19] and part labeling, where we show a 6.6 point gain over a strong baseline.', '저자': 'Bharath Hariharan, Pablo Arbeláez, Ross Girshick, Jitendra Malik', '전체 인용횟수': '1856회 인용2014201520162017201820192020202120222023653182293290270227192166111', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on', '학술 문서': 'Hypercolumns for object segmentation and fine-grained localizationB Hariharan, P Arbeláez, R Girshick, J Malik\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20151856회 인용 관련 학술자료 전체 16개의 버전 '}, title='Hypercolumns for object segmentation and fine-grained localization', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning rich features from RGB-D images for object detection and segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '게시자': 'Springer International Publishing', '설명': ' In this paper we study the problem of object detection for RGB-D images using semantically rich image and depth features. We propose a new geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity. We demonstrate that this geocentric embedding works better than using raw depth images for learning feature representations with convolutional neural networks. Our final object detection system achieves an average precision of 37.3%, which is a 56% relative improvement over existing methods. We then focus on the task of instance segmentation where we label pixels belonging to object instances found by our detector. For this task, we propose a decision forest approach that classifies pixels in the detection window as foreground or background using a family of unary and binary tests that query shape and\\xa0…', '저자': 'Saurabh Gupta, Ross Girshick, Pablo Arbeláez, Jitendra Malik', '전체 인용횟수': '1844회 인용20142015201620172018201920202021202220238103188256279237221214170133', '컨퍼런스': 'Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII 13', '페이지': '345-360', '학술 문서': 'Learning rich features from RGB-D images for object detection and segmentationS Gupta, R Girshick, P Arbeláez, J Malik\\xa0- Computer Vision–ECCV 2014: 13th European\\xa0…, 20141844회 인용 관련 학술자료 전체 17개의 버전 Learning Rich Features from RGB-D Images for Object Detection and Segmentation: Supplementary Material*S Gupta, R Girshick, P Arbeláez, J Malik관련 학술자료 전체 2개의 버전 '}, title='Learning rich features from RGB-D images for object detection and segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Simultaneous detection and segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '게시자': 'Springer International Publishing', '설명': ' We aim to detect all instances of a category in an image and, for each instance, mark the pixels that belong to it. We call this task Simultaneous Detection and Segmentation (SDS). Unlike classical bounding box detection, SDS requires a segmentation and not just a box. Unlike classical semantic segmentation, we require individual object instances. We build on recent work that uses convolutional neural networks to classify category-independent region proposals (R-CNN\\xa0[16]), introducing a novel architecture tailored for SDS. We then use category-specific, top-down figure-ground predictions to refine our bottom-up proposals. We show a 7 point boost (16% relative) over our baselines on SDS, a 5 point boost (10% relative) over state-of-the-art on semantic segmentation, and state-of-the-art performance in object detection. Finally, we provide diagnostic tools that unpack performance and provide directions\\xa0…', '저자': 'Bharath Hariharan, Pablo Arbeláez, Ross Girshick, Jitendra Malik', '전체 인용횟수': '1529회 인용201420152016201720182019202020212022202314112155194196188156187157127', '컨퍼런스': 'Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII 13', '페이지': '297-312', '학술 문서': 'Simultaneous detection and segmentationB Hariharan, P Arbeláez, R Girshick, J Malik\\xa0- Computer Vision–ECCV 2014: 13th European\\xa0…, 20141529회 인용 관련 학술자료 전체 12개의 버전 '}, title='Simultaneous detection and segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': 'It is well known that contextual and multi-scale representations are important for accurate visual recognition. In this paper we present the Inside-Outside Net (ION), an object detector that exploits information both inside and outside the region of interest. Contextual information outside the region of interest is integrated using spatial recurrent neural networks. Inside, we use skip pooling to extract information at multiple scales and levels of abstraction. Through extensive experiments we evaluate the design space and provide readers with an overview of what tricks of the trade are important. ION improves state-of-the-art on PASCAL VOC 2012 object detection from 73.9% to 77.9% mAP. On the new and more challenging MS COCO dataset, we improve state-of-the-art from 19.7% to 33.1% mAP. In the 2015 MS COCO Detection Challenge, our ION model won\" Best Student Entry\" and finished 3rd place overall. As intuition suggests, our detection results provide strong evidence that context and multi-scale representations improve small object detection.', '저자': 'Sean Bell, C Lawrence Zitnick, Kavita Bala, Ross Girshick', '전체 인용횟수': '1493회 인용2016201720182019202020212022202344138192266255233207127', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '2874-2883', '학술 문서': 'Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networksS Bell, CL Zitnick, K Bala, R Girshick\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20161493회 인용 관련 학술자료 전체 12개의 버전 '}, title='Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Part-based R-CNNs for fine-grained category detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '게시자': 'Springer International Publishing', '설명': ' Semantic part localization can facilitate fine-grained categorization by explicitly isolating subtle appearance differences associated with specific object parts. Methods for pose-normalized representations have been proposed, but generally presume bounding box annotations at test time due to the difficulty of object detection. We propose a model for fine-grained categorization that overcomes these limitations by leveraging deep convolutional features computed on bottom-up region proposals. Our method learns whole-object and part detectors, enforces learned geometric constraints between them, and predicts a fine-grained category from a pose-normalized representation. Experiments on the Caltech-UCSD bird dataset confirm that our method outperforms state-of-the-art fine-grained categorization methods in an end-to-end evaluation without requiring a bounding box at test time.', '저자': 'Ning Zhang, Jeff Donahue, Ross Girshick, Trevor Darrell', '전체 인용횟수': '1380회 인용20142015201620172018201920202021202220231083131146155162156184182141', '컨퍼런스': 'Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13', '페이지': '834-849', '학술 문서': 'Part-based R-CNNs for fine-grained category detectionN Zhang, J Donahue, R Girshick, T Darrell\\xa0- Computer Vision–ECCV 2014: 13th European\\xa0…, 20141380회 인용 관련 학술자료 전체 9개의 버전 '}, title='Part-based R-CNNs for fine-grained category detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Exploring the limits of weakly supervised pretraining': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'State-of-the-art visual perception models for a wide range of tasks rely on supervised pretraining. ImageNet classification is the de facto pretraining task for these models. Yet, ImageNet is now nearly ten years old and is by modern standards\" small\". Even so, relatively little is known about the behavior of pretraining with datasets that are multiple orders of magnitude larger. The reasons are obvious: such datasets are difficult to collect and annotate. In this paper, we present a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images. Our experiments demonstrate that training for large-scale hashtag prediction leads to excellent results. We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4%(97.6% top-5). We also perform extensive experiments that provide novel empirical data on the relationship between large-scale pretraining and transfer learning performance.', '저자': 'Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, Laurens Van Der Maaten', '전체 인용횟수': '1379회 인용20182019202020212022202330162258313339270', '컨퍼런스': 'Proceedings of the European conference on computer vision (ECCV)', '페이지': '181-196', '학술 문서': 'Exploring the limits of weakly supervised pretrainingD Mahajan, R Girshick, V Ramanathan, K He, M Paluri…\\xa0- Proceedings of the European conference on computer\\xa0…, 20181379회 인용 관련 학술자료 전체 8개의 버전 '}, title='Exploring the limits of weakly supervised pretraining', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Designing network design spaces': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020', '설명': 'In this work, we present a new network design paradigm. Our goal is to help advance the understanding of network design and discover design principles that generalize across settings. Instead of focusing on designing individual network instances, we design network design spaces that parametrize populations of networks. The overall process is analogous to classic manual design of networks, but elevated to the design space level. Using our methodology we explore the structure aspect of network design and arrive at a low-dimensional design space consisting of simple, regular networks that we call RegNet. The core insight of the RegNet parametrization is surprisingly simple: widths and depths of good networks can be explained by a quantized linear function. We analyze the RegNet design space and arrive at interesting findings that do not match the current practice of network design. The RegNet design space provides simple and fast networks that work well across a wide range of flop regimes. Under comparable training settings and flops, the RegNet models outperform the popular EfficientNet models while being up to 5x faster on GPUs.', '저자': 'Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollár', '전체 인용횟수': '1336회 인용202020212022202351292479499', '컨퍼런스': 'Proceedings of the IEEE/CVF conference on computer vision and pattern recognition', '페이지': '10428-10436', '학술 문서': 'Designing network design spacesI Radosavovic, RP Kosaraju, R Girshick, K He, P Dollár\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 20201336회 인용 관련 학술자료 전체 11개의 버전 '}, title='Designing network design spaces', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Panoptic segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': 'We propose and study a task we name panoptic segmentation (PS). Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more unified view of image segmentation. For more analysis and up-to-date results, please check the arXiv version of the paper:\\\\smallhttps://arxiv. org/abs/1801.00868.', '저자': 'Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, Piotr Dollár', '전체 인용횟수': '1307회 인용2018201920202021202220231881176282345395', '컨퍼런스': 'Proceedings of the IEEE/CVF conference on computer vision and pattern recognition', '페이지': '9404-9413', '학술 문서': 'Panoptic segmentationA Kirillov, K He, R Girshick, C Rother, P Dollár\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 20191307회 인용 관련 학술자료 전체 8개의 버전 '}, title='Panoptic segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Cascade object detection with deformable part models': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/6/13', '게시자': 'Ieee', '설명': 'We describe a general method for building cascade classifiers from part-based deformable models such as pictorial structures. We focus primarily on the case of star-structured models and show how a simple algorithm based on partial hypothesis pruning can speed up object detection by more than one order of magnitude without sacrificing detection accuracy. In our algorithm, partial hypotheses are pruned with a sequence of thresholds. In analogy to probably approximately correct (PAC) learning, we introduce the notion of probably approximately admissible (PAA) thresholds. Such thresholds provide theoretical guarantees on the performance of the cascade method and can be computed from a small sample of positive examples. Finally, we outline a cascade detection algorithm for a general class of models defined by a grammar formalism. This class includes not only tree-structured pictorial structures but also\\xa0…', '저자': 'Pedro F Felzenszwalb, Ross B Girshick, David McAllester', '전체 인용횟수': '1265회 인용201020112012201320142015201620172018201920202021202220231465871151231201161107483771048956', '컨퍼런스': '2010 IEEE Computer society conference on computer vision and pattern recognition', '페이지': '2241-2248', '학술 문서': 'Cascade object detection with deformable part modelsPF Felzenszwalb, RB Girshick, D McAllester\\xa0- 2010 IEEE Computer society conference on computer\\xa0…, 20101265회 인용 관련 학술자료 전체 20개의 버전 '}, title='Cascade object detection with deformable part models', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Rethinking imagenet pre-training': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': \"We report competitive results on object detection and instance segmentation on the COCO dataset using standard models trained from random initialization. The results are no worse than their ImageNet pre-training counterparts even when using the hyper-parameters of the baseline system (Mask R-CNN) that were optimized for fine-tuning pre-trained models, with the sole exception of increasing the number of training iterations so the randomly initialized models may converge. Training from random initialization is surprisingly robust; our results hold even when:(i) using only 10% of the training data,(ii) for deeper and wider models, and (iii) for multiple tasks and metrics. Experiments show that ImageNet pre-training speeds up convergence early in training, but does not necessarily provide regularization or improve final target task accuracy. To push the envelope we demonstrate 50.9 AP on COCO object detection without using any external data---a result on par with the top COCO 2017 competition results that used ImageNet pre-training. These observations challenge the conventional wisdom of ImageNet pre-training for dependent tasks and we expect these discoveries will encourage people to rethink the current de facto paradigm ofpre-training and fine-tuning'in computer vision.\", '저자': 'Kaiming He, Ross Girshick, Piotr Dollár', '전체 인용횟수': '1153회 인용2018201920202021202220236108217280285250', '컨퍼런스': 'Proceedings of the IEEE/CVF International Conference on Computer Vision', '페이지': '4918-4927', '학술 문서': 'Rethinking imagenet pre-trainingK He, R Girshick, P Dollár\\xa0- Proceedings of the IEEE/CVF International Conference\\xa0…, 20191153회 인용 관련 학술자료 전체 7개의 버전 '}, title='Rethinking imagenet pre-training', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Segment anything': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2023/4/5', '설명': 'We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.', '저널': 'arXiv preprint arXiv:2304.02643', '저자': 'Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick', '전체 인용횟수': '1130회 인용2022202331112', '학술 문서': 'Segment anythingA Kirillov, E Mintun, N Ravi, H Mao, C Rolland…\\xa0- arXiv preprint arXiv:2304.02643, 20231117회 인용 관련 학술자료 전체 3개의 버전 Segment anything. arXiv 2023A Kirillov, E Mintun, N Ravi, H Mao, C Rolland…\\xa0- arXiv preprint arXiv:2304.0264353회 인용 관련 학술자료 '}, title='Segment anything', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Panoptic feature pyramid networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': \"The recently introduced panoptic segmentation task has renewed our community's interest in unifying the tasks of instance segmentation (for thing classes) and semantic segmentation (for stuff classes). However, current state-of-the-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation, without performing any shared computation. In this work, we aim to unify these methods at the architectural level, designing a single network for both tasks. Our approach is to endow Mask R-CNN, a popular instance segmentation method, with a semantic segmentation branch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for instance segmentation, but also yields a lightweight, top-performing method for semantic segmentation. In this work, we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and accurate baseline for both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and aid future research in panoptic segmentation.\", '저자': 'Alexander Kirillov, Ross Girshick, Kaiming He, Piotr Dollár', '전체 인용횟수': '1116회 인용2019202020212022202341139230337364', '컨퍼런스': 'Proceedings of the IEEE/CVF conference on computer vision and pattern recognition', '페이지': '6399-6408', '학술 문서': 'Panoptic feature pyramid networksA Kirillov, R Girshick, K He, P Dollár\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 20191116회 인용 관련 학술자료 전체 7개의 버전 '}, title='Panoptic feature pyramid networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Densenet: Implementing efficient convnet descriptor pyramids': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/4/7', '설명': 'Convolutional Neural Networks (CNNs) can provide accurate object classification. They can be extended to perform object detection by iterating over dense or selected proposed object regions. However, the runtime of such detectors scales as the total number and/or area of regions to examine per image, and training such detectors may be prohibitively slow. However, for some CNN classifier topologies, it is possible to share significant work among overlapping regions to be classified. This paper presents DenseNet, an open source system that computes dense, multiscale features from the convolutional layers of a CNN based object classifier. Future work will involve training efficient object detectors with DenseNet feature descriptors.', '저널': 'arXiv preprint arXiv:1404.1869', '저자': 'Forrest Iandola, Matt Moskewicz, Sergey Karayev, Ross Girshick, Trevor Darrell, Kurt Keutzer', '전체 인용횟수': '979회 인용201420152016201720182019202020212022202372119253693124177272201', '학술 문서': 'Densenet: Implementing efficient convnet descriptor pyramidsF Iandola, M Moskewicz, S Karayev, R Girshick…\\xa0- arXiv preprint arXiv:1404.1869, 2014961회 인용 관련 학술자료 전체 6개의 버전 Densenet: Implementing efficient convnet descriptor pyramids. arXiv 2014F Iandola, M Moskewicz, S Karayev, R Girshick…\\xa0- arXiv preprint arXiv:1404.186992회 인용 관련 학술자료 '}, title='Densenet: Implementing efficient convnet descriptor pyramids', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Lvis: A dataset for large vocabulary instance segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': \"Progress on object detection is enabled by datasets that focus the research community's attention on open challenges. This process led us from simple images to complex scenes and from bounding boxes to segmentation masks. In this work, we introduce LVIS (pronounced'el-vis'): a new dataset for Large Vocabulary Instance Segmentation. We plan to collect 2.2 million high-quality instance segmentation masks for over 1000 entry-level object categories in 164k images. Due to the Zipfian distribution of categories in natural images, LVIS naturally has a long tail of categories with few training samples. Given that state-of-the-art deep learning methods for object detection perform poorly in the low-sample regime, we believe that our dataset poses an important and exciting new scientific challenge. LVIS is available at http://www. lvisdataset. org.\", '저자': 'Agrim Gupta, Piotr Dollar, Ross Girshick', '전체 인용횟수': '891회 인용20192020202120222023977169257376', '컨퍼런스': 'Proceedings of the IEEE/CVF conference on computer vision and pattern recognition', '페이지': '5356-5364', '학술 문서': 'Lvis: A dataset for large vocabulary instance segmentationA Gupta, P Dollar, R Girshick\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 2019891회 인용 관련 학술자료 전체 7개의 버전 '}, title='Lvis: A dataset for large vocabulary instance segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Low-shot Visual Recognition by Shrinking and Hallucinating Features': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/6/9', '설명': 'Low-shot visual learning--the ability to recognize novel object categories from very few examples--is a hallmark of human visual intelligence. Existing machine learning approaches fail to generalize in the same way. To make progress on this foundational problem, we present a low-shot learning benchmark on complex images that mimics challenges faced by recognition systems in the wild. We then propose (1) representation regularization techniques, and (2) techniques to hallucinate additional training examples for data-starved classes. Together, our methods improve the effectiveness of convolutional networks in low-shot learning, improving the one-shot accuracy on novel classes by 2.3 x on the challenging ImageNet dataset.', '저널': 'arXiv preprint arXiv:1606.02819', '저자': 'Bharath Hariharan, Ross Girshick', '전체 인용횟수': '853회 인용2016201720182019202020212022202343077118155183149127', '학술 문서': 'Low-shot visual recognition by shrinking and hallucinating featuresB Hariharan, R Girshick\\xa0- Proceedings of the IEEE international conference on\\xa0…, 2017816회 인용 관련 학술자료 전체 7개의 버전 Low-shot visual object recognition*B Hariharan, R Girshick\\xa0- arXiv preprint arXiv:1606.02819, 201644회 인용 관련 학술자료 '}, title='Low-shot Visual Recognition by Shrinking and Hallucinating Features', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Low-shot learning from imaginary data': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': \"Humans can quickly learn new visual concepts, perhaps because they can easily visualize or imagine what novel objects look like from different views. Incorporating this ability to hallucinate novel instances of new concepts might help machine vision systems perform better low-shot learning, ie, learning concepts from few examples. We present a novel approach to low-shot learning that uses this idea. Our approach builds on recent progress in meta-learning (''learning to learn'') by combining a meta-learner with a''hallucinator''that produces additional training examples, and optimizing both models jointly. Our hallucinator can be incorporated into a variety of meta-learners and provides significant gains: up to a 6 point boost in classification accuracy when only a single training example is available, yielding state-of-the-art performance on the challenging ImageNet low-shot classification benchmark.\", '저자': 'Yu-Xiong Wang, Ross Girshick, Martial Hebert, Bharath Hariharan', '전체 인용횟수': '768회 인용20182019202020212022202321107153183170131', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '7278-7286', '학술 문서': 'Low-shot learning from imaginary dataYX Wang, R Girshick, M Hebert, B Hariharan\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2018768회 인용 관련 학술자료 전체 12개의 버전 '}, title='Low-shot learning from imaginary data', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Pointrend: Image segmentation as rendering': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020', '설명': \"We present a new method for efficient high-quality image segmentation of objects and scenes. By analogizing classical computer graphics methods for efficient rendering with over-and undersampling challenges faced in pixel labeling tasks, we develop a unique perspective of image segmentation as a rendering problem. From this vantage, we present the PointRend (Point-based Rendering) neural network module: a module that performs point-based segmentation predictions at adaptively selected locations based on an iterative subdivision algorithm. PointRend can be flexibly applied to both instance and semantic segmentation tasks by building on top of existing state-of-the-art models. While many concrete implementations of the general idea are possible, we show that a simple design already achieves excellent results. Qualitatively, PointRend outputs crisp object boundaries in regions that are over-smoothed by previous methods. Quantitatively, PointRend yields significant gains on COCO and Cityscapes, for both instance and semantic segmentation. PointRend's efficiency enables output resolutions that are otherwise impractical in terms of memory or computation compared to existing approaches. Code has been made available at https://github. com/facebookresearch/detectron2/tree/master/projects/PointRend.\", '저자': 'Alexander Kirillov, Yuxin Wu, Kaiming He, Ross Girshick', '전체 인용횟수': '767회 인용202020212022202340176247301', '컨퍼런스': 'Proceedings of the IEEE/CVF conference on computer vision and pattern recognition', '페이지': '9799-9808', '학술 문서': 'Pointrend: Image segmentation as renderingA Kirillov, Y Wu, K He, R Girshick\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 2020767회 인용 관련 학술자료 전체 10개의 버전 '}, title='Pointrend: Image segmentation as rendering', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Efficient human pose estimation from single depth images': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012/11/10', '게시자': 'IEEE', '권': '35', '설명': 'We describe two new approaches to human pose estimation. Both can quickly and accurately predict the 3D positions of body joints from a single depth image without using any temporal information. The key to both approaches is the use of a large, realistic, and highly varied synthetic set of training images. This allows us to learn models that are largely invariant to factors such as pose, body shape, field-of-view cropping, and clothing. Our first approach employs an intermediate body parts representation, designed so that an accurate per-pixel classification of the parts will localize the joints of the body. The second approach instead directly regresses the positions of body joints. By using simple depth pixel comparison features and parallelizable decision forests, both approaches can run super-real time on consumer hardware. Our evaluation investigates many aspects of our methods, and compares the approaches\\xa0…', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Jamie Shotton, Ross Girshick, Andrew Fitzgibbon, Toby Sharp, Mat Cook, Mark Finocchio, Richard Moore, Pushmeet Kohli, Antonio Criminisi, Alex Kipman, Andrew Blake', '전체 인용횟수': '706회 인용2013201420152016201720182019202020212022202331479097103916061374137', '페이지': '2821-2840', '학술 문서': 'Efficient human pose estimation from single depth imagesJ Shotton, R Girshick, A Fitzgibbon, T Sharp, M Cook…\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 2012706회 인용 관련 학술자료 전체 11개의 버전 ', '호': '12'}, title='Efficient human pose estimation from single depth images', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Detecting and recognizing human-object interactions': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'To understand the visual world, a machine must not only recognize individual object instances but also how they interact. Humans are often at the center of such interactions and detecting human-object interactions is an important practical and scientific problem. In this paper, we address the task of detecting (human, verb, object) triplets in challenging everyday photos. We propose a novel model that is driven by a human-centric approach. Our hypothesis is that the appearance of a person--their pose, clothing, action--is a powerful cue for localizing the objects they are interacting with. To exploit this cue, our model learns to predict an action-specific density over target object locations based on the appearance of a detected person. Our model also jointly learns to detect people and objects, and by fusing these predictions it efficiently infers interaction triplets in a clean, jointly trained end-to-end system we call InteractNet. We validate our approach on the recently introduced Verbs in COCO (V-COCO) and HICO-DET datasets, where we show quantitatively compelling results.', '저자': 'Georgia Gkioxari, Ross Girshick, Piotr Dollár, Kaiming He', '전체 인용횟수': '606회 인용201720182019202020212022202363267120128132116', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '8359-8367', '학술 문서': 'Detecting and recognizing human-object interactionsG Gkioxari, R Girshick, P Dollár, K He\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2018606회 인용 관련 학술자료 전체 12개의 버전 '}, title='Detecting and recognizing human-object interactions', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning Features by Watching Objects Move': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/7/1', '권': '1', '설명': \"This paper presents a novel yet intuitive approach to unsupervised feature learning. Inspired by the human visual system, we explore whether low-level motion-based grouping cues can be used to learn an effective visual representation. Specifically, we use unsupervised motion-based segmentation on videos to obtain segments, which we use as' pseudo ground truth'to train a convolutional network to segment objects from a single frame. Given the extensive evidence that motion plays a key role in the development of the human visual system, we hope that this straightforward approach to unsupervised learning will be more effective than cleverly designed'pretext'tasks studied in the literature. Indeed, our extensive experiments show that this is the case. When used for transfer learning on object detection, our representation significantly outperforms previous unsupervised approaches across multiple settings, especially when training data for the target task is scarce.\", '저널': 'CVPR', '저자': 'Deepak Pathak, Ross B Girshick, Piotr Dollár, Trevor Darrell, Bharath Hariharan', '전체 인용횟수': '571회 인용2016201720182019202020212022202323383841181186364', '페이지': '7', '학술 문서': 'Learning features by watching objects moveD Pathak, R Girshick, P Dollár, T Darrell, B Hariharan\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2017571회 인용 관련 학술자료 전체 10개의 버전 ', '호': '2'}, title='Learning Features by Watching Objects Move', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Inferring and executing programs for visual reasoning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'Existing methods for visual reasoning attempt to directly map inputs to outputs using black-box architectures without explicitly modeling the underlying reasoning processes. As a result, these black-box models often learn to exploit biases in the data rather than learning to perform visual reasoning. Inspired by module networks, this paper proposes a model for visual reasoning that consists of a program generator that constructs an explicit representation of the reasoning process to be performed, and an execution engine that executes the resulting program to produce an answer. Both the program generator and the execution engine are implemented by neural networks, and are trained using a combination of backpropagation and REINFORCE. Using the CLEVR benchmark for visual reasoning, we show that our model significantly outperforms strong baselines and generalizes better in a variety of settings.', '저자': 'Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zitnick, Ross Girshick', '전체 인용횟수': '567회 인용20172018201920202021202220232390115911097857', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '2989-2998', '학술 문서': 'Inferring and executing programs for visual reasoningJ Johnson, B Hariharan, L Van Der Maaten, J Hoffman…\\xa0- Proceedings of the IEEE international conference on\\xa0…, 2017567회 인용 관련 학술자료 전체 16개의 버전 '}, title='Inferring and executing programs for visual reasoning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Analyzing the performance of multilayer neural networks for object recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '게시자': 'Springer International Publishing', '설명': ' In the last two years, convolutional neural networks (CNNs) have achieved an impressive suite of results on standard recognition datasets and tasks. CNN-based features seem poised to quickly replace engineered representations, such as SIFT and HOG. However, compared to SIFT and HOG, we understand much less about the nature of the features learned by large CNNs. In this paper, we experimentally probe several aspects of CNN feature learning in an attempt to help practitioners gain useful, evidence-backed intuitions about how to apply CNNs to computer vision problems.', '저자': 'Pulkit Agrawal, Ross Girshick, Jitendra Malik', '전체 인용횟수': '558회 인용20142015201620172018201920202021202220235626969885765535122', '컨퍼런스': 'Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII 13', '페이지': '329-344', '학술 문서': 'Analyzing the performance of multilayer neural networks for object recognitionP Agrawal, R Girshick, J Malik\\xa0- Computer Vision–ECCV 2014: 13th European\\xa0…, 2014558회 인용 관련 학술자료 전체 15개의 버전 '}, title='Analyzing the performance of multilayer neural networks for object recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deformable part models are convolutional neural networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'Deformable part models (DPMs) and convolutional neural networks (CNNs) are two widely used tools for visual recognition. They are typically viewed as distinct approaches: DPMs are graphical models (Markov random fields), while CNNs are\" black-box\" non-linear classifiers. In this paper, we show that a DPM can be formulated as a CNN, thus providing a synthesis of the two ideas. Our construction involves unrolling the DPM inference algorithm and mapping each step to an equivalent CNN layer. From this perspective, it is natural to replace the standard image features used in DPMs with a learned feature extractor. We call the resulting model a DeepPyramid DPM and experimentally validate it on PASCAL VOC object detection. We find that DeepPyramid DPMs significantly outperform DPMs based on histograms of oriented gradients features (HOG) and slightly outperforms a comparable version of the recently introduced R-CNN detection system, while running significantly faster.', '저자': 'Ross Girshick, Forrest Iandola, Trevor Darrell, Jitendra Malik', '전체 인용횟수': '553회 인용20142015201620172018201920202021202220234478190666451534333', '컨퍼런스': 'Proceedings of the IEEE conference on Computer Vision and Pattern Recognition', '페이지': '437-446', '학술 문서': 'Deformable part models are convolutional neural networksR Girshick, F Iandola, T Darrell, J Malik\\xa0- Proceedings of the IEEE conference on Computer\\xa0…, 2015553회 인용 관련 학술자료 전체 17개의 버전 '}, title='Deformable part models are convolutional neural networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Discriminatively Trained Deformable Part Models (DPM) Code': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012', '저자': 'RB Girshick, PF Felzenszwalb, D McAllester', '전체 인용횟수': '544회 인용201020112012201320142015201620172018201920202021202220239445864928068332412111394', '출처': 'http://www.cs.berkeley.edu/~rbg/latent/', '학술 문서': 'Discriminatively trained deformable part models, release 5*RB Girshick, PF Felzenszwalb, D McAllester - 2012305회 인용 관련 학술자료 Discriminatively trained deformable part models, release 4*PF Felzenszwalb\\xa0- http://people. cs. uchicago. edu/pff/latent-release4/, 2010250회 인용 관련 학술자료 전체 2개의 버전 Lsvm-mdpm release 4 notes*R Girshick, P Felzenszwalb, D McAllester\\xa0- Apr, 20103회 인용 관련 학술자료 전체 4개의 버전 '}, title='Discriminatively Trained Deformable Part Models (DPM) Code', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Efficient Regression of General-Activity Human Poses from Depth Images': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011/11', '설명': 'We present a new approach to general-activity human pose estimation from depth images, building on Hough forests. We extend existing techniques in several ways: real time prediction of multiple 3D joints, explicit learning of voting weights, vote compression to allow larger training sets, and a comparison of several decision-tree training objectives. Key aspects of our work include: regression directly from the raw depth image, without the use of an arbitrary intermediate representation; applicability to general motions (not constrained to particular activities) and the ability to localize occluded as well as visible body joints. Experimental results demonstrate that our method produces state of the art results on several data sets including the challenging MSRC-5000 pose estimation test set, at a speed of about 200 frames per second. Results on silhouettes suggest broader applicability to other imaging modalities.', '저자': 'Ross Girshick, Jamie Shotton, Pushmeet Kohli, Antonio Criminisi, Andrew Fitzgibbon', '전체 인용횟수': '521회 인용2011201220132014201520162017201820192020202120222023249717462714334382815106', '컨퍼런스': 'ICCV', '학술 문서': 'Efficient regression of general-activity human poses from depth imagesR Girshick, J Shotton, P Kohli, A Criminisi, A Fitzgibbon\\xa0- 2011 International Conference on Computer Vision, 2011521회 인용 관련 학술자료 전체 16개의 버전 '}, title='Efficient Regression of General-Activity Human Poses from Depth Images', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Long-term feature banks for detailed video understanding': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': 'To understand the world, we humans constantly need to relate the present to the past, and put events in context. In this paper, we enable existing video models to do the same. We propose a long-term feature bank--supportive information extracted over the entire span of a video--to augment state-of-the-art video models that otherwise would only view short clips of 2-5 seconds. Our experiments demonstrate that augmenting 3D convolutional networks with a long-term feature bank yields state-of-the-art results on three challenging video datasets: AVA, EPIC-Kitchens, and Charades. Code is available online.', '저자': 'Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming He, Philipp Krahenbuhl, Ross Girshick', '전체 인용횟수': '494회 인용201920202021202220233183140127110', '컨퍼런스': 'Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition', '페이지': '284-293', '학술 문서': 'Long-term feature banks for detailed video understandingCY Wu, C Feichtenhofer, H Fan, K He, P Krahenbuhl…\\xa0- Proceedings of the IEEE/CVF Conference on Computer\\xa0…, 2019494회 인용 관련 학술자료 전체 12개의 버전 '}, title='Long-term feature banks for detailed video understanding', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Early convolutions help transformers see better': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021/12/6', '권': '34', '설명': 'Vision transformer (ViT) models exhibit substandard optimizability. In particular, they are sensitive to the choice of optimizer (AdamW vs. SGD), optimizer hyperparameters, and training schedule length. In comparison, modern convolutional neural networks are easier to optimize. Why is this the case? In this work, we conjecture that the issue lies with the patchify stem of ViT models, which is implemented by a stride-p p× p convolution (p= 16 by default) applied to the input image. This large-kernel plus large-stride convolution runs counter to typical design choices of convolutional layers in neural networks. To test whether this atypical design choice causes an issue, we analyze the optimization behavior of ViT models with their original patchify stem versus a simple counterpart where we replace the ViT stem by a small number of stacked stride-two 3× 3 convolutions. While the vast majority of computation in the two ViT designs is identical, we find that this small change in early visual processing results in markedly different training behavior in terms of the sensitivity to optimization settings as well as the final model accuracy. Using a convolutional stem in ViT dramatically increases optimization stability and also improves peak performance (by∼ 1-2% top-1 accuracy on ImageNet-1k), while maintaining flops and runtime. The improvement can be observed across the wide spectrum of model complexities (from 1G to 36G flops) and dataset scales (from ImageNet-1k to ImageNet-21k). These findings lead us to recommend using a standard, lightweight convolutional stem for ViT models in this regime as a more robust architectural choice compared to the\\xa0…', '저널': 'Advances in Neural Information Processing Systems', '저자': 'Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Dollár, Ross Girshick', '전체 인용횟수': '493회 인용20212022202321219251', '페이지': '30392-30400', '학술 문서': 'Early convolutions help transformers see betterT Xiao, M Singh, E Mintun, T Darrell, P Dollár…\\xa0- Advances in neural information processing systems, 2021493회 인용 관련 학술자료 전체 6개의 버전 '}, title='Early convolutions help transformers see better', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Object detection networks on convolutional feature maps': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/8/17', '게시자': 'IEEE', '권': '39', '설명': 'Most object detectors contain two important components: a feature extractor and an object classifier. The feature extractor has rapidly evolved with significant research efforts leading to better deep convolutional architectures. The object classifier, however, has not received much attention and many recent systems (like SPPnet and Fast/Faster R-CNN) use simple multi-layer perceptrons. This paper demonstrates that carefully designing deep networks for object classification is just as important. We experiment with region-wise classifier networks that use shared, region-independent convolutional features. We call them “Networks on Convolutional feature maps” (NoCs). We discover that aside from deep feature maps, a  deep  and  convolutional  per-region classifier is of particular importance for object detection, whereas latest superior image classification models (such as ResNets and GoogLeNets) do not directly\\xa0…', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Shaoqing Ren, Kaiming He, Ross Girshick, Xiangyu Zhang, Jian Sun', '전체 인용횟수': '491회 인용20152016201720182019202020212022202362335708583795647', '페이지': '1476-1481', '학술 문서': 'Object detection networks on convolutional feature mapsS Ren, K He, R Girshick, X Zhang, J Sun\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 2016491회 인용 관련 학술자료 전체 11개의 버전 ', '호': '7'}, title='Object detection networks on convolutional feature maps', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Contextual action recognition with r* cnn': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'There are multiple cues in an image which reveal what action a person is performing. For example, a jogger has a pose that is characteristic for jogging, but the scene (eg road, trail) and the presence of other joggers can be an additional source of information. In this work, we exploit the simple observation that actions are accompanied by contextual cues to build a strong action recognition system. We adapt RCNN to use more than one region for classification while still maintaining the ability to localize the action. We call our system R* CNN. The action-specific models and the feature maps are trained jointly, allowing for action specific representations to emerge. R* CNN achieves 90.2% mean AP on the PASAL VOC Action dataset, outperforming all other approaches in the field by a significant margin. Last, we show that R* CNN is not limited to action recognition. In particular, R* CNN can also be used to tackle fine-grained tasks such as attribute classification. We validate this claim by reporting state-of-the-art performance on the Berkeley Attributes of People dataset.', '저자': 'Georgia Gkioxari, Ross Girshick, Jitendra Malik', '전체 인용횟수': '490회 인용20152016201720182019202020212022202333962727977665531', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '1080-1088', '학술 문서': 'Contextual action recognition with r* cnnG Gkioxari, R Girshick, J Malik\\xa0- Proceedings of the IEEE international conference on\\xa0…, 2015490회 인용 관련 학술자료 전체 15개의 버전 '}, title='Contextual action recognition with r* cnn', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '게시자': 'Springer International Publishing', '설명': ' As 3D movie viewing becomes mainstream and the Virtual Reality (VR) market emerges, the demand for 3D contents is growing rapidly. Producing 3D videos, however, remains challenging. In this paper we propose to use deep neural networks to automatically convert 2D videos and images to a stereoscopic 3D format. In contrast to previous automatic 2D-to-3D conversion algorithms, which have separate stages and need ground truth depth map as supervision, our approach is trained end-to-end directly on stereo pairs extracted from existing 3D movies. This novel training scheme makes it possible to exploit orders of magnitude more data and significantly increases performance. Indeed, Deep3D outperforms baselines in both quantitative and human subject evaluations.', '저자': 'Junyuan Xie, Ross Girshick, Ali Farhadi', '전체 인용횟수': '475회 인용20162017201820192020202120222023820599986876642', '컨퍼런스': 'Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14', '페이지': '842-857', '학술 문서': 'Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networksJ Xie, R Girshick, A Farhadi\\xa0- Computer Vision–ECCV 2016: 14th European\\xa0…, 2016475회 인용 관련 학술자료 전체 6개의 버전 '}, title='Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Reducing overfitting in deep networks by decorrelating representations': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/11/19', '설명': 'One major challenge in training Deep Neural Networks is preventing overfitting. Many techniques such as data augmentation and novel regularizers such as Dropout have been proposed to prevent overfitting without requiring a massive amount of training data. In this work, we propose a new regularizer called DeCov which leads to significantly reduced overfitting (as indicated by the difference between train and val performance), and better generalization. Our regularizer encourages diverse or non-redundant representations in Deep Neural Networks by minimizing the cross-covariance of hidden activations. This simple intuition has been explored in a number of past works but surprisingly has never been applied as a regularizer in supervised learning. Experiments across a range of datasets and network architectures show that this loss always reduces overfitting while almost always maintaining or increasing generalization performance and often improving performance over Dropout.', '저널': 'arXiv preprint arXiv:1511.06068', '저자': 'Michael Cogswell, Faruk Ahmed, Ross Girshick, Larry Zitnick, Dhruv Batra', '전체 인용횟수': '448회 인용201620172018201920202021202220231634566477765268', '학술 문서': 'Reducing overfitting in deep networks by decorrelating representationsM Cogswell, F Ahmed, R Girshick, L Zitnick, D Batra\\xa0- arXiv preprint arXiv:1511.06068, 2015448회 인용 관련 학술자료 전체 6개의 버전 '}, title='Reducing overfitting in deep networks by decorrelating representations', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Data distillation: Towards omni-supervised learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'We investigate omni-supervised learning, a special regime of semi-supervised learning in which the learner exploits all available labeled data plus internet-scale sources of unlabeled data. Omni-supervised learning is lower-bounded by performance on existing labeled datasets, offering the potential to surpass state-of-the-art fully supervised methods. To exploit the omni-supervised setting, we propose data distillation, a method that ensembles predictions from multiple transformations of unlabeled data, using a single model, to automatically generate new training annotations. We argue that visual recognition models have recently become accurate enough that it is now possible to apply classic ideas about self-training to challenging real-world data. Our experimental results show that in the cases of human keypoint detection and general object detection, state-of-the-art models trained with data distillation surpass the performance of using labeled data from the COCO dataset alone.', '저자': 'Ilija Radosavovic, Piotr Dollár, Ross Girshick, Georgia Gkioxari, Kaiming He', '전체 인용횟수': '438회 인용201720182019202020212022202322766881187561', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '4119-4128', '학술 문서': 'Data distillation: Towards omni-supervised learningI Radosavovic, P Dollár, R Girshick, G Gkioxari, K He\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2018438회 인용 관련 학술자료 전체 16개의 버전 '}, title='Data distillation: Towards omni-supervised learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Visual storytelling': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/6', '설명': 'We introduce the first dataset for sequential vision-to-language, and explore how this data may be used for the task of visual storytelling. The first release of this dataset, SIND1 v. 1, includes 81,743 unique photos in 20,211 sequences, aligned to both descriptive (caption) and story language. We establish several strong baselines for the storytelling task, and motivate an automatic metric to benchmark progress. Modelling concrete description as well as figurative and social language, as provided in this dataset and the storytelling task, has the potential to move artificial intelligence from basic understandings of typical visual scenes towards more and more human-like understanding of grounded event structure and subjective expression.', '저자': 'Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, C Lawrence Zitnick, Devi Parikh, Lucy Vanderwende, Michel Galley, Margaret Mitchell', '전체 인용횟수': '409회 인용20162017201820192020202120222023727486071606766', '컨퍼런스': 'Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: Human language technologies', '페이지': '1233-1239', '학술 문서': 'Visual storytellingTH Huang, F Ferraro, N Mostafazadeh, I Misra…\\xa0- Proceedings of the 2016 conference of the North\\xa0…, 2016406회 인용 관련 학술자료 전체 12개의 버전 Visual storytelling*H Ting-Hao, F Ferraro, N Mostafazadeh, I Misra…\\xa0- In Proceedings of NAACL, 20166회 인용 관련 학술자료 '}, title='Visual storytelling', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Exploring randomly wired neural networks for image recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': 'Neural networks for image recognition have evolved through extensive manual design from simple chain-like models to structures with multiple wiring paths. The success of ResNets and DenseNets is due in large part to their innovative wiring plans. Now, neural architecture search (NAS) studies are exploring the joint optimization of wiring and operation types, however, the space of possible wirings is constrained and still driven by manual design despite being searched. In this paper, we explore a more diverse set of connectivity patterns through the lens of randomly wired neural networks. To do this, we first define the concept of a stochastic network generator that encapsulates the entire network generation process. Encapsulation provides a unified view of NAS and randomly wired networks. Then, we use three classical random graph models to generate randomly wired graphs for networks. The results are surprising: several variants of these random generators yield network instances that have competitive accuracy on the ImageNet benchmark. These results suggest that new efforts focusing on designing better network generators may lead to new breakthroughs by exploring less constrained search spaces with more room for novel design.', '저자': 'Saining Xie, Alexander Kirillov, Ross Girshick, Kaiming He', '전체 인용횟수': '397회 인용20192020202120222023571021117057', '컨퍼런스': 'Proceedings of the IEEE/CVF International Conference on Computer Vision', '페이지': '1284-1293', '학술 문서': 'Exploring randomly wired neural networks for image recognitionS Xie, A Kirillov, R Girshick, K He\\xa0- Proceedings of the IEEE/CVF International Conference\\xa0…, 2019397회 인용 관련 학술자료 전체 8개의 버전 '}, title='Exploring randomly wired neural networks for image recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'LSDA: Large scale detection through adaptation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '권': '27', '설명': 'A major challenge in scaling object detection is the difficulty of obtaining labeled images for large numbers of categories. Recently, deep convolutional neural networks (CNNs) have emerged as clear winners on object classification benchmarks, in part due to training with 1.2 M+ labeled classification images. Unfortunately, only a small fraction of those labels are available for the detection task. It is much cheaper and easier to collect large quantities of image-level labels from search engines than it is to collect detection data and label it with precise bounding boxes. In this paper, we propose Large Scale Detection through Adaptation (LSDA), an algorithm which learns the difference between the two tasks and transfers this knowledge to classifiers for categories without bounding box annotated data, turning them into detectors. Our method has the potential to enable detection for the tens of thousands of categories that lack bounding box annotations, yet have plenty of classification data. Evaluation on the ImageNet LSVRC-2013 detection challenge demonstrates the efficacy of our approach. This algorithm enables us to produce a> 7.6 K detector by using available classification data from leaf nodes in the ImageNet tree. We additionally demonstrate how to modify our architecture to produce a fast detector (running at 2fps for the 7.6 K detector). Models and software are available at', '저널': 'Advances in neural information processing systems', '저자': 'Judy Hoffman, Sergio Guadarrama, Eric S Tzeng, Ronghang Hu, Jeff Donahue, Ross Girshick, Trevor Darrell, Kate Saenko', '전체 인용횟수': '364회 인용20142015201620172018201920202021202220234215342475152411922', '학술 문서': 'LSDA: Large scale detection through adaptationJ Hoffman, S Guadarrama, ES Tzeng, R Hu, J Donahue…\\xa0- Advances in neural information processing systems, 2014364회 인용 관련 학술자료 전체 15개의 버전 '}, title='LSDA: Large scale detection through adaptation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Tensormask: A foundation for dense object segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': 'Sliding-window object detectors that generate bounding-box object predictions over a dense, regular grid have advanced rapidly and proven popular. In contrast, modern instance segmentation approaches are dominated by methods that first detect object bounding boxes, and then crop and segment these regions, as popularized by Mask R-CNN. In this work, we investigate the paradigm of dense sliding-window instance segmentation, which is surprisingly under-explored. Our core observation is that this task is fundamentally different than other dense prediction tasks such as semantic segmentation or bounding-box object detection, as the output at every spatial location is itself a geometric structure with its own spatial dimensions. To formalize this, we treat dense instance segmentation as a prediction task over 4D tensors and present a general framework called TensorMask that explicitly captures this geometry and enables novel operators on 4D tensors. We demonstrate that the tensor view leads to large gains over baselines that ignore this structure, and leads to results comparable to Mask R-CNN. These promising results suggest that TensorMask can serve as a foundation for novel advances in dense mask prediction and a more complete understanding of the task. Code will be made available.', '저자': 'Xinlei Chen, Ross Girshick, Kaiming He, Piotr Dollár', '전체 인용횟수': '361회 인용2019202020212022202365911310081', '컨퍼런스': 'Proceedings of the IEEE/CVF international conference on computer vision', '페이지': '2061-2069', '학술 문서': 'Tensormask: A foundation for dense object segmentationX Chen, R Girshick, K He, P Dollár\\xa0- Proceedings of the IEEE/CVF international conference\\xa0…, 2019361회 인용 관련 학술자료 전체 9개의 버전 '}, title='Tensormask: A foundation for dense object segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Object Detection with Grammar Models': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011', '설명': 'Compositional models provide an elegant formalism for representing the visual appearance of highly variable objects. While such models are appealing from a theoretical point of view, it has been difficult to demonstrate that they lead to performance advantages on challenging datasets. Here we develop a grammar model for person detection and show that it outperforms previous high-performance systems on the PASCAL benchmark. Our model represents people using a hierarchy of deformable parts, variable structure and an explicit model of occlusion for partially visible objects. To train the model, we introduce a new discriminative framework for learning structured prediction models from weakly-labeled data.', '저자': 'Ross B Girshick, Pedro F Felzenszwalb, David McAllester', '전체 인용횟수': '356회 인용2010201120122013201420152016201720182019202020212022202311193857444423211521252612', '컨퍼런스': 'NIPS 2011', '학술 문서': 'Object detection with grammar modelsR Girshick, P Felzenszwalb, D McAllester\\xa0- Advances in neural information processing systems, 2011356회 인용 관련 학술자료 전체 13개의 버전 '}, title='Object Detection with Grammar Models', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Exploring plain vision transformer backbones for object detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2022/10/23', '게시자': 'Springer Nature Switzerland', '도서': 'European Conference on Computer Vision', '설명': 'We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection. This design enables the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for fine-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is sufficient to build a simple feature pyramid from a single-scale feature map (without the common FPN design) and (ii) it is sufficient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain ViT backbones pre-trained as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the previous leading methods that were all based on hierarchical backbones, reaching up to 61.3 AP on the COCO dataset using only ImageNet-1K pre-training. We hope our\\xa0…', '저자': 'Yanghao Li, Hanzi Mao, Ross Girshick, Kaiming He', '전체 인용횟수': '342회 인용202120222023174263', '페이지': '280-296', '학술 문서': 'Exploring plain vision transformer backbones for object detectionY Li, H Mao, R Girshick, K He\\xa0- European Conference on Computer Vision, 2022342회 인용 관련 학술자료 전체 5개의 버전 '}, title='Exploring plain vision transformer backbones for object detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning to segment every thing': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'Most methods for object instance segmentation require all training examples to be labeled with segmentation masks. This requirement makes it expensive to annotate new categories and has restricted instance segmentation models to~ 100 well-annotated classes. The goal of this paper is to propose a new partially supervised training paradigm, together with a novel weight transfer function, that enables training instance segmentation models on a large set of categories all of which have box annotations, but only a small fraction of which have mask annotations. These contributions allow us to train Mask R-CNN to detect and segment 3000 visual concepts using box annotations from the Visual Genome dataset and mask annotations from the 80 classes in the COCO dataset. We evaluate our approach in a controlled study on the COCO dataset. This work is a first step towards instance segmentation models that have broad comprehension of the visual world.', '저자': 'Ronghang Hu, Piotr Dollár, Kaiming He, Trevor Darrell, Ross Girshick', '전체 인용횟수': '324회 인용201820192020202120222023256880705128', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '4233-4241', '학술 문서': 'Learning to segment every thingR Hu, P Dollár, K He, T Darrell, R Girshick\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2018324회 인용 관련 학술자료 전체 11개의 버전 '}, title='Learning to segment every thing', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Aligning 3D models to RGB-D images of cluttered scenes': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'The goal of this work is to represent objects in an RGB-D scene with corresponding 3D models from a library. We approach this problem by first detecting and segmenting object instances in the scene and then using a convolutional neural network (CNN) to predict the pose of the object. This CNN is trained using pixel surface normals in images containing renderings of synthetic objects. When tested on real data, our method outperforms alternative algorithms trained on real data. We then use this coarse pose estimate along with the inferred pixel support to align a small number of prototypical models to the data, and place into the scene the model that fits best. We observe a 48% relative improvement in performance at the task of 3D detection over the current state-of-the-art, while being an order of magnitude faster.', '저자': 'Saurabh Gupta, Pablo Arbeláez, Ross Girshick, Jitendra Malik', '전체 인용횟수': '292회 인용2014201520162017201820192020202120222023163345454640302112', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '4731-4740', '학술 문서': 'Aligning 3D models to RGB-D images of cluttered scenesS Gupta, P Arbeláez, R Girshick, J Malik\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2015292회 인용 관련 학술자료 전체 15개의 버전 '}, title='Aligning 3D models to RGB-D images of cluttered scenes', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'On learning to localize objects with minimal supervision': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/6/18', '게시자': 'PMLR', '설명': 'Learning to localize objects with minimal supervision is an important problem in computer vision, since large fully annotated datasets are extremely costly to obtain. In this paper, we propose a new method that achieves this goal with only image-level labels of whether the objects are present or not. Our approach combines a discriminative submodular cover problem for automatically discovering a set of positive object windows with a smoothed latent SVM formulation. The latter allows us to leverage efficient quasi-Newton optimization techniques. Our experiments demonstrate that the proposed approach provides a 50% relative improvement in mean average precision over the current state-of-the-art on PASCAL VOC 2007 detection.', '저자': 'Hyun Oh Song, Ross Girshick, Stefanie Jegelka, Julien Mairal, Zaid Harchaoui, Trevor Darrell', '전체 인용횟수': '285회 인용2014201520162017201820192020202120222023927484641391817166', '컨퍼런스': 'International Conference on Machine Learning', '페이지': '1611-1619', '학술 문서': 'On learning to localize objects with minimal supervisionHO Song, R Girshick, S Jegelka, J Mairal, Z Harchaoui…\\xa0- International Conference on Machine Learning, 2014285회 인용 관련 학술자료 전체 18개의 버전 '}, title='On learning to localize objects with minimal supervision', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Indoor scene understanding with rgb-d images: Bottom-up segmentation, object detection and semantic segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/4', '게시자': 'Springer US', '권': '112', '설명': ' In this paper, we address the problems of contour detection, bottom-up grouping, object detection and semantic segmentation on RGB-D data. We focus on the challenging setting of cluttered indoor scenes, and evaluate our approach on the recently introduced NYU-Depth V2 (NYUD2) dataset (Silberman et al., ECCV, 2012). We propose algorithms for object boundary detection and hierarchical segmentation that generalize the  approach of Arbelaez et al. (TPAMI, 2011) by making effective use of depth information. We show that our system can label each contour with its type (depth, normal or albedo). We also propose a generic method for long-range amodal completion of surfaces and show its effectiveness in grouping. We train RGB-D object detectors by analyzing and computing histogram of oriented gradients on the depth image and using them with deformable part models (Felzenszwalb et\\xa0…', '저널': 'International Journal of Computer Vision', '저자': 'Saurabh Gupta, Pablo Arbeláez, Ross Girshick, Jitendra Malik', '전체 인용횟수': '279회 인용20152016201720182019202020212022202316283946463636189', '페이지': '133-149', '학술 문서': 'Indoor scene understanding with rgb-d images: Bottom-up segmentation, object detection and semantic segmentationS Gupta, P Arbeláez, R Girshick, J Malik\\xa0- International Journal of Computer Vision, 2015276회 인용 관련 학술자료 전체 10개의 버전 I1. 4: Invited Paper: Indoor Scene Understanding from RGB‐D Images*S Gupta, R Girshick, P Arbeláez, J Malik\\xa0- SID Symposium Digest of Technical Papers, 20153회 인용 관련 학술자료 Indoor scene understanding from RGB-D images*S Gupta, P Arbeláez, R Girshick, J Malik\\xa0- Digest of Technical Papers-SID International\\xa0…, 2015'}, title='Indoor scene understanding with rgb-d images: Bottom-up segmentation, object detection and semantic segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/4/29', '저자': 'Francisco Massa, Ross Girshick', '전체 인용횟수': '261회 인용20182019202020212022202335784742815', '학술 문서': 'maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorchF Massa, R Girshick - 2018255회 인용 관련 학술자료 maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch*M Francisco, G Ross\\xa0- 2020-07-24]. https://github. com/facebookresearch\\xa0…, 20188회 인용 관련 학술자료 Maskrcnn-Benchmark: Fast, Modular Reference Implementation of Instance Segmentation and Object Detection Algorithms in PyTorch. 2018.[(accessed on 2 February 2019)]F Massa, R Girshick3회 인용 관련 학술자료 '}, title='maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Seeing through the Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/6', '설명': 'When human annotators are given a choice about what to label in an image, they apply their own subjective judgments on what to ignore and what to mention. We refer to these noisy\" human-centric\" annotations as exhibiting human reporting bias. Examples of such annotations include image tags and keywords found on photo sharing sites, or in datasets containing image captions. In this paper, we use these noisy annotations for learning visually correct image classifiers. Such annotations do not use consistent vocabulary, and miss a significant amount of the information present in an image; however, we demonstrate that the noise in these annotations exhibits structure and can be modeled. We propose an algorithm to decouple the human reporting bias from the correct visually grounded labels. Our results are highly interpretable for reporting\" what\\'s in the image\" versus\" what\\'s worth saying.\" We demonstrate the algorithm\\'s efficacy along a variety of metrics and datasets, including MS COCO and Yahoo Flickr 100M. We show significant improvements over traditional algorithms for both image classification and image captioning, doubling the performance of existing methods in some cases.', '저널': 'CVPR', '저자': 'Ishan Misra, C Lawrence Zitnick, Margaret Mitchell, Ross Girshick', '전체 인용횟수': '233회 인용20162017201820192020202120222023520293438363728', '학술 문서': 'Seeing through the human reporting bias: Visual classifiers from noisy human-centric labelsI Misra, C Lawrence Zitnick, M Mitchell, R Girshick\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2016230회 인용 관련 학술자료 전체 11개의 버전 Learning visual classifiers using human-centric annotations*I Misra, CL Zitnick, M Mitchell, RB Girshick\\xa0- CoRR, abs/1512.06974, 20153회 인용 관련 학술자료 '}, title='Seeing through the Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Exploring nearest neighbor approaches for image captioning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/5/17', '설명': 'We explore a variety of nearest neighbor baseline approaches for image captioning. These approaches find a set of nearest neighbor images in the training set from which a caption may be borrowed for the query image. We select a caption for the query image by finding the caption that best represents the \"consensus\" of the set of candidate captions gathered from the nearest neighbor images. When measured by automatic evaluation metrics on the MS COCO caption evaluation server, these approaches perform as well as many recent approaches that generate novel captions. However, human studies show that a method that generates novel captions is still preferred over the nearest neighbor approach.', '저널': 'arXiv preprint arXiv:1505.04467', '저자': 'Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C Lawrence Zitnick', '전체 인용횟수': '214회 인용20142015201620172018201920202021202220231112332352625182017', '학술 문서': 'Exploring nearest neighbor approaches for image captioningJ Devlin, S Gupta, R Girshick, M Mitchell, CL Zitnick\\xa0- arXiv preprint arXiv:1505.04467, 2015214회 인용 관련 학술자료 전체 2개의 버전 '}, title='Exploring nearest neighbor approaches for image captioning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Using k-poselets for detecting people and localizing their keypoints': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '설명': 'A k-poselet is a deformable part model (DPM) with k parts, where each of the parts is a poselet, aligned to a specific configuration of keypoints based on ground-truth annotations. A separate template is used to learn the appearance of each part. The parts are allowed to move with respect to each other with a deformation cost that is learned at training time. This model is richer than both the traditional version of poselets and DPMs. It enables a unified approach to person detection and keypoint prediction which, barring contemporaneous approaches based on CNN features, achieves state-of-the-art keypoint prediction while maintaining competitive detection performance.', '저자': 'Georgia Gkioxari, Bharath Hariharan, Ross Girshick, Jitendra Malik', '전체 인용횟수': '210회 인용20142015201620172018201920202021202220234172127222522222715', '컨퍼런스': 'Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition', '페이지': '3582-3589', '학술 문서': 'Using k-poselets for detecting people and localizing their keypointsG Gkioxari, B Hariharan, R Girshick, J Malik\\xa0- Proceedings of the IEEE Conference on Computer\\xa0…, 2014210회 인용 관련 학술자료 전체 13개의 버전 '}, title='Using k-poselets for detecting people and localizing their keypoints', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A large-scale study on unsupervised spatiotemporal representation learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021', '설명': 'We present a large-scale study on unsupervised spatiotemporal representation learning from videos. With a unified perspective on four recent image-based frameworks, we study a simple objective that can easily generalize all these methods to space-time. Our objective encourages temporally-persistent features in the same video, and in spite of its simplicity, it works surprisingly well across:(i) different unsupervised frameworks,(ii) pre-training datasets,(iii) downstream datasets, and (iv) backbone architectures. We draw a series of intriguing observations from this study, eg, we discover that encouraging long-spanned persistency can be effective even if the timespan is 60 seconds. In addition to state-of-the-art results in multiple benchmarks, we report a few promising cases in which unsupervised pre-training can outperform its supervised counterpart. Code will be made available at https://github. com/facebookresearch/SlowFast.', '저자': 'Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, Kaiming He', '전체 인용횟수': '196회 인용20202021202220232208886', '컨퍼런스': 'Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition', '페이지': '3299-3309', '학술 문서': 'A large-scale study on unsupervised spatiotemporal representation learningC Feichtenhofer, H Fan, B Xiong, R Girshick, K He\\xa0- Proceedings of the IEEE/CVF Conference on Computer\\xa0…, 2021196회 인용 관련 학술자료 전체 6개의 버전 '}, title='A large-scale study on unsupervised spatiotemporal representation learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Actions and attributes from wholes and parts': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'We investigate the importance of parts for the tasks of action and attribute classification. We develop a part-based approach by leveraging convolutional network features inspired by recent advances in computer vision. Our part detectors are a deep version of poselets and capture parts of the human body under a distinct set of poses. For the tasks of action and attribute classification, we train holistic convolutional neural networks and show that adding parts leads to top-performing results for both tasks. We observe that for deeper networks parts are less significant. In addition, we demonstrate the effectiveness of our approach when we replace an oracle person detector, as is the default in the current evaluation protocol for both tasks, with a state-of-the-art person detection system.', '저자': 'Georgia Gkioxari, Ross Girshick, Jitendra Malik', '전체 인용횟수': '183회 인용2014201520162017201820192020202120222023131820281826192014', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '2470-2478', '학술 문서': 'Actions and attributes from wholes and partsG Gkioxari, R Girshick, J Malik\\xa0- Proceedings of the IEEE international conference on\\xa0…, 2015183회 인용 관련 학술자료 전체 13개의 버전 '}, title='Actions and attributes from wholes and parts', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'R-CNNs for pose estimation and action detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/6/19', '설명': 'We present convolutional neural networks for the tasks of keypoint (pose) prediction and action classification of people in unconstrained images. Our approach involves training an R-CNN detector with loss functions depending on the task being tackled. We evaluate our method on the challenging PASCAL VOC dataset and compare it to previous leading approaches. Our method gives state-of-the-art results for keypoint and action prediction. Additionally, we introduce a new dataset for action detection, the task of simultaneously localizing people and classifying their actions, and present results using our approach.', '저널': 'arXiv preprint arXiv:1406.5212', '저자': 'Georgia Gkioxari, Bharath Hariharan, Ross Girshick, Jitendra Malik', '전체 인용횟수': '177회 인용20152016201720182019202020212022202320202123252322138', '학술 문서': 'R-cnns for pose estimation and action detectionG Gkioxari, B Hariharan, R Girshick, J Malik\\xa0- arXiv preprint arXiv:1406.5212, 2014177회 인용 관련 학술자료 전체 6개의 버전 '}, title='R-CNNs for pose estimation and action detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Boundary iou: Improving object-centric image segmentation evaluation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021', '설명': 'We present Boundary IoU (Intersection-over-Union), a new segmentation evaluation measure focused on boundary quality. We perform an extensive analysis across different error types and object sizes and show that Boundary IoU is significantly more sensitive than the standard Mask IoU measure to boundary errors for large objects and does not over-penalize errors on smaller objects. The new quality measure displays several desirable characteristics like symmetry wrt prediction/ground truth pairs and balanced responsiveness across scales, which makes it more suitable for segmentation evaluation than other boundary-focused measures like Trimap IoU and F-measure. Based on Boundary IoU, we update the standard evaluation protocols for instance and panoptic segmentation tasks by proposing the Boundary AP (Average Precision) and Boundary PQ (Panoptic Quality) metrics, respectively. Our experiments show that the new evaluation metrics track boundary quality improvements that are generally overlooked by current Mask IoU-based evaluation metrics. We hope that the adoption of the new boundary-sensitive evaluation metrics will lead to rapid progress in segmentation methods that improve boundary quality.', '저자': 'Bowen Cheng, Ross Girshick, Piotr Dollár, Alexander C Berg, Alexander Kirillov', '전체 인용횟수': '174회 인용20202021202220232207078', '컨퍼런스': 'Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition', '페이지': '15334-15342', '학술 문서': 'Boundary IoU: Improving object-centric image segmentation evaluationB Cheng, R Girshick, P Dollár, AC Berg, A Kirillov\\xa0- Proceedings of the IEEE/CVF Conference on Computer\\xa0…, 2021174회 인용 관련 학술자료 전체 7개의 버전 '}, title='Boundary iou: Improving object-centric image segmentation evaluation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Object detection and classification in images': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/1/2', '발명자': 'Jian Sun, Ross Girshick, Shaoqing Ren, Kaiming He', '설명': 'Systems, methods, and computer-readable media for providing fast and accurate object detection and classification in images are described herein. In some examples, a computing device can receive an input image. The computing device can process the image, and generate a convolutional feature map. In some configurations, the convolutional feature map can be processed through a Region Proposal Network (RPN) to generate proposals for candidate objects in the image. In various examples, the computing device can process the convolutional feature map with the proposals through a Fast Region-Based Convolutional Neural Network (FRCN) proposal classifier to determine a class of each object in the image and a confidence score associated therewith. The computing device can then provide a requestor with an output including the object classification and/or confidence score.', '전체 인용횟수': '130회 인용20182019202020212022202312824283316', '출원번호': '15001417', '특허 번호': '9858496', '특허청': 'US', '학술 문서': 'Object detection and classification in imagesJ Sun, R Girshick, S Ren, K He\\xa0- US Patent 9,858,496, 2018130회 인용 관련 학술자료 전체 4개의 버전 '}, title='Object detection and classification in images', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Understanding objects in detail with fine-grained attributes': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '설명': 'We study the problem of understanding objects in detail, intended as recognizing a wide array of fine-grained object attributes. To this end, we introduce a dataset of 7,413 airplanes annotated in detail with parts and their attributes, leveraging images donated by airplane spotters and crowdsourcing both the design and collection of the detailed annotations. We provide a number of insights that should help researchers interested in designing fine-grained datasets for other basic level categories. We show that the collected data can be used to study the relation between part detection and attribute prediction by diagnosing the performance of classifiers that pool information from different parts of an object. We note that the prediction of certain attributes can benefit substantially from accurate part detection. We also show that, differently from previous results in object detection, employing a large number of part templates can improve detection accuracy at the expenses of detection speed. We finally propose a coarse-to-fine approach to speed up detection through a hierarchical cascade algorithm.', '저자': 'Andrea Vedaldi, Siddharth Mahendran, Stavros Tsogkas, Subhransu Maji, Ross Girshick, Juho Kannala, Esa Rahtu, Iasonas Kokkinos, Matthew B Blaschko, David Weiss, Ben Taskar, Karen Simonyan, Naomi Saphra, Sammy Mohamed', '전체 인용횟수': '125회 인용2014201520162017201820192020202120222023316272517118665', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '3622-3629', '학술 문서': 'Understanding objects in detail with fine-grained attributesA Vedaldi, S Mahendran, S Tsogkas, S Maji, R Girshick…\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2014125회 인용 관련 학술자료 전체 32개의 버전 '}, title='Understanding objects in detail with fine-grained attributes', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Benchmarking detection transfer learning with vision transformers': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021/11/22', '설명': 'Object detection is a central downstream task used to test if pre-trained network parameters confer benefits, such as improved accuracy or training speed. The complexity of object detection methods can make this benchmarking non-trivial when new architectures, such as Vision Transformer (ViT) models, arrive. These difficulties (e.g., architectural incompatibility, slow training, high memory consumption, unknown training formulae, etc.) have prevented recent studies from benchmarking detection transfer learning with standard ViT models. In this paper, we present training techniques that overcome these challenges, enabling the use of standard ViT models as the backbone of Mask R-CNN. These tools facilitate the primary goal of our study: we compare five ViT initializations, including recent state-of-the-art self-supervised learning methods, supervised initialization, and a strong random initialization baseline. Our results show that recent masking-based unsupervised learning methods may, for the first time, provide convincing transfer learning improvements on COCO, increasing box AP up to 4% (absolute) over supervised and prior self-supervised pre-training methods. Moreover, these masking-based initializations scale better, with the improvement growing as model size increases.', '저널': 'arXiv preprint arXiv:2111.11429', '저자': 'Yanghao Li, Saining Xie, Xinlei Chen, Piotr Dollar, Kaiming He, Ross Girshick', '전체 인용횟수': '105회 인용202220234461', '학술 문서': 'Benchmarking detection transfer learning with vision transformersY Li, S Xie, X Chen, P Dollar, K He, R Girshick\\xa0- arXiv preprint arXiv:2111.11429, 2021105회 인용 관련 학술자료 전체 2개의 버전 '}, title='Benchmarking detection transfer learning with vision transformers', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A multigrid method for efficiently training video models': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020', '설명': 'Training competitive deep video models is an order of magnitude slower than training their counterpart image models. Slow training causes long research cycles, which hinders progress in video understanding research. Following standard practice for training image models, video model training has used a fixed mini-batch shape: a specific number of clips, frames, and spatial size. However, what is the optimal shape? High resolution models perform well, but train slowly. Low resolution models train faster, but are less accurate. Inspired by multigrid methods in numerical optimization, we propose to use variable mini-batch shapes with different spatial-temporal resolutions that are varied according to a schedule. The different shapes arise from resampling the training data on multiple sampling grids. Training is accelerated by scaling up the mini-batch size and learning rate when shrinking the other dimensions. We empirically demonstrate a general and robust grid schedule that yields a significant out-of-the-box training speedup without a loss in accuracy for different models (I3D, non-local, SlowFast), datasets (Kinetics, Something-Something, Charades), and training settings (with and without pre-training, 128 GPUs or 1 GPU). As an illustrative example, the proposed multigrid method trains a ResNet-50 SlowFast network 4.5 x faster (wall-clock time, same hardware) while also improving accuracy (+ 0.8% absolute) on Kinetics-400 compared to baseline training. Code is available online.', '저자': 'Chao-Yuan Wu, Ross Girshick, Kaiming He, Christoph Feichtenhofer, Philipp Krahenbuhl', '전체 인용횟수': '104회 인용20192020202120222023113413019', '컨퍼런스': 'Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition', '페이지': '153-162', '학술 문서': 'A multigrid method for efficiently training video modelsCY Wu, R Girshick, K He, C Feichtenhofer…\\xa0- Proceedings of the IEEE/CVF Conference on Computer\\xa0…, 2020104회 인용 관련 학술자료 전체 12개의 버전 '}, title='A multigrid method for efficiently training video models', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Object detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021/10/13', '게시자': 'Springer International Publishing', '도서': 'Computer Vision: A Reference Guide', '설명': 'BackgroundThe goal of object detection is to detect all instances of objects from one or several known classes, such as people, cars, or faces in an image. Typically only a small number of objects are present in the image, but there is a very large number of possible locations and scales at which they can occur and that need to somehow be explored.Each detection is reported with some form of pose information. This could be as simple as the location of the object, a location and scale, a bounding box, or a segmentation mask. In other situations the pose information is more detailed and contains the parameters of a linear or nonlinear transformation. For example a face detector may compute the locations of the eyes, nose, and mouth, in addition to the bounding box of the face. An example of a bicycle detection that specifies the locations of certain parts is shown in Fig. 1. The pose could also be defined by a three\\xa0…', '저자': 'Yali Amit, Pedro Felzenszwalb, Ross Girshick', '전체 인용횟수': '103회 인용201420152016201720182019202020212022202311525117102436', '페이지': '875-883', '학술 문서': 'Object detectionY Amit, P Felzenszwalb, R Girshick\\xa0- Computer Vision: A Reference Guide, 2021103회 인용 관련 학술자료 전체 7개의 버전 '}, title='Object detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Phyre: A new benchmark for physical reasoning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '권': '32', '설명': 'Understanding and reasoning about physics is an important ability of intelligent agents. We develop the PHYRE benchmark for physical reasoning that contains a set of simple classical mechanics puzzles in a 2D physical environment. The benchmark is designed to encourage the development of learning algorithms that are sample-efficient and generalize well across puzzles. We test several modern learning algorithms on PHYRE and find that these algorithms fall short in solving the puzzles efficiently. We expect that PHYRE will encourage the development of novel sample-efficient agents that learn efficient but useful models of physics. For code and to play PHYRE for yourself, please visit https://player. phyre. ai.', '저널': 'Advances in Neural Information Processing Systems', '저자': 'Anton Bakhtin, Laurens van der Maaten, Justin Johnson, Laura Gustafson, Ross Girshick', '전체 인용횟수': '103회 인용20192020202120222023521332123', '학술 문서': 'Phyre: A new benchmark for physical reasoningA Bakhtin, L van der Maaten, J Johnson, L Gustafson…\\xa0- Advances in Neural Information Processing Systems, 2019103회 인용 관련 학술자료 전체 7개의 버전 '}, title='Phyre: A new benchmark for physical reasoning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Sparselet models for efficient multiclass object detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012', '게시자': 'Springer Berlin Heidelberg', '설명': ' We develop an intermediate representation for deformable part models and show that this representation has favorable performance characteristics for multi-class problems when the number of classes is high. Our model uses sparse coding of part filters to represent each filter as a sparse linear combination of shared dictionary elements. This leads to a universal set of parts that are shared among all object classes. Reconstruction of the original part filter responses via sparse matrix-vector product reduces computation relative to conventional part filter convolutions. Our model is well suited to a parallel implementation, and we report a new GPU DPM implementation that takes advantage of sparse coding of part filters. The speed-up offered by our intermediate representation and parallel computation enable real-time DPM detection of 20 different object classes on a laptop computer.', '저자': 'Hyun Oh Song, Stefan Zickler, Tim Althoff, Ross Girshick, Mario Fritz, Christopher Geyer, Pedro Felzenszwalb, Trevor Darrell', '전체 인용횟수': '103회 인용201220132014201520162017201820192020202120222023121222210473111', '컨퍼런스': 'Computer Vision–ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part II 12', '페이지': '802-815', '학술 문서': 'Sparselet models for efficient multiclass object detectionHO Song, S Zickler, T Althoff, R Girshick, M Fritz…\\xa0- Computer Vision–ECCV 2012: 12th European\\xa0…, 2012103회 인용 관련 학술자료 전체 22개의 버전 '}, title='Sparselet models for efficient multiclass object detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'From rigid templates to grammars: Object detection with structured models': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012/1/1', '기관': 'University of Chicago', '설명': 'We develop models for localizing instances of a generic object category, such as cars or people, in images. We define these models using a grammar formalism. In this formalism compositional rules are used to encode models that can range in complexity from simple rigid templates to rich deformable part models with variable structure. A central contribution of this dissertation is an exploration along this axis, wherein we gradually enrich our object category representations. We demonstrate that these richer models lead to improved object detection performance on challenging datasets such as the PASCAL VOC Challenges.', '저자': 'Ross Brook Girshick', '전체 인용횟수': '89회 인용20122013201420152016201720182019202020212022202313748135211111211', '학술 문서': 'From rigid templates to grammars: Object detection with structured modelsRB Girshick - 201289회 인용 관련 학술자료 전체 3개의 버전 '}, title='From rigid templates to grammars: Object detection with structured models', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Visual object detection with deformable part models': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/9/1', '게시자': 'ACM', '권': '56', '설명': 'We describe a state-of-the-art system for finding objects in cluttered images. Our system is based on deformable models that represent objects using local part templates and geometric constraints on the locations of parts. We reduce object detection to classification with latent variables. The latent variables introduce invariances that make it possible to detect objects with highly variable appearance. We use a generalization of support vector machines to incorporate latent information during training. This has led to a general framework for discriminative training of classifiers with latent variables. Discriminative training benefits from large training datasets. In practice we use an iterative algorithm that alternates between estimating latent values for positive examples and solving a large convex optimization problem. Practical optimization of this large convex problem can be done using active set techniques for adaptive\\xa0…', '저널': 'Communications of the ACM', '저자': 'Pedro Felzenszwalb, Ross Girshick, David McAllester, Deva Ramanan', '전체 인용횟수': '83회 인용2014201520162017201820192020202120222023586781313995', '페이지': '97-105', '학술 문서': 'Visual object detection with deformable part modelsP Felzenszwalb, R Girshick, D McAllester, D Ramanan\\xa0- Communications of the ACM, 201383회 인용 관련 학술자료 전체 2개의 버전 ', '호': '9'}, title='Visual object detection with deformable part models', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Fast and accurate model scaling': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021', '설명': 'In this work we analyze strategies for convolutional neural network scaling; that is, the process of scaling a base convolutional network to endow it with greater computational complexity and consequently representational power. Example scaling strategies may include increasing model width, depth, resolution, etc. While various scaling strategies exist, their tradeoffs are not fully understood. Existing analysis typically focuses on the interplay of accuracy and flops (floating point operations). Yet, as we demonstrate, various scaling strategies affect model parameters, activations, and consequently actual runtime quite differently. In our experiments we show the surprising result that numerous scaling strategies yield networks with similar accuracy but with widely varying properties. This leads us to propose a simple fast compound scaling strategy that encourages primarily scaling model width, while scaling depth and resolution to a lesser extent. Unlike currently popular scaling strategies, which result in about O (s) increase in model activation wrt scaling flops by a factor of s, the proposed fast compound scaling results in close to O (sqrt s) increase in activations, while achieving excellent accuracy. Fewer activations leads to speedups on modern memory-bandwidth limited hardware (eg, GPUs). More generally, we hope this work provides a framework for analyzing scaling strategies under various computational constraints.', '저자': 'Piotr Dollár, Mannat Singh, Ross Girshick', '전체 인용횟수': '82회 인용202120222023112446', '컨퍼런스': 'Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition', '페이지': '924-932', '학술 문서': 'Fast and accurate model scalingP Dollár, M Singh, R Girshick\\xa0- Proceedings of the IEEE/CVF Conference on Computer\\xa0…, 202182회 인용 관련 학술자료 전체 8개의 버전 '}, title='Fast and accurate model scaling', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Are labels necessary for neural architecture search?': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020', '게시자': 'Springer International Publishing', '설명': ' Existing neural network architectures in computer vision—whether designed by humans or by machines—were typically found using both images and their associated labels. In this paper, we ask the question: can we find high-quality neural architectures using only images, but no human-annotated labels? To answer this question, we first define a new setup called Unsupervised Neural Architecture Search (UnNAS). We then conduct two sets of experiments. In sample-based experiments, we train a large number (500) of diverse architectures with either supervised or unsupervised objectives, and find that the architecture rankings produced with and without labels are highly correlated. In search-based experiments, we run a well-established NAS algorithm (DARTS) using various unsupervised objectives, and report that the architectures searched without labels can be competitive to their counterparts\\xa0…', '저자': 'Chenxi Liu, Piotr Dollár, Kaiming He, Ross Girshick, Alan Yuille, Saining Xie', '전체 인용횟수': '79회 인용20202021202220235272918', '컨퍼런스': 'Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IV 16', '페이지': '798-813', '학술 문서': 'Are labels necessary for neural architecture search?C Liu, P Dollár, K He, R Girshick, A Yuille, S Xie\\xa0- Computer Vision–ECCV 2020: 16th European\\xa0…, 202079회 인용 관련 학술자료 전체 10개의 버전 '}, title='Are labels necessary for neural architecture search?', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning by asking questions': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'We introduce an interactive learning framework for the development and testing of intelligent visual systems, called learning-by-asking (LBA). We explore LBA in context of the Visual Question Answering (VQA) task. LBA differs from standard VQA training in that most questions are not observed during training time, and the learner must ask questions it wants answers to. Thus, LBA more closely mimics natural learning and has the potential to be more data-efficient than the traditional VQA setting. We present a model that performs LBA on the CLEVR dataset, and show that it automatically discovers an easy-to-hard curriculum when learning interactively from an oracle. Our LBA generated data consistently matches or outperforms the CLEVR train data and is more sample efficient. We also show that our model asks questions that generalize to state-of-the-art VQA models and to novel test time distributions.', '저자': 'Ishan Misra, Ross Girshick, Rob Fergus, Martial Hebert, Abhinav Gupta, Laurens Van Der Maaten', '전체 인용횟수': '79회 인용20182019202020212022202361915171110', '컨퍼런스': 'Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition', '페이지': '11-20', '학술 문서': 'Learning by asking questionsI Misra, R Girshick, R Fergus, M Hebert, A Gupta…\\xa0- Proceedings of the IEEE Conference on Computer\\xa0…, 201879회 인용 관련 학술자료 전체 12개의 버전 '}, title='Learning by asking questions', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Human body pose estimation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/1/28', '발명자': 'Jamie Daniel Joseph Shotton, Shahram Izadi, Otmar Hilliges, David Kim, David Geoffrey Molyneaux, Matthew Darius Cook, Pushmeet Kohli, Antonio Criminisi, Ross Brook Girshick, Andrew William Fitzgibbon', '설명': 'Techniques for human body pose estimation are disclosed herein. Images such as depth images, silhouette images, or volumetric images may be generated and pixels or voxels of the images may be identified. The techniques may process the pixels or voxels to determine a probability that each pixel or voxel is associated with a segment of a body captured in the image or to determine a three-dimensional representation for each pixel or voxel that is associated with a location on a canonical body. These probabilities or three-dimensional representations may then be utilized along with the images to construct a posed model of the body captured in the image.', '전체 인용횟수': '74회 인용201320142015201620172018201920202021202220237413876731081', '출원번호': '13040205', '특허 번호': '8638985', '특허청': 'US', '학술 문서': 'Human body pose estimationJDJ Shotton, S Izadi, O Hilliges, D Kim, DG Molyneaux…\\xa0- US Patent 8,638,985, 201474회 인용 관련 학술자료 전체 4개의 버전 Journal Publications (Peer Reviewed) 1. J. Shotton, R. Girshick, A. Fitzgibbon, T. Sharp, M. Cook, M. Finocchio, R. Moore, P. Kohli, A.*RB Girshick\\xa0- US Patent App, 2012관련 학술자료 '}, title='Human body pose estimation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Inferring 3d object pose in RGB-D images': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/2/16', '설명': 'The goal of this work is to replace objects in an RGB-D scene with corresponding 3D models from a library. We approach this problem by first detecting and segmenting object instances in the scene using the approach from Gupta et al. [13]. We use a convolutional neural network (CNN) to predict the pose of the object. This CNN is trained using pixel normals in images containing rendered synthetic objects. When tested on real data, it outperforms alternative algorithms trained on real data. We then use this coarse pose estimate along with the inferred pixel support to align a small number of prototypical models to the data, and place the model that fits the best into the scene. We observe a 48% relative improvement in performance at the task of 3D detection over the current state-of-the-art [33], while being an order of magnitude faster at the same time.', '저널': 'arXiv preprint arXiv:1502.04652', '저자': 'Saurabh Gupta, Pablo Arbeláez, Ross Girshick, Jitendra Malik', '전체 인용횟수': '56회 인용201520162017201820192020202120222023375987942', '학술 문서': 'Inferring 3d object pose in RGB-D imagesS Gupta, P Arbeláez, R Girshick, J Malik\\xa0- arXiv preprint arXiv:1502.04652, 201556회 인용 관련 학술자료 전체 3개의 버전 '}, title='Inferring 3d object pose in RGB-D images', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Impact of data on generalization of AI for surgical intelligence applications': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020/12/17', '게시자': 'Nature Publishing Group UK', '권': '10', '설명': 'AI is becoming ubiquitous, revolutionizing many aspects of our lives. In surgery, it is still a promise. AI has the potential to improve surgeon performance and impact patient care, from post-operative debrief to real-time decision support. But, how much data is needed by an AI-based system to learn surgical context with high fidelity? To answer this question, we leveraged a large-scale, diverse, cholecystectomy video dataset. We assessed surgical workflow recognition and report a deep learning system, that not only detects surgical phases, but does so with high accuracy and is able to generalize to new settings and unseen medical centers. Our findings provide a solid foundation for translating AI applications from research to practice, ushering in a new era of surgical intelligence.', '저널': 'Scientific reports', '저자': 'Omri Bar, Daniel Neimark, Maya Zohar, Gregory D Hager, Ross Girshick, Gerald M Fried, Tamir Wolf, Dotan Asselmann', '전체 인용횟수': '52회 인용202120222023121623', '페이지': '22208', '학술 문서': 'Impact of data on generalization of AI for surgical intelligence applicationsO Bar, D Neimark, M Zohar, GD Hager, R Girshick…\\xa0- Scientific reports, 202052회 인용 관련 학술자료 전체 10개의 버전 ', '호': '1'}, title='Impact of data on generalization of AI for surgical intelligence applications', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The three R’s of computer vision: Recognition, reconstruction and reorganization': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/3/1', '게시자': 'North-Holland', '권': '72', '설명': 'We argue for the importance of the interaction between recognition, reconstruction and re-organization, and propose that as a unifying framework for computer vision. In this view, recognition of objects is reciprocally linked to re-organization, with bottom-up grouping processes generating candidates, which can be classified using top down knowledge, following which the segmentations can be refined again. Recognition of 3D objects could benefit from a reconstruction of 3D structure, and 3D reconstruction can benefit from object category-specific priors. We also show that reconstruction of 3D structure from video data goes hand in hand with the reorganization of the scene. We demonstrate pipelined versions of two systems, one for RGB-D images, and another for RGB images, which produce rich 3D scene interpretations in this framework.', '저널': 'Pattern Recognition Letters', '저자': 'Jitendra Malik, Pablo Arbeláez, Joao Carreira, Katerina Fragkiadaki, Ross Girshick, Georgia Gkioxari, Saurabh Gupta, Bharath Hariharan, Abhishek Kar, Shubham Tulsiani', '전체 인용횟수': '50회 인용20162017201820192020202120222023461064568', '페이지': '4-14', '학술 문서': 'The three R’s of computer vision: Recognition, reconstruction and reorganizationJ Malik, P Arbeláez, J Carreira, K Fragkiadaki…\\xa0- Pattern Recognition Letters, 201650회 인용 관련 학술자료 전체 8개의 버전 '}, title='The three R’s of computer vision: Recognition, reconstruction and reorganization', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A unified architecture for instance and semantic segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': '[1] He, K., Zhang, X., Ren, S., & Sun, J. Deep residual learning for image recognition. CVPR 2016.[2] Xie, S., Girshick, R., Dollár, P., Tu, Z., & He, K. Aggregated residual transformations for deep neural networks. CVPR 2017.[3] Lin, TY, Dollár, P., Girshick, R., He, K., Hariharan, B., & Belongie, S. Feature pyramid networks for object detection. CVPR 2017.', '저자': 'Alexander Kirillov, Kaiming He, Ross Girshick, Piotr Dollár', '전체 인용횟수': '44회 인용20182019202020212022202332681312', '학술 문서': 'A unified architecture for instance and semantic segmentationA Kirillov, K He, R Girshick, P Dollár\\xa0- CVPR, 201744회 인용 관련 학술자료 '}, title='A unified architecture for instance and semantic segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Method and system for using machine-learning for object instance segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020/7/14', '발명자': 'Kaiming He, Georgia Gkioxari, Piotr Dollar, Ross Girshick', '설명': 'In one embodiment, a method includes a computing system accessing a training image. The system may generate a feature map for the training image using a first neural network. The system may identify a region of interest in the feature map and generate a regional feature map for the region of interest based on sampling locations defined by a sampling region. The sampling region and the region of interest may correspond to the same region in the feature map. The system may generate an instance segmentation mask associated with the region of interest by processing the regional feature map using a second neural network. The second neural network may be trained using the instance segmentation mask. Once trained, the second neural network is configured to generate instance segmentation masks for object instances depicted in images.', '전체 인용횟수': '43회 인용2020202120222023414178', '출원번호': '15922734', '특허 번호': '10713794', '특허청': 'US', '학술 문서': 'Method and system for using machine-learning for object instance segmentationK He, G Gkioxari, P Dollar, R Girshick\\xa0- US Patent 10,713,794, 202043회 인용 관련 학술자료 전체 2개의 버전 '}, title='Method and system for using machine-learning for object instance segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Evaluating large-vocabulary object detectors: The devil is in the details': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021/2/1', '설명': 'By design, average precision (AP) for object detection aims to treat all classes independently: AP is computed independently per category and averaged. On one hand, this is desirable as it treats all classes equally. On the other hand, it ignores cross-category confidence calibration, a key property in real-world use cases. Unfortunately, under important conditions (i.e., large vocabulary, high instance counts) the default implementation of AP is neither category independent, nor does it directly reward properly calibrated detectors. In fact, we show that on LVIS the default implementation produces a gameable metric, where a simple, un-intuitive re-ranking policy can improve AP by a large margin. To address these limitations, we introduce two complementary metrics. First, we present a simple fix to the default AP implementation, ensuring that it is independent across categories as originally intended. We benchmark recent LVIS detection advances and find that many reported gains do not translate to improvements under our new evaluation, suggesting recent improvements may arise from difficult to interpret changes to cross-category rankings. Given the importance of reliably benchmarking cross-category rankings, we consider a pooled version of AP (AP-Pool) that rewards properly calibrated detectors by directly comparing cross-category rankings. Finally, we revisit classical approaches for calibration and find that explicitly calibrating detectors improves state-of-the-art on AP-Pool by 1.7 points', '저널': 'arXiv preprint arXiv:2102.01066', '저자': 'Achal Dave, Piotr Dollár, Deva Ramanan, Alexander Kirillov, Ross Girshick', '전체 인용횟수': '36회 인용20212022202310188', '학술 문서': 'Evaluating large-vocabulary object detectors: The devil is in the detailsA Dave, P Dollár, D Ramanan, A Kirillov, R Girshick\\xa0- arXiv preprint arXiv:2102.01066, 202136회 인용 관련 학술자료 전체 2개의 버전 '}, title='Evaluating large-vocabulary object detectors: The devil is in the details', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Training deformable part models with decorrelated features': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013', '설명': 'In this paper, we show how to train a deformable part model (DPM) fast--typically in less than 20 minutes, or four times faster than the current fastest method--while maintaining high average precision on the PASCAL VOC datasets. At the core of our approach is\" latent LDA,\" a novel generalization of linear discriminant analysis for learning latent variable models. Unlike latent SVM, latent LDA uses efficient closed-form updates and does not require an expensive search for hard negative examples. Our approach also acts as a springboard for a detailed experimental study of DPM training. We isolate and quantify the impact of key training factors for the first time (eg, How important are discriminative SVM filters? How important is joint parameter estimation? How many negative images are needed for training?). Our findings yield useful insights for researchers working with Markov random fields and partbased models, and have practical implications for speeding up tasks such as model selection.', '저자': 'Ross Girshick, Jitendra Malik', '전체 인용횟수': '35회 인용20142015201620172018201920202021202261171232', '컨퍼런스': 'Proceedings of the IEEE International Conference on Computer Vision', '페이지': '3016-3023', '학술 문서': 'Training deformable part models with decorrelated featuresR Girshick, J Malik\\xa0- Proceedings of the IEEE International Conference on\\xa0…, 201335회 인용 관련 학술자료 전체 10개의 버전 '}, title='Training deformable part models with decorrelated features', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'PyTorchVideo: A deep learning library for video understanding': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021/10/17', '도서': 'Proceedings of the 29th ACM International Conference on Multimedia', '설명': 'We introduce PyTorchVideo, an open-source deep-learning library that provides a rich set of modular, efficient, and reproducible components for a variety of video understanding tasks, including classification, detection, self-supervised learning, and low-level processing. The library covers a full stack of video understanding tools including multimodal data loading, transformations, and models that reproduce state-of-the-art performance. PyTorchVideo further supports hardware acceleration that enables real-time inference on mobile devices. The library is based on PyTorch and can be used by any training framework; for example, PyTorchLightning, PySlowFast, or Classy Vision. PyTorchVideo is available at https://pytorchvideo.org/.', '저자': 'Haoqi Fan, Tullie Murrell, Heng Wang, Kalyan Vasudev Alwala, Yanghao Li, Yilei Li, Bo Xiong, Nikhila Ravi, Meng Li, Haichuan Yang, Jitendra Malik, Ross Girshick, Matt Feiszli, Aaron Adcock, Wan-Yen Lo, Christoph Feichtenhofer', '전체 인용횟수': '33회 인용20212022202322110', '페이지': '3783-3786', '학술 문서': 'PyTorchVideo: A deep learning library for video understandingH Fan, T Murrell, H Wang, KV Alwala, Y Li, Y Li…\\xa0- Proceedings of the 29th ACM international conference\\xa0…, 202133회 인용 관련 학술자료 전체 5개의 버전 PyTorchVideoH Fan, T Murrell, H Wang, KV Alwala, Y Li, Y Li…\\xa0- Proceedings of the 29th ACM International Conference\\xa0…, 2021'}, title='PyTorchVideo: A deep learning library for video understanding', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Predicting joint positions': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/10/29', '발명자': 'Jamie Daniel Joseph Shotton, Pushmeet Kohli, Ross Brook Girshick, Andrew Fitzgibbon, Antonio Criminisi', '설명': 'Predicting joint positions is described, for example, to find joint positions of humans or animals (or parts thereof) in an image to control a computer game or for other applications. In an embodiment image elements of a depth image make joint position votes so that for example, an image element depicting part of a torso may vote for a position of a neck joint, a left knee joint and a right knee joint. A random decision forest may be trained to enable image elements to vote for the positions of one or more joints and the training process may use training images of bodies with specified joint positions. In an example a joint position vote is expressed as a vector representing a distance and a direction of a joint position from an image element making the vote. The random decision forest may be trained using a mixture of objectives.', '전체 인용횟수': '33회 인용2013201420152016201720182019202020212022161356533', '출원번호': '13050858', '특허 번호': '8571263', '특허청': 'US', '학술 문서': 'Predicting joint positionsJDJ Shotton, P Kohli, RB Girshick, A Fitzgibbon…\\xa0- US Patent 8,571,263, 201333회 인용 관련 학술자료 전체 4개의 버전 '}, title='Predicting joint positions', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Discriminatively activated sparselets': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/2/13', '게시자': 'PMLR', '설명': 'Shared representations are highly appealing due to their potential for gains in computational and statistical efficiency. Compressing a shared representation leads to greater computational savings, but at the same time can severely decrease performance on a target task. Recently, sparselets (Song et al., 2012) were introduced as a new shared intermediate representation for multiclass object detection with deformable part models (Felzenszwalb et al., 2010a), showing significant speedup factors, but with a large decrease in task performance. In this paper we describe a new training framework that learns which sparselets to activate in order to optimize a discriminative objective, leading to larger speedup factors with no decrease in task performance. We first reformulate sparselets in a general structured output prediction framework, then analyze when sparselets lead to computational efficiency gains, and lastly show experimental results on object detection and image classification tasks. Our experimental results demonstrate that discriminative activation substantially outperforms the previous reconstructive approach which, together with our structured output prediction formulation, make sparselets broadly applicable and significantly more effective.', '저자': 'Ross Girshick, Hyun Oh Song, Trevor Darrell', '전체 인용횟수': '33회 인용20132014201520162017201820192020212102131', '컨퍼런스': 'International Conference on Machine Learning', '페이지': '196-204', '학술 문서': 'Discriminatively activated sparseletsR Girshick, HO Song, T Darrell\\xa0- International Conference on Machine Learning, 201333회 인용 관련 학술자료 전체 12개의 버전 '}, title='Discriminatively activated sparselets', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Machine-learning models based on non-local neural networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2023/1/24', '발명자': 'Kaiming He, Ross Girshick, Xiaolong Wang', '설명': 'In one embodiment, a method includes training a baseline machine-learning model based on a neural network comprising a plurality of stages, wherein each stage comprises a plurality of neural blocks, accessing a plurality of training samples comprising a plurality of content objects, respectively, determining one or more non-local operations, wherein each non-local operation is based on one or more pairwise functions and one or more unary functions, generating one or more non-local blocks based on the plurality of training samples and the one or more non-local operations, determining a stage from the plurality of stages of the neural network, and training a non-local machine-learning model by inserting each of the one or more non-local blocks in between at least two of the plurality of neural blocks in the determined stage of the neural network.', '전체 인용횟수': '29회 인용202020212022202387113', '출원번호': '16192649', '특허 번호': '11562243', '특허청': 'US', '학술 문서': 'Machine-learning models based on non-local neural networksK He, R Girshick, X Wang\\xa0- US Patent 11,562,243, 202329회 인용 관련 학술자료 전체 4개의 버전 '}, title='Machine-learning models based on non-local neural networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Systems and methods for optimizing pose estimation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020/8/4', '발명자': 'Peizhao Zhang, Peter Vajda, Kevin Matzen, Ross Girshick', '설명': 'In one embodiment, a system may access first, second, and third probability models that are respectively associated with predetermined first and second body parts and a predetermined segment connecting the first and second body parts. Each model includes probability values associated with regions in an image, with each value representing the probability of the associated region containing the associated body part or segment. The system may select a first and second region based on the first probability model and a third region based on the second probability model. Based on the third probability model, the system may compute a first probability score for regions connecting the first and third regions and a second probability score for regions connecting the second and third regions. Based on the first and second probability scores, the system may select the first region to indicate where the predetermined first\\xa0…', '전체 인용횟수': '26회 인용202020212022202321266', '출원번호': '16236974', '특허 번호': '10733431', '특허청': 'US', '학술 문서': 'Systems and methods for optimizing pose estimationP Zhang, P Vajda, K Matzen, R Girshick\\xa0- US Patent 10,733,431, 202026회 인용 관련 학술자료 전체 4개의 버전 '}, title='Systems and methods for optimizing pose estimation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Revisiting weakly supervised pre-training of visual perception models': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2022', '설명': 'Model pre-training is a cornerstone of modern visual recognition systems. Although fully supervised pre-training on datasets like ImageNet is still the de-facto standard, recent studies suggest that large-scale weakly supervised pre-training can outperform fully supervised approaches. This paper revisits weakly-supervised pre-training of models using hashtag supervision with modern versions of residual networks and the largest-ever dataset of images and corresponding hashtags. We study the performance of the resulting models in various transfer-learning settings including zero-shot transfer. We also compare our models with those obtained via large-scale self-supervised learning. We find our weakly-supervised models to be very competitive across all settings, and find they substantially outperform their self-supervised counterparts. We also include an investigation into whether our models learned potentially troubling associations or stereotypes. Overall, our results provide a compelling argument for the use of weakly supervised learning in the development of visual recognition systems. Our models, Supervised Weakly through hashtAGs (SWAG), are available publicly.', '저자': 'Mannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de Freitas Reis, Bugra Gedik, Raj Prateek Kosaraju, Dhruv Mahajan, Ross Girshick, Piotr Dollár, Laurens Van Der Maaten', '전체 인용횟수': '25회 인용20222023419', '컨퍼런스': 'Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition', '페이지': '804-814', '학술 문서': 'Revisiting weakly supervised pre-training of visual perception modelsM Singh, L Gustafson, A Adcock, V de Freitas Reis…\\xa0- Proceedings of the IEEE/CVF Conference on Computer\\xa0…, 202225회 인용 관련 학술자료 전체 7개의 버전 '}, title='Revisiting weakly supervised pre-training of visual perception models', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Generalized sparselet models for real-time multiclass object recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/9/4', '게시자': 'IEEE', '권': '37', '설명': 'The problem of real-time multiclass object recognition is of great practical importance in object recognition. In this paper, we describe a framework that simultaneously utilizes shared representation, reconstruction sparsity, and parallelism to enable real-time multiclass object detection with deformable part models at 5Hz on a laptop computer with almost no decrease in task performance. Our framework is trained in the standard structured output prediction formulation and is generically applicable for speeding up object recognition systems where the computational bottleneck is in multiclass, multi-convolutional inference. We experimentally demonstrate the efficiency and task performance of our method on PASCAL VOC, subset of ImageNet, Caltech101 and Caltech256 dataset.', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Hyun Oh Song, Ross Girshick, Stefan Zickler, Christopher Geyer, Pedro Felzenszwalb, Trevor Darrell', '전체 인용횟수': '25회 인용201520162017201820192020202120222435317', '페이지': '1001-1012', '학술 문서': 'Generalized sparselet models for real-time multiclass object recognitionHO Song, R Girshick, S Zickler, C Geyer…\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 201425회 인용 관련 학술자료 전체 11개의 버전 ', '호': '5'}, title='Generalized sparselet models for real-time multiclass object recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Visibility constraints on features of 3D objects': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009/6/20', '게시자': 'IEEE', '설명': 'To recognize three-dimensional objects it is important to model how their appearances can change due to changes in viewpoint. A key aspect of this involves understanding which object features can be simultaneously visible under different viewpoints. We address this problem in an image-based framework, in which we use a limited number of images of an object taken from unknown viewpoints to determine which subsets of features might be simultaneously visible in other views. This leads to the problem of determining whether a set of images, each containing a set of features, is consistent with a single 3D object. We assume that each feature is visible from a disk of viewpoints on the viewing sphere. In this case we show the problem is NP-hard in general, but can be solved efficiently when all views come from a circle on the viewing sphere. We also give iterative algorithms that can handle noisy data and\\xa0…', '저자': 'Ronen Basri, Pedro F Felzenszwalb, Ross B Girshick, David W Jacobs, Caroline J Klivans', '전체 인용횟수': '22회 인용20092010201120122013201420152016201720182019202020212022211124312112', '컨퍼런스': '2009 IEEE Conference on Computer Vision and Pattern Recognition', '페이지': '1231-1238', '학술 문서': 'Visibility constraints on features of 3D objectsR Basri, PF Felzenszwalb, RB Girshick, DW Jacobs…\\xa0- 2009 IEEE Conference on Computer Vision and\\xa0…, 200922회 인용 관련 학술자료 전체 21개의 버전 '}, title='Visibility constraints on features of 3D objects', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Discriminatively trained mixtures of deformable part models': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2008', '설명': 'Discriminatively Trained Mixtures of Deformable Part Models Page 1 Discriminatively \\nTrained Mixtures of Deformable Part Models Pedro Felzenszwalb and Ross Girshick \\nUniversity of Chicago David McAllester Toyota Technological Institute at Chicago Deva \\nRamanan UC Irvine http://www.cs.uchicago.edu/~pff/latent Page 2 Model Overview • \\nMixture of deformable part models (pictorial structures) • Each component has global \\ntemplate + deformable parts • Fully trained from bounding boxes alone Page 3 2 component \\nbicycle model root filters coarse resolution part filters finer resolution deformation models \\nPage 4 Object Hypothesis Image pyramid HOG feature pyramid Multiscale model captures \\nfeatures at two resolutions Score of object hypothesis is sum of filter scores minus \\ndeformation costs Score of filter is dot product of filter with HOG features underneath it Page \\n5 Connection with linear classifier concatenation …', '저널': 'PASCAL VOC Challenge', '저자': 'Pedro Felzenszwalb, Ross Girshick, David McAllester, Deva Ramanan', '전체 인용횟수': '16회 인용2009201020112012201320142015201620172018201920202021202220233118111', '학술 문서': 'Discriminatively trained mixtures of deformable part modelsP Felzenszwalb, R Girshick, D McAllester, D Ramanan\\xa0- PASCAL VOC Challenge, 200816회 인용 관련 학술자료 전체 9개의 버전 '}, title='Discriminatively trained mixtures of deformable part models', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Simulating Chinese brush painting: the parametric hairy brush': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2004/8/8', '도서': 'ACM SIGGRAPH 2004 Posters', '설명': 'In Chinese brush painting each stroke exists on its own as a piece of art. The distinct characteristics of a brush stroke come from the combination of a painter’s skill with the unique physical properties of the paper, ink, and brush. As an artist paints the brush bristles deposit ink and water on the paper, the ink and water diffuse through the paper, and the bristles deform due to external forces.Digitizing the painting process requires a solution to three distinct problems: modeling ink and water diffusion in paper, modeling soft brush dynamics, and creating a stroke input method. Recent research demonstrates that solutions to the latter two problems are more fundamental for capturing the style of Chinese brush painting [Chu and Tai 2002]. Until recently, however, nearly all research has focused exclusively on the first problem. The Parametric Hairy Brush (PHB) is a new brush model designed to realistically simulate the\\xa0…', '저자': 'Ross B Girshick', '전체 인용횟수': '15회 인용200620072008200920102011201220132014201520162017201820192020202122122111111', '페이지': '22', '학술 문서': 'Simulating Chinese brush painting: the parametric hairy brushRB Girshick\\xa0- ACM SIGGRAPH 2004 Posters, 200415회 인용 관련 학술자료 전체 5개의 버전 '}, title='Simulating Chinese brush painting: the parametric hairy brush', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The effectiveness of MAE pre-pretraining for billion-scale pretraining': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2023/3/23', '저널': 'arXiv preprint arXiv:2303.13496', '저자': 'Mannat Singh, Quentin Duval, Kalyan Vasudev Alwala, Haoqi Fan, Vaibhav Aggarwal, Aaron Adcock, Armand Joulin, Piotr Dollár, Christoph Feichtenhofer, Ross Girshick, Rohit Girdhar, Ishan Misra', '전체 인용횟수': '9회 인용20239', '학술 문서': 'The effectiveness of MAE pre-pretraining for billion-scale pretrainingM Singh, Q Duval, KV Alwala, H Fan, V Aggarwal…\\xa0- arXiv preprint arXiv:2303.13496, 20239회 인용 관련 학술자료 전체 2개의 버전 '}, title='The effectiveness of MAE pre-pretraining for billion-scale pretraining', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Large scale weakly and semi-supervised learning for low-resource video ASR': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020/5/16', '설명': 'Many semi- and weakly-supervised approaches have been investigated for overcoming the labeling cost of building high quality speech recognition systems. On the challenging task of transcribing social media videos in low-resource conditions, we conduct a large scale systematic comparison between two self-labeling methods on one hand, and weakly-supervised pretraining using contextual metadata on the other. We investigate distillation methods at the frame level and the sequence level for hybrid, encoder-only CTC-based, and encoder-decoder speech recognition systems on Dutch and Romanian languages using 27,000 and 58,000 hours of unlabeled audio respectively. Although all approaches improved upon their respective baseline WERs by more than 8%, sequence-level distillation for encoder-decoder models provided the largest relative WER reduction of 20% compared to the strongest data-augmented supervised baseline.', '저널': 'arXiv preprint arXiv:2005.07850', '저자': 'Kritika Singh, Vimal Manohar, Alex Xiao, Sergey Edunov, Ross Girshick, Vitaliy Liptchinsky, Christian Fuegen, Yatharth Saraf, Geoffrey Zweig, Abdelrahman Mohamed', '전체 인용횟수': '9회 인용202020212022153', '학술 문서': 'Large scale weakly and semi-supervised learning for low-resource video asrK Singh, V Manohar, A Xiao, S Edunov, R Girshick…\\xa0- arXiv preprint arXiv:2005.07850, 20209회 인용 관련 학술자료 전체 6개의 버전 '}, title='Large scale weakly and semi-supervised learning for low-resource video ASR', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Training asr models by generation of contextual information': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020/5/4', '게시자': 'IEEE', '설명': 'Supervised ASR models have reached unprecedented levels of accuracy, thanks in part to ever-increasing amounts of labelled training data. However, in many applications and locales, only moderate amounts of data are available, which has led to a surge in semi- and weakly-supervised learning research. In this paper, we conduct a large-scale study evaluating the effectiveness of weakly-supervised learning for speech recognition by using loosely related contextual information as a surrogate for ground-truth labels. For weakly supervised training, we use 50k hours of public English social media videos along with their respective titles and post text to train an encoder-decoder transformer model. Our best encoder-decoder models achieve an average of 20.8% WER reduction over a 1000 hours supervised baseline, and an average of 13.4% WER reduction when using only the weakly supervised encoder for CTC\\xa0…', '저자': 'Kritika Singh, Dmytro Okhonko, Jun Liu, Yongqiang Wang, Frank Zhang, Ross Girshick, Sergey Edunov, Fuchun Peng, Yatharth Saraf, Geoffrey Zweig, Abdelrahman Mohamed', '전체 인용횟수': '6회 인용2020202120222023231', '컨퍼런스': 'ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)', '페이지': '7864-7868', '학술 문서': 'Training asr models by generation of contextual informationK Singh, D Okhonko, J Liu, Y Wang, F Zhang…\\xa0- ICASSP 2020-2020 IEEE International Conference on\\xa0…, 20206회 인용 관련 학술자료 전체 3개의 버전 '}, title='Training asr models by generation of contextual information', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Discriminative Latent Variable Models for Object Detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010', '설명': '(To be presented by Deva Ramanan). In this talk, I will discuss recent work by colleagues and myself on discriminative latent-variable models for object detection. Object recognition is one of the fundamental challenges of computer vision. We specifically consider the task of localizing and detecting instances of a generic object category, such as people or cars, in cluttered real-word images. Recent benchmark competitions such as the PASCAL Visual Object Challenge suggest our method is the state-of-the-art system for such tasks. This success, combined with publicallyavailable code that runs orders of magnitude faster than comparable approaches, has turned our system into a standard baseline for contemporary research on object recognition (Felzenszwalb et al., 2008; 2009).This talk will focus on the machine learning aspects of our approach. Our system is trained with a latent variable extension of support vector machines that we call a latent SVM. The formulation is equivalent to the MI-SVM framework for multiple instance learning. Latent variables provide a formalism for modeling structured variation in object appearance due to deformation, viewpoint, and other factors. The resulting learning problem is no longer convex, but admits a coordinate descent algorithm that exploits a ‘semiconvex’property. Notable aspects of our system involve (a) weakly-supervised learning, in which hidden latent structure is automatically inferred;(b) out-ofcore learning algorithms for learning from large-scale datasets that do not fit in memory; and (c) efficient algorithms for searching over latent variables.', '저자': 'Pedro Felzenszwalb, Ross Girshick, David McAllester, Deva Ramanan, UC Irvine', '전체 인용횟수': '4회 인용20142015201631', '컨퍼런스': 'ICML', '학술 문서': 'Discriminative latent variable models for object detectionPF Felzenszwalb, RB Girshick, DA McAllester…\\xa0- Proceedings of the 27th International Conference on\\xa0…, 20104회 인용 관련 학술자료 전체 6개의 버전 '}, title='Discriminative Latent Variable Models for Object Detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Object detection with heuristic coarse-to-fine search': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009/5/29', '기관': \"Master's Thesis. Chicago: University of Chicago\", '설명': 'We consider the task of localizing and labeling instances of a generic object class within real-world images. Our focus is on a generalized class of pictorial structure models that are defined in terms of visual grammars. In particular, we address the challenging problem of performing detection efficiently even as model complexity grows within this class. Our proposed solution is a blend of heuristic best-first search and a coarse-to-fine detection process. This paper demonstrates that our algorithm can be successfully applied to two special cases of visual grammars: multiscale star models and mixtures of multiscale star models. We show that for problems where the desired output is the local optima of a thresholded function, best-first search gives additional pruning power to coarse-to-fine processes. Unfortunately, admissible heuristics that also provide good best-first search behavior can be difficult or impossible to find in practice. To resolve this deficiency, we provide theoretical results demonstrating that inadmissible heuristics can be used to increase detection speed while only slightly increasing the likelihood of suffering mistakes. The theoretical results are bolstered by strong experimental evidence obtained by applying inadmissible heuristic coarse-to-fine detection to our object recognition system during both training and testing. We increase testing speed by a factor of 2-3 for some classes while maintaining comparable average precision scores on the challenging PASCAL 2007 dataset. Ultimately we expect to see even more significant speed gains when we explore more complex grammar models in future work.', '저자': 'Ross Girshick', '전체 인용횟수': '3회 인용20172018201920202021202212', '학술 문서': 'Object detection with heuristic coarse-to-fine searchR Girshick - 20093회 인용 관련 학술자료 전체 3개의 버전 '}, title='Object detection with heuristic coarse-to-fine search', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Very deep convolutional networks for large-scale image recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/9/4', '설명': 'In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.', '저널': 'arXiv preprint arXiv:1409.1556', '저자': 'Karen Simonyan, Andrew Zisserman', '전체 인용횟수': '114016회 인용20152016201720182019202020212022202369726186048108641494217120204842121917520', '학술 문서': 'Very deep convolutional networks for large-scale image recognitionK Simonyan, A Zisserman\\xa0- arXiv preprint arXiv:1409.1556, 2014113275회 인용 관련 학술자료 전체 43개의 버전 Very deep convolutional networks for large-scale image recognition. arXiv 2014K Simonyan, A Zisserman\\xa0- arXiv preprint arXiv:1409.1556, 20144531회 인용 관련 학술자료 Very deep convolutional networks for large-scale image recognition. arXiv preprint (2014)K Simonyan, A Zisserman\\xa0- arXiv preprint arXiv:1409.1556, 2014206회 인용 관련 학술자료 Very deep convolutional networks for large-scale image recognition. arXiv: 14091556*K Simonyan, A Zisserman - 2014187회 인용 관련 학술자료 Very deep convolutional neural networks for large-scale image recognition*K Simonyan, A Zisserman\\xa0- Proceedings of the International Conference on\\xa0…, 2013106회 인용 관련 학술자료 Very deep convnets for large-scale image recognition*K Simonyan, A Zisserman\\xa0- Computing Research Repository, 20149회 인용 관련 학술자료 전체 5개의 버전 Very Deep Convolutional Networks for Large-Scale Image Recognition. 1409.1556 [cs]*K Simonyan, A Zisserman - 20143회 인용 관련 학술자료 '}, title='Very deep convolutional networks for large-scale image recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Multiple view geometry in computer vision': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2000', '설명': 'A basic problem in computer vision is to understand the structure of a real world scene given several images of it. Techniques for solving this problem are taken from projective geometry and photogrammetry. Here, the authors cover the geometric principles and their algebraic representation in terms of camera projection matrices, the fundamental matrix and the trifocal tensor. The theory and methods of computation of these entities are discussed with real examples, as is their use in the reconstruction of scenes from multiple images. The new edition features an extended introduction covering the key ideas in the book (which itself has been updated with additional examples and appendices) and significant new results which have appeared since the first edition. Comprehensive background material is provided, so readers familiar with linear algebra and basic numerical methods can understand the projective geometry and estimation algorithms presented, and implement the algorithms directly from the book.', '저자': 'Richard Hartley, A. Zisserman', '전체 인용횟수': '33978회 인용20012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023184368578725949120313801562169118711865196220462083200619241816175517181655159114161186', '학술 문서': 'Multiple view geometry in computer visionR Hartley, A Zisserman - 200333772회 인용 관련 학술자료 전체 21개의 버전 Multiple view geometry in computer vision*AM Andrew\\xa0- Kybernetes, 2001225회 인용 관련 학술자료 전체 2개의 버전 Multiple view geometry in computer*R Hartley, A Zisserman\\xa0- Vision, 2nd ed., New York: Cambridge, 20036회 인용 관련 학술자료 '}, title='Multiple view geometry in computer vision', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The pascal visual object classes (voc) challenge': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/6/1', '게시자': 'Springer Netherlands', '권': '88', '설명': ' The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.', '저널': 'International journal of computer vision', '저자': 'Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, Andrew Zisserman', '전체 인용횟수': '20413회 인용20092010201120122013201420152016201720182019202020212022202360912493584986278449331137149519652335283630162695', '페이지': '303-338', '학술 문서': 'The pascal visual object classes (voc) challengeM Everingham, L Van Gool, CKI Williams, J Winn…\\xa0- International journal of computer vision, 201020410회 인용 관련 학술자료 전체 28개의 버전 The pascal voc challengeM Everingham, L Van Gool, CKI Williams, J Winn…\\xa0- Int. J. Comput. Vis5회 인용 관련 학술자료 Thepascal visual object classes (voc) challenge*M Everingham, L Van Gool, CKI Williams, J Winn…\\xa0- International Journal of ComputerVision, 88 (2): 303\\xa0…4회 인용 관련 학술자료 ', '호': '2'}, title='The pascal visual object classes (voc) challenge', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Two-stream convolutional networks for action recognition in videos': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '권': '27', '설명': 'We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multi-task learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.', '저널': 'Advances in neural information processing systems', '저자': 'Karen Simonyan, Andrew Zisserman', '전체 인용횟수': '8546회 인용20152016201720182019202020212022202314338271310511270130713721170995', '학술 문서': 'Two-stream convolutional networks for action recognition in videosK Simonyan, A Zisserman\\xa0- Advances in neural information processing systems, 20148546회 인용 관련 학술자료 전체 18개의 버전 '}, title='Two-stream convolutional networks for action recognition in videos', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Video Google: A text retrieval approach to object matching in videos': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2003/10/13', '게시자': 'IEEE', '설명': 'We describe an approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video. The object is represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion. The temporal continuity of the video within a shot is used to track the regions in order to reject unstable regions and reduce the effects of noise in the descriptors. The analogy with text retrieval is in the implementation where matches on descriptors are pre-computed (using vector quantization), and inverted file systems and document rankings are used. The result is that retrieved is immediate, returning a ranked list of key frames/shots in the manner of Google. The method is illustrated for matching in two full length feature films.', '저자': 'Josef Sivic, Andrew Zisserman', '전체 인용횟수': '8397회 인용200420052006200720082009201020112012201320142015201620172018201920202021202220233978117165238326494518643738765834676621503398364320259170', '컨퍼런스': 'ICCV 2003', '페이지': '1470', '학술 문서': 'Video Google: A text retrieval approach to object matching in videosSivic, Zisserman\\xa0- Proceedings ninth IEEE international conference on\\xa0…, 20038394회 인용 관련 학술자료 전체 39개의 버전 Zisserman, and W*J Sivic, B Russell, A Efros - 20052회 인용 관련 학술자료 Video Google*J Sivic, A Zisserman, F Schaffalitzky1회 인용 관련 학술자료 전체 3개의 버전 '}, title='Video Google: A text retrieval approach to object matching in videos', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Spatial transformer networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '권': '28', '설명': 'Convolutional Neural Networks define an exceptionallypowerful class of model, but are still limited by the lack of abilityto be spatially invariant to the input data in a computationally and parameterefficient manner. In this work we introduce a new learnable module, theSpatial Transformer, which explicitly allows the spatial manipulation ofdata within the network. This differentiable module can be insertedinto existing convolutional architectures, giving neural networks the ability toactively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the useof spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-artperformance on several benchmarks, and for a numberof classes of transformations.', '저널': 'Advances in neural information processing systems', '저자': 'Max Jaderberg, Karen Simonyan, Andrew Zisserman', '전체 인용횟수': '8075회 인용2015201620172018201920202021202220232517537773511331281148714961282', '학술 문서': 'Spatial transformer networksM Jaderberg, K Simonyan, A Zisserman\\xa0- Advances in neural information processing systems, 20158075회 인용 관련 학술자료 전체 15개의 버전 '}, title='Spatial transformer networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Quo vadis, action recognition? a new model and the kinetics dataset': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.2% on HMDB-51 and 97.9% on UCF-101.', '저자': 'Joao Carreira, Andrew Zisserman', '전체 인용횟수': '7786회 인용2017201820192020202120222023283547831174168718861830', '컨퍼런스': 'proceedings of the IEEE Conference on Computer Vision and Pattern Recognition', '페이지': '6299-6308', '학술 문서': 'Quo vadis, action recognition? a new model and the kinetics datasetJ Carreira, A Zisserman\\xa0- proceedings of the IEEE Conference on Computer\\xa0…, 20177630회 인용 관련 학술자료 전체 14개의 버전 Quo vadis, action recognition*J Carreira, A Zisserman\\xa0- A new model and the kinetics dataset. CoRR, abs\\xa0…, 2017288회 인용 관련 학술자료 '}, title='Quo vadis, action recognition? a new model and the kinetics dataset', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep inside convolutional networks: Visualising image classification models and saliency maps': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/12/20', '설명': 'This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].', '저널': 'arXiv preprint arXiv:1312.6034', '저자': 'Karen Simonyan, Andrea Vedaldi, Andrew Zisserman', '전체 인용횟수': '7404회 인용201420152016201720182019202020212022202321842163506238561057137814411305', '학술 문서': 'Deep inside convolutional networks: Visualising image classification models and saliency mapsK Simonyan, A Vedaldi, A Zisserman\\xa0- arXiv preprint arXiv:1312.6034, 20137286회 인용 관련 학술자료 전체 11개의 버전 Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv 2013K Simonyan, A Vedaldi, A Zisserman\\xa0- arXiv preprint arXiv:1312.6034, 2019309회 인용 관련 학술자료 '}, title='Deep inside convolutional networks: Visualising image classification models and saliency maps', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The pascal visual object classes challenge: A retrospective': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/1', '게시자': 'Springer US', '권': '111', '설명': ' The Pascal Visual Object Classes (VOC) challenge consists of two components: (i)\\xa0a publicly available dataset of images together with ground truth annotation and standardised evaluation software; and (ii)\\xa0an annual competition and workshop. There are five challenges: classification, detection, segmentation, action classification, and person layout. In this paper we provide a review of the challenge from 2008–2012. The paper is intended for two audiences: algorithm designers, researchers who want to see what the state of the art is, as measured by performance on the VOC datasets, along with the limitations and weak points of the current generation of algorithms; and, challenge designers, who want to see what we as organisers have learnt from the process and our recommendations for the organisation of future challenges. To analyse the performance of submitted algorithms on the VOC datasets we\\xa0…', '저널': 'International journal of computer vision', '저자': 'Mark Everingham, SM Ali Eslami, Luc Van Gool, Christopher KI Williams, John Winn, Andrew Zisserman', '전체 인용횟수': '6294회 인용20142015201620172018201920202021202220232811725836459278786810511018992', '페이지': '98-136', '학술 문서': 'The pascal visual object classes challenge: A retrospectiveM Everingham, SMA Eslami, L Van Gool, CKI Williams…\\xa0- International journal of computer vision, 20156294회 인용 관련 학술자료 전체 37개의 버전 ', '호': '1'}, title='The pascal visual object classes challenge: A retrospective', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep face recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '게시자': 'British Machine Vision Association', '설명': 'The goal of this paper is face recognition – from either a single photograph or from a set of faces tracked in a video. Recent progress in this area has been due to two factors: (i) end to end learning for the task using a convolutional neural network (CNN), and (ii) the availability of very large scale training datasets. We make two contributions: first, we show how a very large scale dataset (2.6M images, over 2.6K people) can be assembled by a combination of automation and human in the loop, and discuss the trade off between data purity and time; second, we traverse through the complexities of deep network training and face recognition to present methods and procedures to achieve comparable state of the art results on the standard LFW and YTF face benchmarks.', '저널': 'BMVC 2015-Proceedings of the British Machine Vision Conference 2015', '저자': 'Omkar Parkhi, Andrea Vedaldi, Andrew Zisserman', '전체 인용횟수': '6011회 인용201520162017201820192020202120222023202495939301063963883678515', '학술 문서': 'Deep face recognitionO Parkhi, A Vedaldi, A Zisserman\\xa0- BMVC 2015-Proceedings of the British Machine Vision\\xa0…, 20156011회 인용 관련 학술자료 전체 12개의 버전 '}, title='Deep face recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A comparison of affine region detectors': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2005', '권': '65', '저널': 'International Journal of Computer Vision', '저자': 'C Schmid A Zisserman J Matas, F Schaffalitzky T Kadir K Mikolajczyk, T Tuytelaars, L Van Gool', '전체 인용횟수': '4416회 인용20042005200620072008200920102011201220132014201520162017201820192020202120222023133012215925031236836832236031533830424621819714311010373', '페이지': '43-72', '학술 문서': 'A comparison of affine region detectors*K Mikolajczyk, T Tuytelaars, C Schmid, A Zisserman…\\xa0- International journal of computer vision, 20054412회 인용 관련 학술자료 전체 41개의 버전 A comparison of affine region detectors*T Tuytelaars, C Schmid, A Zisser-Man, J Matas…\\xa0- International journal of computer vision, 20054회 인용 관련 학술자료 T. et al.(2005). A comparison of affine region detectors*KT MiNolajczyN\\xa0- International Journal of Computer Vision2회 인용 관련 학술자료 ', '호': '1/2'}, title='A comparison of affine region detectors', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Return of the devil in the details: Delving deep into convolutional nets': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/5/14', '설명': 'The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source code and models to reproduce the experiments in the paper is made publicly available.', '저널': 'arXiv preprint arXiv:1405.3531', '저자': 'Ken Chatfield, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman', '전체 인용횟수': '4162회 인용201420152016201720182019202020212022202329265483664678595446348327211', '학술 문서': 'Return of the devil in the details: Delving deep into convolutional netsK Chatfield, K Simonyan, A Vedaldi, A Zisserman\\xa0- arXiv preprint arXiv:1405.3531, 20144162회 인용 관련 학술자료 전체 13개의 버전 '}, title='Return of the devil in the details: Delving deep into convolutional nets', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Object retrieval with large vocabularies and fast spatial matching': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2007/6/17', '게시자': 'IEEE', '설명': 'In this paper, we present a large-scale object retrieval system. The user supplies a query object by selecting a region of a query image, and the system returns a ranked list of images that contain the same object, retrieved from a large corpus. We demonstrate the scalability and performance of our system on a dataset of over 1 million images crawled from the photo-sharing site, Flickr [3], using Oxford landmarks as queries. Building an image-feature vocabulary is a major time and performance bottleneck, due to the size of our dataset. To address this problem we compare different scalable methods for building a vocabulary and introduce a novel quantization method based on randomized trees which we show outperforms the current state-of-the-art on an extensive ground-truth. Our experiments show that the quantization has a major effect on retrieval quality. To further improve query performance, we add an efficient\\xa0…', '저자': 'James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, Andrew Zisserman', '전체 인용횟수': '3701회 인용200620072008200920102011201220132014201520162017201820192020202120222023111556100176206253306294353351289279231205209161141', '컨퍼런스': '2007 IEEE conference on computer vision and pattern recognition', '페이지': '1-8', '학술 문서': 'Object retrieval with large vocabularies and fast spatial matchingJ Philbin, O Chum, M Isard, J Sivic, A Zisserman\\xa0- 2007 IEEE conference on computer vision and pattern\\xa0…, 20073701회 인용 관련 학술자료 전체 19개의 버전 '}, title='Object retrieval with large vocabularies and fast spatial matching', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The kinetics human action video dataset': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/5/19', '설명': 'We describe the DeepMind Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers.', '저널': 'arXiv preprint arXiv:1705.06950', '저자': 'Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, Andrew Zisserman', '전체 인용횟수': '3632회 인용201720182019202020212022202324196366513794889828', '학술 문서': 'The kinetics human action video datasetW Kay, J Carreira, K Simonyan, B Zhang, C Hillier…\\xa0- arXiv preprint arXiv:1705.06950, 20173632회 인용 관련 학술자료 전체 4개의 버전 '}, title='The kinetics human action video dataset', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The PASCAL visual object classes challenge 2008 (VOC2008) results': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2008', '설명': 'The PASCAL Visual Object Classes Challenge 2008 (VOC2008) Results | CiNii Research \\nCiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ] 論文・データをさがす 大学図書館の本を\\nさがす 日本の博士論文をさがす English 検索 タイトル 人物/団体名 所属機関 ISSN DOI 期間 ~ \\n本文リンク 本文リンクあり データソース JaLC IRDB Crossref DataCite NDL NDL-Digital IDR \\nJDCat NINJAL CiNii Articles CiNii Books CiNii Dissertations RUDA DBpedia Nikkei BP \\nKAKEN Integbio すべて 研究データ 論文 本 博士論文 プロジェクト [4/18更新]CiNii Articlesの\\nCiNii Researchへの統合について The PASCAL Visual Object Classes Challenge 2008 (VOC2008) \\nResults 被引用文献1件 EVERINGHAM M. 収録刊行物 http://www.pascal-network.org/challenges/VOC/voc2008/year=workshop/index.html \\nhttp://www.pascal-network.org/challenges/VOC/voc2008/year=workshop/index.html 2008 …', '저널': 'http://www. pascal-network. org/challenges/VOC/voc2008/year= workshop/index. html', '저자': 'Mark Everingham', '전체 인용횟수': '3437회 인용20052006200720082009201020112012201320142015201620172018201920202021202220239122451111141144136158173215211253307300301284325237', '학술 문서': 'The PASCAL visual object classes challenge 2008 (VOC2008) resultsM Everingham\\xa0- http://www. pascal-network. org/challenges/VOC\\xa0…, 20083210회 인용 관련 학술자료 전체 2개의 버전 The pascal visual object classes challenge 2006 (voc2006) resultsM Everingham, A Zisserman, CKI Williams, L Van Gool - 2007186회 인용 관련 학술자료 Pascal visual object classes challenge results*M Everingham, L Van Gool, C Williams, J Winn…\\xa0- Available from www. pascal-network. org, 200595회 인용 관련 학술자료 전체 4개의 버전 '}, title='The PASCAL visual object classes challenge 2008 (VOC2008) results', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Visual reconstruction': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1987/11/12', '게시자': 'MIT press', '설명': 'This book deals with vision as a computational problem. It presents visual reconstruction from the mechanical viewpoint, which is more natural for representation of a priori knowledge about visible surfaces or about distributions of visual quantities such as intensity, reflectance, optic flow, and curve orientation. An important class of reconstruction processes is presented. Two new concepts are introduced, analyzed, and illustrated: the weak continuity constraint in vision and the graduated nonconvexity algorithm for fitting piecewise continuous functions to visual data. The chapters of the book are as follows:(1) Modeling Piecewise Continuity (2) Applications of Piecewise Continuous Reconstructions (3) Introduction to Weak Continuity Constraints (4) Properties of the Weak String and Membrane (5) Properties of the Weak Rod and Plates (6) The Discrete Problem (7) The Graduated Non Convexity Algorithm and (8\\xa0…', '저자': 'Andrew Blake, Andrew Zisserman', '전체 인용횟수': '3251회 인용1988198919901991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220232160104119108124151129119127142120103988310779851049598758179918171796154695341564160', '학술 문서': 'Visual reconstructionA Blake, A Zisserman - 19873251회 인용 관련 학술자료 전체 7개의 버전 '}, title='Visual reconstruction', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Automated flower classification over a large number of classes': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2008/12/16', '게시자': 'IEEE', '설명': 'We investigate to what extent combinations of features can improve classification performance on a large dataset of similar classes. To this end we introduce a 103 class flower dataset. We compute four different features for the flowers, each describing different aspects, namely the local shape/texture, the shape of the boundary, the overall spatial distribution of petals, and the colour. We combine the features using a multiple kernel framework with a SVM classifier. The weights for each class are learnt using the method of Varma and Ray, which has achieved state of the art performance on other large dataset, such as Caltech 101/256. Our dataset has a similar challenge in the number of classes, but with the added difficulty of large between class similarity and small within class similarity. Results show that learning the optimum kernel combination of multiple features vastly improves the performance, from 55.1% for\\xa0…', '저자': 'Maria-Elena Nilsback, Andrew Zisserman', '전체 인용횟수': '3145회 인용20092010201120122013201420152016201720182019202020212022202312283743567395133137215260304455552708', '컨퍼런스': '2008 Sixth Indian conference on computer vision, graphics & image processing', '페이지': '722-729', '학술 문서': 'Automated flower classification over a large number of classesME Nilsback, A Zisserman\\xa0- 2008 Sixth Indian conference on computer vision\\xa0…, 20083145회 인용 관련 학술자료 전체 10개의 버전 '}, title='Automated flower classification over a large number of classes', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Convolutional two-stream network fusion for video action recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': 'Recent applications of Convolutional Neural Networks (ConvNets) for human action recognition in videos have proposed different solutions for incorporating the appearance and motion information. We study a number of ways of fusing ConvNet towers both spatially and temporally in order to best take advantage of this spatio-temporal information. We make the following findings:(i) that rather than fusing at the softmax layer, a spatial and temporal network can be fused at a convolution layer without loss of performance, but with a substantial saving in parameters;(ii) that it is better to fuse such networks spatially at the last convolutional layer than earlier, and that additionally fusing at the class prediction layer can boost accuracy; finally (iii) that pooling of abstract convolutional features over spatiotemporal neighbourhoods further boosts performance. Based on these studies we propose a new ConvNet architecture for spatiotemporal fusion of video snippets, and evaluate its performance on standard benchmarks where this architecture achieves state-of-the-art results.', '저자': 'Christoph Feichtenhofer, Axel Pinz, Andrew Zisserman', '전체 인용횟수': '3139회 인용2016201720182019202020212022202317226386498554578465374', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '1933-1941', '학술 문서': 'Convolutional two-stream network fusion for video action recognitionC Feichtenhofer, A Pinz, A Zisserman\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20163139회 인용 관련 학술자료 전체 19개의 버전 '}, title='Convolutional two-stream network fusion for video action recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Object class recognition by unsupervised scale-invariant learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2003/6/18', '게시자': 'IEEE', '권': '2', '설명': 'We present a method to learn and recognize object class models from unlabeled and unsegmented cluttered scenes in a scale invariant manner. Objects are modeled as flexible constellations of parts. A probabilistic representation is used for all aspects of the object: shape, appearance, occlusion and relative scale. An entropy-based feature detector is used to select regions and their scale within the image. In learning the parameters of the scale-invariant object model are estimated. This is done using expectation-maximization in a maximum-likelihood setting. In recognition, this model is used in a Bayesian manner to classify images. The flexible nature of the model is demonstrated by excellent results over a range of datasets including geometrically constrained classes (e.g. faces, cars) and flexible objects (such as animals).', '저자': 'Robert Fergus, Pietro Perona, Andrew Zisserman', '전체 인용횟수': '3043회 인용2003200420052006200720082009201020112012201320142015201620172018201920202021202220231611120224624726524225124421420716515010288645137352424', '컨퍼런스': '2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings.', '페이지': 'II-II', '학술 문서': 'Object class recognition by unsupervised scale-invariant learningR Fergus, P Perona, A Zisserman\\xa0- 2003 IEEE Computer Society Conference on Computer\\xa0…, 20033043회 인용 관련 학술자료 전체 41개의 버전 '}, title='Object class recognition by unsupervised scale-invariant learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'MLESAC: A new robust estimator with application to estimating image geometry': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2000/4/1', '게시자': 'Academic Press', '권': '78', '설명': 'A new method is presented for robustly estimating multiple view relations from point correspondences. The method comprises two parts. The first is a new robust estimator MLESAC which is a generalization of the RANSAC estimator. It adopts the same sampling strategy as RANSAC to generate putative solutions, but chooses the solution that maximizes the likelihood rather than just the number of inliers. The second part of the algorithm is a general purpose method for automatically parameterizing these relations, using the output of MLESAC. A difficulty with multiview image relations is that there are often nonlinear constraints between the parameters, making optimization a difficult task. The parameterization method overcomes the difficulty of nonlinear constraints and conducts a constrained optimization. The method is general and its use is illustrated for the estimation of fundamental matrices, image–image\\xa0…', '저널': 'Computer vision and image understanding', '저자': 'Philip HS Torr, Andrew Zisserman', '전체 인용횟수': '2887회 인용20012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023122641394655627411210988104121124162176225237250217233167171', '페이지': '138-156', '학술 문서': 'MLESAC: A new robust estimator with application to estimating image geometryPHS Torr, A Zisserman\\xa0- Computer vision and image understanding, 20002887회 인용 관련 학술자료 전체 13개의 버전 ', '호': '1'}, title='MLESAC: A new robust estimator with application to estimating image geometry', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/5/28', '게시자': 'Nature Publishing Group UK', '권': '521', '설명': 'Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.', '저자': 'Yann LeCun, Yoshua Bengio, Geoffrey Hinton', '전체 인용횟수': '72040회 인용201520162017201820192020202120222023213165741527073981311535127941276911288', '출처': 'nature', '페이지': '436-444', '학술 문서': 'Deep learningY LeCun, Y Bengio, G Hinton\\xa0- nature, 201571811회 인용 관련 학술자료 전체 89개의 버전 Yoshua bengio, and geoffrey hinton*Y LeCun\\xa0- Deep learning. nature, 2015130회 인용 관련 학술자료 Deep learning*G Hinton, Y LeCun, Y Bengio\\xa0- Nature, 2015112회 인용 관련 학술자료 et Hinton, G.(2015)*Y LeCun, Y Bengio\\xa0- Deep learning. nature31회 인용 관련 학술자료 Deep learning. nature*Y LeCun, Y Bengio, G Hinton\\xa0- Nature, 201821회 인용 관련 학술자료 Imagenet classification with deep convolutional neural networks*Y LeCun, J Denker, D Henderson, R Howard…\\xa0- Advances in neural information processing systems, 199012회 인용 관련 학술자료 Y., Bengio, Y., Hinton*G LeCun\\xa0- Nature, 20158회 인용 관련 학술자료 ', '호': '7553'}, title='Deep learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Gradient-based learning applied to document recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1998/11', '게시자': 'Ieee', '권': '86', '설명': 'Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows\\xa0…', '저자': 'Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner', '전체 인용횟수': '60099회 인용200620072008200920102011201220132014201520162017201820192020202120222023173193204226304360368549814162126203934594675848553918790447592', '출처': 'Proceedings of the IEEE', '페이지': '2278-2324', '학술 문서': 'Gradient-based learning applied to document recognitionY LeCun, L Bottou, Y Bengio, P Haffner\\xa0- Proceedings of the IEEE, 199860099회 인용 관련 학술자료 전체 41개의 버전 ', '호': '11'}, title='Gradient-based learning applied to document recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Backpropagation applied to handwritten zip code recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1989/12', '게시자': 'MIT Press', '권': '1', '설명': 'The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.', '저널': 'Neural computation', '저자': 'Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, Lawrence D Jackel', '전체 인용횟수': '15620회 인용198919901991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220233946739311315110414613816813015012282595454657566758084106131180382593882134016761848215722791659', '페이지': '541-551', '학술 문서': 'Backpropagation applied to handwritten zip code recognitionY LeCun, B Boser, JS Denker, D Henderson…\\xa0- Neural computation, 198915616회 인용 관련 학술자료 전체 15개의 버전 Neural Comput.*Y LeCun, BE Boser, JS Denker, D Henderson… - 19908회 인용 관련 학술자료 Backpropagation Applied to Handwritten Zip Code Recognition, Neural Computation*Y LeCun, B Boser, JS Denker, D Henderson…\\xa0- Neural Computation, 19906회 인용 관련 학술자료 Denker, 1*Y LeCun, B Boser\\xa0- S., Henderson, D., Howard, RE, Hubbard, W., and\\xa0…, 19893회 인용 관련 학술자료 ', '호': '4'}, title='Backpropagation applied to handwritten zip code recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Convolutional networks for images, speech, and time series': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1995/4', '권': '3361', '설명': 'The ability of multilayer back-propagation networks to learn complex, high-dimensional, nonlinear mappings from large collections of examples makes them obvious candidates for image recognition or speech recognition tasks (see PATTERN RECOGNITION AND NEURAL NETWORKS). In the traditional model of pattern recognition, a hand-designed feature extractor gathers relevant information from the input and eliminates irrelevant variabilities. A trainable classi er then categorizes the resulting feature vectors (or strings of symbols) into classes. In this scheme, standard, fully-connected multilayer networks can be used as classi ers. A potentially more interesting scheme is to eliminate the feature extractor, feeding the network with\\\\raw\" inputs (eg normalized images), and to rely on backpropagation to turn the rst few layers into an appropriate feature extractor. While this can be done with an ordinary fully connected feed-forward network with some success for tasks such as character recognition, there are problems.Firstly, typical images, or spectral representations of spoken words, are large, often with several hundred variables. A fully-connected rst layer with, say a few 100 hidden units, would already contain several 10,000 weights. Over tting problems may occur if training data is scarce. In addition, the memory requirement for that many weights may rule out certain hardware implementations. But, the main de ciency of unstructured nets for image or speech aplications is that they have no built-in invariance with respect to translations, or', '저널': 'The handbook of brain theory and neural networks', '저자': 'Yann LeCun, Yoshua Bengio', '전체 인용횟수': '7517회 인용2007200820092010201120122013201420152016201720182019202020212022202323202133333560111210328507753919104612241134878', '페이지': '1995', '학술 문서': 'Convolutional networks for images, speech, and time seriesY LeCun, Y Bengio\\xa0- The handbook of brain theory and neural networks, 19957277회 인용 관련 학술자료 전체 21개의 버전 The handbook of brain theory and neural networks*Y LeCun, Y Bengio - 1998456회 인용 관련 학술자료 ', '호': '10'}, title='Convolutional networks for images, speech, and time series', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/4/14', '게시자': 'arXiv preprint arXiv:1312.6229', '설명': 'We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.', '저자': 'Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, Yann LeCun', '전체 인용횟수': '7323회 인용19881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202319353634384471568380998210688501418122415243329325046171463667765868856748633504341', '컨퍼런스': 'International Conference on Learning Representations (ICLR 2014)', '학술 문서': 'Overfeat: Integrated recognition, localization and detection using convolutional networksP Sermanet, D Eigen, X Zhang, M Mathieu, R Fergus…\\xa0- arXiv preprint arXiv:1312.6229, 20137323회 인용 관련 학술자료 전체 30개의 버전 '}, title='OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The MNIST database of handwritten digits': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1998', '저자': 'Yann LeCun, Corinna Cortes', '전체 인용횟수': '6954회 인용200420052006200720082009201020112012201320142015201620172018201920202021202220231923242741474257507211416427542473085194210411037904', '학술 문서': 'The MNIST database of handwritten digitsY LeCun\\xa0- http://yann. lecun. com/exdb/mnist/, 19986944회 인용 관련 학술자료 전체 2개의 버전 The MNIST Database of handwritten images*Y LeCun, C Cortes, CJC Burgess - 201210회 인용 관련 학술자료 ', '호': 'http://yann.lecun.com/exdb/mnist/'}, title='The MNIST database of handwritten digits', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Efficient backprop': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2002/3/28', '게시자': 'Springer Berlin Heidelberg', '도서': 'Neural networks: Tricks of the trade', '설명': ' The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most “classical” second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.', '저자': 'Yann LeCun, Léon Bottou, Genevieve B Orr, Klaus-Robert Müller', '전체 인용횟수': '6856회 인용19881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202319343734364271548077988111592614537526458709496102138160174231395510651669642668563428', '페이지': '9-50', '학술 문서': 'Efficient backpropY LeCun, L Bottou, GB Orr, KR Müller\\xa0- Neural networks: Tricks of the trade, 20026856회 인용 관련 학술자료 전체 17개의 버전 '}, title='Efficient backprop', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Character-level convolutional networks for text classification': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '권': '28', '설명': 'This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.', '저널': 'Advances in neural information processing systems', '저자': 'Xiang Zhang, Junbo Zhao, Yann LeCun', '전체 인용횟수': '6021회 인용201620172018201920202021202220231423246358629999691097949', '학술 문서': 'Character-level convolutional networks for text classificationX Zhang, J Zhao, Y LeCun\\xa0- Advances in neural information processing systems, 20156021회 인용 관련 학술자료 전체 15개의 버전 '}, title='Character-level convolutional networks for text classification', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Handwritten digit recognition with a back-propagation network': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1990', '게시자': 'Morgan Kaufmann Publishers', '설명': 'We present an application of back-propagation networks to hand (cid: 173) written digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the US Postal Service.', '저자': 'Y LeCun, B Boser, JS Denker, D Henderson, RE Howard, W Hubbard, LD Jackel', '전체 인용횟수': '5962회 인용199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202316274040485224492822272032282534313330362440355580135275419661664777753752599', '컨퍼런스': 'Advances in neural information processing systems 2, NIPS 1989', '페이지': '396-404', '학술 문서': 'Handwritten digit recognition with a back-propagation networkY LeCun, B Boser, J Denker, D Henderson, R Howard…\\xa0- Advances in neural information processing systems, 19895958회 인용 관련 학술자료 전체 12개의 버전 Handwritten digit recognition with*BP Network - 19895회 인용 관련 학술자료 전체 5개의 버전 '}, title='Handwritten digit recognition with a back-propagation network', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Optimal Brain Damage': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1990/2', '게시자': 'Morgan-Kaufmann Publishers', '권': '2', '설명': 'We have used information-theoretic ideas to derive a class of prac (cid: 173) tical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, sev (cid: 173) eral improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative informa (cid: 173) tion to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.', '저자': 'Yann LeCun, John S Denker, Sara A Solla', '전체 인용횟수': '5541회 인용19901991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220232245609110210511910711986869898747864941177684636567715978125175308408570585652527', '컨퍼런스': 'Advances in neural information processing systems 2, NIPS 1989', '페이지': '598-605', '학술 문서': 'Optimal brain damageY LeCun, J Denker, S Solla\\xa0- Advances in neural information processing systems, 19895472회 인용 관련 학술자료 전체 16개의 버전 Solla. 1990. Optimal brain damage*Y LeCun, JS Denker, A Sara\\xa0- Advances in neural information processing systems100회 인용 관련 학술자료 '}, title='Optimal Brain Damage', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Spectral Networks and Locally Connected Networks on Graphs': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/4/14', '게시자': 'arXiv preprint arXiv:1312.6203', '설명': 'Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.', '저자': 'Joan Bruna, Wojciech Zaremba, Arthur Szlam, Yann LeCun', '전체 인용횟수': '5400회 인용2015201620172018201920202021202220231441106287592891117312461010', '컨퍼런스': 'International Conference on Learning Representations (ICLR 2014)', '학술 문서': 'Spectral networks and locally connected networks on graphsJ Bruna, W Zaremba, A Szlam, Y LeCun\\xa0- arXiv preprint arXiv:1312.6203, 20135400회 인용 관련 학술자료 전체 14개의 버전 '}, title='Spectral Networks and Locally Connected Networks on Graphs', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Dimensionality reduction by learning an invariant mapping': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2006', '게시자': 'IEEE', '권': '2', '설명': 'Dimensionality reduction involves mapping a set of high dimensional input points onto a low dimensional manifold so that \\'similar\" points in input space are mapped to nearby points on the manifold. We present a method - called Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) - for learning a globally coherent nonlinear function that maps the data evenly to the output manifold. The learning relies solely on neighborhood relationships and does not require any distancemeasure in the input space. The method can learn mappings that are invariant to certain transformations of the inputs, as is demonstrated with a number of experiments. Comparisons are made to other techniques, in particular LLE.', '저자': 'Raia Hadsell, Sumit Chopra, Yann LeCun', '전체 인용횟수': '5375회 인용20092010201120122013201420152016201720182019202020212022202317201722383063133193324490724102711751036', '컨퍼런스': 'Computer vision and pattern recognition 2006. CVPR 2006. IEEE computer society conference on', '페이지': '1735-1742', '학술 문서': 'Dimensionality reduction by learning an invariant mappingR Hadsell, S Chopra, Y LeCun\\xa0- 2006 IEEE computer society conference on computer\\xa0…, 20065375회 인용 관련 학술자료 전체 18개의 버전 '}, title='Dimensionality reduction by learning an invariant mapping', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'MNIST handwritten digit database': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/1', '권': '2', '저자': 'Yann LeCun, Corinna Cortes, Chris Burges', '전체 인용횟수': '5017회 인용20132014201520162017201820192020202120222023151836641413705778781179955732', '페이지': '18', '학술 문서': 'MNIST handwritten digit databaseY LeCun, C Cortes, C Burges - 20105017회 인용 관련 학술자료 '}, title='MNIST handwritten digit database', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning a similarity metric discriminatively, with application to face verification': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2005/6/20', '게시자': 'IEEE', '권': '1', '설명': 'We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the L/sub 1/ norm in the target space approximates the \"semantic\" distance in the input space. The method is applied to a face verification task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons. The mapping from raw to the target space is a convolutional network whose architecture is designed for robustness to geometric distortions. The system is tested on the Purdue/AR face database which has a very high degree of\\xa0…', '저자': 'Sumit Chopra, Raia Hadsell, Yann LeCun', '전체 인용횟수': '4875회 인용20062007200820092010201120122013201420152016201720182019202020212022202321262730362727325299207362510576656712751661', '컨퍼런스': \"2005 IEEE computer society conference on computer vision and pattern recognition (CVPR'05)\", '페이지': '539-546', '학술 문서': 'Learning a similarity metric discriminatively, with application to face verificationS Chopra, R Hadsell, Y LeCun\\xa0- 2005 IEEE computer society conference on computer\\xa0…, 20054875회 인용 관련 학술자료 전체 24개의 버전 '}, title='Learning a similarity metric discriminatively, with application to face verification', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Signature verification using a\" siamese\" time delay neural network': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1993', '권': '6', '설명': 'This paper describes an algorithm for verification of signatures written on a pen-input tablet. The algorithm is based on a novel, artificial neural network, called a\" Siamese\" neural network. This network consists of two identical sub-networks joined at their out (cid: 173) puts. During training the two sub-networks extract features from two signatures, while the joining neuron measures the distance be (cid: 173) tween the two feature vectors. Verification consists of comparing an extracted feature vector~ ith a stored feature vector for the signer. Signatures closer to this stored representation than a chosen thresh (cid: 173) old are accepted, all other signatures are rejected as forgeries.', '저널': 'Advances in neural information processing systems', '저자': 'Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Säckinger, Roopak Shah', '전체 인용횟수': '4500회 인용20112012201320142015201620172018201920202021202220231511101454146259382588700762781657', '학술 문서': 'Signature verification using a\" siamese\" time delay neural networkJ Bromley, I Guyon, Y LeCun, E Säckinger, R Shah\\xa0- Advances in neural information processing systems, 19934500회 인용 관련 학술자료 전체 22개의 버전 Signature Veriﬁcation using a “Siamese” Time Delay Neural Network*J Bromley, I Guyon, Y LeCun, E Siickinger, R Shah - 1994관련 학술자료 전체 2개의 버전 '}, title='Signature verification using a\" siamese\" time delay neural network', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Geometric deep learning: going beyond euclidean data': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/7/11', '게시자': 'IEEE', '권': '34', '설명': 'Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains, such as graphs and manifolds. The purpose of this article is to overview different examples of geometric deep-learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.', '저널': 'IEEE Signal Processing Magazine', '저자': 'Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, Pierre Vandergheynst', '전체 인용횟수': '3445회 인용201720182019202020212022202343240436585673783647', '페이지': '18-42', '학술 문서': 'Geometric deep learning: going beyond euclidean dataMM Bronstein, J Bruna, Y LeCun, A Szlam…\\xa0- IEEE Signal Processing Magazine, 20173445회 인용 관련 학술자료 전체 14개의 버전 ', '호': '4'}, title='Geometric deep learning: going beyond euclidean data', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning Hierarchical Features for Scene Labeling': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/8', '게시자': 'IEEE', '권': '8', '설명': 'Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona\\xa0…', '저널': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', '저자': 'Clément Farabet, Camille Couprie, Laurent Najman, Yann LeCun', '전체 인용횟수': '3385회 인용2013201420152016201720182019202020212022202334120273420487474400387289288151', '페이지': '1915-1929', '학술 문서': 'Learning hierarchical features for scene labelingC Farabet, C Couprie, L Najman, Y LeCun\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20123385회 인용 관련 학술자료 전체 35개의 버전 ', '호': '35'}, title='Learning Hierarchical Features for Scene Labeling', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Regularization of neural networks using dropconnect': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/5/26', '게시자': 'PMLR', '설명': 'We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recoginition benchmarks can be obtained by aggregating multiple DropConnect-trained models.', '저자': 'Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, Rob Fergus', '전체 인용횟수': '3104회 인용201320142015201620172018201920202021202220231681156271335414451413362321246', '컨퍼런스': 'International conference on machine learning', '페이지': '1058-1066', '학술 문서': 'Regularization of neural networks using dropconnectL Wan, M Zeiler, S Zhang, Y Le Cun, R Fergus\\xa0- International conference on machine learning, 20133104회 인용 관련 학술자료 전체 8개의 버전 '}, title='Regularization of neural networks using dropconnect', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A Closer Look at Spatiotemporal Convolutions for Action Recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/11/30', '게시자': 'arXiv preprint arXiv:1711.11248', '설명': \"In this paper we discuss several forms of spatiotemporal convolutions for video analysis and study their effects on action recognition. Our motivation stems from the observation that 2D CNNs applied to individual frames of the video have remained solid performers in action recognition. In this work we empirically demonstrate the accuracy advantages of 3D CNNs over 2D CNNs within the framework of residual learning. Furthermore, we show that factorizing the 3D convolutional filters into separate spatial and temporal components yields significantly gains in accuracy. Our empirical study leads to the design of a new spatiotemporal convolutional block``R (2+ 1) D''which produces CNNs that achieve results comparable or superior to the state-of-the-art on Sports-1M, Kinetics, UCF101, and HMDB51.\", '저자': 'Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, Manohar Paluri', '전체 인용횟수': '2882회 인용20182019202020212022202339217426697782700', '컨퍼런스': 'computer vision and pattern recognition conference (CVPR 2018)', '학술 문서': 'A closer look at spatiotemporal convolutions for action recognitionD Tran, H Wang, L Torresani, J Ray, Y LeCun, M Paluri\\xa0- Proceedings of the IEEE conference on Computer\\xa0…, 20182882회 인용 관련 학술자료 전체 15개의 버전 '}, title='A Closer Look at Spatiotemporal Convolutions for Action Recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'What is the best multi-stage architecture for object recognition?': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009/9/29', '게시자': 'IEEE', '설명': 'In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition\\xa0…', '저자': \"Kevin Jarrett, Koray Kavukcuoglu, Marc'Aurelio Ranzato, Yann LeCun\", '전체 인용횟수': '2871회 인용20102011201220132014201520162017201820192020202120222023347374120165196274299334328304281201132', '컨퍼런스': '2009 IEEE 12th international conference on computer vision', '페이지': '2146-2153', '학술 문서': 'What is the best multi-stage architecture for object recognition?K Jarrett, K Kavukcuoglu, MA Ranzato, Y LeCun\\xa0- 2009 IEEE 12th international conference on computer\\xa0…, 20092871회 인용 관련 학술자료 전체 26개의 버전 '}, title='What is the best multi-stage architecture for object recognition?', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep residual learning for image recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': 'Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.', '저자': 'Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun', '전체 인용횟수': '189280회 인용2016201720182019202020212022202311814991117422035228043377524409339614', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '770-778', '학술 문서': 'Deep residual learning for image recognitionK He, X Zhang, S Ren, J Sun\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2016189280회 인용 관련 학술자료 전체 76개의 버전 '}, title='Deep residual learning for image recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Faster r-cnn: Towards real-time object detection with region proposal networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/6/4', '설명': 'State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP) using 300 proposals per image. Code is available at https://github. com/ShaoqingRen/faster_rcnn.', '저널': 'Conference on Neural Information Processing Systems (NIPS 2015)', '저자': 'Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun', '전체 인용횟수': '67084회 인용2016201720182019202020212022202369123324971809410149128971420011938', '학술 문서': 'Faster r-cnn: Towards real-time object detection with region proposal networksS Ren, K He, R Girshick, J Sun\\xa0- Advances in neural information processing systems, 201567038회 인용 관련 학술자료 전체 42개의 버전 Faster r-cnn: Towards real-time object detection with region proposal networks. arXiv 2015*S Ren, K He, R Girshick, J Sun\\xa0- arXiv preprint arXiv:1506.01497, 2015381회 인용 관련 학술자료 Faster R 鄄 CNN: towards real 鄄 time object detection with region proposal networks*S REN, K HE, R GIRSHICK\\xa0- Proc of the 28th International Conference on Neural\\xa0…, 20157회 인용 관련 학술자료 Faster R-CNN: 利用区域提案网络实现实时目标检测*S Ren, K He, R Girshick, J Sun\\xa0- arXiv preprint arXiv:1506.01497, 2015관련 학술자료 전체 2개의 버전 '}, title='Faster r-cnn: Towards real-time object detection with region proposal networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Delving deep into rectifiers: Surpassing human-level performance on imagenet classification': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%) on this dataset.', '저자': 'Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun', '전체 인용횟수': '22483회 인용2015201620172018201920202021202220232176471284213729213475378236452966', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '1026-1034', '학술 문서': 'Delving deep into rectifiers: Surpassing human-level performance on imagenet classificationK He, X Zhang, S Ren, J Sun\\xa0- Proceedings of the IEEE international conference on\\xa0…, 201522483회 인용 관련 학술자료 전체 21개의 버전 '}, title='Delving deep into rectifiers: Surpassing human-level performance on imagenet classification', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Spatial pyramid pooling in deep convolutional networks for visual recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/1/9', '게시자': 'IEEE', '권': '37', '설명': 'Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224   224) input image. This requirement is “artificial” and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, “spatial pyramid pooling”, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image\\xa0…', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun', '전체 인용횟수': '13825회 인용201420152016201720182019202020212022202357214462772103814431634210725662200', '페이지': '1904-1916', '학술 문서': 'Spatial pyramid pooling in deep convolutional networks for visual recognitionK He, X Zhang, S Ren, J Sun\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 201513825회 인용 관련 학술자료 전체 26개의 버전 ', '호': '9'}, title='Spatial pyramid pooling in deep convolutional networks for visual recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Identity mappings in deep residual networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '게시자': 'Springer International Publishing', '설명': ' Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62\\xa0% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at:                      https://github.com/KaimingHe/resnet-1k-layers                                        .', '저자': 'Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun', '전체 인용횟수': '10833회 인용2016201720182019202020212022202312049698314151772207821321722', '컨퍼런스': 'Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14', '페이지': '630-645', '학술 문서': 'Identity mappings in deep residual networksK He, X Zhang, S Ren, J Sun\\xa0- Computer Vision–ECCV 2016: 14th European\\xa0…, 201610833회 인용 관련 학술자료 전체 8개의 버전 '}, title='Identity mappings in deep residual networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Face Alignment at 3000 FPS via Regressing Local Binary Features': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '설명': 'This paper presents a highly efficient, very accurate regression approach for face alignment. Our approach has two novel components: a set of local binary features, and a locality principle for learning those features. The locality principle guides us to learn a set of highly discriminative local binary features for each facial landmark independently. The obtained local binary features are used to jointly learn a linear regression for the final output. Our approach achieves the state-of-the-art results when tested on the current most challenging benchmarks. Furthermore, because extracting and regressing local binary features is computationally very cheap, our system is much faster than previous methods. It achieves over 3,000 fps on a desktop or 300 fps on a mobile phone for locating a few dozens of landmarks.', '저널': 'IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2014)', '저자': 'Shaoqing Ren, Xudong Cao, Yichen Wei, Jian Sun', '전체 인용횟수': '1186회 인용20132014201520162017201820192020202120222023313981601991711701341096235', '학술 문서': 'Face alignment at 3000 fps via regressing local binary featuresS Ren, X Cao, Y Wei, J Sun\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20141113회 인용 관련 학술자료 전체 20개의 버전 Face alignment via regressing local binary features*S Ren, X Cao, Y Wei, J Sun\\xa0- IEEE Transactions on Image Processing, 2016110회 인용 관련 학술자료 전체 5개의 버전 '}, title='Face Alignment at 3000 FPS via Regressing Local Binary Features', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Joint cascade face detection and alignment': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '게시자': 'Springer International Publishing', '설명': ' We present a new state-of-the-art approach for face detection. The key idea is to combine face alignment with detection, observing that aligned face shapes provide better features for face classification. To make this combination more effective, our approach learns the two tasks jointly in the same cascade framework, by exploiting recent advances in face alignment. Such joint learning greatly enhances the capability of cascade detection and still retains its realtime performance. Extensive experiments show that our approach achieves the best accuracy on challenging datasets, where all existing solutions are either inaccurate or too slow.', '저자': 'Dong Chen, Shaoqing Ren, Yichen Wei, Xudong Cao, Jian Sun', '전체 인용횟수': '521회 인용2013201420152016201720182019202020212022202322334987748757542929', '컨퍼런스': 'Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13', '페이지': '109-122', '학술 문서': 'Joint cascade face detection and alignmentD Chen, S Ren, Y Wei, X Cao, J Sun\\xa0- Computer Vision–ECCV 2014: 13th European\\xa0…, 2014521회 인용 관련 학술자료 전체 7개의 버전 '}, title='Joint cascade face detection and alignment', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Instance-sensitive fully convolutional networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '게시자': 'Springer International Publishing', '설명': ' Fully convolutional networks (FCNs) have been proven very successful for semantic segmentation, but the FCN outputs are unaware of object instances. In this paper, we develop FCNs that are capable of proposing instance-level segment candidates. In contrast to the previous FCN that generates one score map, our FCN is designed to compute a small set of instance-sensitive score maps, each of which is the outcome of a pixel-wise classifier of a relative position to instances. On top of these instance-sensitive score maps, a simple assembling module is able to output instance candidate at each position. In contrast to the recent DeepMask method for segmenting instances, our method does not have any high-dimensional layer related to the mask resolution, but instead exploits image local coherence for estimating instances. We present competitive results of instance segment proposal on both PASCAL\\xa0…', '저자': 'Jifeng Dai, Kaiming He, Yi Li, Shaoqing Ren, Jian Sun', '전체 인용횟수': '499회 인용20152016201720182019202020212022202329397110375756749', '컨퍼런스': 'Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI 14', '페이지': '534-549', '학술 문서': 'Instance-sensitive fully convolutional networksJ Dai, K He, Y Li, S Ren, J Sun\\xa0- Computer Vision–ECCV 2016: 14th European\\xa0…, 2016499회 인용 관련 학술자료 전체 4개의 버전 '}, title='Instance-sensitive fully convolutional networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Faster R-CNN: towards real-time object detection with region proposal networks. İn: International Conference on Neural Information Processing Systems': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '저자': 'SQ Ren, KM He, R Girshick, J Sun', '전체 인용횟수': '207회 인용202120222023713186', '페이지': '91-99', '학술 문서': 'Faster R-CNN: towards real-time object detection with region proposal networks. İn: International Conference on Neural Information Processing SystemsSQ Ren, KM He, R Girshick, J Sun - 2018207회 인용 관련 학술자료 '}, title='Faster R-CNN: towards real-time object detection with region proposal networks. İn: International Conference on Neural Information Processing Systems', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep residual learning for image recognition. arXiv e-prints': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/12', '권': '10', '저널': 'arXiv preprint arXiv:1512.03385', '저자': 'Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun', '전체 인용횟수': '157회 인용2016201720182019202020212022202331093831252019', '학술 문서': 'Deep residual learning for image recognition. arXiv e-printsK He, X Zhang, S Ren, J Sun\\xa0- arXiv preprint arXiv:1512.03385, 2015157회 인용 관련 학술자료 '}, title='Deep residual learning for image recognition. arXiv e-prints', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Global refinement of random forest': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'Random forest is well known as one of the best learning methods. In spite of its great success, it also has certain drawbacks: the heuristic learning rule does not effectively minimize the global training loss; the model size is usually too large for many real applications. To address the issues, we propose two techniques, global refinement and global pruning, to improve a pre-trained random forest. The proposed global refinement jointly relearns the leaf nodes of all trees under a global objective function so that the complementary information between multiple trees is well exploited. In this way, the fitting power of the forest is significantly enhanced. The global pruning is developed to reduce the model size as well as the over-fitting risk. The refined model has better performance and smaller storage cost, as verified in extensive experiments.', '저자': 'Shaoqing Ren, Xudong Cao, Yichen Wei, Jian Sun', '전체 인용횟수': '143회 인용20152016201720182019202020212022202391423171914132013', '컨퍼런스': 'IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2015)', '페이지': '723-730', '학술 문서': 'Global refinement of random forestS Ren, X Cao, Y Wei, J Sun\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2015143회 인용 관련 학술자료 전체 13개의 버전 '}, title='Global refinement of random forest', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep Learning and Image Recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2023/7/21', '게시자': 'IEEE', '설명': \"Over the next decade, we can expect artificial intelligence (AI) to have a profound impact on our society. As AI technologies continue to advance and become more integrated into various industries, we will witness significant changes that are shaping the human being's future for the better. This paper reviews the major deep learning algorithms that have accomplished latest achievements on various fields such as manufacturing robots, smart assistants, automated financial investing, social media monitoring, marketing chat-bots and self-driving car. These algorithms are trained on large datasets of labeled images, where each image is associated with a set of labels or categories. This paper provides an overview of popular techniques for image recognition. It discusses the convolutional neural network (CNN), which is made up of multiple convolutional layers followed by pooling layers, dropout layers, batch\\xa0…\", '저자': 'Chaoyang Li, Xiaohan Li, Manni Chen, Xinyao Sun', '전체 인용횟수': '110회 인용201720182019202020212022202359716332513', '출처': '2023 IEEE 6th International Conference on Electronic Information and Communication Technology (ICEICT)', '페이지': '557-562', '학술 문서': 'Deep Learning and Image RecognitionC Li, X Li, M Chen, X Sun\\xa0- 2023 IEEE 6th International Conference on Electronic\\xa0…, 2023106회 인용 관련 학술자료 Deep Learning Image Recognition*DQT Le, SN Tiwari, B Merialdo\\xa0- Stuwww. eurecom. fr, 20154회 인용 관련 학술자료 '}, title='Deep Learning and Image Recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Computer Vision–ECCV 2016': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/10', '저널': 'European conference on computer vision', '저자': 'Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, B Leibe, J Matas, N Sebe, M Welling', '전체 인용횟수': '101회 인용2015201620172018201920202021202220233131316162523', '페이지': '645', '학술 문서': 'Computer Vision–ECCV 2016K He, X Zhang, S Ren, J Sun, B Leibe, J Matas, N Sebe…\\xa0- European conference on computer vision, 2016101회 인용 관련 학술자료 '}, title='Computer Vision–ECCV 2016', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep residual learning for image recognition In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 770–778': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '저널': 'IEEE. https://doi. org/10.1109/cvpr', '저자': 'Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun', '전체 인용횟수': '98회 인용20162017201820192020202120222023151115133122', '학술 문서': 'Deep residual learning for image recognition In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 770–778K He, X Zhang, S Ren, J Sun\\xa0- IEEE. https://doi. org/10.1109/cvpr, 201698회 인용 관련 학술자료 '}, title='Deep residual learning for image recognition In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 770–778', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep residual learning for image recognition. CoRR (2015)': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '저널': 'arXiv preprint arXiv:1512.03385', '저자': 'Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun', '전체 인용횟수': '80회 인용20172018201920202021202220233787162017', '학술 문서': 'Deep residual learning for image recognition. CoRR (2015)K He, X Zhang, S Ren, J Sun\\xa0- arXiv preprint arXiv:1512.03385, 201680회 인용 관련 학술자료 '}, title='Deep residual learning for image recognition. CoRR (2015)', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep residual learning for image recognition. 2016 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '게시자': 'IEEE', '저자': 'K He, X Zhang, Sh Ren, J Sun', '전체 인용횟수': '64회 인용201720182019202020212022202313212141715', '페이지': '770-778', '학술 문서': 'Deep residual learning for image recognition. 2016 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NVK He, X Zhang, S Ren, J Sun - 201664회 인용 관련 학술자료 '}, title='Deep residual learning for image recognition. 2016 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep Residual Learning for Image Recognition. 2015. doi: 10.48550': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'저널': 'arXiv preprint ARXIV.1512.03385', '저자': 'Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun', '전체 인용횟수': '63회 인용20182019202020212022202313230', '학술 문서': 'Deep Residual Learning for Image Recognition. 2015. doi: 10.48550K He, X Zhang, S Ren, J Sun\\xa0- arXiv preprint ARXIV.1512.0338563회 인용 관련 학술자료 '}, title='Deep Residual Learning for Image Recognition. 2015. doi: 10.48550', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Shufflenet: An extremely efficient convolutional neural network for mobile devices': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (eg, 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, eg lower top-1 error (absolute 7.8%) than recent MobileNet~ cite {howard2017mobilenets} on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves $ sim $13 $ imes $ actual speedup over AlexNet while maintaining comparable accuracy.', '저자': 'Xiangyu Zhang*, Xinyu Zhou*, Mengxiao Lin, Jian Sun', '전체 인용횟수': '7035회 인용2017201820192020202120222023212756741095139017951740', '컨퍼런스': 'Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition', '페이지': '6848-6856', '학술 문서': 'Shufflenet: An extremely efficient convolutional neural network for mobile devicesX Zhang, X Zhou, M Lin, J Sun\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20187035회 인용 관련 학술자료 전체 14개의 버전 '}, title='Shufflenet: An extremely efficient convolutional neural network for mobile devices', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Shufflenet v2: Practical guidelines for efficient cnn architecture design': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'Current network architecture design is mostly guided by the indirect metric of computation complexity, ie, FLOPs. However, the direct metric, such as speed, also depends on the other factors such as memory access cost and platform characterics. Taking these factors into account, this work proposes practical guidelines for efficient network de-sign. Accordingly, a new architecture called ShuffleNet V2 is presented. Comprehensive experiments verify that it is the state-of-the-art in both speed and accuracy.', '저자': 'Ningning Ma*, Xiangyu Zhang*, Hai-Tao Zheng, Jian Sun', '전체 인용횟수': '4706회 인용2018201920202021202220232030860491613401490', '컨퍼런스': 'Proceedings of the European conference on computer vision (ECCV)', '페이지': '116-131', '학술 문서': 'Shufflenet v2: Practical guidelines for efficient cnn architecture designN Ma, X Zhang, HT Zheng, J Sun\\xa0- Proceedings of the European conference on computer\\xa0…, 20184706회 인용 관련 학술자료 전체 10개의 버전 '}, title='Shufflenet v2: Practical guidelines for efficient cnn architecture design', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Channel pruning for accelerating very deep neural networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'In this paper, we introduce a new channel pruning method to accelerate very deep convolutional neural networks. Given a trained CNN model, we propose an iterative two-step algorithm to effectively prune each layer, by a LASSO regression based channel selection and least square reconstruction. We further generalize this algorithm to multi-layer and multi-branch cases. Our method reduces the accumulated error and enhance the compatibility with various architectures. Our pruned VGG-16 achieves the state-of-the-art results by 5x speed-up along with only 0.3% increase of error. More importantly, our method is able to accelerate modern networks like ResNet, Xception and suffers only 1.4%, 1.0% accuracy loss under 2x speed-up respectively, which is significant.', '저자': 'Yihui He, Xiangyu Zhang, Jian Sun', '전체 인용횟수': '2660회 인용201820192020202120222023159307515556629482', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '1389-1397', '학술 문서': 'Channel pruning for accelerating very deep neural networksY He, X Zhang, J Sun\\xa0- Proceedings of the IEEE international conference on\\xa0…, 20172660회 인용 관련 학술자료 전체 12개의 버전 '}, title='Channel pruning for accelerating very deep neural networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Large kernel matters--improve semantic segmentation by global convolutional network': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'Convolution Neural Network (CNN) has boosted the per-formanceofalotofcomputervisiontasks, likeimageclassi-fication [31], segmentation [25], and detection [28]. Based on the observations from [31, 32, 14], recent model design-ers prefer to employ stacking of small kernels, like 3 x 3 over large-size filters. However, in the field of semantic seg-mentation, where we need to perform dense per-pixel pre-diction, we find that large kernel plays an important role to relieve the contradictories when optimizing the classi-fication and localization tasks simultaneously. Following the design principle of large-size kernel, We propose the Global Convolutional Network to address both the classi-fication and localization issue in the semantic segmentation task. To further refine the object category boundaries, we presentBoundaryRefinementblockbasedonresidualstruc-ture. Qualitatively, our model achieves state-of-art perfor-mance on two public benchmarks and outperforms previous results on a large margin, 82.2%(vs 80.2%) on PASCAL VOC 2012 dataset and 76.9%(vs 71.8%) on Cityscapes dataset.', '저자': 'Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, Jian Sun', '전체 인용횟수': '1626회 인용201720182019202020212022202319135267298315300283', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '4353-4361', '학술 문서': 'Large kernel matters--improve semantic segmentation by global convolutional networkC Peng, X Zhang, G Yu, G Luo, J Sun\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20171626회 인용 관련 학술자료 전체 11개의 버전 '}, title='Large kernel matters--improve semantic segmentation by global convolutional network', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Repvgg: Making vgg-style convnets great again': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021', '설명': 'We present a simple but powerful architecture of convolutional neural network, which has a VGG-like inference-time body composed of nothing but a stack of 3x3 convolution and ReLU, while the training-time model has a multi-branch topology. Such decoupling of the training-time and inference-time architecture is realized by a structural re-parameterization technique so that the model is named RepVGG. On ImageNet, RepVGG reaches over 80% top-1 accuracy, which is the first time for a plain model, to the best of our knowledge. On NVIDIA 1080Ti GPU, RepVGG models run 83% faster than ResNet-50 or 101% faster than ResNet-101 with higher accuracy and show favorable accuracy-speed trade-off compared to the state-of-the-art models like EfficientNet and RegNet. The code and trained models are available at https://github. com/megvii-model/RepVGG.', '저자': 'Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, Jian Sun', '전체 인용횟수': '1030회 인용20212022202378347595', '컨퍼런스': 'Proceedings of the IEEE/CVF conference on computer vision and pattern recognition', '페이지': '13733-13742', '학술 문서': 'Repvgg: Making vgg-style convnets great againX Ding, X Zhang, N Ma, J Han, G Ding, J Sun\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 20211027회 인용 관련 학술자료 전체 13개의 버전 Repvgg: Making vgg-style convnets great again. arXiv 2021*X Ding, X Zhang, N Ma, J Han, G Ding, J Sun\\xa0- arXiv preprint arXiv:2101.0369713회 인용 관련 학술자료 '}, title='Repvgg: Making vgg-style convnets great again', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Accelerating very deep convolutional networks for classification and detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/11/20', '게시자': 'IEEE', '권': '38', '설명': 'This paper aims to accelerate the test-time computation of convolutional neural networks (CNNs), especially very deep CNNs    that have substantially impacted the computer vision community. Unlike previous methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We develop an effective solution to the resulting nonlinear optimization problem without the need of stochastic gradient descent (SGD). More importantly, while previous methods mainly focus on optimizing one or two layers, our nonlinear method enables an asymmetric reconstruction that reduces the rapidly accumulated error when multiple (e.g.,   10) layers are approximated. For the widely used very deep VGG-16 model    , our method achieves a whole-model speedup of 4   with merely a 0.3 percent increase of top-5 error in ImageNet classification. Our 4    accelerated VGG\\xa0…', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Xiangyu Zhang, Jianhua Zou, Kaiming He, Jian Sun', '전체 인용횟수': '942회 인용2015201620172018201920202021202220237214288132140188193117', '페이지': '1943-1955', '학술 문서': 'Accelerating very deep convolutional networks for classification and detectionX Zhang, J Zou, K He, J Sun\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 2015942회 인용 관련 학술자료 전체 10개의 버전 ', '호': '10'}, title='Accelerating very deep convolutional networks for classification and detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Single path one-shot neural architecture search with uniform sampling': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020/8/23', '게시자': 'Springer, Cham', '설명': ' We revisit the one-shot Neural Architecture Search (NAS) paradigm and analyze its advantages over existing NAS approaches. Existing one-shot method, however, is hard to train and not yet effective on large scale datasets like ImageNet. This work propose a Single Path One-Shot model to address the challenge in the training. Our central idea is to construct a simplified supernet, where all architectures are single paths so that weight co-adaption problem is alleviated. Training is performed by uniform path sampling. All architectures (and their weights) are trained fully and equally. Comprehensive experiments verify that our approach is flexible and effective. It is easy to train and fast to search. It effortlessly supports complex search spaces (e.g., building blocks, channel, mixed-precision quantization) and different search constraints (e.g., FLOPs, latency). It is thus convenient to use for various needs\\xa0…', '저자': 'Zichao Guo*, Xiangyu Zhang*, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, Jian Sun', '전체 인용횟수': '798회 인용2019202020212022202336111219220208', '컨퍼런스': 'European Conference on Computer Vision', '페이지': '544-560', '학술 문서': 'Single path one-shot neural architecture search with uniform samplingZ Guo, X Zhang, H Mu, W Heng, Z Liu, Y Wei, J Sun\\xa0- Computer Vision–ECCV 2020: 16th European\\xa0…, 2020771회 인용 관련 학술자료 전체 10개의 버전 Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun*Z Guo, X Zhang\\xa0- Single path one-shot neural architecture search with\\xa0…, 201949회 인용 관련 학술자료 '}, title='Single path one-shot neural architecture search with uniform sampling', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Bounding box regression with uncertainty for accurate object detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': 'Large-scale object detection datasets (eg, MS-COCO) try to define the ground truth bounding boxes as clear as possible. However, we observe that ambiguities are still introduced when labeling the bounding boxes. In this paper, we propose a novel bounding box regression loss for learning bounding box transformation and localization variance together. Our loss greatly improves the localization accuracies of various architectures with nearly no additional computation. The learned localization variance allows us to merge neighboring bounding boxes during non-maximum suppression (NMS), which further improves the localization performance. On MS-COCO, we boost the Average Precision (AP) of VGG-16 Faster R-CNN from 23.6% to 29.1%. More importantly, for ResNet-50-FPN Mask R-CNN, our method improves the AP and AP90 by 1.8% and 6.2% respectively, which significantly outperforms previous state-of-the-art bounding box refinement methods. Our code and models are available at github. com/yihui-he/KL-Loss', '저자': 'Yihui He, Chenchen Zhu, Jianren Wang, Marios Savvides, Xiangyu Zhang', '전체 인용횟수': '578회 인용201920202021202220232796161160130', '컨퍼런스': 'Proceedings of the ieee/cvf conference on computer vision and pattern recognition', '페이지': '2888-2897', '학술 문서': 'Bounding box regression with uncertainty for accurate object detectionY He, C Zhu, J Wang, M Savvides, X Zhang\\xa0- Proceedings of the ieee/cvf conference on computer\\xa0…, 2019506회 인용 관련 학술자료 전체 11개의 버전 Softer-nms: Rethinking bounding box regression for accurate object detection*Y He, X Zhang, M Savvides, K Kitani\\xa0- arXiv preprint arXiv:1809.08545, 201886회 인용 관련 학술자료 '}, title='Bounding box regression with uncertainty for accurate object detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Crowdhuman: A benchmark for detecting human in a crowd': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/4/30', '설명': 'Human detection has witnessed impressive progress in recent years. However, the occlusion issue of detecting human in highly crowded environments is far from solved. To make matters worse, crowd scenarios are still under-represented in current human detection benchmarks. In this paper, we introduce a new dataset, called CrowdHuman, to better evaluate detectors in crowd scenarios. The CrowdHuman dataset is large, rich-annotated and contains high diversity. There are a total of  human instances from the train and validation subsets, and  persons per image, with various kinds of occlusions in the dataset. Each human instance is annotated with a head bounding-box, human visible-region bounding-box and human full-body bounding-box. Baseline performance of state-of-the-art detection frameworks on CrowdHuman is presented. The cross-dataset generalization results of CrowdHuman dataset demonstrate state-of-the-art performance on previous dataset including Caltech-USA, CityPersons, and Brainwash without bells and whistles. We hope our dataset will serve as a solid baseline and help promote future research in human detection tasks.', '저널': 'arXiv preprint arXiv:1805.00123', '저자': 'Shuai Shao, Zijian Zhao, Boxun Li, Tete Xiao, Gang Yu, Xiangyu Zhang, Jian Sun', '전체 인용횟수': '567회 인용20182019202020212022202331960107186192', '학술 문서': 'Crowdhuman: A benchmark for detecting human in a crowdS Shao, Z Zhao, B Li, T Xiao, G Yu, X Zhang, J Sun\\xa0- arXiv preprint arXiv:1805.00123, 2018567회 인용 관련 학술자료 전체 3개의 버전 '}, title='Crowdhuman: A benchmark for detecting human in a crowd', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Exfuse: Enhancing feature fusion for semantic segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'Modern semantic segmentation frameworks usually combine low-level and high-level features from pre-trained backbone convolutional models to boost performance. In this paper, we first point out that a simple fusion of low-level and high-level features could be less effective because of the gap in semantic levels and spatial resolution. We find that introducing semantic information into low-level features and high-resolution details into high-level features are more effective for the later fusion. Based on this observation, we propose a new framework, named ExFuse, to bridge the gap between low-level and high-level features thus significantly improve the segmentation quality by 4.0% in total. Furthermore, we evaluate our approach on the challenging PASCAL VOC 2012 segmentation benchmark and achieve 87.9% mean IoU, which outperforms the previous state-of-the-art results.', '저자': 'Zhenli Zhang, Xiangyu Zhang, Chao Peng, Xiangyang Xue, Jian Sun', '전체 인용횟수': '529회 인용20182019202020212022202357597108130111', '컨퍼런스': 'Proceedings of the European conference on computer vision (ECCV)', '페이지': '269-284', '학술 문서': 'Exfuse: Enhancing feature fusion for semantic segmentationZ Zhang, X Zhang, C Peng, X Xue, J Sun\\xa0- Proceedings of the European conference on computer\\xa0…, 2018529회 인용 관련 학술자료 전체 6개의 버전 '}, title='Exfuse: Enhancing feature fusion for semantic segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Detnet: A backbone network for object detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/4/17', '설명': 'Recent CNN based object detectors, no matter one-stage methods like YOLO, SSD, and RetinaNe or two-stage detectors like Faster R-CNN, R-FCN and FPN are usually trying to directly finetune from ImageNet pre-trained models designed for image classification. There has been little work discussing on the backbone feature extractor specifically designed for the object detection. More importantly, there are several differences between the tasks of image classification and object detection. 1. Recent object detectors like FPN and RetinaNet usually involve extra stages against the task of image classification to handle the objects with various scales. 2. Object detection not only needs to recognize the category of the object instances but also spatially locate the position. Large downsampling factor brings large valid receptive field, which is good for image classification but compromises the object location ability. Due to the gap between the image classification and object detection, we propose DetNet in this paper, which is a novel backbone network specifically designed for object detection. Moreover, DetNet includes the extra stages against traditional backbone network for image classification, while maintains high spatial resolution in deeper layers. Without any bells and whistles, state-of-the-art results have been obtained for both object detection and instance segmentation on the MSCOCO benchmark based on our DetNet~(4.8G FLOPs) backbone. The code will be released for the reproduction.', '저널': 'Proceedings of the European Conference on Computer Vision (ECCV), 334-350', '저자': 'Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yangdong Deng, Jian Sun', '전체 인용횟수': '519회 인용20182019202020212022202387411013110586', '학술 문서': 'Detnet: A backbone network for object detectionZ Li, C Peng, G Yu, X Zhang, Y Deng, J Sun\\xa0- arXiv preprint arXiv:1804.06215, 2018308회 인용 관련 학술자료 전체 2개의 버전 Detnet: Design backbone for object detection*Z Li, C Peng, G Yu, X Zhang, Y Deng, J Sun\\xa0- Proceedings of the European conference on computer\\xa0…, 2018213회 인용 관련 학술자료 전체 6개의 버전 '}, title='Detnet: A backbone network for object detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Metapruning: Meta learning for automatic neural network channel pruning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': 'In this paper, we propose a novel meta learning approach for automatic channel pruning of very deep neural networks. We first train a PruningNet, a kind of meta network, which is able to generate weight parameters for any pruned structure given the target network. We use a simple stochastic structure sampling method for training the PruningNet. Then, we apply an evolutionary procedure to search for good-performing pruned networks. The search is highly efficient because the weights are directly generated by the trained PruningNet and we do not need any finetuning at search time. With a single PruningNet trained for the target network, we can search for various Pruned Networks under different constraints with little human participation. Compared to the state-of-the-art pruning methods, we have demonstrated superior performances on MobileNet V1/V2 and ResNet. Codes are available on https://github. com/liuzechun/MetaPruning.', '저자': 'Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Kwang-Ting Cheng, Jian Sun', '전체 인용횟수': '515회 인용201920202021202220231470139157133', '컨퍼런스': 'Proceedings of the IEEE/CVF international conference on computer vision', '페이지': '3296-3305', '학술 문서': 'Metapruning: Meta learning for automatic neural network channel pruningZ Liu, H Mu, X Zhang, Z Guo, X Yang, KT Cheng, J Sun\\xa0- Proceedings of the IEEE/CVF international conference\\xa0…, 2019380회 인용 관련 학술자료 전체 9개의 버전 Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Kwang-Ting Cheng, and Jian Sun. Metapruning: Meta learning for automatic neural network channel pruning*Z Liu\\xa0- Proceedings of the IEEE/CVF international conference\\xa0…, 2019202회 인용 관련 학술자료 '}, title='Metapruning: Meta learning for automatic neural network channel pruning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'You only look one-level feature': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021', '설명': 'This paper revisits feature pyramids networks (FPN) for one-stage detectors and points out that the success of FPN is due to its divide-and-conquer solution to the optimization problem in object detection rather than multi-scale feature fusion. From the perspective of optimization, we introduce an alternative way to address the problem instead of adopting the complex feature pyramids--utilizing only one-level feature for detection. Based on the simple and efficient solution, we present You Only Look One-level Feature (YOLOF). In our method, two key components, Dilated Encoder and Uniform Matching, are proposed and bring considerable improvements. Extensive experiments on the COCO benchmark prove the effectiveness of the proposed model. Our YOLOF achieves comparable results with its feature pyramids counterpart RetinaNet while being 2.5 times faster. Without transformer layers, YOLOF can match the performance of DETR in a single-level feature manner with 7 times less training epochs.', '저자': 'Qiang Chen, Yingming Wang, Tong Yang, Xiangyu Zhang, Jian Cheng, Jian Sun', '전체 인용횟수': '481회 인용20212022202326208241', '컨퍼런스': 'Proceedings of the IEEE/CVF conference on computer vision and pattern recognition', '페이지': '13039-13048', '학술 문서': 'You only look one-level featureQ Chen, Y Wang, T Yang, X Zhang, J Cheng, J Sun\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 2021481회 인용 관련 학술자료 전체 8개의 버전 '}, title='You only look one-level feature', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Objects365: A large-scale, high-quality dataset for object detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': 'In this paper, we introduce a new large-scale object detection dataset, Objects365, which has 365 object categories over 600K training images. More than 10 million, high-quality bounding boxes are manually labeled through a three-step, carefully designed annotation pipeline. It is the largest object detection dataset (with full annotation) so far and establishes a more challenging benchmark for the community. Objects365 can serve as a better feature learning dataset for localization-sensitive tasks like object detection and semantic segmentation. The Objects365 pre-trained models significantly outperform ImageNet pre-trained models with 5.6 points gain (42 vs 36.4) based on the standard setting of 90K iterations on COCO benchmark. Even compared with much long training time like 540K iterations, our Objects365 pretrained model with 90K iterations still have 2.7 points gain (42 vs 39.3). Meanwhile, the finetuning time can be greatly reduced (up to 10 times) when reaching the same accuracy. Better generalization ability of Object365 has also been verified on CityPersons, VOC segmentation, and ADE tasks. The dataset as well as the pretrained-models have been released at www. objects365. org.', '저자': 'Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, Jian Sun', '전체 인용횟수': '410회 인용2019202020212022202322558126196', '컨퍼런스': 'Proceedings of the IEEE/CVF international conference on computer vision', '페이지': '8430-8439', '학술 문서': 'Objects365: A large-scale, high-quality dataset for object detectionS Shao, Z Li, T Zhang, C Peng, G Yu, X Zhang, J Li…\\xa0- Proceedings of the IEEE/CVF international conference\\xa0…, 2019410회 인용 관련 학술자료 전체 8개의 버전 '}, title='Objects365: A large-scale, high-quality dataset for object detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Scaling up your kernels to 31x31: Revisiting large kernel design in cnns': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2022', '설명': 'We revisit large kernel design in modern convolutional neural networks (CNNs). Inspired by recent advances in vision transformers (ViTs), in this paper, we demonstrate that using a few large convolutional kernels instead of a stack of small kernels could be a more powerful paradigm. We suggested five guidelines, eg, applying re-parameterized large depth-wise convolutions, to design efficient high-performance large-kernel CNNs. Following the guidelines, we propose RepLKNet, a pure CNN architecture whose kernel size is as large as 31x31, in contrast to commonly used 3x3. RepLKNet greatly closes the performance gap between CNNs and ViTs, eg, achieving comparable or superior results than Swin Transformer on ImageNet and a few typical downstream tasks, with lower latency. RepLKNet also shows nice scalability to big data and large models, obtaining 87.8% top-1 accuracy on ImageNet and 56.0% mIoU on ADE20K, which is very competitive among the state-of-the-arts with similar model sizes. Our study further reveals that, in contrast to small-kernel CNNs, large-kernel CNNs have much larger effective receptive fields and higher shape bias rather than texture bias. Code & models at https://github. com/megvii-research/RepLKNet.', '저자': 'Xiaohan Ding, Xiangyu Zhang**, Jungong Han, Guiguang Ding', '전체 인용횟수': '407회 인용2022202391310', '컨퍼런스': 'Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition', '페이지': '11963-11975', '학술 문서': 'Scaling up your kernels to 31x31: Revisiting large kernel design in cnnsX Ding, X Zhang, J Han, G Ding\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 2022407회 인용 관련 학술자료 전체 7개의 버전 '}, title='Scaling up your kernels to 31x31: Revisiting large kernel design in cnns', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The effects of titania nanotubes with embedded silver oxide nanoparticles on bacteria and osteoblasts': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/4/1', '게시자': 'Elsevier', '권': '35', '설명': 'A versatile strategy to endow biomaterials with long-term antibacterial ability without compromising the cytocompatibility is highly desirable to combat biomaterial related infection. TiO2 nanotube (NT) arrays can significantly enhance the functions of many cell types including osteoblasts thus having promising applications in orthopedics, orthodontics, as well as other biomedical fields. In this study, TiO2 NT arrays with Ag2O nanoparticle embedded in the nanotube wall (NT-Ag2O arrays) are prepared on titanium (Ti) by TiAg magnetron sputtering and anodization. Well-defined NT arrays containing Ag concentrations in a wide range from 0 to 15 at % are formed. Ag incorporation has little influence on the NT diameter, but significantly decreases the tube length. Crystallized Ag2O nanoparticles with diameters ranging from 5\\xa0nm to 20\\xa0nm are embedded in the amorphous TiO2 nanotube wall and this unique structure\\xa0…', '저널': 'Biomaterials', '저자': 'Ang Gao, Ruiqiang Hang, Xiaobo Huang, Lingzhou Zhao, Xiangyu Zhang, Lin Wang, Bin Tang, Shengli Ma, Paul K Chu', '전체 인용횟수': '355회 인용201420152016201720182019202020212022202310293557503139512524', '페이지': '4223-4235', '학술 문서': 'The effects of titania nanotubes with embedded silver oxide nanoparticles on bacteria and osteoblastsA Gao, R Hang, X Huang, L Zhao, X Zhang, L Wang…\\xa0- Biomaterials, 2014355회 인용 관련 학술자료 전체 11개의 버전 ', '호': '13'}, title='The effects of titania nanotubes with embedded silver oxide nanoparticles on bacteria and osteoblasts', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A multifaceted coating on titanium dictates osteoimmunomodulation and osteo/angio-genesis towards ameliorative osseointegration': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/4/1', '게시자': 'Elsevier', '권': '162', '설명': \"A multifaceted coating for hard tissue implants, with favorable osteogenesis, angiogenesis, and osteoimmunomodulation abilities, would be of great value since it could improve osseointegration and alleviate prosthesis loosening. However, to date there are few coatings that fully satisfy these criteria. Herein we describe a microporous TiO2 coating decorated with hydroxyapatite (HA) nanoparticles that is generated by micro-arc oxidation of pure titanium (Ti) and followed annealing. By altering the annealing temperature, it is possible to simultaneously tune the coating's physical (morphology and wettability) and chemical (composites and crystallinity) properties. A coating produced with micro-arc oxidization (MAO) with an annealing temperature of 650\\u202f°C (MAO-650) exhibits numerous favorable physicochemical properties, such as hybrid micro-nano morphology, superhydrophilicity, and highly crystalline HA\\xa0…\", '저널': 'Biomaterials', '저자': 'Long Bai, Zhibin Du, Jingjing Du, Wei Yao, Jiaming Zhang, Zeming Weng, Si Liu, Ya Zhao, Yanlian Liu, Xiangyu Zhang, Xiaobo Huang, Xiaohong Yao, Ross Crawford, Ruiqiang Hang, Di Huang, Bin Tang, Yin Xiao', '전체 인용횟수': '212회 인용201820192020202120222023112128575635', '페이지': '154-169', '학술 문서': 'A multifaceted coating on titanium dictates osteoimmunomodulation and osteo/angio-genesis towards ameliorative osseointegrationL Bai, Z Du, J Du, W Yao, J Zhang, Z Weng, S Liu…\\xa0- Biomaterials, 2018212회 인용 관련 학술자료 전체 6개의 버전 '}, title='A multifaceted coating on titanium dictates osteoimmunomodulation and osteo/angio-genesis towards ameliorative osseointegration', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Microstructure and antibacterial properties of Cu-doped TiO2 coating on titanium by micro-arc oxidation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/2/15', '게시자': 'North-Holland', '권': '292', '설명': 'Infection associated with titanium implants remains the most common serious complication after surgery. In this work, Cu-doped antibacterial TiO2 coating was synthesized by micro-arc oxidation of titanium in an electrolyte bearing Cu nanoparticles. Surface morphology and structure of the coating were characterized with scanning electron microscopy (SEM), X-ray diffraction (XRD), and X-ray photoelectron spectroscopy (XPS). The results indicated that Cu nanoparticles were not only distributed on the surface and inside the pores but also embedded in the coating. Cu mainly exists in the Cu2+ state in the TiO2 coating. The Cu-doped coating exhibited excellent antibacterial activities against Escherichia coli (E. coli) and Staphylococcus aureus (S. aureus).', '저널': 'Applied Surface Science', '저자': 'Xiaohong Yao, Xiangyu Zhang, Haibo Wu, Linhai Tian, Yong Ma, Bin Tang', '전체 인용횟수': '148회 인용2014201520162017201820192020202120222023351716131726211910', '페이지': '944-947', '학술 문서': 'Microstructure and antibacterial properties of Cu-doped TiO2 coating on titanium by micro-arc oxidationX Yao, X Zhang, H Wu, L Tian, Y Ma, B Tang\\xa0- Applied Surface Science, 2014148회 인용 관련 학술자료 전체 9개의 버전 '}, title='Microstructure and antibacterial properties of Cu-doped TiO2 coating on titanium by micro-arc oxidation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Microenvironment of alginate-based microcapsules for cell culture and tissue engineering': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012/7/1', '게시자': 'Elsevier', '권': '114', '설명': 'As a type of 3D model, the technology of microencapsulation holds significant promise for tissue engineering and cell therapy due to its unique performance. The microenvironmental factors within microcapsules play an important role in influencing the behaviors of encapsulated cells. The aim of this review article is to give an overview on the construction of the microenvironmental factors, which include 3D space, physicochemical properties of alginate matrix, cell spheroids, nutritional status, and so on. Furthermore, we clarified the effect of microenvironmental factors on the behaviors of encapsulated cells and the methods about improving the microenvironment of microcapsules. This review will help to understand the interaction of the microenvironment and the encapsulated cells and lay a solid foundation for microcapsule-based cell therapy and tissue engineering.', '저자': 'Xiaobo Huang, Xiangyu Zhang, Xiaoguang Wang, Chan Wang, Bin Tang', '전체 인용횟수': '124회 인용201220132014201520162017201820192020202120222023111171715138914955', '출처': 'Journal of bioscience and bioengineering', '페이지': '1-8', '학술 문서': 'Microenvironment of alginate-based microcapsules for cell culture and tissue engineeringX Huang, X Zhang, X Wang, C Wang, B Tang\\xa0- Journal of bioscience and bioengineering, 2012124회 인용 관련 학술자료 전체 11개의 버전 ', '호': '1'}, title='Microenvironment of alginate-based microcapsules for cell culture and tissue engineering', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A bifunctional hydrogel incorporated with CuS@MoS2 microspheres for disinfection and improved wound healing': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020/2', '권': '382', '설명': 'Recent advances in antibacterial technology has made it possible to obviate the needs for antibiotics to combat bacterial infection during would healing. However, few current wound dressings can simultaneously kill bacteria efficiently and promote would healing by facilitating revascularization. Herein, a hybrid hydrogel embedded with CuS@MoS2 microspheres is synthesized. This hydrogel exhibits outstanding antibacterial activities in a short time and enhances wound healing at the same time. Within 15\\u202fmin, 99.3% of Escherichia coli (E. coli) and 99.5% of Staphylococcus aureus (S. aureus) are killed due to the synergistic effects rendered by the photodynamic and photothermal antibacterial treatments under co-irradiation of 660\\u202fnm visible light (VL) and 808 near infrared (NIR) light. The synergistic effects rendered by hyperthermia and reactive oxygen species (ROS) generated by the hydrogel during light\\xa0…', '저널': 'Chemical Engineering Journal', '저자': 'Xiangyu Zhang Xingyu Zhang, Guannan Zhang, Hongyu Zhang, Xiaoping Liu, Jing Shi, Huixian Shi, Xiaohong Yao, Paul K. Chu', '전체 인용횟수': '122회 인용202020212022202311294932', '페이지': '122849', '학술 문서': 'A bifunctional hydrogel incorporated with CuS@ MoS2 microspheres for disinfection and improved wound healingX Zhang, G Zhang, H Zhang, X Liu, J Shi, H Shi, X Yao…\\xa0- Chemical Engineering Journal, 2020122회 인용 관련 학술자료 전체 4개의 버전 ', '호': '15'}, title='A bifunctional hydrogel incorporated with CuS@MoS2 microspheres for disinfection and improved wound healing', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Biocompatibility, corrosion resistance and antibacterial activity of TiO2/CuO coating on titanium': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'In this work, TiO2/CuO coating was prepared on titanium (Ti) by combination of magnetron sputtering and annealing treatment. The microstructure, biocompatibility, corrosion resistance and antibacterial property of TiO2/CuO coating were investigated in comparison with pure Ti and TiO2 coating. The results show that TiO2/CuO coating is mainly composed of TiO2 and CuO. In vitro cytocompatibility evaluation suggests that no obvious toxicity appears on the TiO2/CuO coating, and the coating stimulates the osteoblast spreading and proliferation. Compared with Ti and TiO2 coating, TiO2/CuO coating exhibits improved corrosion resistance and antibacterial ability against S.aureus. This study is the first attempt to apply the combination of magnetron sputtering and annealing treatment to introduce the Cu into TiO2 coating for surface modification of Ti-based implant materials, which may provide a research foundation\\xa0…', '저널': 'Ceramics Internationa', '저자': 'Xiangyu Zhang Xiaojing He, Guannan Zhang, Xin Wang, Ruiqiang Hang, Xiaobo Huang, Lin Qin, Bin Tang', '전체 인용횟수': '114회 인용20172018201920202021202220231141924211619', '학술 문서': 'Biocompatibility, corrosion resistance and antibacterial activity of TiO2/CuO coating on titaniumX He, G Zhang, X Wang, R Hang, X Huang, L Qin…\\xa0- Ceramics International, 2017114회 인용 관련 학술자료 '}, title='Biocompatibility, corrosion resistance and antibacterial activity of TiO2/CuO coating on titanium', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Antimicrobial property, cytocompatibility and corrosion resistance of Zn-doped ZrO2/TiO2 coatings on Ti6Al4V implants': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/6/1', '게시자': 'Elsevier', '권': '75', '설명': 'Zn-doped ZrO2/TiO2 porous coatings (Zn-ZrO2/TiO2) were prepared on the surface of titanium alloy (Ti6Al4V) by a hybrid approach of magnetron sputtering and micro-arc oxidation (MAO). The microstructures, phase constituents and elemental states of the coating were investigated by scanning electron microscope (SEM) equipped with energy dispersive spectroscopy (EDS), X-ray diffraction (XRD), and X-ray photoelectron spectroscopy (XPS). The results demonstrate that the Zn-ZrO2/TiO2 coatings are porous and its thickness is approximately 13\\xa0μm. The major phases in the oxidation coating are tetragonal ZrO2 (t-ZrO2), cubic ZrO2 (c-ZrO2) and rutile TiO2. XPS result reveals that Zn exists as ZnO in the Zn-ZrO2/TiO2 coatings. The biological experiments indicate that Zn-ZrO2/TiO2 coatings exhibit not only excellent antibacterial property against Gram-positive Staphylococcus aureus (S. aureus), but also favorable\\xa0…', '저널': 'Materials Science and Engineering: C', '저자': 'Ruoyun Wang, Xiaojing He, Yuee Gao, Xiangyu Zhang, Xiaohong Yao, Bin Tang', '전체 인용횟수': '110회 인용20172018201920202021202220237151319221618', '페이지': '7-15', '학술 문서': 'Antimicrobial property, cytocompatibility and corrosion resistance of Zn-doped ZrO2/TiO2 coatings on Ti6Al4V implantsR Wang, X He, Y Gao, X Zhang, X Yao, B Tang\\xa0- Materials Science and Engineering: C, 2017110회 인용 관련 학술자료 전체 5개의 버전 '}, title='Antimicrobial property, cytocompatibility and corrosion resistance of Zn-doped ZrO2/TiO2 coatings on Ti6Al4V implants', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Synergistic antibacterial activity of physical-chemical multi-mechanism by TiO2 nanorod arrays for safe biofilm eradication on implant': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021/1/1', '게시자': 'Elsevier', '권': '6', '설명': 'Treatment of implant-associated infection is becoming more challenging, especially when bacterial biofilms form on the surface of the implants. Developing multi-mechanism antibacterial methods to combat bacterial biofilm infections by the synergistic effects are superior to those based on single modality due to avoiding the adverse effects arising from the latter. In this work, TiO2 nanorod arrays in combination with irradiation with 808 near-infrared (NIR) light are proven to eradicate single specie biofilms by combining photothermal therapy, photodynamic therapy, and physical killing of bacteria. The TiO2 nanorod arrays possess efficient photothermal conversion ability and produce a small amount of reactive oxygen species (ROS). Physiologically, the combined actions of hyperthermia, ROS, and puncturing by nanorods give rise to excellent antibacterial properties on titanium requiring irradiation for only 15\\xa0min as\\xa0…', '저널': 'Bioactive materials', '저자': 'Xiangyu Zhang, Guannan Zhang, Maozhou Chai, Xiaohong Yao, Weiyi Chen, Paul K Chu', '전체 인용횟수': '107회 인용202120222023293840', '페이지': '12-25', '학술 문서': 'Synergistic antibacterial activity of physical-chemical multi-mechanism by TiO2 nanorod arrays for safe biofilm eradication on implantX Zhang, G Zhang, M Chai, X Yao, W Chen, PK Chu\\xa0- Bioactive materials, 2021107회 인용 관련 학술자료 전체 10개의 버전 ', '호': '1'}, title='Synergistic antibacterial activity of physical-chemical multi-mechanism by TiO2 nanorod arrays for safe biofilm eradication on implant', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'DLP printing photocurable chitosan to build bio-constructs for tissue engineering': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020/5/1', '게시자': 'Elsevier', '권': '235', '설명': 'In this study, we provide a photocurable chitosan bioink (CHI-MA), which can be used for the digital light processing (DLP) technology. The CHI-MA precursors were facilely synthesized by grafting chitosan molecular chains with methacryloyl groups. We investigated the effect of parameters, including the concentration and substitution degree (DS) of CHI-MA, on the rheology and the photocuring of bioinks and the mechanical property of photo-crosslinked gels. Using the CHI-MA with a high DS (33.6 %), the curing time to print a 150\\u202fμm thick hydrogel layer can be controlled within a reasonable short time period. Additionally, the cytotoxicity test shows that both the photocuring process and the photo-crosslinked hydrogels exhibit an excellent biocompatibility. Through the DLP printing, the CHI-MA bioink can be processed into complex 3D hydrogel structures with high-resolution, high-fidelity and good biocompatibility\\xa0…', '저널': 'Carbohydrate polymers', '저자': 'Yi Shen, Haifeng Tang, Xiaobo Huang, Ruiqiang Hang, Xiangyu Zhang, Yueyue Wang, Xiaohong Yao', '전체 인용횟수': '105회 인용20202021202220236233638', '페이지': '115970', '학술 문서': 'DLP printing photocurable chitosan to build bio-constructs for tissue engineeringY Shen, H Tang, X Huang, R Hang, X Zhang, Y Wang…\\xa0- Carbohydrate polymers, 2020105회 인용 관련 학술자료 전체 5개의 버전 '}, title='DLP printing photocurable chitosan to build bio-constructs for tissue engineering', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Differential effect of hydroxyapatite nano-particle versus nano-rod decorated titanium micro-surface on osseointegration': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/8/1', '게시자': 'Elsevier', '권': '76', '설명': 'Coating materials applied for intraosseous implants must be optimized to stimulate osseointegration. Osseointegration is a temporal and spatial physiological process that not only requires interactions between osteogenesis and angiogenesis but also necessitates a favorable immune microenvironment. It is now well-documented that hierarchical nano-micro surface structures promote the long-term stability of implants, the interactions between nano-micro structure and the immune response are largely unknown. Here, we report the effects of microporous titanium (Ti) surfaces coated with nano-hydroxyapatite (HA) produced by micro-arc oxidation and steam-hydrothermal treatment (SHT) on multiple cell behavior and osseointegration. By altering the processing time of SHT it was possible to shift HA structures from nano-particles to nano-rods on the microporous Ti surfaces. Ti surfaces coated with HA nano-particles\\xa0…', '저널': 'Acta biomaterialia', '저자': 'Long Bai, Yanlian Liu, Zhibin Du, Zeming Weng, Wei Yao, Xiangyu Zhang, Xiaobo Huang, Xiaohong Yao, Ross Crawford, Ruiqiang Hang, Di Huang, Bin Tang, Yin Xiao', '전체 인용횟수': '100회 인용2018201920202021202220232916282122', '페이지': '344-358', '학술 문서': 'Differential effect of hydroxyapatite nano-particle versus nano-rod decorated titanium micro-surface on osseointegrationL Bai, Y Liu, Z Du, Z Weng, W Yao, X Zhang, X Huang…\\xa0- Acta biomaterialia, 2018100회 인용 관련 학술자료 전체 5개의 버전 '}, title='Differential effect of hydroxyapatite nano-particle versus nano-rod decorated titanium micro-surface on osseointegration', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Effects of copper nanoparticles in porous TiO2 coatings on bacterial resistance and cytocompatibility of osteoblasts and endothelial cells': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'Copper (Cu) has garnered increasing interest due to its excellent antimicrobial activity and important roles in human metabolism. Although the biological effects of Cu have been studied, the effects of Cu nanoparticles (NPs) on cell behavior are not well understood. In this study, porous TiO2 coatings doped with different amounts of Cu NPs (designated as 0 Cu, 0.3 Cu, and 3.0 Cu) are deposited on titanium by micro-arc oxidation (MAO). The Cu NPs coated samples exhibit excellent antibacterial activity against Staphylococcus aureus (S. aureus). In vitro cytocompatibility evaluation discloses that 0 Cu and 0.3 Cu have no toxicity to osteoblasts but 3.0 Cu shows cytotoxicity. 0.3 Cu promotes proliferation and adhesion of osteoblasts and enhances extracellular matrix mineralization (ECM), but has little effects on the alkaline phosphatase activity (ALP) and collagen secretion. Surprisingly, the Cu NPs coated samples\\xa0…', '저널': 'Materials Science and Engineering C,', '저자': 'Paul K. Chu Xiangyu Zhang, Jianfang Li, Xin Wang, Yueyue Wang, Ruiqiang Hang, Xiaobo Huang, Bin Tang', '전체 인용횟수': '95회 인용2018201920202021202220235119222224', '페이지': '110–120', '학술 문서': 'Effects of copper nanoparticles in porous TiO2 coatings on bacterial resistance and cytocompatibility of osteoblasts and endothelial cellsX Zhang, J Li, X Wang, Y Wang, R Hang, X Huang…\\xa0- Materials Science and Engineering: C, 201895회 인용 관련 학술자료 전체 7개의 버전 ', '호': '82'}, title='Effects of copper nanoparticles in porous TiO2 coatings on bacterial resistance and cytocompatibility of osteoblasts and endothelial cells', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Preparation, antibacterial effects and corrosion resistant of porous Cu–TiO2 coatings': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/7/30', '게시자': 'North-Holland', '권': '308', '설명': 'Antibacterial TiO2 coatings with different concentrations of Cu (Cu–TiO2) were prepared by micro-arc oxidation (MAO) on pre-sputtered CuTi films. The effect of Cu concentrations in CuTi films on the MAO process was investigated. The Cu–TiO2 coatings were analyzed by scanning electron microscopy (SEM), X-ray photoelectron spectroscopy (XPS) and X-ray diffraction (XRD). The corrosion resistance of Cu–TiO2 coatings was evaluated via potentiodynamic polarization method. The antibacterial properties were assessed by two methods: spread plate method and fluorescence staining. The experimental results demonstrate that the coatings are porous and consist of anatase phase, rutile phase and unoxidized titanium. The CuTi films are almost completely oxidized and the thickness of all MAO coatings is about 5–10\\xa0μm. Cu mainly exists as CuO in the TiO2 coatings. The Cu–TiO2 coatings exhibit excellent\\xa0…', '저널': 'Applied Surface Science', '저자': 'Haibo Wu, Xiangyu Zhang, Zhenhua Geng, Yan Yin, Ruiqiang Hang, Xiaobo Huang, Xiaohong Yao, Bin Tang', '전체 인용횟수': '95회 인용2014201520162017201820192020202120222023221214712178714', '페이지': '43-49', '학술 문서': 'Preparation, antibacterial effects and corrosion resistant of porous Cu–TiO2 coatingsH Wu, X Zhang, Z Geng, Y Yin, R Hang, X Huang…\\xa0- Applied Surface Science, 201495회 인용 관련 학술자료 전체 9개의 버전 '}, title='Preparation, antibacterial effects and corrosion resistant of porous Cu–TiO2 coatings', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'In vitro assessments on bacterial adhesion and corrosion performance of TiN coating on Ti6Al4V titanium alloy synthesized by multi-arc ion plating': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012/7/1', '게시자': 'North-Holland', '권': '258', '설명': 'TiN coating was synthesized on Ti6Al4V titanium alloy surface by multi-arc ion plating (MIP) technique. Surface morphology, cross sectional microstructure, elemental distributions and phase compositions of the obtained coating were analyzed by means of scanning electron microscope (SEM), optical microscope (OM), glow discharge optical emission spectroscope (GDOES) and X-ray diffraction (XRD). Bacterial adhesion and corrosion performance of Ti6Al4V and the TiN coating were assessed via in vitro bacterial adhesion tests and corrosion experiments, respectively. The results indicated that continuous and compact coating which was built up by pure TiN with a typical columnar crystal structure has reached a thickness of 1.5μm. This TiN coating could significantly reduce the bacterial adhesion and enhance the corrosion resistance of Ti6Al4V substrate.', '저널': 'Applied surface science', '저자': 'Naiming Lin, Xiaobo Huang, Xiangyu Zhang, Ailan Fan, Lin Qin, Bin Tang', '전체 인용횟수': '90회 인용20122013201420152016201720182019202020212022202323611510610119116', '페이지': '7047-7051', '학술 문서': 'In vitro assessments on bacterial adhesion and corrosion performance of TiN coating on Ti6Al4V titanium alloy synthesized by multi-arc ion platingN Lin, X Huang, X Zhang, A Fan, L Qin, B Tang\\xa0- Applied surface science, 201290회 인용 관련 학술자료 전체 8개의 버전 ', '호': '18'}, title='In vitro assessments on bacterial adhesion and corrosion performance of TiN coating on Ti6Al4V titanium alloy synthesized by multi-arc ion plating', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Antibacterial activity and cytocompatibility of Cu–Ti–O nanotubes': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/6', '권': '102', '설명': ' TiO2 nanotubes (NTs) have favorable biological properties, but the poor antibacterial activity limits their application especially in orthopedics fields. In this article, Cu–Ti–O nanotubes with different Cu contents are fabricated on sputtered TiCu films. Scanning electron microscopy reveals the NTs can be formed on sputtered TiCu films when the Cu content is less than 14.6 at %. X‐ray photoelectron spectroscopy results indicate the NTs are consist of CuO mixed with TiO2 and the Cu content in NTs decreases dramatically compared with that in TiCu films. Biological experiments show that although these NTs have poor release antibacterial activity, their contact antibacterial activity has proven to be excellent, indicating the NT surface can effectively inhibit biomaterial‐associated infections. The cytocompatibility of the NTs is closely related to the Cu content and when its content is relatively low (1.01 at %), there is no\\xa0…', '저널': 'Journal of Biomedical Materials Research Part A', '저자': 'Ruiqiang Hang, Ang Gao, Xiaobo Huang, Xiaoguang Wang, Xiangyu Zhang, Lin Qin, Bin Tang', '전체 인용횟수': '87회 인용20142015201620172018201920202021202220234592011410977', '페이지': '1850-1858', '학술 문서': 'Antibacterial activity and cytocompatibility of Cu–Ti–O nanotubesR Hang, A Gao, X Huang, X Wang, X Zhang, L Qin…\\xa0- Journal of Biomedical Materials Research Part A, 201487회 인용 관련 학술자료 전체 6개의 버전 ', '호': '6'}, title='Antibacterial activity and cytocompatibility of Cu–Ti–O nanotubes', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Near-infrared light II-assisted rapid biofilm elimination platform for bone implants at mild temperature': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021/2/1', '게시자': 'Elsevier', '권': '269', '설명': 'Light-triggered therapy is a prospective method to combat implant-associated infection but near-infrared I (NIR-I) light has insufficient penetrating ability in tissues and local hyperthermia induced by the photothermal treatment may destroy surrounding healthy tissues. Herein, a near-infrared II (NIR-II) phototherapy system composed of upconversion elements doped titanium dioxide nanorods (TiO2 NRs)/curcumin (Cur)/hyaluronic acid (HA)/bone morphogenetic protein-2 (BMP-2) is designed for biomedical titanium and demonstrated to overcome the above hurdles simultaneously. Incorporation of F, Yb, and Ho not only improves the photocatalytic ability, but also renders the implants with the upconversion capability, so that the NRs can generate enough reactive oxygen species (ROS) when irradiated by the NIR-II laser. Furthermore, the combined actions of quorum sensing inhibitors, ROS, and physical puncture by\\xa0…', '저널': 'Biomaterials', '저자': 'Guannan Zhang, Yongqiang Yang, Jing Shi, Xiaohong Yao, Weiyi Chen, Xiaochun Wei, Xiangyu Zhang, Paul K Chu', '전체 인용횟수': '80회 인용20212022202363935', '페이지': '120634', '학술 문서': 'Near-infrared light II-assisted rapid biofilm elimination platform for bone implants at mild temperatureG Zhang, Y Yang, J Shi, X Yao, W Chen, X Wei…\\xa0- Biomaterials, 202180회 인용 관련 학술자료 전체 7개의 버전 '}, title='Near-infrared light II-assisted rapid biofilm elimination platform for bone implants at mild temperature', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Corrosion behavior of Zn-incorporated antibacterial TiO2 porous coating on titanium': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/11/15', '게시자': 'Elsevier', '권': '42', '설명': 'Incorporation of antibacterial agents (e.g. Ag and Cu) at the surface of biomedical materials has evolved as a potentially effective method for preventing the bacterial infections. However, the antibacterial efficacy of medical device implants must necessarily be balanced by good corrosion resistance and the corrosion behavior of the antibacterial coatings has seldom been reported. In this work, Zn-incorporated antibacterial TiO2 coating was produced on pure titanium (Ti) by micro-arc oxidization (MAO) and the electrochemical behavior was assessed. The results obtained from the antibacterial studies suggest that the Zn-incorporated TiO2 coating provides bactericidal activity against both Gram-negative Escherichia coli (E. coli) and Gram-positive Staphylococcus aureus (S. aureus) over 90%. The corrosion behavior of Zn-incorporated TiO2 coating were investigated using a combination of complementary\\xa0…', '저널': 'Ceramics International', '저자': 'Xiangyu Zhang, Huizhen Wang, Jiangfang Li, Xiaojing He, Ruiqiang Hang, Xiaobo Huang, Linhai Tian, Bin Tang', '전체 인용횟수': '80회 인용2016201720182019202020212022202319169919512', '페이지': '17095-17100', '학술 문서': 'Corrosion behavior of Zn-incorporated antibacterial TiO2 porous coating on titaniumX Zhang, H Wang, J Li, X He, R Hang, X Huang, L Tian…\\xa0- Ceramics International, 201680회 인용 관련 학술자료 전체 3개의 버전 ', '호': '15'}, title='Corrosion behavior of Zn-incorporated antibacterial TiO2 porous coating on titanium', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Facile synthesis of mesoporous Mn 3 O 4 nanorods as a promising anode material for high performance lithium-ion batteries': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '게시자': 'Royal Society of Chemistry', '권': '2', '설명': 'In this work, porous Mn3O4 nanorods have been fabricated through the decomposition of MnOOH nanorods under an inert gas. The sample shows a high BET surface area of 27.6 m2 g−1 and a narrow pore size distribution of 3.9 nm. Because of the excellent porous geometry and one-dimensional structure, the porous Mn3O4 nanorods display outstanding electrochemical performance, such as high specific capacity (901.5 mA h g−1 at a current density of 500 mA g−1), long cycling stability (coulombic efficiency of 99.3% after 150 cycles) and high rate capability (387.5 mA h g−1 at 2000 mA g−1). Very interestingly, the porous Mn3O4 nanorods are converted to Mn3O4 following electrochemical reaction, which does not occur with nonporous Mn3O4 nanorods. The possible reason may be ascribed to the improved kinetics of the porous structure.', '저널': 'Journal of Materials Chemistry A', '저자': 'Zhongchao Bai, Xiangyu Zhang, Yuwen Zhang, Chunli Guo, Bin Tang', '전체 인용횟수': '77회 인용2014201520162017201820192020202120222023113131410611351', '페이지': '16755-16760', '학술 문서': 'Facile synthesis of mesoporous Mn 3 O 4 nanorods as a promising anode material for high performance lithium-ion batteriesZ Bai, X Zhang, Y Zhang, C Guo, B Tang\\xa0- Journal of Materials Chemistry A, 201477회 인용 관련 학술자료 전체 2개의 버전 ', '호': '39'}, title='Facile synthesis of mesoporous Mn 3 O 4 nanorods as a promising anode material for high performance lithium-ion batteries', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Review of antibacterial activity of titanium-based implants’ surfaces fabricated by micro-arc oxidation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/3/22', '게시자': 'MDPI', '권': '7', '설명': 'Ti and its alloys are the most commonly-used materials for biomedical applications. However, bacterial infection after implant placement is still one of the significant rising complications. Therefore, the application of the antimicrobial agents into implant surfaces to prevent implant-associated infection has attracted much attention. Scientific papers have shown that inorganic antibacterial metal elements (e.g., Ag, Cu, Zn) can be introduced into implant surfaces with the addition of metal nanoparticles or metallic compounds into an electrolyte via micro-arc oxidation (MAO) technology. In this review, the effects of the composition and concentration of electrolyte and process parameters (e.g., voltage, current density, oxidation time) on the morphological characteristics (e.g., surface morphology, bonding strength), antibacterial ability and biocompatibility of MAO antimicrobial coatings are discussed in detail. Anti-infection and osseointegration can be simultaneously accomplished with the selection of the proper antibacterial elements and operating parameters. Besides, MAO assisted by magnetron sputtering (MS) to endow Ti-based implant materials with superior antibacterial ability and biocompatibility is also discussed. Finally, the development trend of MAO technology in the future is forecasted.', '저자': 'Xiaojing He, Xiangyu Zhang, Xin Wang, Lin Qin', '전체 인용횟수': '74회 인용201720182019202020212022202331181714714', '출처': 'Coatings', '페이지': '45', '학술 문서': 'Review of antibacterial activity of titanium-based implants’ surfaces fabricated by micro-arc oxidationX He, X Zhang, X Wang, L Qin\\xa0- Coatings, 201774회 인용 관련 학술자료 전체 10개의 버전 ', '호': '3'}, title='Review of antibacterial activity of titanium-based implants’ surfaces fabricated by micro-arc oxidation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Bactericidal behavior of Cu-containing stainless steel surfaces': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012/10/1', '게시자': 'North-Holland', '권': '258', '설명': 'Stainless steels are one of the most common materials used in health care environments. However, the lack of antibacterial advantage has limited their use in practical application. In this paper, antibacterial stainless steel surfaces with different Cu contents have been prepared by plasma surface alloying technology (PSAT). The steel surface with Cu content 90wt.% (Cu-SS) exhibits strong bactericidal activity against Escherichia coli (E. coli) and Staphylococcus aureus (S. aureus) within 3h. Although the Cu-containing surface with Cu content 2.5wt.% (CuNi-SS) can also kill all tested bacteria, this process needs 12h. SEM observation of the bacterial morphology and an agarose gel electrophoresis were performed to study the antibacterial mechanism of Cu-containing stainless steel surfaces against E. coli. The results indicated that Cu ions are released when the Cu-containing surfaces are in contact with bacterial\\xa0…', '저널': 'Applied Surface Science', '저자': 'Xiangyu Zhang, Xiaobo Huang, Yong Ma, Naiming Lin, Ailan Fan, Bin Tang', '전체 인용횟수': '73회 인용201320142015201620172018201920202021202220236771114626752', '페이지': '10058-10063', '학술 문서': 'Bactericidal behavior of Cu-containing stainless steel surfacesX Zhang, X Huang, Y Ma, N Lin, A Fan, B Tang\\xa0- Applied Surface Science, 201273회 인용 관련 학술자료 전체 7개의 버전 ', '호': '24'}, title='Bactericidal behavior of Cu-containing stainless steel surfaces', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Fabrication of Ni-Ti-O nanotube arrays by anodization of NiTi alloy and their potential applications': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/12/17', '게시자': 'Nature Publishing Group UK', '권': '4', '설명': 'Nickel-titanium-oxide (Ni-Ti-O) nanotube arrays (NTAs) prepared on nearly equiatomic NiTi alloy shall have broad application potential such as for energy storage and biomedicine, but their precise structure control is a great challenge because of the high content of alloying element of Ni, a non-valve metal that cannot form a compact electronic insulating passive layer when anodized. In the present work, we systemically investigated the influence of various anodization parameters on the formation and structure of Ni-Ti-O NTAs and their potential applications. Our results show that well controlled NTAs can be fabricated during relatively wide ranges of the anodization voltage (5–90\\u2005V), electrolyte temperature (10–50°C) and electrolyte NH4F content (0.025–0.8\\u2005wt%) but within a narrow window of the electrolyte H2O content (0.0–1.0\\u2005vol%). Through modulating these parameters, the Ni-Ti-O NTAs with different\\xa0…', '저널': 'Scientific reports', '저자': 'Ruiqiang Hang, Yanlian Liu, Lingzhou Zhao, Ang Gao, Long Bai, Xiaobo Huang, Xiangyu Zhang, Bin Tang, Paul K Chu', '전체 인용횟수': '69회 인용20152016201720182019202020212022202319710591477', '페이지': '7547', '학술 문서': 'Fabrication of Ni-Ti-O nanotube arrays by anodization of NiTi alloy and their potential applicationsR Hang, Y Liu, L Zhao, A Gao, L Bai, X Huang, X Zhang…\\xa0- Scientific reports, 201469회 인용 관련 학술자료 전체 13개의 버전 ', '호': '1'}, title='Fabrication of Ni-Ti-O nanotube arrays by anodization of NiTi alloy and their potential applications', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Exploiting macrophage autophagy-lysosomal biogenesis as a therapy for atherosclerosis': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/6/7', '게시자': 'Nature Publishing Group UK', '권': '8', '설명': 'Macrophages specialize in removing lipids and debris present in the atherosclerotic plaque. However, plaque progression renders macrophages unable to degrade exogenous atherogenic material and endogenous cargo including dysfunctional proteins and organelles. Here we show that a decline in the autophagy–lysosome system contributes to this as evidenced by a derangement in key autophagy markers in both mouse and human atherosclerotic plaques. By augmenting macrophage TFEB, the master transcriptional regulator of autophagy–lysosomal biogenesis, we can reverse the autophagy dysfunction of plaques, enhance aggrephagy of p62-enriched protein aggregates and blunt macrophage apoptosis and pro-inflammatory IL-1β levels, leading to reduced atherosclerosis. In order to harness this degradative response therapeutically, we also describe a natural sugar called trehalose as an inducer of\\xa0…', '저널': 'Nature communications', '저자': 'Ismail Sergin, Trent D Evans, Xiangyu Zhang, Somashubhra Bhattacharya, Carl J Stokes, Eric Song, Sahl Ali, Babak Dehestani, Karyn B Holloway, Paul S Micevych, Ali Javaheri, Jan R Crowley, Andrea Ballabio, Joel D Schilling, Slava Epelman, Conrad C Weihl, Abhinav Diwan, Daping Fan, Mohamed A Zayed, Babak Razani', '전체 인용횟수': '268회 인용20172018201920202021202220235244347625234', '페이지': '15750', '학술 문서': 'Exploiting macrophage autophagy-lysosomal biogenesis as a therapy for atherosclerosisI Sergin, TD Evans, X Zhang, S Bhattacharya…\\xa0- Nature communications, 2017268회 인용 관련 학술자료 전체 15개의 버전 ', '호': '1'}, title='Exploiting macrophage autophagy-lysosomal biogenesis as a therapy for atherosclerosis', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'TFEB and trehalose drive the macrophage autophagy-lysosome system to protect against atherosclerosis': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/4/3', '게시자': 'Taylor & Francis', '권': '14', '설명': 'In the atherosclerotic plaque, macrophages are the key catabolic workhorse responsible for clearing lipid and dead cell debris. To survive the highly proinflammatory and lipotoxic plaque environment, macrophages must adopt strategies for maintaining tight homeostasis and self-renewal. Macroautophagy/autophagy is a pro-survival cellular pathway wherein damaged or excess cellular cargoes are encapsulated by a double-membrane compartment and delivered to the lysosome for hydrolysis. Previously, macrophage-specific autophagy deficiency has been shown to be atherogenic through several complementary mechanisms including hyperactivation of the inflammasome, defective efferocytosis, accumulation of cytotoxic protein aggregates, and impaired lipid degradation. Conversely, in a recent study we hypothesized that enhancing the macrophage autophagy-lysosomal system through genetic or\\xa0…', '저널': 'Autophagy', '저자': 'Trent D Evans, Se-Jin Jeong, Xiangyu Zhang, Ismail Sergin, Babak Razani', '전체 인용횟수': '134회 인용20182019202020212022202321527333522', '페이지': '724-726', '학술 문서': 'TFEB and trehalose drive the macrophage autophagy-lysosome system to protect against atherosclerosisTD Evans, SJ Jeong, X Zhang, I Sergin, B Razani\\xa0- Autophagy, 2018134회 인용 관련 학술자료 전체 8개의 버전 ', '호': '4'}, title='TFEB and trehalose drive the macrophage autophagy-lysosome system to protect against atherosclerosis', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Acetyl-CoA derived from hepatic peroxisomal β-oxidation inhibits autophagy and promotes steatosis via mTORC1 activation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020/7/2', '게시자': 'Elsevier', '권': '79', '설명': 'Autophagy is activated by prolonged fasting but cannot overcome the ensuing hepatic lipid overload, resulting in fatty liver. Here, we describe a peroxisome-lysosome metabolic link that restricts autophagic degradation of lipids. Acyl-CoA oxidase 1 (Acox1), the enzyme that catalyzes the first step in peroxisomal β-oxidation, is enriched in liver and further increases with fasting or high-fat diet (HFD). Liver-specific Acox1 knockout (Acox1-LKO) protected mice against hepatic steatosis caused by starvation or HFD due to induction of autophagic degradation of lipid droplets. Hepatic Acox1 deficiency markedly lowered total cytosolic acetyl-CoA levels, which led to decreased Raptor acetylation and reduced lysosomal localization of mTOR, resulting in impaired activation of mTORC1, a central regulator of autophagy. Dichloroacetic acid treatment elevated acetyl-CoA levels, restored mTORC1 activation, inhibited autophagy\\xa0…', '저널': 'Molecular cell', '저자': 'Anyuan He, Xiaowen Chen, Min Tan, Yali Chen, Dongliang Lu, Xiangyu Zhang, John M Dean, Babak Razani, Irfan J Lodhi', '전체 인용횟수': '109회 인용20202021202220236163155', '페이지': '30-42. e4', '학술 문서': 'Acetyl-CoA derived from hepatic peroxisomal β-oxidation inhibits autophagy and promotes steatosis via mTORC1 activationA He, X Chen, M Tan, Y Chen, D Lu, X Zhang, JM Dean…\\xa0- Molecular cell, 2020109회 인용 관련 학술자료 전체 9개의 버전 ', '호': '1'}, title='Acetyl-CoA derived from hepatic peroxisomal β-oxidation inhibits autophagy and promotes steatosis via mTORC1 activation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Protective Effect of Procyanidin B2 against CCl4-Induced Acute Liver Injury in Mice': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/7/3', '게시자': 'MDPI', '권': '20', '설명': 'Procyanidin B2 has demonstrated several health benefits and medical properties. However, its protective effects against CCl4-induced hepatotoxicity have not been clarified. The present study aimed to investigate the hepatoprotective effects of procyanidin B2 in CCl4-treated mice. Our data showed that procyanidin B2 significantly decreased the CCl4-induced elevation of serum alanine aminotransferase activities, as well as improved hepatic histopathological abnormalities. Procyanidin B2 also significantly decreased the content of MDA but enhanced the activities of antioxidant enzymes SOD, CAT and GSH-Px. Further research demonstrated that procyanidin B2 decreased the expression of TNF-α, IL-1β, cyclooxygenase-2 (COX-2) and inducible nitric oxide synthase (iNOS), as well as inhibited the translocation of nuclear factor-kappa B (NF-κB) p65 from the cytosol to the nuclear fraction in mouse liver. Moreover, CCl4-induced apoptosis in mouse liver was measured by (terminal-deoxynucleotidyl transferase mediated nick end labeling) TUNEL assay and the cleaved caspase-3. Meanwhile, the expression of apoptosis-related proteins Bax and Bcl-xL was analyzed by Western blot. Results showed that procyanidin B2 significantly inhibited CCl4-induced hepatocyte apoptosis, markedly suppressed the upregulation of Bax expression and restored the downregulation of Bcl-xL expression. Overall, the findings indicated that procyanidin B2 exhibited a protective effect on CCl4-induced hepatic injury by elevating the antioxidative defense potential and consequently suppressing the inflammatory response and apoptosis of liver tissues.', '저널': 'Molecules', '저자': 'Bing-Ya Yang, Xiang-Yu Zhang, Sheng-Wen Guan, Zi-Chun Hua', '전체 인용횟수': '109회 인용2016201720182019202020212022202315921131413158', '페이지': '12250-12265', '학술 문서': 'Protective effect of procyanidin B2 against CCl4-induced acute liver injury in miceBY Yang, XY Zhang, SW Guan, ZC Hua\\xa0- Molecules, 2015109회 인용 관련 학술자료 전체 11개의 버전 ', '호': '7'}, title='Protective Effect of Procyanidin B2 against CCl4-Induced Acute Liver Injury in Mice', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Peroxisome-derived lipids regulate adipose thermogenesis by mediating cold-induced mitochondrial fission': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019/2/1', '게시자': 'American Society for Clinical Investigation', '권': '129', '설명': 'Peroxisomes perform essential functions in lipid metabolism, including fatty acid oxidation and plasmalogen synthesis. Here, we describe a role for peroxisomal lipid metabolism in mitochondrial dynamics in brown and beige adipocytes. Adipose tissue peroxisomal biogenesis was induced in response to cold exposure through activation of the thermogenic coregulator PRDM16. Adipose-specific knockout of the peroxisomal biogenesis factor Pex16 (Pex16-AKO) in mice impaired cold tolerance, decreased energy expenditure, and increased diet-induced obesity. Pex16 deficiency blocked cold-induced mitochondrial fission, decreased mitochondrial copy number, and caused mitochondrial dysfunction. Adipose-specific knockout of the peroxisomal β-oxidation enzyme acyl-CoA oxidase 1 (Acox1-AKO) was not sufficient to affect adiposity, thermogenesis, or mitochondrial copy number, but knockdown of the\\xa0…', '저널': 'The Journal of clinical investigation', '저자': 'Hongsuk Park, Anyuan He, Min Tan, Jordan M Johnson, John M Dean, Terri A Pietka, Yali Chen, Xiangyu Zhang, Fong-Fu Hsu, Babak Razani, Katsuhiko Funai, Irfan J Lodhi', '전체 인용횟수': '106회 인용20192020202120222023422252727', '페이지': '694-711', '학술 문서': 'Peroxisome-derived lipids regulate adipose thermogenesis by mediating cold-induced mitochondrial fissionH Park, A He, M Tan, JM Johnson, JM Dean, TA Pietka…\\xa0- The Journal of clinical investigation, 2019106회 인용 관련 학술자료 전체 13개의 버전 ', '호': '2'}, title='Peroxisome-derived lipids regulate adipose thermogenesis by mediating cold-induced mitochondrial fission', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Inclusion bodies enriched for p62 and polyubiquitinated proteins in macrophages protect against atherosclerosis': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/1/5', '게시자': 'American Association for the Advancement of Science', '권': '9', '설명': 'Autophagy is a catabolic cellular mechanism that degrades dysfunctional proteins and organelles. Atherosclerotic plaque formation is enhanced in mice with macrophages deficient for the critical autophagy protein ATG5. We showed that exposure of macrophages to lipids that promote atherosclerosis increased the abundance of the autophagy chaperone p62 and that p62 colocalized with polyubiquitinated proteins in cytoplasmic inclusions, which are characterized by insoluble protein aggregates. ATG5-null macrophages developed further p62 accumulation at the sites of large cytoplasmic ubiquitin-positive inclusion bodies. Aortas from atherosclerotic mice and plaques from human endarterectomy samples showed increased abundance of p62 and polyubiquitinated proteins that colocalized with plaque macrophages, suggesting that p62-enriched protein aggregates were characteristic of atherosclerosis. The\\xa0…', '저널': 'Science signaling', '저자': 'Ismail Sergin, Somashubhra Bhattacharya, Roy Emanuel, Emel Esen, Carl J Stokes, Trent D Evans, Batool Arif, John A Curci, Babak Razani', '전체 인용횟수': '93회 인용20172018201920202021202220231511916141610', '페이지': 'ra2-ra2', '학술 문서': 'Inclusion bodies enriched for p62 and polyubiquitinated proteins in macrophages protect against atherosclerosisI Sergin, S Bhattacharya, R Emanuel, E Esen…\\xa0- Science signaling, 201693회 인용 관련 학술자료 전체 8개의 버전 ', '호': '409'}, title='Inclusion bodies enriched for p62 and polyubiquitinated proteins in macrophages protect against atherosclerosis', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'High-protein diets increase cardiovascular risk by activating macrophage mTOR to suppress mitophagy': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020/1', '게시자': 'Nature Publishing Group UK', '권': '2', '설명': 'High-protein diets are commonly utilized for weight loss, yet they have been reported to raise cardiovascular risk. The mechanisms underlying this risk are unknown. Here, we show that dietary protein drives atherosclerosis and lesion complexity. Protein ingestion acutely elevates amino acid levels in blood and atherosclerotic plaques, stimulating macrophage mammalian target of rapamycin (mTOR) signalling. This is causal in plaque progression, because the effects of dietary protein are abrogated in macrophage-specific Raptor-null mice. Mechanistically, we find amino acids exacerbate macrophage apoptosis induced by atherogenic lipids, a process that involves mammalian target of rapamycin complex 1 (mTORC1)-dependent inhibition of mitochondrial autophagy (mitophagy), accumulation of dysfunctional mitochondria and mitochondrial apoptosis. Using macrophage-specific mTORC1- and autophagy\\xa0…', '저널': 'Nature metabolism', '저자': 'Xiangyu Zhang, Ismail Sergin, Trent D Evans, Se-Jin Jeong, Astrid Rodriguez-Velez, Divya Kapoor, Sunny Chen, Eric Song, Karyn B Holloway, Jan R Crowley, Slava Epelman, Conrad C Weihl, Abhinav Diwan, Daping Fan, Bettina Mittendorfer, Nathan O Stitziel, Joel D Schilling, Irfan J Lodhi, Babak Razani', '전체 인용횟수': '85회 인용20202021202220238202233', '페이지': '110-125', '학술 문서': 'High-protein diets increase cardiovascular risk by activating macrophage mTOR to suppress mitophagyX Zhang, I Sergin, TD Evans, SJ Jeong…\\xa0- Nature metabolism, 202085회 인용 관련 학술자료 전체 9개의 버전 ', '호': '1'}, title='High-protein diets increase cardiovascular risk by activating macrophage mTOR to suppress mitophagy', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'p62/SQSTM1 and Selective Autophagy in Cardiometabolic Diseases': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019/8/20', '게시자': 'Mary Ann Liebert, Inc., publishers', '권': '31', '설명': \" Significance: p62/SQSTM1 is a multifunctional scaffolding protein involved in the regulation of various signaling pathways as well as autophagy. In particular, p62/SQSTM1 serves as an essential adaptor to identify and deliver specific organelles and protein aggregates to autophagosomes for degradation, a process known as selective autophagy. Critical Issues: With the emergence of autophagy as a critical process in cellular metabolism and the development of cardiometabolic diseases, it is increasingly important to understand p62's role in the integration of signaling and autophagic pathways. Recent Advances: This review first discusses the features that make p62/SQSTM1 an ideal chaperone in integrating signaling pathways with autophagy and details the current understanding of its diverse roles in selective autophagy processes. Distinct and overlapping roles of other chaperones with similar functions are\\xa0…\", '저자': 'Se-Jin Jeong, Xiangyu Zhang, Astrid Rodriguez-Velez, Trent D Evans, Babak Razani', '전체 인용횟수': '76회 인용2019202020212022202326152824', '출처': 'Antioxidants & Redox Signaling', '페이지': '458-471', '학술 문서': 'p62/SQSTM1 and selective autophagy in cardiometabolic diseasesSJ Jeong, X Zhang, A Rodriguez-Velez, TD Evans…\\xa0- Antioxidants & Redox Signaling, 201976회 인용 관련 학술자료 전체 8개의 버전 ', '호': '6'}, title='p62/SQSTM1 and Selective Autophagy in Cardiometabolic Diseases', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Classical and alternative roles for autophagy in lipid metabolism': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/6', '게시자': 'NIH Public Access', '권': '29', '설명': 'As the field of autophagy has evolved and expanded to include functional roles in various aspects of cellular degradation, so has its role in intracellular lipid metabolism. Understanding the mechanisms underlying these classical and alternative roles of autophagy will not only enhance our knowledge in lipid biology but also provide new avenues of translation to human lipid disorders.', '저자': 'Xiangyu Zhang, Trent D Evans, Se-Jin Jeong, Babak Razani', '전체 인용횟수': '73회 인용20182019202020212022202311119131413', '출처': 'Current opinion in lipidology', '페이지': '203', '학술 문서': 'Classical and alternative roles for autophagy in lipid metabolismX Zhang, TD Evans, SJ Jeong, B Razani\\xa0- Current opinion in lipidology, 201873회 인용 관련 학술자료 전체 9개의 버전 ', '호': '3'}, title='Classical and alternative roles for autophagy in lipid metabolism', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Endoplasmic reticulum stress regulates rat mandibular cartilage thinning under compressive mechanical stress': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/6/21', '게시자': 'Elsevier', '권': '288', '설명': 'Compressive mechanical stress-induced cartilage thinning has been characterized as a key step in the progression of temporomandibular joint diseases, such as osteoarthritis. However, the regulatory mechanisms underlying this loss have not been thoroughly studied. Here, we used an established animal model for loading compressive mechanical stress to induce cartilage thinning in vivo. The mechanically stressed mandibular chondrocytes were then isolated to screen potential candidates using a proteomics approach. A total of 28 proteins were identified that were directly or indirectly associated with endoplasmic reticulum stress, including protein disulfide-isomerase, calreticulin, translationally controlled tumor protein, and peptidyl-prolyl cis/trans-isomerase protein. The altered expression of these candidates was validated at both the mRNA and protein levels. The induction of endoplasmic reticulum stress by\\xa0…', '저널': 'Journal of Biological Chemistry', '저자': 'Huang Li, Xiang-Yu Zhang, Tuo-Jiang Wu, Wei Cheng, Xin Liu, Ting-Ting Jiang, Juan Wen, Jie Li, Qiao-Ling Ma, Zi-Chun Hua', '전체 인용횟수': '73회 인용201320142015201620172018201920202021202220232768975411104', '페이지': '18172-18183', '학술 문서': 'Endoplasmic reticulum stress regulates rat mandibular cartilage thinning under compressive mechanical stressH Li, XY Zhang, TJ Wu, W Cheng, X Liu, TT Jiang…\\xa0- Journal of Biological Chemistry, 201373회 인용 관련 학술자료 전체 9개의 버전 ', '호': '25'}, title='Endoplasmic reticulum stress regulates rat mandibular cartilage thinning under compressive mechanical stress', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Target acquired: selective autophagy in cardiometabolic disease': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/2/28', '게시자': 'American Association for the Advancement of Science', '권': '10', '설명': 'The accumulation of damaged or excess proteins and organelles is a defining feature of metabolic disease in nearly every tissue. Thus, a central challenge in maintaining metabolic homeostasis is the identification, sequestration, and degradation of these cellular components, including protein aggregates, mitochondria, peroxisomes, inflammasomes, and lipid droplets. A primary route through which this challenge is met is selective autophagy, the targeting of specific cellular cargo for autophagic compartmentalization and lysosomal degradation. In addition to its roles in degradation, selective autophagy is emerging as an integral component of inflammatory and metabolic signaling cascades. In this Review, we focus on emerging evidence and key questions about the role of selective autophagy in the cell biology and pathophysiology of metabolic diseases such as obesity, diabetes, atherosclerosis, and\\xa0…', '저자': 'Trent D Evans, Ismail Sergin, Xiangyu Zhang, Babak Razani', '전체 인용횟수': '61회 인용2017201820192020202120222023871112698', '출처': 'Science signaling', '페이지': 'eaag2298', '학술 문서': 'Target acquired: selective autophagy in cardiometabolic diseaseTD Evans, I Sergin, X Zhang, B Razani\\xa0- Science signaling, 201761회 인용 관련 학술자료 전체 8개의 버전 ', '호': '468'}, title='Target acquired: selective autophagy in cardiometabolic disease', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Modulation of Salmonella tumor-colonization and intratumoral anti-angiogenesis by triptolide and its mechanism': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '게시자': 'Ivyspring International Publisher', '권': '7', '설명': 'The weakened tumour colonization of attenuated Salmonella has severely hampered its clinical development. In this study, we investigated whether an anti-inflammation and antiangiogenesis compound triptolide could improve the efficacy of', '저널': 'Theranostics', '저자': 'Jianxiang Chen, Yiting Qiao, Bo Tang, Guo Chen, Xiufeng Liu, Bingya Yang, Jing Wei, Xiangyu Zhang, Xiawei Cheng, Pan Du, Wenhui Jiang, Qingang Hu, Zi-Chun Hua', '전체 인용횟수': '53회 인용20182019202020212022202339910139', '페이지': '2250', '학술 문서': 'Modulation of Salmonella tumor-colonization and intratumoral anti-angiogenesis by triptolide and its mechanismJ Chen, Y Qiao, B Tang, G Chen, X Liu, B Yang, J Wei…\\xa0- Theranostics, 201753회 인용 관련 학술자료 전체 5개의 버전 ', '호': '8'}, title='Modulation of Salmonella tumor-colonization and intratumoral anti-angiogenesis by triptolide and its mechanism', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Trehalose causes low-grade lysosomal stress to activate TFEB and the autophagy-lysosome biogenesis response': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021/11/2', '게시자': 'Taylor & Francis', '권': '17', '설명': 'The autophagy-lysosome system is an important cellular degradation pathway that recycles dysfunctional organelles and cytotoxic protein aggregates. A decline in this system is pathogenic in many human diseases including neurodegenerative disorders, fatty liver disease, and atherosclerosis. Thus there is intense interest in discovering therapeutics aimed at stimulating the autophagy-lysosome system. Trehalose is a natural disaccharide composed of two glucose molecules linked by a ɑ-1,1-glycosidic bond with the unique ability to induce cellular macroautophagy/autophagy and with reported efficacy on mitigating several diseases where autophagy is dysfunctional. Interestingly, the mechanism by which trehalose induces autophagy is unknown. One suggested mechanism is its ability to activate TFEB (transcription factor EB), the master transcriptional regulator of autophagy-lysosomal biogenesis. Here we\\xa0…', '저널': 'Autophagy', '저자': 'Se-Jin Jeong, Jeremiah Stitham, Trent D Evans, Xiangyu Zhang, Astrid Rodriguez-Velez, Yu-Sheng Yeh, Joan Tao, Koki Takabatake, Slava Epelman, Irfan J Lodhi, Joel D Schilling, Brian J DeBosch, Abhinav Diwan, Babak Razani', '전체 인용횟수': '51회 인용20212022202352224', '페이지': '3740-3752', '학술 문서': 'Trehalose causes low-grade lysosomal stress to activate TFEB and the autophagy-lysosome biogenesis responseSJ Jeong, J Stitham, TD Evans, X Zhang…\\xa0- Autophagy, 202151회 인용 관련 학술자료 전체 6개의 버전 ', '호': '11'}, title='Trehalose causes low-grade lysosomal stress to activate TFEB and the autophagy-lysosome biogenesis response', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'TFEB drives PGC-1α expression in adipocytes to protect against diet-induced metabolic dysfunction': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019/11/5', '게시자': 'American Association for the Advancement of Science', '권': '12', '설명': 'TFEB is a basic helix-loop-helix transcription factor that confers protection against metabolic diseases such as atherosclerosis by targeting a network of genes involved in autophagy-lysosomal biogenesis and lipid catabolism. In this study, we sought to characterize the role of TFEB in adipocyte and adipose tissue physiology and evaluate the therapeutic potential of adipocyte-specific TFEB overexpression in obesity. We demonstrated that mice with adipocyte-specific TFEB overexpression (Adipo-TFEB) were protected from diet-induced obesity, insulin resistance, and metabolic sequelae. Adipo-TFEB mice were lean primarily through increased metabolic rate, suggesting a role for adipose tissue browning and enhanced nonshivering thermogenesis in fat. Transcriptional characterization revealed that TFEB targeted genes involved in adipose tissue browning rather than those involved in autophagy. One such gene\\xa0…', '저널': 'Science signaling', '저자': 'Trent D Evans, Xiangyu Zhang, Se-Jin Jeong, Anyuan He, Eric Song, Somashubhra Bhattacharya, Karyn B Holloway, Irfan J Lodhi, Babak Razani', '전체 인용횟수': '49회 인용2020202120222023817915', '페이지': 'eaau2281', '학술 문서': 'TFEB drives PGC-1α expression in adipocytes to protect against diet-induced metabolic dysfunctionTD Evans, X Zhang, SJ Jeong, A He, E Song…\\xa0- Science signaling, 201949회 인용 관련 학술자료 전체 8개의 버전 ', '호': '606'}, title='TFEB drives PGC-1α expression in adipocytes to protect against diet-induced metabolic dysfunction', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Assessment of copper nanoclusters for accurate in vivo tumor imaging and potential for translation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019/5/10', '게시자': 'American Chemical Society', '권': '11', '설명': 'Nanoparticles have been widely used for preclinical cancer imaging. However, their successful clinical translation is largely hampered by potential toxicity, unsatisfactory detection of malignancy at early stages, inaccurate diagnosis of tumor biomarkers, and histology for imaging-guided treatment. Herein, a targeted copper nanocluster (CuNC) is reported with high potential to address these challenges for future translation. Its ultrasmall structure enables efficient renal/bowel clearance, minimized off-target effects in nontargeted organs, and low nonspecific tumor retention. The pH-dependent in vivo dissolution of CuNCs affords minimal toxicity and potentially selective drug delivery to tumors. The intrinsic radiolabeling through the direct addition of 64Cu to CuNC (64Cu-CuNCs-FC131) synthesis offers high specific activity for sensitive and accurate detection of CXCR4 via FC131-directed targeting in novel triple\\xa0…', '저널': 'ACS applied materials & interfaces', '저자': 'Gyu Seong Heo, Yongfeng Zhao, Deborah Sultan, Xiaohui Zhang, Lisa Detering, Hannah P Luehmann, Xiangyu Zhang, Richen Li, Ankur Choksi, Savannah Sharp, Sidney Levingston, Tina Primeau, David E Reichert, Guorong Sun, Babak Razani, Shunqiang Li, Katherine N Weilbaecher, Farrokh Dehdashti, Karen L Wooley, Yongjian Liu', '전체 인용횟수': '43회 인용201920202021202220233131574', '페이지': '19669-19678', '학술 문서': 'Assessment of copper nanoclusters for accurate in vivo tumor imaging and potential for translationGS Heo, Y Zhao, D Sultan, X Zhang, L Detering…\\xa0- ACS applied materials & interfaces, 201943회 인용 관련 학술자료 전체 8개의 버전 ', '호': '22'}, title='Assessment of copper nanoclusters for accurate in vivo tumor imaging and potential for translation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Suppression of HSP70 expression sensitizes NSCLC cell lines to TRAIL-induced apoptosis by upregulating DR4 and DR5 and downregulating c-FLIP-L expressions': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/2', '게시자': 'Springer-Verlag', '권': '91', '설명': ' Many cancer cell types are resistant to tumor necrosis factor-related apoptosis-inducing ligand (TRAIL)-induced apoptosis. Here, we examined whether HSP70 suppression by small interfering RNA (siRNA) sensitized non-small cell lung cancer (NSCLC) cells to TRAIL-induced apoptosis and the underlying mechanisms. We demonstrated that HSP70 suppression by siRNA sensitized NSCLC cells to TRAIL-induced apoptosis by upregulating the expressions of death receptor 4 (DR4) and death receptor 5 (DR5) through activating NF-κB, JNK, and, subsequently, p53, consequently significantly amplifying TRAIL-mediated caspase-8 processing and activity, cytosolic translocation of cytochrome c, and cell death. Consistently, the pro-apoptotic proteins Bad and Bax were upregulated, while the anti-apoptotic protein Bcl-2 was downregulated. The luciferase activity of the DR4 promoter was blocked by a NF-κB\\xa0…', '저널': 'Journal of molecular medicine', '저자': 'Hongqin Zhuang, Weiwei Jiang, Xiangyu Zhang, Fan Qiu, Ziyi Gan, Wei Cheng, Jing Zhang, Shengwen Guan, Bo Tang, Qilai Huang, Xinhua Wu, Xiaofeng Huang, Wenhui Jiang, Qingang Hu, Min Lu, Zi-Chun Hua', '전체 인용횟수': '42회 인용201320142015201620172018201920202021202220232778237311', '페이지': '219-235', '학술 문서': 'Suppression of HSP70 expression sensitizes NSCLC cell lines to TRAIL-induced apoptosis by upregulating DR4 and DR5 and downregulating c-FLIP-L expressionsH Zhuang, W Jiang, X Zhang, F Qiu, Z Gan, W Cheng…\\xa0- Journal of molecular medicine, 201342회 인용 관련 학술자료 전체 7개의 버전 '}, title='Suppression of HSP70 expression sensitizes NSCLC cell lines to TRAIL-induced apoptosis by upregulating DR4 and DR5 and downregulating c-FLIP-L expressions', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Role of H-1 and H-2 subunits of soybean seed ferritin in oxidative deposition of iron in protein': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/10/15', '게시자': 'Elsevier', '권': '285', '설명': 'Naturally occurring phytoferritin is a heteropolymer consisting of two different H-type subunits, H-1 and H-2. Prior to this study, however, the function of the two subunits in oxidative deposition of iron in ferritin was unknown. The data show that, upon aerobic addition of 48–200 Fe2+/shell to apoferritin, iron oxidation occurs only at the diiron ferroxidase center of recombinant H1 (rH-1). In addition to the diiron ferroxidase mechanism, such oxidation is catalyzed by the extension peptide (a specific domain found in phytoferritin) of rH-2, because the H-1 subunit is able to remove Fe3+ from the center to the inner cavity better than the H-2 subunit. These findings support the idea that the H-1 and H-2 subunits play different roles in iron mineralization in protein. Interestingly, at medium iron loading (200 irons/shell), wild-type (WT) soybean seed ferritin (SSF) exhibits a stronger activity in catalyzing iron oxidation (1.10 ± 0.13\\xa0…', '저널': 'Journal of Biological Chemistry', '저자': 'Jianjun Deng, Xiayun Liao, Haixia Yang, Xiangyu Zhang, Zichun Hua, Taro Masuda, Fumiyuki Goto, Toshihiro Yoshihara, Guanghua Zhao', '전체 인용횟수': '38회 인용20112012201320142015201620172018201920202021202220231436431222334', '페이지': '32075-32086', '학술 문서': 'Role of H-1 and H-2 subunits of soybean seed ferritin in oxidative deposition of iron in proteinJ Deng, X Liao, H Yang, X Zhang, Z Hua, T Masuda…\\xa0- Journal of Biological Chemistry, 201038회 인용 관련 학술자료 전체 10개의 버전 ', '호': '42'}, title='Role of H-1 and H-2 subunits of soybean seed ferritin in oxidative deposition of iron in protein', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Proteomic screening of anaerobically regulated promoters from Salmonella and its antitumor applications': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011/6/1', '게시자': 'Elsevier', '권': '10', '설명': 'Solid tumors often contain hypoxic and necrotic areas that can be targeted by attenuated Salmonella typhimurium VNP20009 (VNP). We sought to develop a hypoxia- inducible promoter system based on the tumor-specific delivered strain VNP to confine expression of therapeutic gene specifically or selectively within the tumor microenvironment. A hypoxia-inducible promoter - adhE promoter was screened from the hypoxia-regulated endogenous proteins of Salmonella through two-dimensional gel electrophoresis and matrix-assisted laser desorption ionization-time-of-flight/time-of-flight MS-based proteomics approaches. The efficiency and specificity of the selected adhE promoter were validated first in both bacteria and animal tumor models. The adhE promoter could specifically drive GFP gene expression under hypoxia, but not under normoxia. Furthermore, luciferase reporter expression controlled by the system\\xa0…', '저널': 'Molecular & Cellular Proteomics', '저자': 'Jianxiang Chen, Dongping Wei, Hongqin Zhuang, Yiting Qiao, Bo Tang, Xiangyu Zhang, Jing Wei, Shentong Fang, Guo Chen, Pan Du, Xiaofeng Huang, Wenhui Jiang, Qingang Hu, Zi-Chun Hua', '전체 인용횟수': '30회 인용20122013201420152016201720182019202020212022135310121112', '학술 문서': 'Proteomic screening of anaerobically regulated promoters from Salmonella and its antitumor applicationsJ Chen, D Wei, H Zhuang, Y Qiao, B Tang, X Zhang…\\xa0- Molecular & Cellular Proteomics, 201130회 인용 관련 학술자료 전체 8개의 버전 ', '호': '6'}, title='Proteomic screening of anaerobically regulated promoters from Salmonella and its antitumor applications', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Systemic administration of attenuated Salmonella typhimurium in combination with interleukin-21 for cancer therapy': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/5/1', '게시자': 'Spandidos Publications', '권': '1', '설명': 'Attenuated Salmonella typhimurium (S. typhimurium) strain VNP20009 has been employed as a powerful anticancer agent due to its selective accumulation in tumors for targeted therapy. S. typhimurium has been demonstrated to constitute a delivery tool carrying antiangiogenic or proapoptotic genes that treat cancer. The hydrodynamic tail vein (HTV) injection of naked plasmid DNA has been developed as an effective gene delivery strategy, which has been successfully used in vivo. The aim of this study was to develop a combination therapy of S. typhimurium VNP20009 and HTV injection of interleukin-21 (IL-21) expression plasmid to evaluate the antitumor potential on an experimental melanoma model. Consistent with previous results, single VNP20009 treatment was demonstrated to possess effective activities to suppress tumor growth and prolong animal survival. Moreover, HTV injection of IL-21 plasmid\\xa0…', '저널': 'Molecular and clinical oncology', '저자': 'Yuxuan Wang, Jianxiang Chen, BO Tang, Xiangyu Zhang, Zi-Chun Hua', '전체 인용횟수': '23회 인용2014201520162017201820192020202120222023232124333', '페이지': '461-465', '학술 문서': 'Systemic administration of attenuated Salmonella typhimurium in combination with interleukin-21 for cancer therapyY Wang, J Chen, BO Tang, X Zhang, ZC Hua\\xa0- Molecular and clinical oncology, 201323회 인용 관련 학술자료 전체 10개의 버전 ', '호': '3'}, title='Systemic administration of attenuated Salmonella typhimurium in combination with interleukin-21 for cancer therapy', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'FADD is a key regulator of lipid metabolism': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/8', '권': '8', '설명': ' FADD, a classical apoptotic signaling adaptor, was recently reported to have non‐apoptotic functions. Here, we report the discovery that FADD regulates lipid metabolism. PPAR‐α is a dietary lipid sensor, whose activation results in hypolipidemic effects. We show that FADD interacts with RIP140, which is a corepressor for PPAR‐α, and FADD phosphorylation‐mimic mutation (FADD‐D) or FADD deficiency abolishes RIP140‐mediated transcriptional repression, leading to the activation of PPAR‐α. FADD‐D‐mutant mice exhibit significantly decreased adipose tissue mass and triglyceride accumulation. Also, they exhibit increased energy expenditure with enhanced fatty acid oxidation in adipocytes due to the activation of PPAR‐α. Similar metabolic phenotypes, such as reduced fat formation, insulin resistance, and resistance to HFD‐induced obesity, are shown in adipose‐specific FADD knockout mice. Additionally\\xa0…', '저널': 'EMBO molecular medicine', '저자': 'Hongqin Zhuang, Xueshi Wang, Daolong Zha, Ziyi Gan, Fangfang Cai, Pan Du, Yunwen Yang, Bingya Yang, Xiangyu Zhang, Chun Yao, Yuqiang Zhou, Chizhou Jiang, Shengwen Guan, Xuerui Zhang, Jing Zhang, Wenhui Jiang, Qingang Hu, Zi‐Chun Hua', '전체 인용횟수': '17회 인용20172018201920202021202220231114235', '페이지': '895-918', '학술 문서': 'FADD is a key regulator of lipid metabolismH Zhuang, X Wang, D Zha, Z Gan, F Cai, P Du, Y Yang…\\xa0- EMBO molecular medicine, 201617회 인용 관련 학술자료 전체 7개의 버전 ', '호': '8'}, title='FADD is a key regulator of lipid metabolism', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Data clustering: a review': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1999/9/1', '게시자': 'Acm', '권': '31', '설명': 'Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify  cross-cutting themes and recent advances. We also describe some important\\xa0…', '저자': 'Anil K Jain, M Narasimha Murty, Patrick J Flynn', '전체 인용횟수': '19814회 인용2001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202311819235544968878489310021082109010491128119411501169115011151030981859829697540', '출처': 'ACM computing surveys (CSUR)', '페이지': '264-323', '학술 문서': 'Data clustering: a reviewAK Jain, MN Murty, PJ Flynn\\xa0- ACM computing surveys (CSUR), 199919814회 인용 관련 학술자료 전체 77개의 버전 ', '호': '3'}, title='Data clustering: a review', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Algorithms for Clustering Data': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1988', '설명': \"A cluster is a group of similar objects; objects from different clusters are not alike. Clustering is an important tool in exploratory data analysis and is used in several disciplines, such as artificial intelligence, pattern recognition, geology, biology, psychology, and information retrieval. A clustering algorithm generates clusters from the definitions of objects, and cluster analysis is the formal analysis of these algorithms. This excellent book emphasizes informal algorithms for clustering data and interpreting results. The authors, whose names should be familiar to researchers working in cluster analysis, masterfully introduce mathematical and statistical theory only when necessary. The book consists of five chapters. Chapter 1 introduces the general concepts and the literature. Chapter 2 presents the authors' view of data and introduces the representation of objects (pattern matrix), the idea of a proximity matrix, different ways\\xa0…\", '저자': 'Anil K Jain, Richard C Dubes', '전체 인용횟수': '16834회 인용19911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202371100111140146201188284304303397490535586688700760746763744725749784791769735701634634568455438348', '학술 문서': 'Algorithms for clustering dataAK Jain, RC Dubes - 198816834회 인용 관련 학술자료 전체 4개의 버전 '}, title='Algorithms for Clustering Data', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Data clustering: 50 years beyond K-means': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/6/1', '게시자': 'North-Holland', '권': '31', '설명': 'Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms into a system of ranked taxa: domain, kingdom, phylum, class, etc. Cluster analysis is the formal study of methods and algorithms for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). The aim of clustering is to find structure in data and is therefore exploratory in nature. Clustering has a long and rich history in a variety of scientific fields. One of the most popular and simple clustering algorithms, K-means, was first published in\\xa0…', '저널': 'Pattern recognition letters', '저자': 'Anil K Jain', '전체 인용횟수': '10916회 인용201020112012201320142015201620172018201920202021202220236618728349268976790810241198127911731118967645', '페이지': '651-666', '학술 문서': 'Data clustering: 50 years beyond K-meansAK Jain\\xa0- Pattern recognition letters, 201010914회 인용 관련 학술자료 전체 42개의 버전 Data Clustering: 50 Years Beyond K-Means. ECML/PKDD (1)*AK Jain\\xa0- Lecture Notes in Computer Science, Springer20082회 인용 관련 학술자료 ', '호': '8'}, title='Data clustering: 50 years beyond K-means', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Statistical pattern recognition: a review': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2000/1/1', '권': '1', '설명': 'The primary goal of pattern recognition is supervised or unsupervised classification. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical approach has been most intensively studied and used in practice. More recently, neural network techniques and methods imported from statistical learning theory have been receiving increasing attention. The design of a recognition system requires careful attention to the following issues: definition of pattern classes, sensing environment, pattern representation, feature extraction and selection, cluster analysis, classifier design and learning, selection of training and test samples, and performance evaluation. In spite of almost 50 years of research and development in this field, the general problem of recognizing complex patterns with arbitrary orientation, location, and scale remains unsolved. New and emerging applications\\xa0…', '저널': 'IEEE Transaction on Pattern Analysis and Machine Intelligence, PAMI-22', '저자': 'Anil K Jain, Robert PW Duin, Jianchang Mao', '전체 인용횟수': '9260회 인용20002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202347129213240395394437447487493524537574501544519456421404369304284242180', '페이지': '4-37', '학술 문서': 'Statistical pattern recognition: A reviewAK Jain, RPW Duin, J Mao\\xa0- IEEE Transactions on pattern analysis and machine\\xa0…, 20009112회 인용 관련 학술자료 전체 12개의 버전 J. Mao Statistical Pattern Recognition: A Review*AK Jain, RPW Duin\\xa0- IEEE Transactions on pattern analysis and machine\\xa0…, 2000221회 인용 관련 학술자료 '}, title='Statistical pattern recognition: a review', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Introduction to biometric recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011', '게시자': 'Springer US', '도서': 'Introduction to Biometrics', '저자': 'Anil K Jain, Arun A Ross, Karthik Nandakumar', '전체 인용횟수': '6953회 인용2004200520062007200820092010201120122013201420152016201720182019202020212022202346132143198218301366427426468467514539442503452359359304191', '페이지': '1-49', '학술 문서': 'An introduction to biometric recognition*AK Jain, A Ross, S Prabhakar\\xa0- IEEE Transactions on circuits and systems for video\\xa0…, 20046953회 인용 관련 학술자료 전체 26개의 버전 '}, title='Introduction to biometric recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Handbook Of Fingerprint Recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009', '권': '1', '설명': 'Biometric recognition, or simply biometrics, refers to the use of distinctive anatomical and/or behavioral characteristics or identifiers (eg, fingerprints, face, iris, voice, and hand geometry) for automatically recognizing a person. Questions such as “Is this person authorized to enter the facility?”,“Is this individual entitled to access the privileged information?”, and “Did this person previously apply for a passport?” are routinely asked in a variety of organizations in both public and private sectors. Traditional person recognition systems that are based on ID documents and password/PIN no longer suffice to verify a person’s identity. Because biometric identifiers cannot be easily misplaced, forged, or shared, they are considered more reliable for person recognition than traditional token-(eg, keys or ID cards) or knowledge-(eg, password or PIN) based methods. Biometric recognition provides better security, higher efficiency\\xa0…', '저자': 'Davide Maltoni, Dario Maio, Anil K Jain, Salil Probhakar', '전체 인용횟수': '6804회 인용20042005200620072008200920102011201220132014201520162017201820192020202120222023111180227304319328414435481425487497415399420361244265222134', '학술 문서': 'Handbook of fingerprint recognitionD Maltoni, D Maio, AK Jain, S Prabhakar - 20096781회 인용 관련 학술자료 전체 16개의 버전 Handbook of fingerprint recognition*S Prabhakar, D Maltoni, D Maio, AK Jain\\xa0- New York, 200322회 인용 관련 학술자료 K., S. Prabhakar (2003). Handbook of Fingerprint RecognitionD Maltoni, D Maio, A Jain6회 인용 관련 학술자료 '}, title='Handbook Of Fingerprint Recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Handbook of fingerprint recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009/4/21', '게시자': 'springer', '권': '2', '설명': 'Biometric recognition, or simply biometrics, refers to the use of distinctive anatomical and/or behavioral characteristics or identifiers (eg, fingerprints, face, iris, voice, and hand geometry) for automatically recognizing a person. Questions such as “Is this person authorized to enter the facility?”,“Is this individual entitled to access the privileged information?”, and “Did this person previously apply for a passport?” are routinely asked in a variety of organizations in both public and private sectors. Traditional person recognition systems that are based on ID documents and password/PIN no longer suffice to verify a person’s identity. Because biometric identifiers cannot be easily misplaced, forged, or shared, they are considered more reliable for person recognition than traditional token-(eg, keys or ID cards) or knowledge-(eg, password or PIN) based methods. Biometric recognition provides better security, higher efficiency\\xa0…', '저자': 'Davide Maltoni, Dario Maio, Anil K Jain, Salil Prabhakar', '전체 인용횟수': '6781회 인용20042005200620072008200920102011201220132014201520162017201820192020202120222023111180226304314327412435480425485496413399418359244264220134', '학술 문서': 'Handbook of fingerprint recognitionD Maltoni, D Maio, AK Jain, S Prabhakar - 20096781회 인용 관련 학술자료 전체 16개의 버전 '}, title='Handbook of fingerprint recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Artificial neural networks: A tutorial': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1996/3', '게시자': 'IEEE', '권': '29', '설명': 'Artificial neural nets (ANNs) are massively parallel systems with large numbers of interconnected simple processors. The article discusses the motivations behind the development of ANNs and describes the basic biological neuron and the artificial computational model. It outlines network architectures and learning processes, and presents some of the most commonly used ANN models. It concludes with character recognition, a successful ANN application.', '저자': 'Anil K Jain, Jianchang Mao, K Moidin Mohiuddin', '전체 인용횟수': '4761회 인용199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023363431434158455375638184110114120153182224172227263294409405435462483', '출처': 'Computer', '페이지': '31-44', '학술 문서': 'Artificial neural networks: A tutorialAK Jain, J Mao, KM Mohiuddin\\xa0- Computer, 19964761회 인용 관련 학술자료 전체 16개의 버전 ', '호': '3'}, title='Artificial neural networks: A tutorial', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Handbook of multibiometrics': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2006/8/11', '게시자': 'Springer Science & Business Media', '권': '6', '설명': 'Reliable human authentication schemes are of paramount importance in our highly networked society. Advances in biometrics help address the myriad of problems associated with traditional human recognition methods. The performance and benefits of a biometric system can be significantly enhanced by consolidating the evidence presented by multiple biometric sources. Multibiometric systems are expected to meet the stringent performance requirements imposed by large-scale authentication systems. Handbook of Multibiometrics, a professional book, introduces multibiometric systems, and demonstrates the noteworthy advantages of these systems over their unimodal counterparts. In addition, this book describes in detail the various scenarios possible when fusing biometric evidence from multiple information sources. This comprehensive volume on multibiometric systems, concisely and clearly outlines the different fusion methodologies that have been proposed by researchers to integrate multiple biometric traits.', '저자': 'Arun A Ross, Karthik Nandakumar, Anil K Jain', '전체 인용횟수': '1981회 인용20062007200820092010201120122013201420152016201720182019202020212022202313491041311661381791651511581441251029565725230', '학술 문서': 'Handbook of multibiometricsAA Ross, K Nandakumar, AK Jain - 20061981회 인용 관련 학술자료 전체 8개의 버전 '}, title='Handbook of multibiometrics', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Biometrics: personal identification in networked society': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1999/1/31', '게시자': 'Springer Science & Business Media', '권': '479', '설명': 'Biometrics: Personal Identification in Networked Society is a comprehensive and accessible source of state-of-the-art information on all existing and emerging biometrics: the science of automatically identifying individuals based on their physiological or behavior characteristics. In particular, the book covers:* General principles and ideas of designing biometric-based systems and their underlying tradeoffs* Identification of important issues in the evaluation of biometrics-based systems* Integration of biometric cues, and the integration of biometrics with other existing technologies* Assessment of the capabilities and limitations of different biometrics* The comprehensive examination of biometric methods in commercial use and in research development* Exploration of some of the numerous privacy and security implications of biometrics. Also included are chapters on face and eye identification, speaker recognition, networking, and other timely technology-related issues. All chapters are written by leading internationally recognized experts from academia and industry. Biometrics: Personal Identification in Networked Society is an invaluable work for scientists, engineers, application developers, systems integrators, and others working in biometrics.', '저자': 'Anil Jain, Ruud Bolle, Sharath Pankanti', '전체 인용횟수': '3769회 인용199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202320367810014219420616618717819617320222523823719116815416314685897645', '학술 문서': 'Biometrics: personal identification in networked societyA Jain, R Bolle, S Pankanti - 19993769회 인용 관련 학술자료 전체 12개의 버전 '}, title='Biometrics: personal identification in networked society', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Unsupervised texture segmentation using Gabor filters': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1991/1/1', '게시자': 'Pergamon', '권': '24', '설명': 'This paper presents a texture segmentation algorithm inspired by the multi-channel filtering theory for visual information processing in the early stages of human visual system. The channels are characterized by a bank of Gabor filters that nearly uniformly covers the spatial-frequency domain, and a systematic filter selection scheme is proposed, which is based on reconstruction of the input image from the filtered images. Texture features are obtained by subjecting each (selected) filtered image to a nonlinear transformation and computing a measure of “energy” in a window around each pixel. A square-error clustering algorithm is then used to integrate the feature images and produce a segmentation. A simple procedure to incorporate spatial information in the clustering process is proposed. A relative index is used to estimate the “true” number of texture categories.', '저널': 'Pattern recognition', '저자': 'Anil K Jain, Farshid Farrokhnia', '전체 인용횟수': '3524회 인용19921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023212448447368848294100134121124151160159148160150141153158152144117124130110114846351', '페이지': '1167-1186', '학술 문서': 'Unsupervised texture segmentation using Gabor filtersAK Jain, F Farrokhnia\\xa0- Pattern recognition, 19913524회 인용 관련 학술자료 전체 13개의 버전 ', '호': '12'}, title='Unsupervised texture segmentation using Gabor filters', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Fingerprint image enhancement: algorithm and performance evaluation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1998/8', '게시자': 'IEEE', '권': '20', '설명': 'In order to ensure that the performance of an automatic fingerprint identification/verification system will be robust with respect to the quality of input fingerprint images, it is essential to incorporate a fingerprint enhancement algorithm in the minutiae extraction module. We present a fast fingerprint enhancement algorithm, which can adaptively improve the clarity of ridge and valley structures of input fingerprint images based on the estimated local ridge orientation and frequency. We have evaluated the performance of the image enhancement algorithm using the goodness index of the extracted minutiae and the accuracy of an online fingerprint verification system. Experimental results show that incorporating the enhancement algorithm improves both the goodness index and the verification accuracy.', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Lin Hong, Yifei Wan, Anil Jain', '전체 인용횟수': '3523회 인용1999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023102034667011615218620419823320622025917923418715815014813485868255', '페이지': '777-789', '학술 문서': 'Fingerprint image enhancement: algorithm and performance evaluationL Hong, Y Wan, A Jain\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 19983523회 인용 관련 학술자료 전체 11개의 버전 ', '호': '8'}, title='Fingerprint image enhancement: algorithm and performance evaluation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Face detection in color images': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2002/5', '게시자': 'IEEE', '권': '24', '설명': 'Human face detection plays an important role in applications such as video surveillance, human computer interface, face recognition, and face image database management. We propose a face detection algorithm for color images in the presence of varying lighting conditions as well as complex backgrounds. Based on a novel lighting compensation technique and a nonlinear color transformation, our method detects skin regions over the entire image and then generates face candidates based on the spatial arrangement of these skin patches. The algorithm constructs eye, mouth, and boundary maps for verifying each face candidate. Experimental results demonstrate successful face detection over a wide range of facial variations in color, position, scale, orientation, 3D pose, and expression in images from several photo collections (both indoors and outdoors).', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Rein-Lien Hsu, Mohamed Abdel-Mottaleb, Anil K Jain', '전체 인용횟수': '3371회 인용2001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202312391091591871962132382402552382312011971671321031106581655029', '페이지': '696-706', '학술 문서': 'Face detection in color imagesRL Hsu, M Abdel-Mottaleb, AK Jain\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20023371회 인용 관련 학술자료 전체 17개의 버전 ', '호': '5'}, title='Face detection in color images', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Handbook of face recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011', '게시자': 'springer', '설명': 'Face recognition is one of the most important abilities that we use in our daily lives. There are several reasons for the growing interest in automated face recognition, including rising concerns for public security, the need for identity verification for physical and logical access, and the need for face analysis and modeling techniques in multimedia data management and digital entertainment. Research in automatic face recognition started in the 1960s. Recent years have seen significant progress in this area and a number of face recognition and modeling systems have been developed and deployed. However, accurate and robust face recognition still offers a number of challenges to computer vision and pattern recognition researchers, especially under unconstrained environments.This book is written with two primary motivations. The first is to compile major approaches, algorithms, and technologies available for\\xa0…', '저자': 'Anil K Jain, Stan Z Li', '전체 인용횟수': '3181회 인용200420052006200720082009201020112012201320142015201620172018201920202021202220238356311814016917417122622721323221918422517614616213999', '학술 문서': \"Handbook of face recognitionAK Jain, SZ Li - 20113128회 인용 관련 학술자료 전체 16개의 버전 Psychological and neural perspectives on human face recognition*SZ Li, AK Jain, AJ O'Toole\\xa0- Handbook of face recognition, 200557회 인용 관련 학술자료 전체 10개의 버전 Handbook of face recognition. 2011*AK Jain, SZ Li4회 인용 관련 학술자료 \"}, title='Handbook of face recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Feature selection: Evaluation, application, and small sample performance': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1997/2', '게시자': 'IEEE', '권': '19', '설명': 'A large number of algorithms have been proposed for feature subset selection. Our experimental results show that the sequential forward floating selection algorithm, proposed by Pudil et al. (1994), dominates the other algorithms tested. We study the problem of choosing an optimal feature set for land use classification based on SAR satellite images using four different texture models. Pooling features derived from different texture models, followed by a feature selection results in a substantial improvement in the classification accuracy. We also illustrate the dangers of using feature selection in small sample size situations.', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Anil Jain, Douglas Zongker', '전체 인용횟수': '2999회 인용19981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023243358558687122135144148182145136135137170142153130137131125107959053', '페이지': '153-158', '학술 문서': 'Feature selection: Evaluation, application, and small sample performanceA Jain, D Zongker\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 19972999회 인용 관련 학술자료 전체 17개의 버전 ', '호': '2'}, title='Feature selection: Evaluation, application, and small sample performance', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Unsupervised learning of finite mixture models': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2002/3', '게시자': 'Ieee', '권': '24', '설명': 'This paper proposes an unsupervised algorithm for learning a finite mixture model from multivariate data. The adjective \"unsupervised\" is justified by two properties of the algorithm: 1) it is capable of selecting the number of components and 2) unlike the standard expectation-maximization (EM) algorithm, it does not require careful initialization. The proposed method also avoids another drawback of EM for mixture fitting: the possibility of convergence toward a singular estimate at the boundary of the parameter space. The novelty of our approach is that we do not use a model selection criterion to choose one among a set of preestimated candidate models; instead, we seamlessly integrate estimation and model selection in a single algorithm. Our technique can be applied to any type of parametric mixture model for which it is possible to write an EM algorithm; in this paper, we illustrate it with experiments involving\\xa0…', '저널': 'IEEE Transactions on pattern analysis and machine intelligence', '저자': 'Mario A. T.  Figueiredo, Anil K.  Jain', '전체 인용횟수': '2907회 인용2002200320042005200620072008200920102011201220132014201520162017201820192020202120222023185578126129150149166159178184164194174163124147139110958579', '페이지': '381-396', '학술 문서': 'Unsupervised learning of finite mixture modelsMAT Figueiredo, AK Jain\\xa0- IEEE Transactions on pattern analysis and machine\\xa0…, 20022907회 인용 관련 학술자료 전체 9개의 버전 ', '호': '3'}, title='Unsupervised learning of finite mixture models', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Score normalization in multimodal biometric systems': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2005/12/1', '게시자': 'Pergamon', '권': '38', '설명': 'Multimodal biometric systems consolidate the evidence presented by multiple biometric sources and typically provide better recognition performance compared to systems based on a single biometric modality. Although information fusion in a multimodal system can be performed at various levels, integration at the matching score level is the most common approach due to the ease in accessing and combining the scores generated by different matchers. Since the matching scores output by the various modalities are heterogeneous, score normalization is needed to transform these scores into a common domain, prior to combining them. In this paper, we have studied the performance of different normalization techniques and fusion rules in the context of a multimodal biometric system based on the face, fingerprint and hand-geometry traits of a user. Experiments conducted on a database of 100 users indicate that the\\xa0…', '저널': 'Pattern recognition', '저자': 'Anil Jain, Karthik Nandakumar, Arun Ross', '전체 인용횟수': '2632회 인용200520062007200820092010201120122013201420152016201720182019202020212022202329506811995131141157174175184147147169176165179178112', '페이지': '2270-2285', '학술 문서': 'Score normalization in multimodal biometric systemsA Jain, K Nandakumar, A Ross\\xa0- Pattern recognition, 20052632회 인용 관련 학술자료 전체 17개의 버전 ', '호': '12'}, title='Score normalization in multimodal biometric systems', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Fully convolutional networks for semantic segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build\" fully convolutional\" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.', '저자': 'Jonathan Long, Evan Shelhamer, Trevor Darrell', '전체 인용횟수': '44928회 인용20152016201720182019202020212022202323710202471432461436785753280006719', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '3431-3440', '학술 문서': 'Fully convolutional networks for semantic segmentationJ Long, E Shelhamer, T Darrell\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 201544729회 인용 관련 학술자료 전체 52개의 버전 Proceedings of the IEEE conference on computer vision and pattern recognition*J Long, E Shelhamer, T Darrell\\xa0- Proceedings of the IEEE Conference on Computer\\xa0…, 2015294회 인용 관련 학술자료 Fully convolutional models for semantic segmentation*J Long, E Shelhamer, T Darrell\\xa0- CVPR, 2015141회 인용 관련 학술자료 Fully convolutional networks for semantic segmentation. arXiv 2015*J Long, E Shelhamer, T Darrell, UC Berkeley\\xa0- arXiv preprint arXiv:1411.4038, 2014139회 인용 관련 학술자료 Fully convolutional networks for semantic segmentation. 2015 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Boston, MA*J Long, E Shelhamer, T Darrell\\xa0- IEEE, https://doi. org/10.1109/CVPR, 201516회 인용 관련 학술자료 Fully convolutional networks for semantic segmentation. Paper presented at: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015 (pp. 3431–3440)*J Long, E Shelhamer, T Darrell6회 인용 관련 학술자료 Fully convolutional networks for semantic segmentation, Conf*J Long, E Shelhamer, T Darrell\\xa0- Computer Vision and Pattern Recognition (Boston\\xa0…2회 인용 관련 학술자료 '}, title='Fully convolutional networks for semantic segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Long-term recurrent convolutional networks for visual recognition and description': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'Models comprised of deep convolutional network layers have dominated recent image interpretation tasks; we investigate whether models which are also compositional, or\" deep\", temporally are effective on tasks involving visual sequences or label sequences. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image to sentence generation problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are\" doubly deep\" in that they can be compositional in spatial and temporal\" layers\". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable length inputs (ie video frames) to variable length outputs (ie natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to state-of-the-art visual convnet models and can jointly trained, updating temporal dynamics and convolutional perceptual representations simultaneously. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized.', '저자': 'Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell', '전체 인용횟수': '7291회 인용2015201620172018201920202021202220231314187801053112310341057875664', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '2625-2634', '학술 문서': 'Long-term recurrent convolutional networks for visual recognition and descriptionJ Donahue, L Anne Hendricks, S Guadarrama…\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20157291회 인용 관련 학술자료 전체 31개의 버전 '}, title='Long-term recurrent convolutional networks for visual recognition and description', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Pfinder: Real-time tracking of the human body': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1997/7', '게시자': 'IEEE', '권': '19', '설명': 'Pfinder is a real-time system for tracking people and interpreting their behavior. It runs at 10 Hz on a standard SGI Indy computer, and has performed reliably on thousands of people in many different physical locations. The system uses a multiclass statistical model of color and shape to obtain a 2D representation of head and hands in a wide range of viewing conditions. Pfinder has been successfully used in a wide range of applications including wireless interfaces, video databases, and low-bandwidth coding.', '저널': 'IEEE Transactions on pattern analysis and machine intelligence', '저자': 'Christopher Richard  Wren, Ali Azarbayejani, Trevor Darrell, Alex Paul Pentland', '전체 인용횟수': '7038회 인용19961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220232659111136187188238268313396419402420423425356392357359324283244165155116877337', '페이지': '780-785', '학술 문서': \"Pfinder: Real-time tracking of the human bodyCR Wren, A Azarbayejani, T Darrell, AP Pentland\\xa0- IEEE Transactions on pattern analysis and machine\\xa0…, 19976735회 인용 관련 학술자료 전체 19개의 버전 Pfinder: real-time tracking of the human bodyC Wren, A Azarbayejani, T Darrell, A Pentland\\xa0- Proceedings of the second international conference on\\xa0…, 1996343회 인용 관련 학술자료 전체 16개의 버전 Pfinder: Real-time tracking of tge human body*C Wren, A Azarbayejani, T Darrell, A Pentland\\xa0- FG'96, 19959회 인용 관련 학술자료 \", '호': '7'}, title='Pfinder: Real-time tracking of the human body', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Decaf: A deep convolutional activation feature for generic visual recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/1/27', '게시자': 'PMLR', '설명': 'We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.', '저자': 'Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, Trevor Darrell', '전체 인용횟수': '5779회 인용2014201520162017201820192020202120222023165421611721787784735592487354', '컨퍼런스': 'International conference on machine learning', '페이지': '647-655', '학술 문서': 'Decaf: A deep convolutional activation feature for generic visual recognitionJ Donahue, Y Jia, O Vinyals, J Hoffman, N Zhang…\\xa0- International conference on machine learning, 20145779회 인용 관련 학술자료 전체 15개의 버전 '}, title='Decaf: A deep convolutional activation feature for generic visual recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Context encoders: Feature learning by inpainting': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': 'We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders--a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part (s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.', '저자': 'Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, Alexei A Efros', '전체 인용횟수': '5759회 인용201620172018201920202021202220232721052772989011111218975', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '2536-2544', '학술 문서': 'Context encoders: Feature learning by inpaintingD Pathak, P Krahenbuhl, J Donahue, T Darrell…\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20165759회 인용 관련 학술자료 전체 14개의 버전 '}, title='Context encoders: Feature learning by inpainting', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Adversarial discriminative domain adaptation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/2/17', '게시자': 'arXiv preprint arXiv:1702.05464', '설명': 'Adversarial learning methods are a promising approach to training robust deep networks, and can generate complex samples across diverse domains. They can also improve recognition despite the presence of domain shift or dataset bias: recent adversarial approaches to unsupervised domain adaptation reduce the difference between the training and test domain distributions and thus improve generalization performance. However, while generative adversarial networks (GANs) show compelling visualizations, they are not optimal on discriminative tasks and can be limited to smaller shifts. On the other hand, discriminative approaches can handle larger domain shifts, but impose tied weights on the model and do not exploit a GAN-based loss. In this work, we first outline a novel generalized framework for adversarial adaptation, which subsumes recent state-of-the-art approaches as special cases, and use this generalized view to better relate prior approaches. We then propose a previously unexplored instance of our general framework which combines discriminative modeling, untied weight sharing, and a GAN loss, which we call Adversarial Discriminative Domain Adaptation (ADDA). We show that ADDA is more effective yet considerably simpler than competing domain-adversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adaptation results on standard domain adaptation tasks as well as a difficult cross-modality object classification task.', '저자': 'Eric Tzeng, Judy Hoffman, Kate Saenko, Trevor Darrell', '전체 인용횟수': '4804회 인용20172018201920202021202220234925961987210371004933', '컨퍼런스': 'Computer Vision and Pattern Recognition (CVPR)', '학술 문서': 'Adversarial discriminative domain adaptationE Tzeng, J Hoffman, K Saenko, T Darrell\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20174804회 인용 관련 학술자료 전체 11개의 버전 '}, title='Adversarial discriminative domain adaptation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'End-to-end training of deep visuomotor policies': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/1/1', '게시자': 'JMLR. org', '권': '17', '설명': 'Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-toend provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot’s motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.', '저널': 'The Journal of Machine Learning Research', '저자': 'Sergey Levine, Chelsea Finn, Trevor Darrell, Pieter Abbeel', '전체 인용횟수': '3622회 인용20152016201720182019202020212022202334145324468576571548507416', '페이지': '1334-1373', '학술 문서': 'End-to-end training of deep visuomotor policiesS Levine, C Finn, T Darrell, P Abbeel\\xa0- The Journal of Machine Learning Research, 20163622회 인용 관련 학술자료 전체 19개의 버전 ', '호': '1'}, title='End-to-end training of deep visuomotor policies', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Adapting visual category models to new domains': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010', '게시자': 'Springer Berlin Heidelberg', '설명': ' Domain adaptation is an important emerging topic in computer vision. In this paper, we present one of the first studies of domain shift in the context of object recognition. We introduce a method that adapts object models acquired in a particular visual domain to new imaging conditions by learning a transformation that minimizes the effect of domain-induced changes in the feature distribution. The transformation is learned in a supervised manner and can be applied to categories for which there are no labeled examples in the new domain. While we focus our evaluation on object recognition tasks, the transform-based adaptation technique we develop is general and could be applied to non-image data. Another contribution is a new multi-domain object database, freely available for download. We experimentally demonstrate the ability of our method to improve recognition on categories with few or no target\\xa0…', '저자': 'Kate Saenko, Brian Kulis, Mario Fritz, Trevor Darrell', '전체 인용횟수': '3032회 인용2010201120122013201420152016201720182019202020212022202310263377111131183158216287393455451463', '컨퍼런스': 'Computer Vision–ECCV 2010: 11th European Conference on Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part IV 11', '페이지': '213-226', '학술 문서': 'Adapting visual category models to new domainsK Saenko, B Kulis, M Fritz, T Darrell\\xa0- Computer Vision–ECCV 2010: 11th European\\xa0…, 20103032회 인용 관련 학술자료 전체 14개의 버전 '}, title='Adapting visual category models to new domains', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Cycada: Cycle-consistent adversarial domain adaptation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '게시자': 'arXiv preprint arXiv:1711.03213, published on 2017/11/8', '설명': 'Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models have shown tremendous progress towards adapting to new environments by focusing either on discovering domain invariant representations or by mapping between unpaired image domains. While feature space methods are difficult to interpret and sometimes fail to capture pixel-level and low-level domain shifts, image space methods sometimes fail to incorporate high level semantic knowledge relevant for the end task. We propose a model which adapts between domains using both generative image space alignment and latent representation space alignment. Our approach, Cycle-Consistent Adversarial Domain Adaptation (CyCADA), guides transfer between domains according to a specific discriminatively trained task and avoids divergence by enforcing consistency of the relevant semantics before and after adaptation. We evaluate our method on a variety of visual recognition and prediction settings, including digit classification and semantic segmentation of road scenes, advancing state-of-the-art performance for unsupervised adaptation from synthetic to real world driving domains.', '저자': 'Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A Efros, Trevor Darrell', '전체 인용횟수': '2980회 인용20182019202020212022202390358545703666601', '컨퍼런스': 'International Conference on Machine Learning 2018', '학술 문서': 'Cycada: Cycle-consistent adversarial domain adaptationJ Hoffman, E Tzeng, T Park, JY Zhu, P Isola, K Saenko…\\xa0- International conference on machine learning, 20182980회 인용 관련 학술자료 전체 9개의 버전 '}, title='Cycada: Cycle-consistent adversarial domain adaptation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Region-based convolutional networks for accurate object detection and segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/5/25', '게시자': 'IEEE', '권': '38', '설명': 'Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition. The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50 percent relative to the previous best result on VOC 2012-achieving a mAP of 62.4 percent. Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly. Since we combine region proposals with CNNs, we call the resulting model\\xa0…', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik', '전체 인용횟수': '2905회 인용2015201620172018201920202021202220231499248367396414496453371', '페이지': '142-158', '학술 문서': 'Region-based convolutional networks for accurate object detection and segmentationR Girshick, J Donahue, T Darrell, J Malik\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20152905회 인용 관련 학술자료 전체 9개의 버전 ', '호': '1'}, title='Region-based convolutional networks for accurate object detection and segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep domain confusion: Maximizing for domain invariance': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/12/10', '설명': 'Recent reports suggest that a generic supervised deep CNN model trained on a large-scale dataset reduces, but does not remove, dataset bias on a standard benchmark. Fine-tuning deep models in a new domain can require a significant amount of data, which for many applications is simply not available. We propose a new CNN architecture which introduces an adaptation layer and an additional domain confusion loss, to learn a representation that is both semantically meaningful and domain invariant. We additionally show that a domain confusion metric can be used for model selection to determine the dimension of an adaptation layer and the best position for the layer in the CNN architecture. Our proposed adaptation method offers empirical performance which exceeds previously published results on a standard benchmark visual domain adaptation task.', '저자': 'Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, Trevor Darrell', '전체 인용횟수': '2741회 인용201520162017201820192020202120222023113897161295453567578519', '출처': 'arXiv preprint arXiv:1412.3474', '학술 문서': 'Deep domain confusion: Maximizing for domain invarianceE Tzeng, J Hoffman, N Zhang, K Saenko, T Darrell\\xa0- arXiv preprint arXiv:1412.3474, 20142741회 인용 관련 학술자료 전체 2개의 버전 '}, title='Deep domain confusion: Maximizing for domain invariance', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A convnet for the 2020s': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2022', '설명': 'The\" Roaring 20s\" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (eg, Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually\" modernize\" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.', '저자': 'Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie', '전체 인용횟수': '2650회 인용20212022202376801933', '컨퍼런스': 'Proceedings of the IEEE/CVF conference on computer vision and pattern recognition', '페이지': '11976-11986', '학술 문서': 'A convnet for the 2020sZ Liu, H Mao, CY Wu, C Feichtenhofer, T Darrell, S Xie\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 20222650회 인용 관련 학술자료 전체 11개의 버전 '}, title='A convnet for the 2020s', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Curiosity-driven exploration by self-supervised prediction': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/8', '권': '70', '설명': 'In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent’s ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (eg new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.', '저자': 'Deepak Pathak, Pulkit Agrawal, Alexei A Efros, Trevor Darrell', '전체 인용횟수': '2384회 인용201720182019202020212022202328166314417488502455', '컨퍼런스': 'Proceedings of the 34th International Conference on Machine Learning (ICML)', '페이지': '2778-2787', '학술 문서': 'Curiosity-driven exploration by self-supervised predictionD Pathak, P Agrawal, AA Efros, T Darrell\\xa0- International conference on machine learning, 20172376회 인용 관련 학술자료 전체 14개의 버전 Curiosity-driven exploration by self-supervised prediction (2017)*D Pathak, P Agrawal, AA Efros, T Darrell\\xa0- arXiv preprint arXiv:1705.05363, 201713회 인용 관련 학술자료 '}, title='Curiosity-driven exploration by self-supervised prediction', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Adversarial feature learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/5/31', '설명': 'The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.', '저널': 'arXiv preprint arXiv:1605.09782', '저자': 'Jeff Donahue, Philipp Krähenbühl, Trevor Darrell', '전체 인용횟수': '2192회 인용2016201720182019202020212022202324122257295382403392305', '학술 문서': 'Adversarial feature learningJ Donahue, P Krähenbühl, T Darrell\\xa0- arXiv preprint arXiv:1605.09782, 20162192회 인용 관련 학술자료 전체 10개의 버전 '}, title='Adversarial feature learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The pyramid match kernel: Discriminative classification with sets of image features': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2005/10/17', '게시자': 'IEEE', '권': '2', '설명': 'Discriminative learning is challenging when examples are sets of features, and the sets vary in cardinality and lack any sort of meaningful ordering. Kernel-based classification methods can learn complex decision boundaries, but a kernel over unordered set inputs must somehow solve for correspondences epsivnerally a computationally expensive task that becomes impractical for large set sizes. We present a new fast kernel function which maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in this space. This \"pyramid match\" computation is linear in the number of features, and it implicitly finds correspondences based on the finest resolution histogram cell where a matched pair first appears. Since the kernel does not penalize the presence of extra features, it is robust to clutter. We show the kernel function is positive-definite, making it valid for use in learning\\xa0…', '저자': 'Kristen Grauman, Trevor Darrell', '전체 인용횟수': '2139회 인용2005200620072008200920102011201220132014201520162017201820192020202120222023105776132149194176170156178149160104836881666239', '컨퍼런스': \"Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1\", '페이지': '1458-1465', '학술 문서': 'The pyramid match kernel: Discriminative classification with sets of image featuresK Grauman, T Darrell\\xa0- Tenth IEEE International Conference on Computer\\xa0…, 20052139회 인용 관련 학술자료 전체 31개의 버전 '}, title='The pyramid match kernel: Discriminative classification with sets of image features', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Sequence to sequence-video to text': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'Real-world videos often have complex dynamics; methods for generating open-domain video descriptions should be senstive to temporal structure and allow both input (sequence of frames) and output (sequence of words) of variable length. To approach this problem we propose a novel end-to-end sequence-to-sequence model to generate captions for videos. For this we exploit recurrent neural networks, specifically LSTMs, which have demonstrated state-of-the-art performance in image caption generation. Our LSTM model is trained on video-sentence pairs and learns to associate a sequence of video frames to a sequence of words in order to generate a description of the event in the video clip. Our model naturally is able to learn the temporal structure of the sequence of frames as well as the sequence model of the generated sentences, ie a language model. We evaluate several variants of our model that exploit different visual features on a standard set of YouTube videos and two movie description datasets (M-VAD and MPII-MD).', '저자': 'Subhashini Venugopalan, Marcus Rohrbach, Jeffrey Donahue, Raymond Mooney, Trevor Darrell, Kate Saenko', '전체 인용횟수': '1642회 인용2015201620172018201920202021202220231684175225259254240204146', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '4534-4542', '학술 문서': 'Sequence to sequence-video to textS Venugopalan, M Rohrbach, J Donahue, R Mooney…\\xa0- Proceedings of the IEEE international conference on\\xa0…, 20151642회 인용 관련 학술자료 전체 21개의 버전 '}, title='Sequence to sequence-video to text', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Multimodal compact bilinear pooling for visual question answering and visual grounding': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/6/6', '설명': 'Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge.', '저널': 'arXiv preprint arXiv:1606.01847', '저자': 'Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, Marcus Rohrbach', '전체 인용횟수': '1634회 인용2016201720182019202020212022202325129217241270256270199', '학술 문서': 'Multimodal compact bilinear pooling for visual question answering and visual groundingA Fukui, DH Park, D Yang, A Rohrbach, T Darrell…\\xa0- arXiv preprint arXiv:1606.01847, 20161634회 인용 관련 학술자료 전체 10개의 버전 '}, title='Multimodal compact bilinear pooling for visual question answering and visual grounding', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Toward multimodal image-to-image translation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '권': '30', '설명': 'Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a distribution of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.', '저널': 'Advances in neural information processing systems', '저자': 'Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, Eli Shechtman', '전체 인용횟수': '1601회 인용20182019202020212022202390258317369298258', '학술 문서': 'Toward multimodal image-to-image translationJY Zhu, R Zhang, D Pathak, T Darrell, AA Efros…\\xa0- Advances in neural information processing systems, 20171601회 인용 관련 학술자료 전체 7개의 버전 '}, title='Toward multimodal image-to-image translation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Imagenet: A large-scale hierarchical image database': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009/6/20', '게시자': 'Ieee', '설명': 'The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets\\xa0…', '저자': 'Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei', '전체 인용횟수': '59890회 인용201120122013201420152016201720182019202020212022202320026635851886613742324393059198168109851232711980', '컨퍼런스': '2009 IEEE conference on computer vision and pattern recognition', '페이지': '248-255', '학술 문서': 'Imagenet: A large-scale hierarchical image databaseJ Deng, W Dong, R Socher, LJ Li, K Li, L Fei-Fei\\xa0- 2009 IEEE conference on computer vision and pattern\\xa0…, 200959553회 인용 관련 학술자료 전체 33개의 버전 ImageNet: A large-scale hierarchical image databaseW Dong, R Socher, L Li-Jia, K Li, L Fei-Fei\\xa0- CVPR, 2009445회 인용 관련 학술자료 ImageNet: A large-scale hierarchical image database. 248–255*J Deng, W Dong, R Socher, LJ Li, K Li, L Fei-Fei\\xa0- 2009 IEEE Conference on Computer Vision and\\xa0…, 2009129회 인용 관련 학술자료 Li, l., 2005,\"*XL Deng, XZ Zhao\\xa0- Vibroacoustic optimization of two cross stamped ribs in\\xa0…17회 인용 관련 학술자료 '}, title='Imagenet: A large-scale hierarchical image database', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Imagenet large scale visual recognition challenge': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/12', '게시자': 'Springer US', '권': '115', '설명': ' The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5\\xa0years of the challenge, and propose future directions and improvements.', '저널': 'International journal of computer vision', '저자': 'Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, Li Fei-Fei', '전체 인용횟수': '41114회 인용20152016201720182019202020212022202352014502737436254826105695970135922', '페이지': '211-252', '학술 문서': 'Imagenet large scale visual recognition challengeO Russakovsky, J Deng, H Su, J Krause, S Satheesh…\\xa0- International journal of computer vision, 201541114회 인용 관련 학술자료 전체 21개의 버전 '}, title='Imagenet large scale visual recognition challenge', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Perceptual losses for real-time style transfer and super-resolution': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '게시자': 'Springer International Publishing', '설명': ' We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al.\\xa0in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment\\xa0…', '저자': 'Justin Johnson, Alexandre Alahi, Li Fei-Fei', '전체 인용횟수': '10296회 인용201620172018201920202021202220234429783113701748207820141819', '컨퍼런스': 'Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14', '페이지': '694-711', '학술 문서': 'Perceptual losses for real-time style transfer and super-resolutionJ Johnson, A Alahi, L Fei-Fei\\xa0- Computer Vision–ECCV 2016: 14th European\\xa0…, 201610296회 인용 관련 학술자료 전체 8개의 버전 '}, title='Perceptual losses for real-time style transfer and super-resolution', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Large-scale video classification with convolutional neural networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '설명': 'Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3% to 63.9%), but only a surprisingly modest improvement compared to single-frame models (59.3% to 60.9%). We further study the generalization performance of our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3% up from 43.9%).', '저자': 'Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, Li Fei-Fei', '전체 인용횟수': '8036회 인용2014201520162017201820192020202120222023382505739361125121611671021902653', '컨퍼런스': 'Proceedings of the IEEE conference on Computer Vision and Pattern Recognition', '페이지': '1725-1732', '학술 문서': 'Large-scale video classification with convolutional neural networksA Karpathy, G Toderici, S Shetty, T Leung…\\xa0- Proceedings of the IEEE conference on Computer\\xa0…, 20148036회 인용 관련 학술자료 전체 44개의 버전 '}, title='Large-scale video classification with convolutional neural networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep visual-semantic alignments for generating image descriptions': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.', '저자': 'Andrej Karpathy, Li Fei-Fei', '전체 인용횟수': '6374회 인용201520162017201820192020202120222023195474666884889806809823685', '컨퍼런스': 'Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition', '페이지': '3128-3137', '학술 문서': 'Deep visual-semantic alignments for generating image descriptionsA Karpathy, L Fei-Fei\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20156374회 인용 관련 학술자료 전체 40개의 버전 '}, title='Deep visual-semantic alignments for generating image descriptions', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2004/6/27', '게시자': 'IEEE', '설명': 'Current computational approaches to learning visual object categories require thousands of training images, are slow, cannot learn in an incremental manner and cannot incorporate prior information into the learning process. In addition, no algorithm presented in the literature has been tested on more than a handful of object categories. We present an method for learning object categories from just a few training images. It is quick and it uses prior information in a principled way. We test it on a dataset composed of images of objects belonging to 101 widely varied categories. Our proposed method is based on making use of prior information, assembled from (unrelated) object categories which were previously learnt. A generative probabilistic model is used, which represents the shape and appearance of a constellation of features belonging to the object. The parameters of the model are learnt incrementally in a\\xa0…', '저자': 'Li Fei-Fei, Rob Fergus, Pietro Perona', '전체 인용횟수': '5101회 인용2004200520062007200820092010201120122013201420152016201720182019202020212022202314437096135165234243282336348362365310303307301314360440', '컨퍼런스': '2004 conference on computer vision and pattern recognition workshop', '페이지': '178-178', '학술 문서': 'Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categoriesL Fei-Fei, R Fergus, P Perona\\xa0- 2004 conference on computer vision and pattern\\xa0…, 20045101회 인용 관련 학술자료 전체 26개의 버전 '}, title='Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A bayesian hierarchical model for learning natural scene categories': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2005/6/20', '게시자': 'IEEE', '권': '2', '설명': 'We propose a novel approach to learn and recognize natural scene categories. Unlike previous work, it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a \"theme\". In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes.', '저자': 'Li Fei-Fei, Pietro Perona', '전체 인용횟수': '4997회 인용20052006200720082009201020112012201320142015201620172018201920202021202220232260109195278383378397439440449408294270229190151117108', '컨퍼런스': \"2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)\", '페이지': '524-531', '학술 문서': 'A bayesian hierarchical model for learning natural scene categoriesL Fei-Fei, P Perona\\xa0- 2005 IEEE Computer Society Conference on Computer\\xa0…, 20054995회 인용 관련 학술자료 전체 35개의 버전 A bayesian hierarchical model for learning natural scene categories; 2005*L Fei-Fei, P Perona\\xa0- Proc. of IEEE Computer Vision and Pattern Recognition4회 인용 관련 학술자료 '}, title='A bayesian hierarchical model for learning natural scene categories', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Visual genome: Connecting language and vision using crowdsourced dense image annotations': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/5', '게시자': 'Springer US', '권': '123', '설명': ' Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked “What vehicle is the person riding?”, computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) to answer correctly that “the person is riding a horse-drawn carriage.” In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense\\xa0…', '저널': 'International journal of computer vision', '저자': 'Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael S Bernstein, Li Fei-Fei', '전체 인용횟수': '4685회 인용201620172018201920202021202220235716227848066686610561086', '페이지': '32-73', '학술 문서': 'Visual genome: Connecting language and vision using crowdsourced dense image annotationsR Krishna, Y Zhu, O Groth, J Johnson, K Hata, J Kravitz…\\xa0- International journal of computer vision, 20174685회 인용 관련 학술자료 전체 15개의 버전 '}, title='Visual genome: Connecting language and vision using crowdsourced dense image annotations', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " '3d object representations for fine-grained categorization': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013', '설명': 'While 3D object representations are being revived in the context of multi-view object class detection and scene understanding, they have not yet attained wide-spread use in fine-grained categorization. State-of-the-art approaches achieve remarkable performance when training data is plentiful, but they are typically tied to flat, 2D representations that model objects as a collection of unconnected views, limiting their ability to generalize across viewpoints. In this paper, we therefore lift two state-of-the-art 2D object representations to 3D, on the level of both local feature appearance and location. In extensive experiments on existing and newly proposed datasets, we show our 3D object representations outperform their state-of-the-art 2D counterparts for fine-grained categorization and demonstrate their efficacy for estimating 3D geometry from images via ultrawide baseline matching and 3D reconstruction.', '저자': 'Jonathan Krause, Michael Stark, Jia Deng, Li Fei-Fei', '전체 인용횟수': '3162회 인용201420152016201720182019202020212022202392055123191290392557689803', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision workshops', '페이지': '554-561', '학술 문서': '3d object representations for fine-grained categorizationJ Krause, M Stark, J Deng, L Fei-Fei\\xa0- Proceedings of the IEEE international conference on\\xa0…, 20133162회 인용 관련 학술자료 전체 20개의 버전 '}, title='3d object representations for fine-grained categorization', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Social lstm: Human trajectory prediction in crowded spaces': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': 'Humans navigate complex crowded environments based on social conventions: they respect personal space, yielding right-of-way and avoid collisions. In our work, we propose a data-driven approach to learn these human-human interactions for predicting their future trajectories. This is in contrast to traditional approaches which use hand-crafted functions such as Social forces. We present a new Long Short-Term Memory (LSTM) model which jointly reasons across multiple individuals in a scene. Different from the conventional LSTM, we share the information between multiple LSTMs through a new pooling layer. This layer pools the hidden representation from LSTMs corresponding to neighboring trajectories to capture interactions within this neighborhood. We demonstrate the performance of our method on several public datasets. Our model outperforms previous forecasting methods by more than 42%. We also analyze the trajectories predicted by our model to demonstrate social behaviours such as collision avoidance and group movement, learned by our model.', '저자': 'Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li Fei-Fei, Silvio Savarese', '전체 인용횟수': '2981회 인용201620172018201920202021202220231591195337510609653545', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '961-971', '학술 문서': 'Social lstm: Human trajectory prediction in crowded spacesA Alahi, K Goel, V Ramanathan, A Robicquet, L Fei-Fei…\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20162981회 인용 관련 학술자료 전체 17개의 버전 '}, title='Social lstm: Human trajectory prediction in crowded spaces', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'One-shot learning of object categories': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2006/2/21', '게시자': 'IEEE', '권': '28', '설명': 'Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models. Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to\\xa0…', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Li Fei-Fei, Robert Fergus, Pietro Perona', '전체 인용횟수': '2837회 인용200620072008200920102011201220132014201520162017201820192020202120222023133560629796100959187103150208273344367351265', '페이지': '594-611', '학술 문서': 'One-shot learning of object categoriesL Fei-Fei, R Fergus, P Perona\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20062799회 인용 관련 학술자료 전체 13개의 버전 One-shot learning of object categories. Pattern Analysis and Machine IntelligenceL Fei-Fei, R Fergus, P Perona\\xa0- IEEE Transactions on, 200667회 인용 관련 학술자료 ', '호': '4'}, title='One-shot learning of object categories', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Unsupervised learning of human action categories using spatial-temporal words': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2008/9/1', '게시자': 'Springer Netherlands', '권': '79', '설명': ' We present a novel unsupervised learning method for human action categories. A video sequence is represented as a collection of spatial-temporal words by extracting space-time interest points. The algorithm automatically learns the probability distributions of the spatial-temporal words and the intermediate topics corresponding to human action categories. This is achieved by using latent topic models such as the probabilistic Latent Semantic Analysis (pLSA) model and Latent Dirichlet Allocation (LDA). Our approach can handle noisy feature points arisen from dynamic background and moving cameras due to the application of the probabilistic models. Given a novel video sequence, the algorithm can categorize and localize the human action(s) contained in the video. We test our algorithm on three challenging datasets: the KTH human motion dataset, the Weizmann human action dataset, and a recent\\xa0…', '저널': 'International Journal of Computer Vision', '저자': 'Juan Carlos Niebles, Hongcheng Wang, Li Fei-Fei', '전체 인용횟수': '2246회 인용2007200820092010201120122013201420152016201720182019202020212022202330961452232242072402151761851361056959523017', '페이지': '299-318', '학술 문서': 'Unsupervised learning of human action categories using spatial-temporal wordsJC Niebles, H Wang, L Fei-Fei\\xa0- International journal of computer vision, 20082246회 인용 관련 학술자료 전체 36개의 버전 ', '호': '3'}, title='Unsupervised learning of human action categories using spatial-temporal words', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Progressive neural architecture search': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'We propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms. Our approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space. Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of Zoph et al.(2018) in terms of number of models evaluated, and 8 times faster in terms of total compute. The structures we discover in this way achieve state of the art classification accuracies on CIFAR-10 and ImageNet.', '저자': 'Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, Kevin Murphy', '전체 인용횟수': '2131회 인용20182019202020212022202375319440508440334', '컨퍼런스': 'Proceedings of the European conference on computer vision (ECCV)', '페이지': '19-34', '학술 문서': 'Progressive neural architecture searchC Liu, B Zoph, M Neumann, J Shlens, W Hua, LJ Li…\\xa0- Proceedings of the European conference on computer\\xa0…, 20182131회 인용 관련 학술자료 전체 11개의 버전 '}, title='Progressive neural architecture search', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'On the opportunities and risks of foundation models': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021/8/16', '설명': 'AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.', '저널': 'arXiv preprint arXiv:2108.07258', '저자': 'Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W Thomas, Florian Tramèr, Rose E Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, Percy Liang', '전체 인용횟수': '1858회 인용202120222023695491230', '학술 문서': 'On the opportunities and risks of foundation modelsR Bommasani, DA Hudson, E Adeli, R Altman, S Arora…\\xa0- arXiv preprint arXiv:2108.07258, 20211858회 인용 관련 학술자료 전체 2개의 버전 '}, title='On the opportunities and risks of foundation models', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Social gan: Socially acceptable trajectories with generative adversarial networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'Understanding human motion behavior is critical for autonomous moving platforms (like self-driving cars and social robots) if they are to navigate human-centric environments. This is challenging because human motion is inherently multimodal: given a history of human motion paths, there are many socially plausible ways that people could move in the future. We tackle this problem by combining tools from sequence prediction and generative adversarial networks: a recurrent sequence-to-sequence model observes motion histories and predicts future behavior, using a novel pooling mechanism to aggregate information across people. We predict socially plausible futures by training adversarially against a recurrent discriminator, and encourage diverse predictions with a novel variety loss. Through experiments on several datasets we demonstrate that our approach outperforms prior work in terms of accuracy, variety, collision avoidance, and computational complexity.', '저자': 'Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, Alexandre Alahi', '전체 인용횟수': '1781회 인용20182019202020212022202319146308421460422', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '2255-2264', '학술 문서': 'Social gan: Socially acceptable trajectories with generative adversarial networksA Gupta, J Johnson, L Fei-Fei, S Savarese, A Alahi\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20181781회 인용 관련 학술자료 전체 12개의 버전 '}, title='Social gan: Socially acceptable trajectories with generative adversarial networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Target-driven visual navigation in indoor scenes using deep reinforcement learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/5/29', '게시자': 'IEEE', '설명': 'Two less addressed issues of deep reinforcement learning are (1) lack of generalization capability to new goals, and (2) data inefficiency, i.e., the model requires several (and often costly) episodes of trial and error to converge, which makes it impractical to be applied to real-world scenarios. In this paper, we address these two issues and apply our model to target-driven visual navigation. To address the first issue, we propose an actor-critic model whose policy is a function of the goal as well as the current state, which allows better generalization. To address the second issue, we propose the AI2-THOR framework, which provides an environment with high-quality 3D scenes and a physics engine. Our framework enables agents to take actions and interact with objects. Hence, we can collect a huge number of training samples efficiently. We show that our proposed method (1) converges faster than the state-of-the-art\\xa0…', '저자': 'Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, Ali Farhadi', '전체 인용횟수': '1658회 인용201720182019202020212022202376185280293307275226', '컨퍼런스': '2017 IEEE international conference on robotics and automation (ICRA)', '페이지': '3357-3364', '학술 문서': 'Target-driven visual navigation in indoor scenes using deep reinforcement learningY Zhu, R Mottaghi, E Kolve, JJ Lim, A Gupta, L Fei-Fei…\\xa0- 2017 IEEE international conference on robotics and\\xa0…, 20171658회 인용 관련 학술자료 전체 12개의 버전 '}, title='Target-driven visual navigation in indoor scenes using deep reinforcement learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Novel dataset for fine-grained image categorization: Stanford dogs': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011/6/25', '게시자': 'Citeseer', '권': '2', '설명': 'We introduce a 120 class Stanford Dogs dataset, a challenging and large-scale dataset aimed at fine-grained image categorization. Stanford Dogs includes over 22,000 annotated images of dogs belonging to 120 species. Each image is annotated with a bounding box and object class label. Fig. 1 shows examples of images from Stanford Dogs. This dataset is extremely challenging due to a variety of reasons. First, being a fine-grained categorization problem, there is little inter-class variation. For example the basset hound and bloodhound share very similar facial characteristics but differ significantly in their color, while the Japanese spaniel and papillion share very similar color but greatly differ in their facial characteristics. Second, there is very large intra-class variation. The images show that dogs within a class could have different ages (eg beagle), poses (eg blenheim spaniel), occlusion/self-occlusion and even color (eg Shih-tzu). Furthermore, compared to other animal datasets that tend to exist in natural scenes, a large proportion of the images contain humans and are taken in manmade environments leading to greater background variation. The aforementioned reasons make this an extremely challenging dataset.', '저널': 'Proc. CVPR workshop on fine-grained visual categorization (FGVC)', '저자': 'Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, Fei-Fei Li', '전체 인용횟수': '1444회 인용20122013201420152016201720182019202020212022202351725325578106150189256254260', '학술 문서': 'Novel dataset for fine-grained image categorization: Stanford dogsA Khosla, N Jayadevaprakash, B Yao, FF Li\\xa0- Proc. CVPR workshop on fine-grained visual\\xa0…, 20111428회 인용 관련 학술자료 전체 4개의 버전 Novel dataset for fgvc: Stanford dogs*A Khosla, N Jayadevaprakash, B Yao, FF Li\\xa0- San Diego: CVPR Workshop on FGVC, 201116회 인용 관련 학술자료 Novel dataset for fine-grained image categorization: Stanford dogsJN KhoslaA, BP Yao\\xa0- 2022-01-04]. https://people. csail. mit. edu/khosla\\xa0…, 20112회 인용 관련 학술자료 ', '호': '1'}, title='Novel dataset for fine-grained image categorization: Stanford dogs', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/7/3', '게시자': 'PMLR', '설명': 'Recent deep networks are capable of memorizing the entire data even when the labels are completely random. To overcome the overfitting on corrupted labels, we propose a novel technique of learning another neural network, called MentorNet, to supervise the training of the base deep networks, namely, StudentNet. During training, MentorNet provides a curriculum (sample weighting scheme) for StudentNet to focus on the sample the label of which is probably correct. Unlike the existing curriculum that is usually predefined by human experts, MentorNet learns a data-driven curriculum dynamically with StudentNet. Experimental results demonstrate that our approach can significantly improve the generalization performance of deep networks trained on corrupted training data. Notably, to the best of our knowledge, we achieve the best-published result on WebVision, a large benchmark containing 2.2 million images of real-world noisy labels.', '저자': 'Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, Li Fei-Fei', '전체 인용횟수': '1425회 인용20182019202020212022202325135244335333346', '컨퍼런스': 'International conference on machine learning', '페이지': '2304-2313', '학술 문서': 'Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labelsL Jiang, Z Zhou, T Leung, LJ Li, L Fei-Fei\\xa0- International conference on machine learning, 20181425회 인용 관련 학술자료 전체 10개의 버전 '}, title='Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Visualizing and understanding recurrent networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/6/5', '설명': 'Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.', '저널': 'arXiv preprint arXiv:1506.02078', '저자': 'Andrej Karpathy, Justin Johnson, Li Fei-Fei', '전체 인용횟수': '1351회 인용2015201620172018201920202021202220231011215022921421716514390', '학술 문서': 'Visualizing and understanding recurrent networksA Karpathy, J Johnson, L Fei-Fei\\xa0- arXiv preprint arXiv:1506.02078, 20151351회 인용 관련 학술자료 전체 13개의 버전 '}, title='Visualizing and understanding recurrent networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Normalized cuts and image segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1997/6/17', '게시자': 'IEEE', '설명': 'We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We have applied this approach to segmenting static images and found results very encouraging.', '저자': 'Jianbo Shi, Jitendra Malik', '전체 인용횟수': '1690회 인용1998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202343484584616064627083711011251411149582694441242722273429', '컨퍼런스': 'Proceedings of IEEE computer society conference on computer vision and pattern recognition', '페이지': '731-737', '학술 문서': 'Normalized cuts and image segmentationJ Shi, J Malik\\xa0- Proceedings of IEEE computer society conference on\\xa0…, 19971690회 인용 관련 학술자료 전체 16개의 버전 '}, title='Normalized cuts and image segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Scale-space and edge detection using anisotropic diffusion': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1990/7', '게시자': 'IEEE', '권': '12', '설명': \"A new definition of scale-space is suggested, and a class of algorithms used to realize a diffusion process is introduced. The diffusion coefficient is chosen to vary spatially in such a way as to encourage intraregion smoothing rather than interregion smoothing. It is shown that the 'no new maxima should be generated at coarse scales' property of conventional scale space is preserved. As the region boundaries in the approach remain sharp, a high-quality edge detector which successfully exploits global information is obtained. Experimental results are shown on a number of images. Parallel hardware implementations are made feasible because the algorithm involves elementary, local operations replicated over the image.< >\", '저널': 'IEEE Transactions on pattern analysis and machine intelligence', '저자': 'Pietro Perona, Jitendra Malik', '전체 인용횟수': '17183회 인용1992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202357781121061451801881842072552963514085687237497668428939059801026951880893777706682566547513406', '페이지': '629-639', '학술 문서': 'Scale-space and edge detection using anisotropic diffusionP Perona, J Malik\\xa0- IEEE Transactions on pattern analysis and machine\\xa0…, 199017183회 인용 관련 학술자료 전체 29개의 버전 ', '호': '7'}, title='Scale-space and edge detection using anisotropic diffusion', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2001/7/7', '게시자': 'IEEE', '권': '2', '설명': \"This paper presents a database containing 'ground truth' segmentations produced by humans for images of a wide variety of natural scenes. We define an error measure which quantifies the consistency between segmentations of differing granularities and find that different human segmentations of the same image are highly consistent. Use of this dataset is demonstrated in two applications: (1) evaluating the performance of segmentation algorithms and (2) measuring probability distributions associated with Gestalt grouping factors as well as statistics of image region properties.\", '저자': 'David Martin, Charless Fowlkes, Doron Tal, Jitendra Malik', '전체 인용횟수': '8710회 인용200220032004200520062007200820092010201120122013201420152016201720182019202020212022202326394885126157195259276326368434416408480484578685758791860808', '컨퍼런스': 'Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001', '페이지': '416-423', '학술 문서': 'A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statisticsD Martin, C Fowlkes, D Tal, J Malik\\xa0- Proceedings Eighth IEEE International Conference on\\xa0…, 20018710회 인용 관련 학술자료 전체 22개의 버전 '}, title='A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Shape matching and object recognition using shape contexts': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2002/4', '게시자': 'IEEE', '권': '24', '설명': 'We present a novel approach to measuring similarity between shapes and exploit it for object recognition. In our framework, the measurement of similarity is preceded by: (1) solving for correspondences between points on the two shapes; (2) using the correspondences to estimate an aligning transform. In order to solve the correspondence problem, we attach a descriptor, the shape context, to each point. The shape context at a reference point captures the distribution of the remaining points relative to it, thus offering a globally discriminative characterization. Corresponding points on two similar shapes will have similar shape contexts, enabling us to solve for correspondences as an optimal assignment problem. Given the point correspondences, we estimate the transformation that best aligns the two shapes; regularized thin-plate splines provide a flexible class of transformation maps for this purpose. The\\xa0…', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Serge Belongie, Jitendra Malik, Jan Puzicha', '전체 인용횟수': '8685회 인용20022003200420052006200720082009201020112012201320142015201620172018201920202021202220232682128238292364461472584602608729669593611508406322263236197155', '페이지': '509-522', '학술 문서': 'Shape matching and object recognition using shape contextsS Belongie, J Malik, J Puzicha\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20028685회 인용 관련 학술자료 전체 36개의 버전 ', '호': '4'}, title='Shape matching and object recognition using shape contexts', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Contour detection and hierarchical image segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/8/26', '게시자': 'IEEE', '권': '33', '설명': 'This paper investigates two fundamental problems in computer vision: contour detection and image segmentation. We present state-of-the-art algorithms for both of these tasks. Our contour detector combines multiple local cues into a globalization framework based on spectral clustering. Our segmentation algorithm consists of generic machinery for transforming the output of any contour detector into a hierarchical region tree. In this manner, we reduce the problem of image segmentation to that of contour detection. Extensive experimental evaluation demonstrates that both our contour detection and segmentation methods significantly outperform competing algorithms. The automatically generated hierarchical segmentations can be interactively refined by user-specified annotations. Computation at multiple image resolutions provides a means of coupling our system to recognition applications.', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Pablo Arbelaez, Michael Maire, Charless Fowlkes, Jitendra Malik', '전체 인용횟수': '5923회 인용201120122013201420152016201720182019202020212022202357169285393500561533558599587569509476', '페이지': '898-916', '학술 문서': 'Contour detection and hierarchical image segmentationP Arbelaez, M Maire, C Fowlkes, J Malik\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20105923회 인용 관련 학술자료 전체 18개의 버전 ', '호': '5'}, title='Contour detection and hierarchical image segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Recovering high dynamic range radiance maps from photographs': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2023/8/1', '도서': 'Seminal Graphics Papers: Pushing the Boundaries, Volume 2', '설명': 'We present a method of recovering high dynamic range radiance maps from photographs taken with conventional imaging equipment. In our method, multiple photographs of the scene are taken with different amounts of exposure. Our algorithm uses these differently exposed photographs to recover the response function of the imaging process, up to factor of scale, using the assumption of reciprocity. With the known response function, the algorithm can fuse the multiple photographs into a single, high dynamic range radiance map whose pixel values are proportional to the true radiance values in the scene. We demonstrate our method on images acquired with both photochemical and digital imaging processes. We discuss how this work is applicable in many areas of computer graphics involving digitized photographs, including image-based modeling, image compositing, and image processing. Lastly, we\\xa0…', '저자': 'Paul E Debevec, Jitendra Malik', '전체 인용횟수': '4427회 인용199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220231220386170106131162176228183218229234237229250244216214206178186205173149', '페이지': '643-652', '학술 문서': 'Recovering high dynamic range radiance maps from photographsPE Debevec, J Malik\\xa0- Seminal Graphics Papers: Pushing the Boundaries\\xa0…, 20234427회 인용 관련 학술자료 전체 50개의 버전 '}, title='Recovering high dynamic range radiance maps from photographs', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Modeling and rendering architecture from photographs: A hybrid geometry-and image-based approach': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2023/8/1', '도서': 'Seminal Graphics Papers: Pushing the Boundaries, Volume 2', '설명': 'We present a new approach for modeling and rendering existing architectural scenes from a sparse set of still photographs. Our modeling approach, which combines both geometry-based and imagebased techniques, has two components. The first component is a photogrammetric modeling method which facilitates the recovery of the basic geometry of the photographed scene. Our photogrammetric modeling approach is effective, convenient, and robust because it exploits the constraints that are characteristic of architectural scenes. The second component is a model-based stereo algorithm, which recovers how the real scene deviates from the basic model. By making use of the model, our stereo technique robustly recovers accurate depth from widely-spaced image pairs. Consequently, our approach canmodel large architectural environmentswith far fewer photographs than current image-based modeling\\xa0…', '저자': 'Paul E Debevec, Camillo J Taylor, Jitendra Malik', '전체 인용횟수': '3256회 인용1997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220234999123146175194182198203172169131130105124118116941037379555050738081', '페이지': '465-474', '학술 문서': 'Modeling and rendering architecture from photographs: A hybrid geometry-and image-based approachPE Debevec, CJ Taylor, J Malik\\xa0- Seminal Graphics Papers: Pushing the Boundaries\\xa0…, 20233256회 인용 관련 학술자료 전체 52개의 버전 '}, title='Modeling and rendering architecture from photographs: A hybrid geometry-and image-based approach', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning to detect natural image boundaries using local brightness, color, and texture cues': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2004/3/15', '게시자': 'IEEE', '권': '26', '설명': 'The goal of this work is to accurately detect and localize boundaries in natural scenes using local image measurements. We formulate features that respond to characteristic changes in brightness, color, and texture associated with natural boundaries. In order to combine the information from these features in an optimal way, we train a classifier using human labeled images as ground truth. The output of this classifier provides the posterior probability of a boundary at each image location and orientation. We present precision-recall curves showing that the resulting detector significantly outperforms existing approaches. Our two main results are 1) that cue combination can be performed adequately with a simple linear model and 2) that a proper, explicit treatment of texture is required to detect boundaries in natural images.', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'David R Martin, Charless C Fowlkes, Jitendra Malik', '전체 인용횟수': '3097회 인용20042005200620072008200920102011201220132014201520162017201820192020202120222023235083102135168179173219204223207212208136165160149132121', '페이지': '530-549', '학술 문서': 'Learning to detect natural image boundaries using local brightness, color, and texture cuesDR Martin, CC Fowlkes, J Malik\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20043097회 인용 관련 학술자료 전체 18개의 버전 ', '호': '5'}, title='Learning to detect natural image boundaries using local brightness, color, and texture cues', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Slowfast networks for video recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': 'We present SlowFast networks for video recognition. Our model involves (i) a Slow pathway, operating at low frame rate, to capture spatial semantics, and (ii) a Fast pathway, operating at high frame rate, to capture motion at fine temporal resolution. The Fast pathway can be made very lightweight by reducing its channel capacity, yet can learn useful temporal information for video recognition. Our models achieve strong performance for both action classification and detection in video, and large improvements are pin-pointed as contributions by our SlowFast concept. We report state-of-the-art accuracy on major video recognition benchmarks, Kinetics, Charades and AVA. Code has been made available at: https://github. com/facebookresearch/SlowFast.', '저자': 'Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, Kaiming He', '전체 인용횟수': '2755회 인용2019202020212022202366295592848940', '컨퍼런스': 'Proceedings of the IEEE/CVF international conference on computer vision', '페이지': '6202-6211', '학술 문서': 'Slowfast networks for video recognitionC Feichtenhofer, H Fan, J Malik, K He\\xa0- Proceedings of the IEEE/CVF international conference\\xa0…, 20192755회 인용 관련 학술자료 전체 10개의 버전 '}, title='Slowfast networks for video recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning a classification model for segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2003/10/13', '게시자': 'IEEE', '설명': 'We propose a two-class classification model for grouping. Human segmented natural images are used as positive examples. Negative examples of grouping are constructed by randomly matching human segmentations and images. In a preprocessing stage an image is over-segmented into super-pixels. We define a variety of features derived from the classical Gestalt cues, including contour, texture, brightness and good continuation. Information-theoretic analysis is applied to evaluate the power of these grouping cues. We train a linear classifier to combine these features. To demonstrate the power of the classification model, a simple algorithm is used to randomly search for good segmentations. Results are shown on a wide range of images.', '저널': 'Proceedings ninth IEEE international conference on computer vision', '저자': 'Ren, Malik', '전체 인용횟수': '2297회 인용200420052006200720082009201020112012201320142015201620172018201920202021202220231521234247668912411113814917420416814817917215414794', '페이지': '10-17 vol. 1', '학술 문서': 'Learning a classification model for segmentationRen, Malik\\xa0- Proceedings ninth IEEE international conference on\\xa0…, 20032297회 인용 관련 학술자료 전체 31개의 버전 '}, title='Learning a classification model for segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Representing and recognizing the visual appearance of materials using three-dimensional textons': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2001/6', '게시자': 'Kluwer Academic Publishers', '권': '43', '설명': ' We study the recognition of surfaces made from different materials such as concrete, rug, marble, or leather on the basis of their textural appearance. Such natural textures arise from spatial variation of two surface attributes: (1) reflectance and (2) surface normal. In this paper, we provide a unified model to address both these aspects of natural texture. The main idea is to construct a vocabulary of prototype tiny surface patches with associated local geometric and photometric properties. We call these 3D textons. Examples might be ridges, grooves, spots or stripes or combinations thereof. Associated with each texton is an appearance vector, which characterizes the local irradiance distribution, represented as a set of linear Gaussian derivative filter outputs, under different lighting and viewing conditions. Given a large collection of images of different materials, a clustering approach is used to acquire a\\xa0…', '저널': 'International journal of computer vision', '저자': 'Thomas Leung, Jitendra Malik', '전체 인용횟수': '2122회 인용2001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202361324334772849010913814412215317016917213211510570415329', '페이지': '29-44', '학술 문서': 'Representing and recognizing the visual appearance of materials using three-dimensional textonsT Leung, J Malik\\xa0- International journal of computer vision, 20012122회 인용 관련 학술자료 전체 16개의 버전 '}, title='Representing and recognizing the visual appearance of materials using three-dimensional textons', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Blobworld: Image segmentation using expectation-maximization and its application to image querying': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2002/8', '게시자': 'IEEE', '권': '24', '설명': 'Retrieving images from large and varied collections using image content as a key is a challenging and important problem. We present a new image representation that provides a transformation from the raw pixel data to a small set of image regions that are coherent in color and texture. This \"Blobworld\" representation is created by clustering pixels in a joint color-texture-position feature space. The segmentation algorithm is fully automatic and has been run on a collection of 10,000 natural images. We describe a system that uses the Blobworld representation to retrieve images from this collection. An important aspect of the system is that the user is allowed to view the internal representation of the submitted image and the query results. Similar systems do not offer the user this view into the workings of the system; consequently, query results from these systems can be inexplicable, despite the availability of knobs for\\xa0…', '저널': 'IEEE Transactions on pattern analysis and machine intelligence', '저자': 'Chad Carson, Serge Belongie, Hayit Greenspan, Jitendra Malik', '전체 인용횟수': '2124회 인용20012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023151286108157158172159168140122121137118776671603437341411', '페이지': '1026-1038', '학술 문서': 'Blobworld: Image segmentation using expectation-maximization and its application to image queryingC Carson, S Belongie, H Greenspan, J Malik\\xa0- IEEE Transactions on pattern analysis and machine\\xa0…, 20022054회 인용 관련 학술자료 전체 13개의 버전 Blobworld: Color-and texture-based image segmentation using EM and its application to image querying and classification*C Carson, S Belongie, H Greenspan, J Malik\\xa0- IEEE Transactions on Pattern Analysis and Machine\\xa0…, 200278회 인용 관련 학술자료 ', '호': '8'}, title='Blobworld: Image segmentation using expectation-maximization and its application to image querying', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Multiscale vision transformers': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021', '설명': 'We present Multiscale Vision Transformers (MViT) for video and image recognition, by connecting the seminal idea of multiscale feature hierarchies with transformer models. Multiscale Transformers have several channel-resolution scale stages. Starting from the input resolution and a small channel dimension, the stages hierarchically expand the channel capacity while reducing the spatial resolution. This creates a multiscale pyramid of features with early layers operating at high spatial resolution to model simple low-level visual information, and deeper layers at spatially coarse, but complex, high-dimensional features. We evaluate this fundamental architectural prior for modeling the dense nature of visual signals for a variety of video recognition tasks where it outperforms concurrent vision transformers that rely on large scale external pre-training and are 5-10 more costly in computation and parameters. We further remove the temporal dimension and apply our model for image classification where it outperforms prior work on vision transformers. Code is available at: https://github. com/facebookresearch/SlowFast.', '저자': 'Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, Christoph Feichtenhofer', '전체 인용횟수': '1894회 인용2021202220231077401033', '컨퍼런스': 'Proceedings of the IEEE/CVF international conference on computer vision', '페이지': '6824-6835', '학술 문서': 'Multiscale vision transformersH Fan, B Xiong, K Mangalam, Y Li, Z Yan, J Malik…\\xa0- Proceedings of the IEEE/CVF international conference\\xa0…, 20211811회 인용 관련 학술자료 전체 11개의 버전 Multiscale Vision Transformers.H Fan, B Xiong, K Mangalam, Y Li, Z Yan, J Malik…\\xa0- ICCV, 2021103회 인용 관련 학술자료 Christoph Feichtenhofer. Multiscale vision transformersH Fan, B Xiong, K Mangalam, Y Li, Z Yan, J Malik\\xa0- Proceedings of the IEEE/CVF International Conference\\xa0…, 20212회 인용 관련 학술자료 '}, title='Multiscale vision transformers', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Recognizing action at a distance': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2003/10/13', '게시자': 'IEEE', '설명': 'Our goal is to recognize human action at a distance, at resolutions where a whole person may be, say, 30 pixels tall. We introduce a novel motion descriptor based on optical flow measurements in a spatiotemporal volume for each stabilized human figure, and an associated similarity measure to be used in a nearest-neighbor framework. Making use of noisy optical flow measurements is the key challenge, which is addressed by treating optical flow not as precise pixel displacements, but rather as a spatial pattern of noisy measurements which are carefully smoothed and aggregated to form our spatiotemporal motion descriptor. To classify the action being performed by a human figure in a query sequence, we retrieve nearest neighbor(s) from a database of stored, annotated video sequences. We can also use these retrieved exemplars to transfer 2D/3D skeletons onto the figures in the query sequence, as well as\\xa0…', '저널': 'Proceedings Ninth IEEE International Conference on Computer Vision', '저자': 'Efros, Berg, Mori, Malik', '전체 인용횟수': '1833회 인용20042005200620072008200920102011201220132014201520162017201820192020202120222023445563851481501591451421611509710271575433403217', '페이지': '726-733 vol. 2', '학술 문서': 'Recognizing action at a distanceEfros, Berg, Mori, Malik\\xa0- Proceedings Ninth IEEE International Conference on\\xa0…, 20031833회 인용 관련 학술자료 전체 43개의 버전 '}, title='Recognizing action at a distance', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Spectral grouping using the nystrom method': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2004/2', '게시자': 'IEEE', '권': '26', '설명': 'Spectral graph theoretic methods have recently shown great promise for the problem of image segmentation. However, due to the computational demands of these approaches, applications to large problems such as spatiotemporal data and high resolution imagery have been slow to appear. The contribution of this paper is a method that substantially reduces the computational requirements of grouping algorithms based on spectral partitioning making it feasible to apply them to very large grouping problems. Our approach is based on a technique for the numerical solution of eigenfunction problems known as the Nystrom method. This method allows one to extrapolate the complete grouping solution using only a small number of samples. In doing so, we leverage the fact that there are far fewer coherent groups in a scene than pixels.', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Charless Fowlkes, Serge Belongie, Fan Chung, Jitendra Malik', '전체 인용횟수': '1779회 인용20042005200620072008200920102011201220132014201520162017201820192020202120222023153739537470791111411141331441461061208690774865', '페이지': '214-225', '학술 문서': 'Spectral grouping using the nystrom methodC Fowlkes, S Belongie, F Chung, J Malik\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20041768회 인용 관련 학술자료 전체 39개의 버전 Spectral grouping using the Nystrom method*F Charless\\xa0- IEEE Trans. PAMI, 200418회 인용 관련 학술자료 ', '호': '2'}, title='Spectral grouping using the nystrom method', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Large displacement optical flow: descriptor matching in variational motion estimation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/8/19', '게시자': 'IEEE', '권': '33', '설명': 'Optical flow estimation is classically marked by the requirement of dense sampling in time. While coarse-to-fine warping schemes have somehow relaxed this constraint, there is an inherent dependency between the scale of structures and the velocity that can be estimated. This particularly renders the estimation of detailed human motion problematic, as small body parts can move very fast. In this paper, we present a way to approach this problem by integrating rich descriptors into the variational optical flow setting. This way we can estimate a dense optical flow field with almost the same high accuracy as known from variational optical flow, while reaching out to new domains of motion analysis where the requirement of dense sampling in time is no longer satisfied.', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Thomas Brox, Jitendra Malik', '전체 인용횟수': '1736회 인용20102011201220132014201520162017201820192020202120222023945751131461622072041871721281017268', '페이지': '500-513', '학술 문서': 'Large displacement optical flow: descriptor matching in variational motion estimationT Brox, J Malik\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20101736회 인용 관련 학술자료 전체 17개의 버전 ', '호': '3'}, title='Large displacement optical flow: descriptor matching in variational motion estimation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'BSurf: Speeded up robust features,’’Computer Vision ECCV 2006': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2006', '저널': 'Lecture Notes in Computer Science', '저자': 'Herbert Bay, Tinne Tuytelaars, Luc Van Gool', '전체 인용횟수': '36468회 인용2007200820092010201120122013201420152016201720182019202020212022202316335671213261809226126652878311930673015296727622533251822351650', '페이지': '404-417', '학술 문서': 'Surf: Speeded up robust features*H Bay, T Tuytelaars, L Van Gool\\xa0- Computer Vision–ECCV 2006: 9th European\\xa0…, 200623066회 인용 관련 학술자료 전체 39개의 버전 Speeded-up robust features (SURF)*H Bay, A Ess, T Tuytelaars, L Van Gool\\xa0- Computer vision and image understanding, 200814444회 인용 관련 학술자료 전체 26개의 버전 European conference on computer vision*H Bay, T Tuytelaars, L Van Gool - 2006123회 인용 관련 학술자료 Luc Van Gool*H Bay, T Tuytelaars\\xa0- SURF: speeded up robust features, 200618회 인용 관련 학술자료 SURF: Speed up robust features (SURF)*H Bay, T Tuytelaars, L Van Gool - 20062회 인용 관련 학술자료 '}, title='BSurf: Speeded up robust features,’’Computer Vision ECCV 2006', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Speeded-up robust features (SURF)': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2008/6/30', '게시자': 'Academic Press', '권': '110', '설명': 'This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURF’s application to two challenging, yet converse goals\\xa0…', '저널': 'Computer vision and image understanding', '저자': 'Herbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool', '전체 인용횟수': '14452회 인용200920102011201220132014201520162017201820192020202120222023150352530780103311201269134214241385121010711064898671', '페이지': '346-359', '학술 문서': 'Speeded-up robust features (SURF)H Bay, A Ess, T Tuytelaars, L Van Gool\\xa0- Computer vision and image understanding, 200814452회 인용 관련 학술자료 전체 26개의 버전 ', '호': '3'}, title='Speeded-up robust features (SURF)', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The Pascal Visual Object Classes Challenge: A Retrospective': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/1', '게시자': 'Springer US', '권': '111', '설명': ' The Pascal Visual Object Classes (VOC) challenge consists of two components: (i)\\xa0a publicly available dataset of images together with ground truth annotation and standardised evaluation software; and (ii)\\xa0an annual competition and workshop. There are five challenges: classification, detection, segmentation, action classification, and person layout. In this paper we provide a review of the challenge from 2008–2012. The paper is intended for two audiences: algorithm designers, researchers who want to see what the state of the art is, as measured by performance on the VOC datasets, along with the limitations and weak points of the current generation of algorithms; and, challenge designers, who want to see what we as organisers have learnt from the process and our recommendations for the organisation of future challenges. To analyse the performance of submitted algorithms on the VOC datasets we\\xa0…', '저널': 'International journal of computer vision', '저자': 'Mark Everingham, SM Ali Eslami, Luc Van Gool, Christopher KI Williams, John Winn, Andrew Zisserman', '전체 인용횟수': '6284회 인용20142015201620172018201920202021202220232811725836459378686710511018983', '페이지': '98-136', '학술 문서': 'The pascal visual object classes challenge: A retrospectiveM Everingham, SMA Eslami, L Van Gool, CKI Williams…\\xa0- International journal of computer vision, 20156284회 인용 관련 학술자료 전체 37개의 버전 '}, title='The Pascal Visual Object Classes Challenge: A Retrospective', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Temporal segment networks: Towards good practices for deep action recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '게시자': 'Springer, Cham', '설명': 'Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51\\xa0…', '저자': 'Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, Luc Van Gool', '전체 인용횟수': '4035회 인용2016201720182019202020212022202317197414576637758718667', '컨퍼런스': 'European conference on computer vision', '페이지': '20-36', '학술 문서': 'Temporal segment networks: Towards good practices for deep action recognitionL Wang, Y Xiong, Z Wang, Y Qiao, D Lin, X Tang…\\xa0- European conference on computer vision, 20164035회 인용 관련 학술자료 전체 13개의 버전 '}, title='Temporal segment networks: Towards good practices for deep action recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Efficient non-maximum suppression': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2006/8/20', '게시자': 'IEEE', '권': '3', '설명': 'In this work we scrutinize a low level computer vision task - non-maximum suppression (NMS) - which is a crucial preprocessing step in many computer vision applications. Especially in real time scenarios, efficient algorithms for such preprocessing algorithms, which operate on the full image resolution, are important. In the case of NMS, it seems that merely the straightforward implementation or slight improvements are known. We show that these are far from being optimal, and derive several algorithms ranging from easy-to-implement to highly-efficient', '저자': 'Alexander Neubeck, Luc Van Gool', '전체 인용횟수': '2010회 인용200920102011201220132014201520162017201820192020202120222023141023242344585070100188263340418363', '컨퍼런스': \"18th international conference on pattern recognition (ICPR'06)\", '페이지': '850-855', '학술 문서': 'Efficient non-maximum suppressionA Neubeck, L Van Gool\\xa0- 18th international conference on pattern recognition\\xa0…, 20062010회 인용 관련 학술자료 전체 12개의 버전 '}, title='Efficient non-maximum suppression', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'An adaptive color-based particle filter': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2003/1/10', '게시자': 'Elsevier', '권': '21', '설명': 'Robust real-time tracking of non-rigid objects is a challenging task. Particle filtering has proven very successful for non-linear and non-Gaussian estimation problems. The article presents the integration of color distributions into particle filtering, which has typically been used in combination with edge-based image features. Color distributions are applied, as they are robust to partial occlusion, are rotation and scale invariant and computationally efficient. As the color of an object can vary over time dependent on the illumination, the visual angle and the camera parameters, the target model is adapted during temporally stable image observations. An initialization based on an appearance condition is introduced since tracked objects may disappear and reappear. Comparisons with the mean shift tracker and a combination between the mean shift tracker and Kalman filtering show the advantages and limitations of the\\xa0…', '저널': 'Image and vision computing', '저자': 'Katja Nummiaro, Esther Koller-Meier, Luc Van Gool', '전체 인용횟수': '1790회 인용20032004200520062007200820092010201120122013201420152016201720182019202020212022202311316065939413915316514813412310512870695641374016', '페이지': '99-110', '학술 문서': 'An adaptive color-based particle filterK Nummiaro, E Koller-Meier, L Van Gool\\xa0- Image and vision computing, 20031790회 인용 관련 학술자료 전체 14개의 버전 ', '호': '1'}, title='An adaptive color-based particle filter', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A benchmark dataset and evaluation methodology for video object segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': 'Over the years, datasets and benchmarks have proven their fundamental importance in computer vision research, enabling targeted progress and objective comparisons in many fields. At the same time, legacy datasets may impend the evolution of a field due to saturated algorithm performance and the lack of contemporary, high quality data. In this work we present a new benchmark dataset and evaluation methodology for the area of video object segmentation. The dataset, named DAVIS (Densely Annotated VIdeo Segmentation), consists of fifty high quality, Full HD video sequences, spanning multiple occurrences of common video object segmentation challenges such as occlusions, motion-blur and appearance changes. Each video is accompanied by densely annotated, pixel-accurate and per-frame ground truth segmentation. In addition, we provide a comprehensive analysis of several state-of-the-art segmentation approaches using three complementary metrics that measure the spatial extent of the segmentation, the accuracy of the silhouette contours and the temporal coherence. The results uncover strengths and weaknesses of current approaches, opening up promising directions for future works.', '저자': 'Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, Alexander Sorkine-Hornung', '전체 인용횟수': '1769회 인용201620172018201920202021202220231094152210262313348366', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '724-732', '학술 문서': 'A benchmark dataset and evaluation methodology for video object segmentationF Perazzi, J Pont-Tuset, B McWilliams, L Van Gool…\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20161769회 인용 관련 학술자료 전체 19개의 버전 '}, title='A benchmark dataset and evaluation methodology for video object segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A+: Adjusted anchored neighborhood regression for fast super-resolution': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '게시자': 'Springer International Publishing', '설명': ' We address the problem of image upscaling in the form of single image super-resolution based on a dictionary of low- and high-resolution exemplars. Two recently proposed methods, Anchored Neighborhood Regression (ANR) and Simple Functions (SF), provide state-of-the-art quality performance. Moreover, ANR is among the fastest known super-resolution methods. ANR learns sparse dictionaries and regressors anchored to the dictionary atoms. SF relies on clusters and corresponding learned functions. We propose A+, an improved variant of ANR, which combines the best qualities of ANR and SF. A+ builds on the features and anchored regressors from ANR but instead of learning the regressors on the dictionary it uses the full training material, similar to SF. We validate our method on standard images and compare with state-of-the-art methods. We obtain improved quality (i.e. 0.2–0.7\\xa0dB PSNR\\xa0…', '저자': 'Radu Timofte, Vincent De Smet, Luc Van Gool', '전체 인용횟수': '1664회 인용20152016201720182019202020212022202338108196312265232209168114', '컨퍼런스': 'Computer Vision--ACCV 2014: 12th Asian Conference on Computer Vision, Singapore, Singapore, November 1-5, 2014, Revised Selected Papers, Part IV 12', '페이지': '111-126', '학술 문서': 'A+: Adjusted anchored neighborhood regression for fast super-resolutionR Timofte, V De Smet, L Van Gool\\xa0- Computer Vision--ACCV 2014: 12th Asian Conference\\xa0…, 20151664회 인용 관련 학술자료 전체 11개의 버전 '}, title='A+: Adjusted anchored neighborhood regression for fast super-resolution', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Food-101–mining discriminative components with random forests': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '게시자': 'Springer International Publishing', '설명': ' In this paper we address the problem of automatically recognizing pictured dishes. To this end, we introduce a novel method to mine discriminative parts using Random Forests (rf), which allows us to mine for parts simultaneously for all classes and to share knowledge among them. To improve efficiency of mining and classification, we only consider patches that are aligned with image superpixels, which we call components. To measure the performance of our rf component mining for food recognition, we introduce a novel and challenging dataset of 101 food categories, with 101’000 images. With an average accuracy of 50.76%, our model outperforms alternative classification methods except for cnn, including svm classification on Improved Fisher Vectors and existing discriminative part-mining algorithms by 11.88% and 8.13%, respectively. On the challenging mit-Indoor dataset, our method compares\\xa0…', '저자': 'Lukas Bossard, Matthieu Guillaumin, Luc Van Gool', '전체 인용횟수': '1644회 인용201520162017201820192020202120222023285476102155171228355455', '컨퍼런스': 'Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13', '페이지': '446-461', '학술 문서': 'Food-101–mining discriminative components with random forestsL Bossard, M Guillaumin, L Van Gool\\xa0- Computer Vision–ECCV 2014: 13th European\\xa0…, 20141644회 인용 관련 학술자료 전체 10개의 버전 '}, title='Food-101–mining discriminative components with random forests', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Self-calibration and metric reconstruction inspite of varying and unknown intrinsic camera parameters': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1999/8', '게시자': 'Kluwer Academic Publishers', '권': '32', '설명': \" In this paper the theoretical and practical feasibility of self-calibration in the presence of varying intrinsic camera parameters is under investigation. The paper's main contribution is to propose a self-calibration method which efficiently deals with all kinds of constraints on the intrinsic camera parameters. Within this framework a practical method is proposed which can retrieve metric reconstruction from image sequences obtained with uncalibrated zooming/focusing cameras. The feasibility of the approach is illustrated on real and synthetic examples. Besides this a theoretical proof is given which shows that the absence of skew in the image plane is sufficient to allow for self-calibration. A counting argument is developed which—depending on the set of constraints—gives the minimum sequence length for self-calibration and a method to detect critical motion sequences is proposed.\", '저널': 'International journal of computer vision', '저자': 'Marc Pollefeys, Reinhard Koch, Luc Van Gool', '전체 인용횟수': '1642회 인용199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220233974937479108109118986888726373576754454331303525222215', '페이지': '7-25', '학술 문서': 'Self-calibration and metric reconstruction inspite of varying and unknown intrinsic camera parametersM Pollefeys, R Koch, LV Gool\\xa0- International journal of computer vision, 19991642회 인용 관련 학술자료 전체 31개의 버전 ', '호': '1'}, title='Self-calibration and metric reconstruction inspite of varying and unknown intrinsic camera parameters', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " \"You'll never walk alone: Modeling social behavior for multi-target tracking\": Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009/9/29', '게시자': 'IEEE', '설명': \"Object tracking typically relies on a dynamic model to predict the object's location from its past trajectory. In crowded scenarios a strong dynamic model is particularly important, because more accurate predictions allow for smaller search regions, which greatly simplifies data association. Traditional dynamic models predict the location for each target solely based on its own history, without taking into account the remaining scene objects. Collisions are resolved only when they happen. Such an approach ignores important aspects of human behavior: people are driven by their future destination, take into account their environment, anticipate collisions, and adjust their trajectories at an early stage in order to avoid them. In this work, we introduce a model of dynamic social behavior, inspired by models developed for crowd simulation. The model is trained with videos recorded from birds-eye view at busy locations, and\\xa0…\", '저자': 'Stefano Pellegrini, Andreas Ess, Konrad Schindler, Luc Van Gool', '전체 인용횟수': '1635회 인용201020112012201320142015201620172018201920202021202220233028587196971089079140178207227201', '컨퍼런스': '2009 IEEE 12th international conference on computer vision', '페이지': '261-268', '학술 문서': \"You'll never walk alone: Modeling social behavior for multi-target trackingS Pellegrini, A Ess, K Schindler, L Van Gool\\xa0- 2009 IEEE 12th international conference on computer\\xa0…, 20091635회 인용 관련 학술자료 전체 23개의 버전 \"}, title=\"You'll never walk alone: Modeling social behavior for multi-target tracking\", authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Procedural modeling of buildings': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2006/7/1', '게시자': 'ACM', '권': '25', '설명': 'CGA shape, a novel shape grammar for the procedural modeling of CG architecture, produces building shells with high visual quality and geometric detail. It produces extensive architectural models for computer games and movies, at low cost. Context sensitive shape rules allow the user to specify interactions between the entities of the hierarchical shape descriptions. Selected examples demonstrate solutions to previously unsolved modeling problems, especially to consistent mass modeling with volumetric shapes of arbitrary orientation. CGA shape is shown to efficiently generate massive urban models with unprecedented level of detail, with the virtual rebuilding of the archaeological site of Pompeii as a case in point.', '저널': 'ACM Transactions on Graphics (TOG)', '저자': 'Pascal Müller, Peter Wonka, Simon Haegler, Andreas Ulmer, Luc Van Gool', '전체 인용횟수': '1580회 인용200620072008200920102011201220132014201520162017201820192020202120222023144276110122123129137117909496817256637348', '페이지': '614-623', '학술 문서': 'Procedural modeling of buildingsP Müller, P Wonka, S Haegler, A Ulmer, L Van Gool\\xa0- ACM SIGGRAPH 2006 Papers, 20061575회 인용 관련 학술자료 전체 31개의 버전 Procedural Modeling of Buildings*PV Gool, P Müller, P Wonka, S Haegler, A Ulmer…\\xa0- ACMT on G.(TOG)(Ed.), ACM SIGGRAPH, 20065회 인용 관련 학술자료 ', '호': '3'}, title='Procedural modeling of buildings', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Ntire 2017 challenge on single image super-resolution: Methods and results': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'This paper reviews the first challenge on single image super-resolution (restoration of rich details in an low resolution image) with focus on proposed solutions and results. A new DIVerse 2K resolution image dataset (DIV2K) was employed. The challenge had 6 competitions divided into 2 tracks with 3 magnification factors each. Track 1 employed the standard bicubic downscaling setup, while Track 2 had unknown downscaling operators (blur kernel and decimation) but learnable through low and high res train images. Each competition had 100 registered participants and 20 teams competed in the final testing phase. They gauge the state-of-the-art in single image super-resolution.', '저자': 'Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-Hsuan Yang, Lei Zhang', '전체 인용횟수': '1488회 인용201720182019202020212022202311111182278302299295', '출처': 'Proceedings of the IEEE conference on computer vision and pattern recognition workshops', '페이지': '114-125', '학술 문서': 'Ntire 2017 challenge on single image super-resolution: Methods and resultsR Timofte, E Agustsson, L Van Gool, MH Yang…\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20171488회 인용 관련 학술자료 전체 19개의 버전 '}, title='Ntire 2017 challenge on single image super-resolution: Methods and results', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Anchored neighborhood regression for fast example-based super-resolution': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013', '설명': 'Recently there have been significant advances in image upscaling or image super-resolution based on a dictionary of low and high resolution exemplars. The running time of the methods is often ignored despite the fact that it is a critical factor for real applications. This paper proposes fast super-resolution methods while making no compromise on quality. First, we support the use of sparse learned dictionaries in combination with neighbor embedding methods. In this case, the nearest neighbors are computed using the correlation with the dictionary atoms rather than the Euclidean distance. Moreover, we show that most of the current approaches reach top performance for the right parameters. Second, we show that using global collaborative coding has considerable speed advantages, reducing the super-resolution mapping to a precomputed projective matrix. Third, we propose the anchored neighborhood regression. That is to anchor the neighborhood embedding of a low resolution patch to the nearest atom in the dictionary and to precompute the corresponding embedding matrix. These proposals are contrasted with current state-ofthe-art methods on standard images. We obtain similar or improved quality and one or two orders of magnitude speed improvements.', '저자': 'Radu Timofte, Vincent De Smet, Luc Van Gool', '전체 인용횟수': '1474회 인용20142015201620172018201920202021202220231869150179256212178151143101', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '1920-1927', '학술 문서': 'Anchored neighborhood regression for fast example-based super-resolutionR Timofte, V De Smet, L Van Gool\\xa0- Proceedings of the IEEE international conference on\\xa0…, 20131474회 인용 관련 학술자료 전체 20개의 버전 '}, title='Anchored neighborhood regression for fast example-based super-resolution', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Swinir: Image restoration using swin transformer': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021', '설명': 'Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from low-quality images (eg, downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by up to 0.14 0.45 dB, while the total number of parameters can be reduced by up to 67%.', '저자': 'Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, Radu Timofte', '전체 인용횟수': '1454회 인용20212022202323439979', '컨퍼런스': 'Proceedings of the IEEE/CVF international conference on computer vision', '페이지': '1833-1844', '학술 문서': 'Swinir: Image restoration using swin transformerJ Liang, J Cao, G Sun, K Zhang, L Van Gool, R Timofte\\xa0- Proceedings of the IEEE/CVF international conference\\xa0…, 20211454회 인용 관련 학술자료 전체 9개의 버전 '}, title='Swinir: Image restoration using swin transformer', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Visual modeling with a hand-held camera': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2004/9', '게시자': 'Kluwer Academic Publishers', '권': '59', '설명': ' In this paper a complete system to build visual models from camera images is presented. The system can deal with uncalibrated image sequences acquired with a hand-held camera. Based on tracked or matched features the relations between multiple views are computed. From this both the structure of the scene and the motion of the camera are retrieved. The ambiguity on the reconstruction is restricted from projective to metric through self-calibration. A flexible multi-view stereo matching scheme is used to obtain a dense estimation of the surface geometry. From the computed data different types of visual models are constructed. Besides the traditional geometry- and image-based approaches, a combined approach with view-dependent geometry and texture is presented. As an application fusion of real and virtual scenes is also shown.', '저널': 'International Journal of Computer Vision', '저자': 'Marc Pollefeys, Luc Van Gool, Maarten Vergauwen, Frank Verbiest, Kurt Cornelis, Jan Tops, Reinhard Koch', '전체 인용횟수': '1336회 인용20042005200620072008200920102011201220132014201520162017201820192020202120222023123877101103126119134878684846260372622211711', '페이지': '207-232', '학술 문서': 'Visual modeling with a hand-held cameraM Pollefeys, L Van Gool, M Vergauwen, F Verbiest…\\xa0- International Journal of Computer Vision, 20041336회 인용 관련 학술자료 전체 33개의 버전 '}, title='Visual modeling with a hand-held camera', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'An efficient dense and scale-invariant spatio-temporal interest point detector': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2008', '게시자': 'Springer Berlin Heidelberg', '설명': ' Over the years, several spatio-temporal interest point detectors have been proposed. While some detectors can only extract a sparse set of scale-invariant features, others allow for the detection of a larger amount of features at user-defined scales. This paper presents for the first time spatio-temporal interest points that are at the same time scale-invariant (both spatially and temporally) and densely cover the video content. Moreover, as opposed to earlier work, the features can be computed efficiently. Applying scale-space theory, we show that this can be achieved by using the determinant of the Hessian as the saliency measure. Computations are speeded-up further through the use of approximative box-filter operations on an integral video structure. A quantitative evaluation and experimental results on action recognition show the strengths of the proposed detector in terms of repeatability, accuracy and\\xa0…', '저자': 'Geert Willems, Tinne Tuytelaars, Luc Van Gool', '전체 인용횟수': '1251회 인용20082009201020112012201320142015201620172018201920202021202220236114667108139124128144118908565513616', '컨퍼런스': 'Computer Vision–ECCV 2008: 10th European Conference on Computer Vision, Marseille, France, October 12-18, 2008, Proceedings, Part II 10', '페이지': '650-663', '학술 문서': 'An efficient dense and scale-invariant spatio-temporal interest point detectorG Willems, T Tuytelaars, L Van Gool\\xa0- Computer Vision–ECCV 2008: 10th European\\xa0…, 20081251회 인용 관련 학술자료 전체 14개의 버전 '}, title='An efficient dense and scale-invariant spatio-temporal interest point detector', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Mask r-cnn': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/10/22', '게시자': 'IEEE', '설명': 'We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, eg, allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.', '저자': 'Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick', '전체 인용횟수': '30941회 인용2017201820192020202120222023159133734505005661174676728', '컨퍼런스': 'Computer Vision (ICCV), 2017 IEEE International Conference on', '페이지': '2980-2988', '학술 문서': 'Mask r-cnnK He, G Gkioxari, P Dollár, R Girshick\\xa0- Proceedings of the IEEE international conference on\\xa0…, 201730662회 인용 관련 학술자료 전체 23개의 버전 Mask R-CNN*GG HE KM, P Dollár\\xa0- 2017 IEEE International Conference on Computer\\xa0…, 2017176회 인용 관련 학술자료 Mask r-cnn. arXiv 2017*K He, G Gkioxari, P Dollár, R Girshick\\xa0- arXiv preprint arXiv:1703.06870, 2020135회 인용 관련 학술자료 CNN [J]*K He, G Gkioxari, P Dollár, R Mask\\xa0- IEEE Transactions on Pattern Analysis and Machine\\xa0…, 2020103회 인용 관련 학술자료 Mask r-cnn. arXiv*K He, G Gkioxari, P Dollár, R Girshick\\xa0- arXiv preprint arXiv:1703.06870, 201772회 인용 관련 학술자료 Mask r-cnn*G Gkioxari, P Dollár, R Girshick\\xa0- IEEE International conference on Computer Vision\\xa0…, 201754회 인용 관련 학술자료 Mask r-cnn*R Mask, K He, G Gkioxari, P Dollár, R Girshick\\xa0- Proceedings of the IEEE International Conference on\\xa0…, 20176회 인용 관련 학술자료 전체 2개의 버전 '}, title='Mask r-cnn', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Pedestrian detection: An evaluation of the state of the art': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012/4', '게시자': 'IEEE', '권': '34', '설명': 'Pedestrian detection is a key problem in computer vision, with several applications that have the potential to positively impact quality of life. In recent years, the number of approaches to detecting pedestrians in monocular images has grown steadily. However, multiple data sets and widely varying evaluation protocols are used, making direct comparisons difficult. To address these shortcomings, we perform an extensive evaluation of the state of the art in a unified framework. We make three primary contributions: 1) We put together a large, well-annotated, and realistic monocular pedestrian detection data set and study the statistics of the size, position, and occlusion patterns of pedestrians in urban scenes, 2) we propose a refined per-frame evaluation methodology that allows us to carry out probing and informative comparisons, including measuring performance in relation to scale and occlusion, and 3) we evaluate\\xa0…', '저널': 'Pattern Analysis and Machine Intelligence (PAMI), IEEE Transactions on', '저자': 'Piotr Dollar, Christian Wojek, Bernt Schiele, Pietro Perona', '전체 인용횟수': '3925회 인용20122013201420152016201720182019202020212022202388220303374408434366403360373294228', '페이지': '743-761', '학술 문서': 'Pedestrian detection: An evaluation of the state of the artP Dollar, C Wojek, B Schiele, P Perona\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20113925회 인용 관련 학술자료 전체 28개의 버전 ', '호': '4'}, title='Pedestrian detection: An evaluation of the state of the art', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Behavior recognition via sparse spatio-temporal features': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2005/10/15', '게시자': 'IEEE', '설명': 'A common trend in object recognition is to detect and leverage the use of sparse, informative feature points. The use of such features makes the problem more manageable while providing increased robustness to noise and pose variation. In this work we develop an extension of these ideas to the spatio-temporal case. For this purpose, we show that the direct 3D counterparts to commonly used 2D interest point detectors are inadequate, and we propose an alternative. Anchoring off of these interest points, we devise a recognition algorithm based on spatio-temporally windowed data. We present recognition results on a variety of datasets including both human and rodent behavior.', '저자': 'Piotr Dollár, Vincent Rabaud, Garrison Cottrell, Serge Belongie', '전체 인용횟수': '3471회 인용20052006200720082009201020112012201320142015201620172018201920202021202220239133691175218261295359361302304252185175151978551', '컨퍼런스': '2005 IEEE international workshop on visual surveillance and performance evaluation of tracking and surveillance', '페이지': '65-72', '학술 문서': 'Behavior recognition via sparse spatio-temporal featuresP Dollár, V Rabaud, G Cottrell, S Belongie\\xa0- 2005 IEEE international workshop on visual\\xa0…, 20053467회 인용 관련 학술자료 전체 18개의 버전 S. Belongie, GC, 2005. Behavior recognition via sparse spatio-temporal features*P Dollar, V Rabaud3회 인용 관련 학술자료 Abstract: Behavior Recognition via Sparse Spatio-Temporal Features*P Dollar\\xa0- Department of Computer Science and Engineering\\xa0…, 20052회 인용 관련 학술자료 '}, title='Behavior recognition via sparse spatio-temporal features', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Edge boxes: Locating object proposals from edges': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '게시자': 'Springer International Publishing', '설명': ' The use of object proposals is an effective recent approach for increasing the computational efficiency of object detection. We propose a novel method for generating object bounding box proposals using edges. Edges provide a sparse yet informative representation of an image. Our main observation is that the number of contours that are wholly contained in a bounding box is indicative of the likelihood of the box containing an object. We propose a simple box objectness score that measures the number of edges that exist in the box minus those that are members of contours that overlap the box’s boundary. Using efficient data structures, millions of candidate boxes can be evaluated in a fraction of a second, returning a ranked set of a few thousand top-scoring proposals. Using standard metrics, we show results that are significantly more accurate than the current state-of-the-art while being faster to compute\\xa0…', '저자': 'C Lawrence Zitnick, Piotr Dollár', '전체 인용횟수': '3297회 인용201420152016201720182019202020212022202313170352513496523371313262189', '컨퍼런스': 'Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13', '페이지': '391-405', '학술 문서': 'Edge boxes: Locating object proposals from edgesCL Zitnick, P Dollár\\xa0- Computer Vision–ECCV 2014: 13th European\\xa0…, 20143297회 인용 관련 학술자료 전체 14개의 버전 '}, title='Edge boxes: Locating object proposals from edges', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Fast feature pyramids for object detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/1/16', '게시자': 'IEEE', '권': '36', '설명': 'Multi-resolution image features may be approximated via extrapolation from nearby scales, rather than being computed explicitly. This fundamental insight allows us to design object detection algorithms that are as accurate, and considerably faster, than the state-of-the-art. The computational bottleneck of many modern detectors is the computation of features at every scale of a finely-sampled image pyramid. Our key insight is that one may compute finely sampled feature pyramids at a fraction of the cost, without sacrificing performance: for a broad family of features we find that features computed at octave-spaced scale intervals are sufficient to approximate features on a finely-sampled pyramid. Extrapolation is inexpensive as compared to direct feature computation. As a result, our approximation yields considerable speedups with negligible loss in detection accuracy. We modify three diverse visual recognition\\xa0…', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Piotr Dollár, Ron Appel, Serge Belongie, Pietro Perona', '전체 인용횟수': '2345회 인용201420152016201720182019202020212022202358196257383323353254200161107', '페이지': '1532-1545', '학술 문서': 'Fast feature pyramids for object detectionP Dollár, R Appel, S Belongie, P Perona\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20142345회 인용 관련 학술자료 전체 21개의 버전 ', '호': '8'}, title='Fast feature pyramids for object detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Microsoft coco captions: Data collection and evaluation server': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/4/1', '설명': 'In this paper we describe the Microsoft COCO Caption dataset and evaluation server. When completed, the dataset will contain over one and a half million captions describing over 330,000 images. For the training and validation images, five independent human generated captions will be provided. To ensure consistency in evaluation of automatic caption generation algorithms, an evaluation server is used. The evaluation server receives candidate captions and scores them using several popular metrics, including BLEU, METEOR, ROUGE and CIDEr. Instructions for using the evaluation server are provided.', '저널': 'arXiv preprint arXiv:1504.00325', '저자': 'Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, C Lawrence Zitnick', '전체 인용횟수': '2044회 인용2015201620172018201920202021202220231966125164226219333425443', '학술 문서': 'Microsoft coco captions: Data collection and evaluation serverX Chen, H Fang, TY Lin, R Vedantam, S Gupta…\\xa0- arXiv preprint arXiv:1504.00325, 20152044회 인용 관련 학술자료 전체 6개의 버전 '}, title='Microsoft coco captions: Data collection and evaluation server', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Pedestrian detection: A benchmark': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009/6/20', '게시자': 'IEEE', '설명': 'Pedestrian detection is a key problem in computer vision, with several applications including robotics, surveillance and automotive safety. Much of the progress of the past few years has been driven by the availability of challenging public datasets. To continue the rapid rate of innovation, we introduce the Caltech Pedestrian Dataset, which is two orders of magnitude larger than existing datasets. The dataset contains richly annotated video, recorded from a moving vehicle, with challenging images of low resolution and frequently occluded people. We propose improved evaluation metrics, demonstrating that commonly used per-window measures are flawed and can fail to predict performance on full images. We also benchmark several promising detection systems, providing an overview of state-of-the-art performance and a direct, unbiased comparison of existing methods. Finally, by analyzing common failure cases\\xa0…', '저자': 'Piotr Dollár, Christian Wojek, Bernt Schiele, Pietro Perona', '전체 인용횟수': '1742회 인용2009201020112012201320142015201620172018201920202021202220231771108879293108108139133138144174174125', '컨퍼런스': '2009 IEEE conference on computer vision and pattern recognition', '페이지': '304-311', '학술 문서': 'Pedestrian detection: A benchmarkP Dollár, C Wojek, B Schiele, P Perona\\xa0- 2009 IEEE conference on computer vision and pattern\\xa0…, 20091742회 인용 관련 학술자료 전체 19개의 버전 '}, title='Pedestrian detection: A benchmark', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Integral channel features': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009', '게시자': 'BMVC Press', '설명': 'We study the performance of ‘integral channel features’ for image classification tasks, focusing in particular on pedestrian detection.  The general idea behind integral channel features is that multiple registered image channels are computed using linear and non-linear transformations of the input image, and then features such as local sums, histograms,  and  Haar  features and  their  various  generalizations are  efficiently  computed using integral images.  Such features have been used in recent literature for a variety of tasks – indeed,  variations appear to have been invented independently multiple times. Although integral channel features have proven effective, little effort has been devoted to analyzing or optimizing the features themselves. In this work we present a unified view of the relevant work in this area and perform a detailed experimental evaluation.   We demonstrate that when designed properly, integral channel features not only outperform other features including histogram of oriented gradient (HOG), they also (1) naturally integrate heterogeneous sources of information, (2) have few parameters and are insensitive to exact parameter settings, (3) allow for more accurate spatial localization during detection, and (4) result in fast detectors when coupled with cascade classifiers.', '저자': 'Piotr Dollár, Zhuowen Tu, Pietro Perona, Serge Belongie', '전체 인용횟수': '1655회 인용20102011201220132014201520162017201820192020202120222023111843891702142091931891641181006639', '페이지': '91.1-91.11', '학술 문서': 'Integral channel featuresP Dollár, Z Tu, P Perona, S Belongie - 20091655회 인용 관련 학술자료 전체 21개의 버전 '}, title='Integral channel features', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'From captions to visual concepts and back': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'This paper presents a novel approach for automatically generating image descriptions: visual detectors, language models, and multimodal similarity models learnt directly from a dataset of image captions. We use multiple instance learning to train visual detectors for words that commonly occur in captions, including many different parts of speech such as nouns, verbs, and adjectives. The word detector outputs serve as conditional inputs to a maximum-entropy language model. The language model learns from a set of over 400,000 image descriptions to capture the statistics of word usage. We capture global semantics by re-ranking caption candidates using sentence-level features and a deep multimodal similarity model. Our system is state-of-the-art on the official Microsoft COCO benchmark, producing a BLEU-4 score of 29.1%. When human judges compare the system captions to ones written by other people on our held-out test set, the system captions have equal or better quality 34% of the time.', '저자': 'Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh K Srivastava, Li Deng, Piotr Dollár, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John C Platt, C Lawrence Zitnick, Geoffrey Zweig', '전체 인용횟수': '1555회 인용2014201520162017201820192020202120222023775139214216239203176130111', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '1473-1482', '학술 문서': 'From captions to visual concepts and backH Fang, S Gupta, F Iandola, RK Srivastava, L Deng…\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20151555회 인용 관련 학술자료 전체 25개의 버전 '}, title='From captions to visual concepts and back', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Structured forests for fast edge detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013', '설명': 'Edge detection is a critical component of many vision systems, including object detectors and image segmentation algorithms. Patches of edges exhibit well-known forms of local structure, such as straight lines or T-junctions. In this paper we take advantage of the structure present in local image patches to learn both an accurate and computationally efficient edge detector. We formulate the problem of predicting local edge masks in a structured learning framework applied to random decision forests. Our novel approach to learning decision trees robustly maps the structured labels to a discrete space on which standard information gain measures may be evaluated. The result is an approach that obtains realtime performance that is orders of magnitude faster than many competing state-of-the-art approaches, while also achieving state-of-the-art edge detection results on the BSDS500 Segmentation dataset and NYU Depth dataset. Finally, we show the potential of our approach as a general purpose edge detector by showing our learned edge models generalize well across datasets.', '저자': 'Piotr Dollár, C Lawrence Zitnick', '전체 인용횟수': '1163회 인용20132014201520162017201820192020202120222023362168194174131118110735539', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '1841-1848', '학술 문서': 'Structured forests for fast edge detectionP Dollár, CL Zitnick\\xa0- Proceedings of the IEEE international conference on\\xa0…, 20131163회 인용 관련 학술자료 전체 28개의 버전 '}, title='Structured forests for fast edge detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Human-level control through deep reinforcement learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/2', '게시자': 'Nature Publishing Group', '권': '518', '설명': 'The theory of reinforcement learning provides a normative account 1, deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems 4, 5, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms 3. While\\xa0…', '저널': 'nature', '저자': 'Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, Demis Hassabis', '전체 인용횟수': '26771회 인용2015201620172018201920202021202220231786911327259136834381477748484069', '페이지': '529-533', '학술 문서': 'Human-level control through deep reinforcement learningV Mnih, K Kavukcuoglu, D Silver, AA Rusu, J Veness…\\xa0- nature, 201526771회 인용 관련 학술자료 전체 59개의 버전 ', '호': '7540'}, title='Human-level control through deep reinforcement learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Mastering the game of Go with deep neural networks and tree search': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/1', '게시자': 'Nature Publishing Group', '권': '529', '설명': 'The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and\\xa0…', '저널': 'nature', '저자': 'David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, Demis Hassabis', '전체 인용횟수': '17214회 인용201620172018201920202021202220237531482213223832565272327442224', '페이지': '484-489', '학술 문서': 'Mastering the game of Go with deep neural networks and tree searchD Silver, A Huang, CJ Maddison, A Guez, L Sifre…\\xa0- nature, 201617214회 인용 관련 학술자료 전체 97개의 버전 ', '호': '7587'}, title='Mastering the game of Go with deep neural networks and tree search', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Highly accurate protein structure prediction with AlphaFold': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021/8', '게시자': 'Nature Publishing Group', '권': '596', '설명': 'Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort 1, 2, 3, 4, the structures of around 100,000 unique proteins have been determined 5, but this represents a small fraction of the billions of known protein sequences 6, 7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years 9. Despite recent progress 10, 11, 12, 13, 14, existing methods fall far short of atomic accuracy\\xa0…', '저널': 'Nature', '저자': 'John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon AA Kohl, Andrew J Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W Senior, Koray Kavukcuoglu, Pushmeet Kohli, Demis Hassabis', '전체 인용횟수': '16619회 인용20212022202388064359148', '페이지': '583-589', '학술 문서': 'Highly accurate protein structure prediction with AlphaFoldJ Jumper, R Evans, A Pritzel, T Green, M Figurnov…\\xa0- Nature, 202116619회 인용 관련 학술자료 전체 20개의 버전 ', '호': '7873'}, title='Highly accurate protein structure prediction with AlphaFold', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Playing atari with deep reinforcement learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/12/19', '설명': 'We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.', '저널': 'arXiv preprint arXiv:1312.5602', '저자': 'Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller', '전체 인용횟수': '13388회 인용201420152016201720182019202020212022202354138289558113416102008241627822242', '학술 문서': 'Playing atari with deep reinforcement learningV Mnih, K Kavukcuoglu, D Silver, A Graves…\\xa0- arXiv preprint arXiv:1312.5602, 201313388회 인용 관련 학술자료 전체 42개의 버전 '}, title='Playing atari with deep reinforcement learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Asynchronous methods for deep reinforcement learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/6/11', '게시자': 'PMLR', '설명': 'We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.', '저자': 'Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu', '전체 인용횟수': '9894회 인용201620172018201920202021202220239938094813761616177719181703', '컨퍼런스': 'International conference on machine learning', '페이지': '1928-1937', '학술 문서': 'Asynchronous methods for deep reinforcement learningV Mnih, AP Badia, M Mirza, A Graves, T Lillicrap…\\xa0- International conference on machine learning, 20169894회 인용 관련 학술자료 전체 25개의 버전 '}, title='Asynchronous methods for deep reinforcement learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Natural language processing (almost) from scratch': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011', '권': '12', '설명': 'We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.', '저널': 'Journal of machine learning research', '저자': 'Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, Pavel Kuksa', '전체 인용횟수': '9716회 인용2012201320142015201620172018201920202021202220233812928659189510961410135913081045873590', '페이지': '2493− 2537', '학술 문서': 'Natural language processing (almost) from scratchR Collobert, J Weston, L Bottou, M Karlen…\\xa0- Journal of machine learning research, 20119716회 인용 관련 학술자료 전체 43개의 버전 ', '호': 'ARTICLE'}, title='Natural language processing (almost) from scratch', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Matching networks for one shot learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '권': '29', '설명': 'Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 82.2% to 87.8% and from 88% accuracy to 95% accuracy on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.', '저널': 'Advances in neural information processing systems', '저자': 'Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra', '전체 인용횟수': '6971회 인용20162017201820192020202120222023181283316221117146916881556', '학술 문서': 'Matching networks for one shot learningO Vinyals, C Blundell, T Lillicrap, D Wierstra\\xa0- Advances in neural information processing systems, 20166971회 인용 관련 학술자료 전체 14개의 버전 '}, title='Matching networks for one shot learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Wavenet: A generative model for raw audio': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/9/12', '설명': 'This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.', '저널': 'arXiv preprint arXiv:1609.03499', '저자': 'Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu', '전체 인용횟수': '6239회 인용2016201720182019202020212022202327263562920103012701161943', '학술 문서': 'Wavenet: A generative model for raw audioA Oord, S Dieleman, H Zen, K Simonyan, O Vinyals…\\xa0- arXiv preprint arXiv:1609.03499, 20166053회 인용 관련 학술자료 전체 13개의 버전 Wavenet: A generative model for raw audio. arXiv 2016AVD Oord, S Dieleman, H Zen, K Simonyan, O Vinyals…\\xa0- arXiv preprint arXiv:1609.03499, 2016133회 인용 관련 학술자료 Wavenet: A generative model for raw audio*S Dieleman, H Zen, K Simonyan, O Vinyals, A Graves…\\xa0- arXiv preprint arXiv:1609.03499, 2016111회 인용 관련 학술자료 Wavenet: a generative model for raw audio (2016)*A van den Oord, S Dieleman, H Zen, K Simonyan…\\xa0- arXiv preprint arXiv:1609.03499, 201684회 인용 관련 학술자료 A generative model for raw audioA Oord, S Dieleman, H Zen, K Simonyan, O Vinyals…\\xa0- arXiv preprint arXiv:1609.03499, 201612회 인용 관련 학술자료 '}, title='Wavenet: A generative model for raw audio', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Bootstrap your own latent-a new approach to self-supervised learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020', '권': '33', '설명': 'We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods intrinsically rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches 74.3% top-1 classification accuracy on ImageNet using the standard linear evaluation protocol with a standard ResNet-50 architecture and 79.6% with a larger ResNet. We also show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks.', '저널': 'Advances in neural information processing systems', '저자': 'Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Remi Munos, Michal Valko', '전체 인용횟수': '4821회 인용20202021202220238276118392107', '페이지': '21271-21284', '학술 문서': 'Bootstrap your own latent-a new approach to self-supervised learningJB Grill, F Strub, F Altché, C Tallec, P Richemond…\\xa0- Advances in neural information processing systems, 20204821회 인용 관련 학술자료 전체 19개의 버전 '}, title='Bootstrap your own latent-a new approach to self-supervised learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Recurrent models of visual attention': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '권': '27', '설명': 'Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.', '저널': 'Advances in neural information processing systems', '저자': 'Volodymyr Mnih, Nicolas Heess, Alex Graves', '전체 인용횟수': '4256회 인용20152016201720182019202020212022202354189289419579665667697623', '학술 문서': 'Recurrent models of visual attentionV Mnih, N Heess, A Graves\\xa0- Advances in neural information processing systems, 20144256회 인용 관련 학술자료 전체 13개의 버전 '}, title='Recurrent models of visual attention', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Grandmaster level in StarCraft II using multi-agent reinforcement learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019/11/14', '게시자': 'Nature Publishing Group UK', '권': '575', '설명': 'Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions–, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement\\xa0…', '저널': 'Nature', '저자': 'Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P Agapiou, Max Jaderberg, Alexander S Vezhnevets, Rémi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James Molloy, Tom L Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, David Silver', '전체 인용횟수': '3604회 인용20192020202120222023395368891134973', '페이지': '350-354', '학술 문서': 'Grandmaster level in StarCraft II using multi-agent reinforcement learningO Vinyals, I Babuschkin, WM Czarnecki, M Mathieu…\\xa0- Nature, 20193604회 인용 관련 학술자료 전체 13개의 버전 ', '호': '7782'}, title='Grandmaster level in StarCraft II using multi-agent reinforcement learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Weight uncertainty in neural network': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/6/1', '게시자': 'PMLR', '설명': 'We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.', '저자': 'Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra', '전체 인용횟수': '3456회 인용201520162017201820192020202120222023206299226327502748745701', '컨퍼런스': 'International conference on machine learning', '페이지': '1613-1622', '학술 문서': 'Weight uncertainty in neural networkC Blundell, J Cornebise, K Kavukcuoglu, D Wierstra\\xa0- International conference on machine learning, 20153456회 인용 관련 학술자료 전체 7개의 버전 '}, title='Weight uncertainty in neural network', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Convolutional networks and applications in vision': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/5/30', '게시자': 'IEEE', '설명': 'Intelligent tasks, such as visual perception, auditory perception, and language understanding require the construction of good internal representations of the world (or \"features\")? which must be invariant to irrelevant variations of the input while, preserving relevant information. A major question for Machine Learning is how to learn such good features automatically. Convolutional Networks (ConvNets) are a biologically-inspired trainable architecture that can learn invariant features. Each stage in a ConvNets is composed of a filter bank, some nonlinearities, and feature pooling layers. With multiple stages, a ConvNet can learn multi-level hierarchies of features. While ConvNets have been successfully deployed in many commercial applications from OCR to video surveillance, they require large amounts of labeled training samples. We describe new unsupervised learning algorithms, and new non-linear stages that\\xa0…', '저자': 'Yann LeCun, Koray Kavukcuoglu, Clément Farabet', '전체 인용횟수': '2795회 인용201120122013201420152016201720182019202020212022202318324579122206301326348360344336236', '컨퍼런스': 'Proceedings of 2010 IEEE international symposium on circuits and systems', '페이지': '253-256', '학술 문서': 'Convolutional networks and applications in visionY LeCun, K Kavukcuoglu, C Farabet\\xa0- Proceedings of 2010 IEEE international symposium on\\xa0…, 20102795회 인용 관련 학술자료 전체 28개의 버전 '}, title='Convolutional networks and applications in vision', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Neural discrete representation learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '권': '30', '설명': \"Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of``posterior collapse''-—where the latents are ignored when they are paired with a powerful autoregressive decoder-—typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.\", '저널': 'Advances in neural information processing systems', '저자': 'Aaron Van Den Oord, Oriol Vinyals', '전체 인용횟수': '2763회 인용201820192020202120222023641473184157501049', '학술 문서': 'Neural discrete representation learningA Van Den Oord, O Vinyals\\xa0- Advances in neural information processing systems, 20172763회 인용 관련 학술자료 전체 12개의 버전 '}, title='Neural discrete representation learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Improved protein structure prediction using potentials from deep learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020/1/30', '게시자': 'Nature Publishing Group UK', '권': '577', '설명': 'Protein structure prediction can be used to determine the three-dimensional shape of a protein from its amino acid sequence. This problem is of fundamental importance as the structure of a protein largely determines its function; however, protein structures can be difficult to determine experimentally. Considerable progress has recently been made by leveraging genetic information. It is possible to infer which amino acid residues are in contact by analysing covariation in homologous sequences, which aids in the prediction of protein structures. Here we show that we can train a neural network to make accurate predictions of the distances between pairs of residues, which convey more information about the structure than contact predictions. Using this information, we construct a potential of mean force that can accurately describe the shape of a protein. We find that the resulting potential can be optimized by a simple\\xa0…', '저널': 'Nature', '저자': 'Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Žídek, Alexander WR Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David T Jones, David Silver, Koray Kavukcuoglu, Demis Hassabis', '전체 인용횟수': '2705회 인용2020202120222023271937845611', '페이지': '706-710', '학술 문서': 'Improved protein structure prediction using potentials from deep learningAW Senior, R Evans, J Jumper, J Kirkpatrick, L Sifre…\\xa0- Nature, 20202695회 인용 관련 학술자료 전체 13개의 버전 Augustin ˇZıdek, Alexander WR Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David T. Jones, David Silver, Koray Kavukcuoglu, and Demis Hassabis. Improved protein structure prediction using potentials from deep learning*AW Senior, R Evans, J Jumper, J Kirkpatrick, L Sifre…\\xa0- Nature, 202016회 인용 관련 학술자료 ', '호': '7792'}, title='Improved protein structure prediction using potentials from deep learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Pixel recurrent neural networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/6/11', '게시자': 'PMLR', '설명': 'Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.', '저자': 'Aäron Van Den Oord, Nal Kalchbrenner, Koray Kavukcuoglu', '전체 인용횟수': '2480회 인용2016201720182019202020212022202362189304373408400406328', '컨퍼런스': 'International conference on machine learning', '페이지': '1747-1756', '학술 문서': 'Pixel recurrent neural networksA Van Den Oord, N Kalchbrenner, K Kavukcuoglu\\xa0- International conference on machine learning, 20162480회 인용 관련 학술자료 전체 10개의 버전 '}, title='Pixel recurrent neural networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Conditional image generation with pixelcnn decoders': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '권': '29', '설명': 'This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.', '저널': 'Advances in neural information processing systems', '저자': 'Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves', '전체 인용횟수': '2403회 인용2016201720182019202020212022202321152261353409426433322', '학술 문서': 'Conditional image generation with pixelcnn decodersA Van den Oord, N Kalchbrenner, L Espeholt…\\xa0- Advances in neural information processing systems, 20162403회 인용 관련 학술자료 전체 12개의 버전 '}, title='Conditional image generation with pixelcnn decoders', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Progressive neural networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/6/15', '설명': 'Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.', '저널': 'arXiv preprint arXiv:1606.04671', '저자': 'Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, Raia Hadsell', '전체 인용횟수': '2399회 인용2016201720182019202020212022202316102180250337433528541', '학술 문서': 'Progressive neural networksAA Rusu, NC Rabinowitz, G Desjardins, H Soyer…\\xa0- arXiv preprint arXiv:1606.04671, 20162399회 인용 관련 학술자료 전체 5개의 버전 '}, title='Progressive neural networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The ATLAS simulation infrastructure': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/12', '게시자': 'Springer-Verlag', '권': '70', '설명': ' The simulation software for the ATLAS Experiment at the Large Hadron Collider is being used for large-scale production of events on the LHC Computing Grid. This simulation requires many components, from the generators that simulate particle collisions, through packages simulating the response of the various detectors and triggers. All of these components come together under the ATLAS simulation infrastructure. In this paper, that infrastructure is discussed, including that supporting the detector description, interfacing the event generation, and combining the GEANT4 simulation of the response of the individual detectors. Also described are the tools allowing the software validation, performance testing, and the validation of the simulated output against known physics processes.', '저널': 'The European Physical Journal C', '저자': 'Georges Aad, B Abbott, J Abdallah, AA Abdelalim, Abdelmalek Abdesselam, B Abi, M Abolins, H Abramowicz, H Abreu, BS Acharya, DL Adams, TN Addy, J Adelman, C Adorisio, P Adragna, T Adye, S Aefsky, JA Aguilar-Saavedra, M Aharrouche, SP Ahlen, F Ahles, A Ahmad, H Ahmed, M Ahsan, G Aielli, T Akdogan, TPA Åkesson, G Akimoto, AV Akimov, A Aktas, MS Alam, MA Alam, S Albrand, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, M Aliev, G Alimonti, J Alison, M Aliyev, PP Allport, SE Allwood-Spiers, J Almond, A Aloisio, R Alon, Alejandro Alonso, MG Alviggi, K Amako, C Amelung, A Amorim, G Amoros, N Amram, C Anastopoulos, T Andeen, CF Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, A Angerami, F Anghinolfi, N Anjos, A Annovi, A Antonaki, M Antonelli, S Antonelli, J Antos, B Antunovic, F Anulli, S Aoun, G Arabidze, I Aracena, Y Arai, ATH Arce, JP Archambault, S Arfaoui, J-F Arguin, T Argyropoulos, M Arik, AJ Armbruster, O Arnaez, C Arnault, A Artamonov, D Arutinov, M Asai, S Asai, R Asfandiyarov, S Ask, B Åsman, D Asner, L Asquith, K Assamagan, A Astbury, A Astvatsatourov, G Atoian, B Auerbach, K Augsten, M Aurousseau, N Austin, G Avolio, R Avramidou, D Axen, C Ay, G Azuelos, Y Azuma, MA Baak, AM Bach, H Bachacou, K Bachas, M Backes, E Badescu, P Bagnaia, Y Bai, T Bain, JT Baines, OK Baker, MD Baker, S Baker, F Baltasar Dos Santos Pedrosa, E Banas, P Banerjee, S Banerjee, D Banfi, A Bangert, V Bansal, SP Baranov, S Baranov, A Barashkou, T Barber, EL Barberio, D Barberis, M Barbero, DY Bardin, T Barillari, M Barisonzi, T Barklow, N Barlow, BM Barnett, RM Barnett, A Baroncelli, AJ Barr, F Barreiro, J Barreiro Guimaraes da Costa, P Barrillon, R Bartoldus, D Bartsch', '전체 인용횟수': '8481회 인용20102011201220132014201520162017201820192020202120222023233517295806459231005929952684482427326406', '페이지': '823-874', '학술 문서': 'The ATLAS simulation infrastructureATLAS Collaboration atlas. secretariat@ cern. ch…\\xa0- The European Physical Journal C, 20108481회 인용 관련 학술자료 전체 79개의 버전 The ATLAS simulation infrastructure*ATLAS Collaboration - 2010관련 학술자료 ', '호': '3'}, title='The ATLAS simulation infrastructure', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Luminosity determination in pp collisions at  TeV using the ATLAS detector at the LHC': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011/4', '게시자': 'Springer-Verlag', '권': '71', '설명': ' Measurements of luminosity obtained using the ATLAS detector during early running of the Large Hadron Collider (LHC) at \\xa0TeV are presented. The luminosity is independently determined using several detectors and multiple algorithms, each having different acceptances, systematic uncertainties and sensitivity to background. The ratios of the luminosities obtained from these methods are monitored as a function of time and of μ, the average number of inelastic interactions per bunch crossing. Residual time- and μ-dependence between the methods is less than 2% for 0<μ<2.5. Absolute luminosity calibrations, performed using beam separation scans, have a common systematic uncertainty of ±11%, dominated by the measurement of the LHC beam currents. After calibration, the luminosities obtained from the different methods differ by at most ±2%. The visible cross sections measured using the beam\\xa0…', '저널': 'The European Physical Journal C', '저자': 'Georges Aad, Brad Abbott, Jalal Abdallah, AA Abdelalim, Abdelmalek Abdesselam, B Abi, M Abolins, H Abramowicz, H Abreu, E Acerbi, BS Acharya, M Ackers, DL Adams, TN Addy, J Adelman, M Aderholz, S Adomeit, P Adragna, T Adye, S Aefsky, JA Aguilar-Saavedra, M Aharrouche, SP Ahlen, F Ahles, A Ahmad, H Ahmed, M Ahsan, G Aielli, T Akdogan, TPA Åkesson, G Akimoto, AV Akimov, MS Alam, MA Alam, S Albrand, M Aleksa, IN Aleksandrov, M Aleppo, F Alessandria, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, M Aliev, G Alimonti, J Alison, M Aliyev, PP Allport, SE Allwood-Spiers, J Almond, A Aloisio, R Alon, A Alonso, J Alonso, MG Alviggi, K Amako, P Amaral, C Amelung, VV Ammosov, A Amorim, G Amoros, N Amram, C Anastopoulos, T Andeen, CF Anders, KJ Anderson, A Andreazza, V Andrei, M-L Andrieux, XS Anduaga, A Angerami, F Anghinolfi, N Anjos, A Annovi, A Antonaki, M Antonelli, S Antonelli, J Antos, F Anulli, S Aoun, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, ATH Arce, JP Archambault, S Arfaoui, J-F Arguin, E Arik, M Arik, AJ Armbruster, KE Arms, SR Armstrong, O Arnaez, C Arnault, A Artamonov, G Artoni, D Arutinov, S Asai, R Asfandiyarov, S Ask, B Åsman, L Asquith, K Assamagan, A Astbury, A Astvatsatourov, G Atoian, B Aubert, B Auerbach, E Auge, K Augsten, M Aurousseau, N Austin, R Avramidou, D Axen, C Ay, G Azuelos, Y Azuma, MA Baak, G Baccaglioni, C Bacci, AM Bach, H Bachacou, K Bachas, G Bachy, M Backes, E Badescu, P Bagnaia, S Bahinipati, Y Bai, DC Bailey, T Bain, JT Baines, OK Baker, S Baker, F Baltasar Dos Santos Pedrosa, E Banas, P Banerjee, Sw Banerjee, D Banfi, A Bangert, V Bansal, HS Bansil, L Barak, SP Baranov, A Barashkou, A Barbaro Galtieri, T Barber', '전체 인용횟수': '5012회 인용201120122013201420152016201720182019202020212022202319365937410794195613713540451389300369', '페이지': '1-37', '학술 문서': 'Luminosity determination in pp collisions at s s= 8 TeV using the ATLAS detector at the LHC*M Aaboud, G Aad, B Abbott, J Abdallah, O Abdinov…\\xa0- The European Physical Journal C, 20163001회 인용 관련 학술자료 전체 84개의 버전 Luminosity determination in pp collisions at s=7 TeV using the ATLAS detector at the LHCATLAS Collaboration atlas. secretariat@ cern. ch…\\xa0- The European Physical Journal C, 20112367회 인용 관련 학술자료 전체 81개의 버전 Luminosity determination in pp collisions at root s= 8 TeV using the ATLAS detector at the LHC*M Aaboud, G Aad, B Abbott, J Abdallah…\\xa0- European Physical Journal C, 20162회 인용 G. Aad et al.,“Luminosity Determination in pp Collisions at√(s)= 7 TeV Using the ATLAS Detector at the LHC”*ATLAS Collaboration\\xa0- European Journal of Physics (submitted)2회 인용 관련 학술자료 Luminosity determination in pp collisions at√ s= 7 TeV using the ATLAS detector at the LHCMS Alam, J Ernst, V Rojo, H Ahmed, S Bahinipati…\\xa0- The European Physical Journal C-Particles and Fields, 20111회 인용 관련 학술자료 전체 2개의 버전 Luminosity determination in pp collisions at $$\\\\sqrt {s}= 13$$ TeV using the ATLAS detector at the LHC*G Aad, B Abbott, K Abeling, SH Abidi, A Aboulhorma…\\xa0- The European Physical Journal C, 2023전체 13개의 버전 Luminosity determination in pp collisions at√ s= 7 TeV using the ATLAS detector at the LHC*XS Anduaga, MT Dova, FG Monticelli, MF Tripiana…\\xa0- The European Physical Journal C, 2011관련 학술자료 전체 4개의 버전 Luminosity Determination in pp Collisions at sqrt (s)= 7 TeV Using the ATLAS Detector at the LHCV Mitsou, A Ferrer Soria, JA Valls Ferrer…\\xa0- European Physical Journal C, 2011, vol. 71, num. 4\\xa0…, 2011관련 학술자료 전체 5개의 버전 ', '호': '4'}, title='Luminosity determination in pp collisions at  TeV using the ATLAS detector at the LHC', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Performance of the ATLAS trigger system in 2015': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/5/1', '게시자': 'Springer Berlin Heidelberg', '권': '77', '설명': 'During 2015 the ATLAS experiment recorded of proton–proton collision data at a centre-of-mass energy of. The ATLAS trigger system is a crucial component of the experiment, responsible for selecting events of interest at a recording rate of approximately 1 kHz from up to 40 MHz of collisions. This paper presents a short overview of the changes to the trigger and data acquisition systems during the first long shutdown of the LHC and shows the performance of the trigger system and its components based on the 2015 proton–proton collision data.', '저널': 'The European Physical Journal C', '저자': 'Morad Aaboud, Georges Aad, Brad Abbott, Jalal Abdallah, B Abeloos, R Aben, OS AbouZeid, NL Abraham, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, S Adachi, L Adamczyk, DL Adams, J Adelman, S Adomeit, T Adye, AA Affolder, T Agatonovic-Jovin, JA Aguilar-Saavedra, SP Ahlen, F Ahmadov, G Aielli, H Akerstedt, TPA Åkesson, AV Akimov, GL Alberghi, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, T Alexopoulos, M Alhroob, B Ali, M Aliev, G Alimonti, J Alison, SP Alkire, BMM Allbrooke, BW Allen, PP Allport, A Aloisio, A Alonso, F Alonso, C Alpigiani, AA Alshehri, M Alstaty, B Alvarez Gonzalez, D Álvarez Piqueras, MG Alviggi, BT Amadio, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, JK Anders, KJ Anderson, A Andreazza, V Andrei, S Angelidakis, I Angelozzi, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, C Antel, M Antonelli, A Antonov, DJ Antrim, F Anulli, M Aoki, L Aperio Bella, G Arabidze, Y Arai, JP Araque, ATH Arce, FA Arduh, J-F Arguin, S Argyropoulos, M Arik, AJ Armbruster, LJ Armitage, O Arnaez, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Artz, S Asai, N Asbah, A Ashkenazi, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, K Augsten, G Avolio, B Axen, MK Ayoub, G Azuelos, MA Baak, AE Baas, MJ Baca, H Bachacou, K Bachas, M Backes, M Backhaus, P Bagiacchi, P Bagnaia, Y Bai, JT Baines, M Bajic, OK Baker, EM Baldin, P Balek, T Balestri, F Balli, WK Balunas, E Banas, Sw Banerjee, AAE Bannoura, L Barak, EL Barberio, D Barberis, M Barbero, T Barillari, M-S Barisits, T Barklow, N Barlow, SL Barnes, BM Barnett, RM Barnett', '전체 인용횟수': '4401회 인용20112012201320142015201620172018201920202021202220231296107137235361517837674488348265317', '페이지': '317', '학술 문서': 'Performance of the ATLAS trigger system in 2015M Aaboud, G Aad, B Abbott, J Abdallah, B Abeloos…\\xa0- The European Physical Journal C, 20174392회 인용 관련 학술자료 전체 120개의 버전 Performance of the ATLAS Trigger System in 2015M Aaboud, G Aad, B Abbott, J Abdallah, B Abcloos…\\xa0- …\\xa0PHYSICAL JOURNAL. C, PARTICLES AND FIELDS, 201711회 인용 관련 학술자료 Performance of the ATLAS trigger system in 2015*A Collaboration\\xa0- Eur. Phys. J. C, 2017관련 학술자료 전체 3개의 버전 ', '호': '5'}, title='Performance of the ATLAS trigger system in 2015', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Luminosity determination in pp collisions at                                           s = 8 TeV using the ATLAS detector at the LHC': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/12', '게시자': 'Springer Berlin Heidelberg', '권': '76', '설명': ' The luminosity determination for the ATLAS detector at the LHC during pp collisions at \\xa08\\xa0TeV in 2012 is presented. The evaluation of the luminosity scale is performed using several luminometers, and comparisons between these luminosity detectors are made to assess the accuracy, consistency and long-term stability of the results. A luminosity uncertainty of  is obtained for the  of pp collision data delivered to ATLAS at \\xa08\\xa0TeV in 2012.', '저널': 'The European Physical Journal C', '저자': 'Morad Aaboud, G Aad, B Abbott, J Abdallah, B Abeloos, R Aben, OS AbouZeid, NL Abraham, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, J Adelman, S Adomeit, T Adye, AA Affolder, T Agatonovic-Jovin, JA Aguilar-Saavedra, SP Ahlen, F Ahmadov, G Aielli, H Akerstedt, TPA Åkesson, AV Akimov, GL Alberghi, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, T Alexopoulos, M Alhroob, M Aliev, G Alimonti, J Alison, SP Alkire, BMM Allbrooke, BW Allen, PP Allport, A Aloisio, A Alonso, F Alonso, C Alpigiani, M Alstaty, B Alvarez Gonzalez, D Álvarez Piqueras, MG Alviggi, BT Amadio, K Amako, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, JK Anders, KJ Anderson, A Andreazza, V Andrei, S Angelidakis, I Angelozzi, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, M Antonelli, A Antonov, F Anulli, M Aoki, L Aperio Bella, G Arabidze, Y Arai, JP Araque, ATH Arce, FA Arduh, J-F Arguin, S Argyropoulos, M Arik, AJ Armbruster, LJ Armitage, O Arnaez, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Artz, S Asai, N Asbah, A Ashkenazi, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, K Augsten, G Avolio, B Axen, MK Ayoub, G Azuelos, MA Baak, AE Baas, MJ Baca, H Bachacou, K Bachas, M Backes, M Backhaus, P Bagiacchi, P Bagnaia, Y Bai, JT Baines, OK Baker, EM Baldin, P Balek, T Balestri, F Balli, WK Balunas, E Banas, Sw Banerjee, AAE Bannoura, L Barak, EL Barberio, D Barberis, M Barbero, T Barillari, T Barklow, N Barlow, SL Barnes, BM Barnett, RM Barnett, Z Barnovska, A Baroncelli, G Barone, AJ Barr, L Barranco Navarro', '전체 인용횟수': '3003회 인용20162017201820192020202120222023102597693504286259213265', '페이지': '1-45', '학술 문서': 'Luminosity determination in pp collisions at s s= 8 TeV using the ATLAS detector at the LHCM Aaboud, G Aad, B Abbott, J Abdallah, O Abdinov…\\xa0- The European Physical Journal C, 20163001회 인용 관련 학술자료 전체 84개의 버전 Luminosity determination in pp collisions at root s= 8 TeV using the ATLAS detector at the LHCM Aaboud, G Aad, B Abbott, J Abdallah…\\xa0- European Physical Journal C, 20162회 인용 ', '호': '12'}, title='Luminosity determination in pp collisions at                                           s = 8 TeV using the ATLAS detector at the LHC', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Topological cell clustering in the ATLAS calorimeters and its performance in LHC Run 1': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/7/24', '게시자': 'SpringerOpen', '설명': 'The reconstruction of the signal from hadrons and jets emerging from the proton–proton collisions at the Large Hadron Collider (LHC) and entering the ATLAS calorimeters is based on a three-dimensional topological clustering of individual calorimeter cell signals. The cluster formation follows cell signal-significance patterns generated by electromagnetic and hadronic showers. In this, the clustering algorithm implicitly performs a topological noise suppression by removing cells with insignificant signals which are not in close proximity to cells with significant signals. The resulting topological cell clusters have shape and location information, which is exploited to apply a local energy calibration and corrections depending on the nature of the cluster. Topological cell clustering is established as a wellperforming calorimeter signal definition for jet and missing transverse momentum reconstruction in ATLAS.', '저자': 'Atlas Collaboration', '전체 인용횟수': '2595회 인용2016201720182019202020212022202398260637474407309188211', '학술 문서': 'Topological cell clustering in the ATLAS calorimeters and its performance in LHC Run 1Atlas Collaboration - 20172424회 인용 관련 학술자료 전체 62개의 버전 Topological cell clustering in the ATLAS calorimeters and its performance in LHC Run 1G Aad, B Abbott, J Abdallah, R Aben, M Abolins…\\xa0- The European Physical Journal C, 2017201회 인용 관련 학술자료 전체 61개의 버전 Topological cell clustering in the ATLAS calorimeters and its performance in LHC RunA Collaboration\\xa0- Eur. Phys. J. C, 20172회 인용 관련 학술자료 '}, title='Topological cell clustering in the ATLAS calorimeters and its performance in LHC Run 1', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Combined Measurement of the Higgs Boson Mass in  Collisions at  and 8\\xa0TeV with the ATLAS and CMS Experiments': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/5/14', '게시자': 'American Physical Society', '권': '114', '설명': 'A measurement of the Higgs boson mass is presented based on the combined data samples of the ATLAS and CMS experiments at the CERN LHC in the  and  decay channels. The results are obtained from a simultaneous fit to the reconstructed invariant mass peaks in the two channels and for the two experiments. The measured masses from the individual channels and the two experiments are found to be consistent among themselves. The combined measured mass of the Higgs boson is .', '저널': 'Physical review letters', '저자': 'Georges Aad, Brad Abbott, Jalal Abdallah, R Aben, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, J Adelman, S Adomeit, T Adye, AA Affolder, T Agatonovic-Jovin, JA Aguilar-Saavedra, SP Ahlen, F Ahmadov, G Aielli, H Akerstedt, TPA Åkesson, G Akimoto, AV Akimov, GL Alberghi, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, T Alexopoulos, M Alhroob, G Alimonti, L Alio, J Alison, SP Alkire, BMM Allbrooke, PP Allport, A Aloisio, A Alonso, F Alonso, C Alpigiani, A Altheimer, B Alvarez Gonzalez, D Álvarez Piqueras, MG Alviggi, BT Amadio, K Amako, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, JK Anders, KJ Anderson, A Andreazza, V Andrei, S Angelidakis, I Angelozzi, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, G Arabidze, Y Arai, JP Araque, ATH Arce, FA Arduh, JF Arguin, S Argyropoulos, M Arik, AJ Armbruster, O Arnaez, V Arnal, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, A Ashkenazi, B Åsman, Lily Asquith, Ketevi Assamagan, Robert Astalos, Markus Atkinson, Naim Bora Atlay, Benjamin Auerbach, Kamil Augsten, Mathieu Aurousseau, Giuseppe Avolio, Bradley Axen, Mohamad Kassem Ayoub, Georges Azuelos, MA Baak, AE Baas, Cesare Bacci, Henri Bachacou, Konstantinos Bachas, Moritz Backes, Malte Backhaus, Elisabeta Badescu, Paolo Bagiacchi, Paolo Bagnaia, Yu Bai, Travis Bain, JT Baines, Oliver Keith Baker, Petr Balek, Thomas Balestri, Fabrice Balli, Elzbieta Banas, Sw Banerjee, Arwa AE Bannoura, Hardeep Singh Bansil, Liron Barak, SP Baranov, Elisabetta Luigia Barberio, Dario Barberis, Marlon Barbero, Teresa Barillari, Marcello Barisonzi, Timothy Barklow, Nick Barlow, Sarah Louise Barnes, BM Barnett, RM Barnett, Zuzana Barnovska', '전체 인용횟수': '2572회 인용20152016201720182019202020212022202323047247246730321316712985', '페이지': '191803', '학술 문서': 'Combined Measurement of the Higgs Boson Mass in $ pp $ Collisions at $\\\\sqrt {s}= 7$ and 8 TeV with the ATLAS and CMS ExperimentsCMS Collaborations\\xa0- arXiv preprint arXiv:1503.07589, 20151164회 인용 관련 학술자료 전체 50개의 버전 Combined Measurement of the Higgs Boson Mass in p p Collisions at s= 7 and 8 TeV with the ATLAS and CMS ExperimentsG Aad, B Abbott, J Abdallah, R Aben, M Abolins…\\xa0- Physical review letters, 2015939회 인용 관련 학술자료 전체 157개의 버전 ATLAS and CMS collaborations*G Aad\\xa0- Phys. Rev. Lett, 2015770회 인용 관련 학술자료 ', '호': '19'}, title='Combined Measurement of the Higgs Boson Mass in  Collisions at  and 8\\xa0TeV with the ATLAS and CMS Experiments', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Electron efficiency measurements with the ATLAS detector using 2012 LHC proton–proton collision data': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/3', '게시자': 'Springer Berlin Heidelberg', '권': '77', '설명': ' This paper describes the algorithms for the reconstruction and identification of electrons in the central region of the ATLAS detector at the Large Hadron Collider (LHC). These algorithms were used for all ATLAS results with electrons in the final state that are based on the 2012 pp collision data produced by the LHC at  = 8\\xa0. The efficiency of these algorithms, together with the charge misidentification rate, is measured in data and evaluated in simulated samples using electrons from ,  and  decays. For these efficiency measurements, the full recorded data set, corresponding to an integrated luminosity of 20.3 fb, is used. Based on a new reconstruction algorithm used in 2012, the electron reconstruction efficiency is 97% for electrons with \\xa0 and 99% at \\xa0. Combining this with the efficiency of additional selection criteria to reject electrons from background\\xa0…', '저널': 'The European Physical Journal C', '저자': 'Morad Aaboud, G Aad, B Abbott, J Abdallah, B Abeloos, OS AbouZeid, NL Abraham, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, S Adachi, L Adamczyk, DL Adams, J Adelman, S Adomeit, T Adye, AA Affolder, T Agatonovic-Jovin, JA Aguilar-Saavedra, SP Ahlen, F Ahmadov, G Aielli, H Akerstedt, TPA Åkesson, AV Akimov, GL Alberghi, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, T Alexopoulos, M Alhroob, B Ali, M Aliev, G Alimonti, J Alison, SP Alkire, BMM Allbrooke, BW Allen, PP Allport, A Aloisio, A Alonso, F Alonso, C Alpigiani, AA Alshehri, M Alstaty, B Alvarez Gonzalez, D Álvarez Piqueras, MG Alviggi, BT Amadio, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, JK Anders, KJ Anderson, A Andreazza, V Andrei, S Angelidakis, I Angelozzi, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, C Antel, M Antonelli, A Antonov, DJ Antrim, F Anulli, M Aoki, L Aperio Bella, G Arabidze, Y Arai, JP Araque, ATH Arce, FA Arduh, J-F Arguin, S Argyropoulos, M Arik, AJ Armbruster, LJ Armitage, O Arnaez, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Artz, S Asai, N Asbah, A Ashkenazi, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, K Augsten, G Avolio, B Axen, MK Ayoub, G Azuelos, MA Baak, AE Baas, MJ Baca, H Bachacou, K Bachas, M Backes, M Backhaus, P Bagiacchi, P Bagnaia, Y Bai, JT Baines, M Bajic, OK Baker, EM Baldin, P Balek, T Balestri, F Balli, WK Balunas, E Banas, Sw Banerjee, AAE Bannoura, L Barak, EL Barberio, D Barberis, M Barbero, T Barillari, M-S Barisits, T Barklow, N Barlow, SL Barnes, BM Barnett, RM Barnett, Z Barnovska-Blenessy, A Baroncelli', '전체 인용횟수': '2500회 인용20162017201820192020202120222023355599692349108714439', '페이지': '1-45', '학술 문서': 'Electron efficiency measurements with the ATLAS detector using 2012 LHC proton–proton collision dataM Aaboud, G Aad, B Abbott, J Abdallah, O Abdinov…\\xa0- The European Physical Journal C, 20172499회 인용 관련 학술자료 전체 119개의 버전 Electron efficiency measurements with the ATLAS detector using 2012 LHC proton–proton collision dataH Arnold - 20171회 인용 관련 학술자료 ', '호': '3'}, title='Electron efficiency measurements with the ATLAS detector using 2012 LHC proton–proton collision data', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Performance of missing transverse momentum reconstruction in proton-proton collisions at  with ATLAS': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012/1', '게시자': 'Springer-Verlag', '권': '72', '설명': ' The measurement of missing transverse momentum in the ATLAS detector, described in this paper, makes use of the full event reconstruction and a calibration based on reconstructed physics objects. The performance of the missing transverse momentum reconstruction is evaluated using data collected in pp collisions at a centre-of-mass energy of 7\\xa0TeV in 2010. Minimum bias events and events with jets of hadrons are used from data samples corresponding to an integrated luminosity of about 0.3\\xa0nb−1 and 600\\xa0nb−1 respectively, together with events containing a Z boson decaying to two leptons (electrons or muons) or a W boson decaying to a lepton (electron or muon) and a neutrino, from a data sample corresponding to an integrated luminosity of about 36\\xa0pb−1. An estimate of the systematic uncertainty on the missing transverse momentum scale is presented.', '저널': 'The European Physical Journal C', '저자': 'Georges Aad, B Abbott, J Abdallah, AA Abdelalim, A Abdesselam, B Abi, M Abolins, H Abramowicz, H Abreu, E Acerbi, BS Acharya, DL Adams, TN Addy, J Adelman, M Aderholz, S Adomeit, P Adragna, T Adye, S Aefsky, JA Aguilar-Saavedra, M Aharrouche, SP Ahlen, F Ahles, A Ahmad, M Ahsan, G Aielli, T Akdogan, TPA Åkesson, G Akimoto, AV Akimov, A Akiyama, MS Alam, MA Alam, J Albert, S Albrand, M Aleksa, IN Aleksandrov, F Alessandria, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, M Aliev, G Alimonti, J Alison, M Aliyev, PP Allport, SE Allwood-Spiers, J Almond, A Aloisio, R Alon, A Alonso, MG Alviggi, K Amako, P Amaral, C Amelung, VV Ammosov, A Amorim, G Amorós, N Amram, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, ML Andrieux, XS Anduaga, A Angerami, F Anghinolfi, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, S Aoun, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, ATH Arce, JP Archambault, S Arfaoui, JF Arguin, E Arik, M Arik, AJ Armbruster, O Arnaez, C Arnault, A Artamonov, G Artoni, D Arutinov, S Asai, R Asfandiyarov, S Ask, B Åsman, L Asquith, K Assamagan, A Astbury, A Astvatsatourov, G Atoian, B Aubert, B Auerbach, E Auge, K Augsten, M Aurousseau, N Austin, G Avolio, R Avramidou, D Axen, C Ay, G Azuelos, Y Azuma, MA Baak, G Baccaglioni, C Bacci, AM Bach, H Bachacou, K Bachas, G Bachy, M Backes, M Backhaus, E Badescu, P Bagnaia, S Bahinipati, Y Bai, DC Bailey, T Bain, JT Baines, OK Baker, MD Baker, S Baker, E Banas, P Banerjee, Sw Banerjee, D Banfi, A Bangert, V Bansal, HS Bansil, L Barak, SP Baranov, A Barashkou, A Barbaro Galtieri', '전체 인용횟수': '2383회 인용201120122013201420152016201720182019202020212022202319416316366485347241793818162312', '페이지': '1-35', '학술 문서': 'Performance of missing transverse momentum reconstruction in proton-proton collisions at 7 TeV with ATLAS*ATLAS collaboration\\xa0- arXiv preprint arXiv:1108.5602, 20111988회 인용 관련 학술자료 전체 16개의 버전 Electron performance measurements with the ATLAS detector using the 2010 LHC proton-proton collision data*ATLAS Collaboration atlas. publications@ cern. ch…\\xa0- The European Physical Journal C, 2012317회 인용 관련 학술자료 전체 76개의 버전 Performance of missing transverse momentum reconstruction in proton-proton collisions at s=7~TeV with ATLASATLAS Collaboration atlas. publications@ cern. ch…\\xa0- The European Physical Journal C, 2012230회 인용 관련 학술자료 전체 71개의 버전 Performance of missing transverse momentum reconstruction in proton-proton collisions at√ s= 7 TeV with ATLAS*G Aad, B Abbott, J Abdallah, AA Abdelalim… - 2012Performance of missing transverse momentum reconstruction in proton-proton collisions at√ s= 7 TeV with atlasMS Alam, J Ernst, V Rojo, S Bahinipati, NJ Buchanan…\\xa0- The European Physical Journal C-Particles and Fields, 2012전체 2개의 버전 ', '호': '1'}, title='Performance of missing transverse momentum reconstruction in proton-proton collisions at  with ATLAS', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Electron performance measurements with the ATLAS detector using the 2010 LHC proton-proton collision data': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012/3', '게시자': 'Springer-Verlag', '권': '72', '설명': 'Detailed measurements of the electron performance of the ATLAS detector at the LHC are reported, using decays of the Z, W and J/psi particles. Data collected in 2010 at sqrt(s)=7 TeV are used, corresponding to an integrated luminosity of almost 40 pb^-1. The inter-alignment of the inner detector and the electromagnetic calorimeter, the determination of the electron energy scale and resolution, and the performance in terms of response uniformity and linearity are discussed. The electron identification, reconstruction and trigger efficiencies, as well as the charge misidentification probability, are also presented.', '저널': 'The European Physical Journal C', '저자': 'Georges Aad, B Abbott, J Abdallah, AA Abdelalim, A Abdesselam, B Abi, M Abolins, H Abramowicz, H Abreu, E Acerbi, BS Acharya, DL Adams, TN Addy, J Adelman, M Aderholz, S Adomeit, P Adragna, T Adye, S Aefsky, JA Aguilar-Saavedra, M Aharrouche, SP Ahlen, F Ahles, A Ahmad, M Ahsan, G Aielli, T Akdogan, TPA Åkesson, G Akimoto, AV Akimov, A Akiyama, MS Alam, MA Alam, J Albert, S Albrand, M Aleksa, IN Aleksandrov, F Alessandria, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, M Aliev, G Alimonti, J Alison, M Aliyev, PP Allport, SE Allwood-Spiers, J Almond, A Aloisio, R Alon, A Alonso, MG Alviggi, K Amako, P Amaral, C Amelung, VV Ammosov, A Amorim, G Amorós, N Amram, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, ML Andrieux, XS Anduaga, A Angerami, F Anghinolfi, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, S Aoun, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, ATH Arce, JP Archambault, S Arfaoui, JF Arguin, E Arik, M Arik, AJ Armbruster, O Arnaez, C Arnault, A Artamonov, G Artoni, D Arutinov, S Asai, R Asfandiyarov, S Ask, B Åsman, L Asquith, K Assamagan, A Astbury, A Astvatsatourov, G Atoian, B Aubert, B Auerbach, E Auge, K Augsten, M Aurousseau, N Austin, G Avolio, R Avramidou, D Axen, C Ay, G Azuelos, Y Azuma, MA Baak, G Baccaglioni, C Bacci, AM Bach, H Bachacou, K Bachas, G Bachy, M Backes, M Backhaus, E Badescu, P Bagnaia, S Bahinipati, Y Bai, DC Bailey, T Bain, JT Baines, OK Baker, MD Baker, S Baker, E Banas, P Banerjee, Sw Banerjee, D Banfi, A Bangert, V Bansal, HS Bansil, L Barak, SP Baranov, A Barashkou, A Barbaro Galtieri', '전체 인용횟수': '2109회 인용2011201220132014201520162017201820192020202120222023315014643862432091224451237155', '페이지': '1-46', '학술 문서': 'Electron performance measurements with the ATLAS detector using the 2010 LHC proton-proton collision dataATLAS collaboration\\xa0- arXiv preprint arXiv:1110.3174, 20111815회 인용 관련 학술자료 전체 15개의 버전 Electron performance measurements with the ATLAS detector using the 2010 LHC proton-proton collision dataATLAS Collaboration atlas. publications@ cern. ch…\\xa0- The European Physical Journal C, 2012317회 인용 관련 학술자료 전체 76개의 버전 ', '호': '3'}, title='Electron performance measurements with the ATLAS detector using the 2010 LHC proton-proton collision data', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Jet energy scale measurements and their systematic uncertainties in proton-proton collisions at  with the ATLAS detector': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/10/13', '게시자': 'American Physical Society', '권': '96', '설명': 'Jet energy scale measurements and their systematic uncertainties are reported for jets measured with the ATLAS detector using proton-proton collision data with a center-of-mass energy of  TeV, corresponding to an integrated luminosity of 3.2 fb collected during 2015 at the LHC. Jets are reconstructed from energy deposits forming topological clusters of calorimeter cells, using the anti- algorithm with radius parameter . Jets are calibrated with a series of simulation-based corrections and in situ techniques. In situ techniques exploit the transverse momentum balance between a jet and a reference object such as a photon,  boson, or multijet system for jets with  GeV and pseudorapidities of , using both data and simulation. An uncertainty in the jet energy scale of less than 1% is found in the central calorimeter region () for jets with  GeV. An uncertainty of about 4.5% is found for low- jets with  GeV in the central region, dominated by uncertainties in the corrections for multiple proton-proton interactions. The calibration of forward jets () is derived from dijet  balance measurements. For jets of  GeV, the additional uncertainty for the forward jet calibration reaches its largest value of about 2% in the range  and in a narrow slice of .', '저널': 'Physical Review D', '저자': 'Morad Aaboud, Georges Aad, B Abbott, J Abdallah, B Abeloos, SH Abidi, OS AbouZeid, NL Abraham, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, S Adachi, L Adamczyk, J Adelman, M Adersberger, T Adye, AA Affolder, T Agatonovic-Jovin, C Agheorghiesei, JA Aguilar-Saavedra, SP Ahlen, F Ahmadov, G Aielli, S Akatsuka, H Akerstedt, TPA Åkesson, AV Akimov, GL Alberghi, J Albert, P Albicocco, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, T Alexopoulos, M Alhroob, B Ali, M Aliev, G Alimonti, J Alison, SP Alkire, BMM Allbrooke, BW Allen, PP Allport, A Aloisio, A Alonso, F Alonso, C Alpigiani, AA Alshehri, M Alstaty, B Alvarez Gonzalez, D Álvarez Piqueras, MG Alviggi, BT Amadio, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, JK Anders, KJ Anderson, A Andreazza, V Andrei, S Angelidakis, I Angelozzi, A Angerami, AV Anisenkov, N Anjos, A Annovi, C Antel, M Antonelli, A Antonov, DJ Antrim, F Anulli, M Aoki, L Aperio Bella, G Arabidze, Y Arai, JP Araque, V Araujo Ferraz, ATH Arce, RE Ardell, FA Arduh, JF Arguin, S Argyropoulos, M Arik, AJ Armbruster, LJ Armitage, O Arnaez, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Artz, S Asai, N Asbah, A Ashkenazi, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, K Augsten, G Avolio, B Axen, MK Ayoub, G Azuelos, AE Baas, MJ Baca, H Bachacou, K Bachas, M Backes, M Backhaus, P Bagnaia, H Bahrasemani, JT Baines, M Bajic, OK Baker, EM Baldin, P Balek, F Balli, WK Balunas, E Banas, Sw Banerjee, AAE Bannoura, L Barak, EL Barberio, D Barberis, M Barbero, T Barillari, MS Barisits, T Barklow, N Barlow, SL Barnes, BM Barnett, RM Barnett, Z Barnovska-Blenessy, A Baroncelli, G Barone', '전체 인용횟수': '2089회 인용201620172018201920202021202220237182507463359241146179', '페이지': '072002', '학술 문서': 'Jet energy scale measurements and their systematic uncertainties in proton-proton collisions at $\\\\sqrt {s}= 13$ TeV with the ATLAS detectorATLAS collaboration\\xa0- arXiv preprint arXiv:1703.09665, 20171920회 인용 관련 학술자료 전체 27개의 버전 Jet energy scale measurements and their systematic uncertainties in proton-proton collisions at s= 13 TeV with the ATLAS detectorM Aaboud, G Aad, B Abbott, J Abdallah, B Abeloos…\\xa0- Physical Review D, 2017217회 인용 관련 학술자료 전체 64개의 버전 Jet energy scale measurements and their systematic uncertainties in proton-proton collisions at root s= 13 TeV with the ATLAS detectorATLA Collaboration\\xa0- Physical review D, 2017관련 학술자료 전체 8개의 버전 ', '호': '7'}, title='Jet energy scale measurements and their systematic uncertainties in proton-proton collisions at  with the ATLAS detector', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Jet energy measurement and its systematic uncertainty in proton–proton collisions at                                                        s                        =            7 \\xa0TeV with the\\xa0…': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/1', '게시자': 'Springer Berlin Heidelberg', '권': '75', '설명': ' The jet energy scale (JES) and its systematic uncertainty are determined for jets measured with the ATLAS detector using proton–proton collision data with a centre-of-mass energy of \\xa0TeV corresponding to an integrated luminosity of  . Jets are reconstructed from energy deposits forming topological clusters of calorimeter cells using the anti- algorithm with distance parameters  or , and are calibrated using MC simulations. A residual JES correction is applied to account for differences between data and MC simulations. This correction and its systematic uncertainty are estimated using a combination of in situ techniques exploiting the transverse momentum balance between a jet and a reference object such as a photon or a  boson, for  and pseudorapidities . The effect of multiple proton–proton interactions is corrected for, and an uncertainty is evaluated using in situ techniques\\xa0…', '저널': 'The European Physical Journal C', '저자': 'Georges Aad, Tatevik Abajyan, Brad Abbott, J Abdallah, S Abdel Khalek, Rosemarie Aben, Babak Abi, Maris Abolins, OS AbouZeid, Halina Abramowicz, H Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, TN Addy, J Adelman, S Adomeit, T Adye, S Aefsky, T Agatonovic-Jovin, JA Aguilar-Saavedra, M Agustoni, SP Ahlen, A Ahmad, F Ahmadov, G Aielli, TPA Åkesson, G Akimoto, AV Akimov, MA Alam, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, F Alessandria, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, M Aliev, G Alimonti, L Alio, J Alison, BMM Allbrooke, LJ Allison, PP Allport, SE Allwood-Spiers, J Almond, A Aloisio, R Alon, A Alonso, F Alonso, A Altheimer, B Alvarez Gonzalez, MG Alviggi, K Amako, Y Amaral Coutinho, C Amelung, VV Ammosov, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, S Angelidakis, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, ATH Arce, S Arfaoui, JF Arguin, S Argyropoulos, E Arik, M Arik, AJ Armbruster, O Arnaez, V Arnal, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, S Ask, B Åsman, L Asquith, K Assamagan, R Astalos, A Astbury, M Atkinson, NB Atlay, B Auerbach, E Auge, K Augsten, M Aurousseau, G Avolio, G Azuelos, Y Azuma, MA Baak, C Bacci, AM Bach, H Bachacou, K Bachas, M Backes, M Backhaus, J Backus Mayes, E Badescu, P Bagiacchi, P Bagnaia, Y Bai, DC Bailey, T Bain, JT Baines, OK Baker, S Baker, P Balek, F Balli, E Banas, Sw Banerjee, D Banfi, A Bangert, V Bansal, HS Bansil, L Barak', '전체 인용횟수': '2063회 인용20142015201620172018201920202021202220237650445945224713052603232', '페이지': '1-101', '학술 문서': 'Jet energy measurement and its systematic uncertainty in proton–proton collisions at s= 7 s= 7 TeV with the ATLAS detectorAtlas Collaboration atlas. publications@ cern. ch…\\xa0- The European Physical Journal C, 20152063회 인용 관련 학술자료 전체 118개의 버전 ', '호': '1'}, title='Jet energy measurement and its systematic uncertainty in proton–proton collisions at                                                        s                        =            7 \\xa0TeV with the\\xa0…', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Measurement of the muon reconstruction performance of the ATLAS detector using 2011 and 2012 LHC proton–proton collision data': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/11', '게시자': 'Springer Berlin Heidelberg', '권': '74', '설명': ' This paper presents the performance of the ATLAS muon reconstruction during the LHC run with  collisions at –8\\xa0TeV in 2011–2012, focusing mainly on data collected in 2012. Measurements of the reconstruction efficiency and of the momentum scale and resolution, based on large reference samples of ,  and  decays, are presented and compared to Monte Carlo simulations. Corrections to the simulation, to be used in physics analysis, are provided. Over most of the covered phase space (muon  and \\xa0GeV) the efficiency is above  and is measured with per-mille precision. The momentum resolution ranges from  at central rapidity and for transverse momentum \\xa0GeV, to  at large rapidity and \\xa0GeV. The momentum scale is known with an uncertainty of  to  depending on rapidity. A method for the recovery of final state\\xa0…', '저널': 'The European Physical Journal C', '저자': 'ATLAS Collaboration atlas. publications@ cern. ch, Georges Aad, B Abbott, J Abdallah, S Abdel Khalek, O Abdinov, R Aben, B Abi, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, J Adelman, S Adomeit, T Adye, T Agatonovic-Jovin, JA Aguilar-Saavedra, M Agustoni, SP Ahlen, F Ahmadov, G Aielli, H Akerstedt, TPA Åkesson, G Akimoto, AV Akimov, GL Alberghi, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, G Alimonti, L Alio, J Alison, BMM Allbrooke, LJ Allison, PP Allport, J Almond, A Aloisio, A Alonso, F Alonso, C Alpigiani, A Altheimer, B Alvarez Gonzalez, MG Alviggi, K Amako, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, S Angelidakis, I Angelozzi, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, JP Araque, ATH Arce, J-F Arguin, S Argyropoulos, M Arik, AJ Armbruster, O Arnaez, V Arnal, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, A Ashkenazi, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, B Auerbach, K Augsten, M Aurousseau, G Avolio, G Azuelos, Y Azuma, MA Baak, A Baas, C Bacci, H Bachacou, K Bachas, M Backes, M Backhaus, J Backus Mayes, E Badescu, P Bagiacchi, P Bagnaia, Y Bai, T Bain, JT Baines, OK Baker, P Balek, F Balli, E Banas, Sw Banerjee, AAE Bannoura, V Bansal, HS Bansil, L Barak, SP Baranov, EL Barberio, D Barberis, M Barbero, T Barillari, M Barisonzi', '전체 인용횟수': '1941회 인용2013201420152016201720182019202020212022202356256051237018310955262819', '페이지': '1-34', '학술 문서': 'Measurement of the muon reconstruction performance of the ATLAS detector using 2011 and 2012 LHC proton–proton collision dataATLAS Collaboration atlas. publications@ cern. ch…\\xa0- The European Physical Journal C, 20141941회 인용 관련 학술자료 전체 118개의 버전 Measurement of the muon reconstruction performance of the ATLAS detector using 2011 and 2012 LHC proton–proton collision dataMJ Alconada Verzini, F Alonso, XS Anduaga, MT Dova…\\xa0- The European Physical Journal C, 2014관련 학술자료 전체 2개의 버전 '}, title='Measurement of the muon reconstruction performance of the ATLAS detector using 2011 and 2012 LHC proton–proton collision data', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Electron reconstruction and identification efficiency measurements with the ATLAS detector using the 2011 LHC proton–proton collision data': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/7', '게시자': 'Springer Berlin Heidelberg', '권': '74', '설명': ' Many of the interesting physics processes to be measured at the LHC have a signature involving one or more isolated electrons. The electron reconstruction and identification efficiencies of the ATLAS detector at the LHC have been evaluated using proton–proton collision data collected in 2011 at \\xa0TeV and corresponding to an integrated luminosity of 4.7\\xa0fb. Tag-and-probe methods using events with leptonic decays of \\xa0and  bosons and  mesons are employed to benchmark these performance parameters. The combination of all measurements results in identification efficiencies determined with an accuracy at the few per mil level for electron transverse energy greater than 30\\xa0GeV.', '저널': 'The European Physical Journal C', '저자': 'ATLAS Collaboration atlas. publications@ cern. ch, Georges Aad, T Abajyan, B Abbott, J Abdallah, S Abdel Khalek, O Abdinov, R Aben, B Abi, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, TN Addy, J Adelman, S Adomeit, T Adye, T Agatonovic-Jovin, JA Aguilar-Saavedra, M Agustoni, SP Ahlen, A Ahmad, F Ahmadov, G Aielli, TPA Åkesson, G Akimoto, AV Akimov, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, G Alimonti, L Alio, J Alison, BMM Allbrooke, LJ Allison, PP Allport, SE Allwood-Spiers, J Almond, A Aloisio, R Alon, A Alonso, F Alonso, C Alpigiani, A Altheimer, B Alvarez Gonzalez, MG Alviggi, K Amako, Y Amaral Coutinho, C Amelung, D Amidei, VV Ammosov, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, S Angelidakis, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, JP Araque, ATH Arce, JF Arguin, S Argyropoulos, M Arik, AJ Armbruster, O Arnaez, V Arnal, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, A Ashkenazi, S Ask, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, B Auerbach, E Auge, K Augsten, M Aurousseau, G Avolio, G Azuelos, Y Azuma, MA Baak, C Bacci, AM Bach, H Bachacou, K Bachas, M Backes, M Backhaus, J Backus Mayes, E Badescu, P Bagiacchi, P Bagnaia, Y Bai, DC Bailey, T Bain, JT Baines, OK Baker, S Baker, P Balek, F Balli, E Banas, Sw Banerjee, D Banfi, A Bangert, AAE Bannoura, V Bansal, HS Bansil, L Barak', '전체 인용횟수': '1883회 인용201320142015201620172018201920202021202220238219560430354159901818912', '페이지': '1-38', '학술 문서': 'Electron reconstruction and identification efficiency measurements with the ATLAS detector using the 2011 LHC proton–proton collision dataATLAS Collaboration atlas. publications@ cern. ch…\\xa0- The European Physical Journal C, 20141881회 인용 관련 학술자료 전체 97개의 버전 Electron reconstruction and identification efficiency measurements with the ATLAS detector using the 2011 LHC proton-proton collision dataG Aad, B Abbott, J Abdallah, S Abdel Khalek, R Aben…\\xa0- The European physical journal. C, Particles and fields, 20143회 인용 관련 학술자료 '}, title='Electron reconstruction and identification efficiency measurements with the ATLAS detector using the 2011 LHC proton–proton collision data', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Performance of missing transverse momentum reconstruction with the ATLAS detector using proton–proton collisions at                                                        s\\xa0…': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/11', '게시자': 'Springer Berlin Heidelberg', '권': '78', '설명': 'The performance of the missing transverse momentum () reconstruction with the ATLAS detector is evaluated using data collected in proton–proton collisions at the LHC at a centre-of-mass energy of 13 TeV in 2015. To reconstruct, fully calibrated electrons, muons, photons, hadronically decaying, and jets reconstructed from calorimeter energy deposits and charged-particle tracks are used. These are combined with the soft hadronic activity measured by reconstructed charged-particle tracks not associated with the hard objects. Possible double counting of contributions from reconstructed charged-particle tracks from the inner detector, energy deposits in the calorimeter, and reconstructed muons from the muon spectrometer is avoided by applying a signal ambiguity resolution procedure which rejects already used signals when combining the various contributions. The individual terms as well as the overall\\xa0…', '저널': 'The European Physical Journal C', '저자': 'Morad Aaboud, Georges Aad, Brad Abbott, B Abeloos, SH Abidi, OS AbouZeid, NL Abraham, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, S Adachi, L Adamczyk, J Adelman, M Adersberger, T Adye, AA Affolder, Y Afik, T Agatonovic-Jovin, C Agheorghiesei, JA Aguilar-Saavedra, F Ahmadov, G Aielli, S Akatsuka, H Akerstedt, TPA Åkesson, E Akilli, AV Akimov, GL Alberghi, J Albert, P Albicocco, MJ Alconada Verzini, S Alderweireldt, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, T Alexopoulos, M Alhroob, B Ali, G Alimonti, J Alison, SP Alkire, BMM Allbrooke, BW Allen, PP Allport, A Aloisio, A Alonso, F Alonso, C Alpigiani, AA Alshehri, MI Alstaty, B Alvarez Gonzalez, D Álvarez Piqueras, MG Alviggi, BT Amadio, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, S Amoroso, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, JK Anders, KJ Anderson, A Andreazza, V Andrei, S Angelidakis, I Angelozzi, A Angerami, AV Anisenkov, N Anjos, A Annovi, C Antel, M Antonelli, Alexey Antonov, DJA Antrim, F Anulli, M Aoki, L Aperio Bella, G Arabidze, Y Arai, JP Araque, V Araujo Ferraz, ATH Arce, RE Ardell, FA Arduh, JF Arguin, S Argyropoulos, M Arik, AJ Armbruster, LJ Armitage, O Arnaez, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Artz, S Asai, N Asbah, A Ashkenazi, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, K Augsten, G Avolio, B Axen, MK Ayoub, G Azuelos, AE Baas, MJ Baca, H Bachacou, K Bachas, M Backes, P Bagnaia, M Bahmani, H Bahrasemani, JT Baines, M Bajic, OK Baker, EM Baldin, P Balek, F Balli, WK Balunas, E Banas, A Bandyopadhyay, S Banerjee, AAE Bannoura, L Barak, EL Barberio, D Barberis, M Barbero, T Barillari, MS Barisits, J Barkeloo, T Barklow, N Barlow, SL Barnes, BM Barnett, RM Barnett, Z Barnovska-Blenessy, A Baroncelli', '전체 인용횟수': '1881회 인용2017201820192020202120222023171333339260254193222', '페이지': '1-46', '학술 문서': 'Performance of missing transverse momentum reconstruction with the ATLAS detector using proton–proton collisions at $$\\\\sqrt {s}= 13~\\\\hbox {TeV} $$ s= 13 TeVM Aaboud, G Aad, B Abbott, B Abeloos, SH Abidi…\\xa0- The European Physical Journal C, 20181881회 인용 관련 학술자료 전체 86개의 버전 ', '호': '11'}, title='Performance of missing transverse momentum reconstruction with the ATLAS detector using proton–proton collisions at                                                        s\\xa0…', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Electron and photon energy calibration with the ATLAS detector using LHC Run 1 data': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/10', '게시자': 'Springer Berlin Heidelberg', '권': '74', '설명': ' This paper presents the electron and photon energy calibration achieved with the ATLAS detector using about 25\\xa0fb of LHC proton–proton collision data taken at centre-of-mass energies of  and 8\\xa0TeV. The reconstruction of electron and photon energies is optimised using multivariate algorithms. The response of the calorimeter layers is equalised in data and simulation, and the longitudinal profile of the electromagnetic showers is exploited to estimate the passive material in front of the calorimeter and reoptimise the detector simulation. After all corrections, the  resonance is used to set the absolute energy scale. For electrons from  decays, the achieved calibration is typically accurate to 0.05\\xa0% in most of the detector acceptance, rising to 0.2\\xa0% in regions with large amounts of passive material. The remaining inaccuracy is less than 0.2–1\\xa0% for electrons with a transverse energy of 10\\xa0GeV, and is\\xa0…', '저널': 'The European Physical Journal C', '저자': 'ATLAS Collaboration atlas. publications@ cern. ch, Georges Aad, B Abbott, J Abdallah, S Abdel Khalek, O Abdinov, R Aben, B Abi, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, J Adelman, S Adomeit, T Adye, T Agatonovic-Jovin, JA Aguilar-Saavedra, M Agustoni, SP Ahlen, F Ahmadov, G Aielli, H Akerstedt, TPA Åkesson, G Akimoto, AV Akimov, GL Alberghi, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, G Alimonti, L Alio, J Alison, BMM Allbrooke, LJ Allison, PP Allport, J Almond, A Aloisio, A Alonso, F Alonso, C Alpigiani, A Altheimer, B Alvarez Gonzalez, MG Alviggi, K Amako, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, S Angelidakis, I Angelozzi, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, JP Araque, ATH Arce, JF Arguin, S Argyropoulos, M Arik, AJ Armbruster, O Arnaez, V Arnal, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, A Ashkenazi, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, B Auerbach, K Augsten, M Aurousseau, G Avolio, G Azuelos, Y Azuma, MA Baak, A Baas, C Bacci, H Bachacou, K Bachas, M Backes, M Backhaus, J Backus Mayes, E Badescu, P Bagiacchi, P Bagnaia, Y Bai, T Bain, JT Baines, OK Baker, P Balek, F Balli, E Banas, Sw Banerjee, AAE Bannoura, V Bansal, HS Bansil, L Barak, SP Baranov, EL Barberio, D Barberis, M Barbero, T Barillari, M Barisonzi', '전체 인용횟수': '1767회 인용2013201420152016201720182019202020212022202364324237539534319664362727', '페이지': '1-48', '학술 문서': 'Electron and photon energy calibration with the ATLAS detector using LHC Run 1 dataATLAS Collaboration atlas. publications@ cern. ch…\\xa0- The European Physical Journal C, 20141767회 인용 관련 학술자료 전체 87개의 버전 '}, title='Electron and photon energy calibration with the ATLAS detector using LHC Run 1 data', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Measurement of the Z/ γ* boson transverse momentum distribution in pp collisions at  TeV with the ATLAS detector': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/9/1', '게시자': 'Springer Berlin Heidelberg', '권': '2014', '설명': 'This paper describes a measurement of the Z/γ* boson transverse momentum spectrum using ATLAS proton-proton collision data at a centre-of-mass energy of TeV at the LHC. The measurement is performed in the Z/γ*→ e+ e− and Z/γ*→ μ+ μ− channels, using data corresponding to an integrated luminosity of 4.7 fb− 1. Normalized differential cross sections as a function of the Z/γ* boson transverse momentum are measured for transverse momenta up to 800 GeV. The measurement is performed inclusively for Z/γ* rapidities up to 2.4, as well as in three rapidity bins. The channel results are combined, compared to perturbative and resummed QCD calculations and used to constrain the parton shower parameters of Monte Carlo generators.', '저널': 'Journal of High Energy Physics', '저자': 'Georges Aad, Brad Abbott, Jalal Abdallah, S Abdel Khalek, R Aben, B Abi, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, J Adelman, Stefanie Adomeit, T Adye, T Agatonovic-Jovin, JA Aguilar-Saavedra, M Agustoni, SP Ahlen, F Ahmadov, G Aielli, H Akerstedt, TPA Åkesson, G Akimoto, AV Akimov, GL Alberghi, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, G Alimonti, L Alio, J Alison, BMM Allbrooke, LJ Allison, PP Allport, J Almond, A Aloisio, A Alonso, F Alonso, C Alpigiani, A Altheimer, B Alvarez Gonzalez, MG Alviggi, K Amako, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, S Angelidakis, I Angelozzi, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, JP Araque, ATH Arce, JF Arguin, S Argyropoulos, M Arik, AJ Armbruster, O Arnaez, V Arnal, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, A Ashkenazi, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, B Auerbach, K Augsten, M Aurousseau, G Avolio, G Azuelos, Y Azuma, MA Baak, C Bacci, H Bachacou, K Bachas, M Backes, M Backhaus, J Backus Mayes, E Badescu, P Bagiacchi, P Bagnaia, Y Bai, T Bain, JT Baines, OK Baker, S Baker, P Balek, F Balli, E Banas, Sw Banerjee, AAE Bannoura, V Bansal, HS Bansil, L Barak, SP Baranov, EL Barberio, D Barberis, M Barbero, T Barillari, M Barisonzi, T Barklow, N Barlow', '전체 인용횟수': '1704회 인용20142015201620172018201920202021202220231364154246328247187192106158', '페이지': '145', '학술 문서': 'Measurement of the Z/γ* boson transverse momentum distribution in pp collisions at s= 7$$\\\\sqrt {s}= 7$$ TeV with the ATLAS detectorG Aad, B Abbott, J Abdallah, S Abdel Khalek, R Aben…\\xa0- Journal of High Energy Physics, 20141692회 인용 관련 학술자료 전체 87개의 버전 Measurement of the Z/γ* boson transverse momentum distribution in pp collisions at√ s= 7 TeV with the ATLAS detectorG Aad, B Abbott, J Abdallah, S Abdel Khalek, R Aben… - 201420회 인용 관련 학술자료 Measurement of the $ Z/\\\\gamma^* $ boson transverse momentum distribution in $ pp $ collisions at $\\\\sqrt {s} $= 7 TeV with the ATLAS detectorY Peters, G Aad, A Pilkington, B Cox, A Oh, C Da Via…\\xa0- JHEP, 2014전체 2개의 버전 ', '호': '9'}, title='Measurement of the Z/ γ* boson transverse momentum distribution in pp collisions at  TeV with the ATLAS detector', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Evidence for the spin-0 nature of the Higgs boson using ATLAS data': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/7/4', '설명': 'Studies of the spin and parity quantum numbers of the Higgs boson are presented, based on proton-proton collision data collected by the ATLAS experiment at the LHC. The Standard Model spin-parity JP = 0+ hypothesis is compared with alternative hypotheses using the Higgs boson decays H->gamma gamma, H -> ZZ -> 4 leptons and H->WW -> l nu l nu, as well as the combination of these channels. The analysed dataset corresponds to an integrated luminosity of 20.7 fb-1 collected at a centre-of-mass energy of sqrt(s) = 8 TeV. For the H -> ZZ -> 4-lepton decay mode the dataset corresponding to an integrated luminosity of 4.6 fb-1 collected at sqrt(s) = 7 TeV is added. The data are compatible with the Standard Model JP = 0+ quantum numbers for the Higgs boson, whereas all alternative hypotheses studied in this letter, namely some specific JP = 0-; 1+; 1-; 2+ models, are excluded at confidence levels above 97.8%. This exclusion holds independently of the assumptions on the coupling strengths to the Standard Model particles and in the case of the JP = 2+ model, of the relative fractions of gluon-fusion and quark-antiquark production of the spin-2 particle. The data thus provide evidence for the spin-0 nature of the Higgs boson, with positive parity being strongly preferred.', '저널': 'arXiv preprint arXiv:1307.1432', '저자': 'Atlas Collaboration', '전체 인용횟수': '1678회 인용201320142015201620172018201920202021202220238333934518115915511381866447', '학술 문서': 'Evidence for the spin-0 nature of the Higgs boson using ATLAS dataAtlas Collaboration\\xa0- arXiv preprint arXiv:1307.1432, 20131107회 인용 관련 학술자료 전체 40개의 버전 Evidence for the spin-0 nature of the Higgs boson using ATLAS dataG Aad, T Abajyan, B Abbott, J Abdallah, SA Khalek…\\xa0- Physics Letters B, 2013506회 인용 관련 학술자료 전체 67개의 버전 ATLAS collaboration*G Aad, T Abajyan, B Abbott, J Abdallah…\\xa0- Nuclear Physics. A, 2014129회 인용 관련 학술자료 전체 33개의 버전 Evidence for the spin-0 nature of the Higgs boson using ATLAS dataP Jackson, N Soni, W Edson, J Ernst, S Guindon…\\xa0- Physics Letters. Section B: Nuclear, Elementary\\xa0…, 20131회 인용 관련 학술자료 '}, title='Evidence for the spin-0 nature of the Higgs boson using ATLAS data', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Jet energy resolution in proton-proton collisions at  recorded in 2010 with the ATLAS detector': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/3', '게시자': 'Springer-Verlag', '권': '73', '설명': ' The measurement of the jet energy resolution is presented using data recorded with the ATLAS detector in proton-proton collisions at . The sample corresponds to an integrated luminosity of 35\\xa0pb−1. Jets are reconstructed from energy deposits measured by the calorimeters and calibrated using different jet calibration schemes. The jet energy resolution is measured with two different in situ methods which are found to be in agreement within uncertainties. The total uncertainties on these measurements range from\\xa020\\xa0% to\\xa010\\xa0% for jets within |y|<2.8 and with transverse momenta increasing from\\xa030\\xa0GeV to\\xa0500\\xa0GeV. Overall, the Monte Carlo simulation of the jet energy resolution agrees with the data within\\xa010\\xa0%.', '저널': 'The European Physical Journal C', '저자': 'ATLAS Collaboration atlas. publications@ cern. ch, Georges Aad, T Abajyan, B Abbott, J Abdallah, S Abdel Khalek, Ahmed Ali Abdelalim, O Abdinov, R Aben, B Abi, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, E Acerbi, BS Acharya, L Adamczyk, DL Adams, TN Addy, J Adelman, S Adomeit, P Adragna, T Adye, S Aefsky, JA Aguilar-Saavedra, M Agustoni, Mohamed Aharrouche, SP Ahlen, F Ahles, A Ahmad, M Ahsan, G Aielli, T Akdogan, TPA Åkesson, G Akimoto, AV Akimov, MS Alam, MA Alam, J Albert, S Albrand, M Aleksa, IN Aleksandrov, F Alessandria, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, M Aliev, G Alimonti, J Alison, BMM Allbrooke, PP Allport, SE Allwood-Spiers, J Almond, A Aloisio, R Alon, A Alonso, F Alonso, B Alvarez Gonzalez, MG Alviggi, K Amako, C Amelung, VV Ammosov, A Amorim, N Amram, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, P Anger, A Angerami, F Anghinolfi, A Anisenkov, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, S Aoun, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, ATH Arce, S Arfaoui, JF Arguin, E Arik, M Arik, AJ Armbruster, O Arnaez, V Arnal, C Arnault, A Artamonov, G Artoni, D Arutinov, S Asai, R Asfandiyarov, S Ask, B Åsman, L Asquith, K Assamagan, A Astbury, M Atkinson, B Aubert, E Auge, K Augsten, M Aurousseau, G Avolio, R Avramidou, D Axen, G Azuelos, Y Azuma, MA Baak, G Baccaglioni, C Bacci, AM Bach, H Bachacou, K Bachas, M Backes, M Backhaus, E Badescu, P Bagnaia, S Bahinipati, Y Bai, DC Bailey, T Bain, JT Baines, OK Baker, MD Baker, S Baker, E Banas, P Banerjee, Sw Banerjee, D Banfi, A Bangert, V Bansal, HS Bansil, L Barak, SP Baranov', '전체 인용횟수': '1429회 인용2012201320142015201620172018201920202021202220236712143091912221499779441923', '페이지': '1-27', '학술 문서': 'Jet energy resolution in proton-proton collisions at s=7TeV recorded in 2010 with the ATLAS detectorATLAS Collaboration atlas. publications@ cern. ch…\\xa0- The European Physical Journal C, 20131429회 인용 관련 학술자료 전체 97개의 버전 Jet energy resolution in proton-proton collisions at [... formula...] recorded in 2010 with the ATLAS detectorG Aad, T Abajyan, B Abbott, J Abdallah, SA Khalek…\\xa0- The European Physical Journal. C, Particles and Fields, 20131회 인용 관련 학술자료 전체 4개의 버전 Jet energy resolution in proton-proton collisions at\\\\documentclass [12pt]{minimal}\\\\usepackage {amsmath}\\\\usepackage {wasysym}\\\\usepackage {amsfonts}\\\\usepackage {amssymb}\\\\usepackage {amsbsy}\\\\usepackage {mathrsfs}\\\\usepackage {upgreek}\\\\setlength {\\\\oddsidemargin}{-69pt}\\\\begin {document} $\\\\sqrt {\\\\mathrm {s}}= 7\\\\mbox {TeV} $\\\\end {document} recorded in 2010 with the ATLAS detector*G Aad, T Abajyan, B Abbott, J Abdallah… - 2013'}, title='Jet energy resolution in proton-proton collisions at  recorded in 2010 with the ATLAS detector', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Observation of a Centrality-Dependent Dijet Asymmetry in Lead-Lead Collisions at  with the ATLAS Detector at the LHC': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/12/13', '게시자': 'American Physical Society', '권': '105', '설명': 'By using the ATLAS detector, observations have been made of a centrality-dependent dijet asymmetry in the collisions of lead ions at the Large Hadron Collider. In a sample of lead-lead events with a per-nucleon center of mass energy of 2.76 TeV, selected with a minimum bias trigger, jets are reconstructed in fine-grained, longitudinally segmented electromagnetic and hadronic calorimeters. The transverse energies of dijets in opposite hemispheres are observed to become systematically more unbalanced with increasing event centrality leading to a large number of events which contain highly asymmetric dijets. This is the first observation of an enhancement of events with such large dijet asymmetries, not observed in proton-proton collisions, which may point to an interpretation in terms of strong jet energy loss in a hot, dense medium.', '저널': 'Physical review letters', '저자': 'Georges Aad, B Abbott, J Abdallah, AA Abdelalim, Abdelmalek Abdesselam, B Abi, M Abolins, H Abramowicz, H Abreu, E Acerbi, BS Acharya, M Ackers, DL Adams, TN Addy, J Adelman, M Aderholz, S Adomeit, P Adragna, T Adye, S Aefsky, JA Aguilar-Saavedra, M Aharrouche, SP Ahlen, F Ahles, A Ahmad, M Ahsan, G Aielli, T Akdogan, TPA Åkesson, G Akimoto, AV Akimov, MS Alam, MA Alam, S Albrand, M Aleksa, IN Aleksandrov, M Aleppo, F Alessandria, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, M Aliev, G Alimonti, J Alison, M Aliyev, PP Allport, SE Allwood-Spiers, J Almond, A Aloisio, R Alon, A Alonso, J Alonso, MG Alviggi, K Amako, P Amaral, C Amelung, VV Ammosov, A Amorim, G Amoros, N Amram, C Anastopoulos, T Andeen, CF Anders, KJ Anderson, A Andreazza, V Andrei, ML Andrieux, XS Anduaga, A Angerami, F Anghinolfi, N Anjos, A Annovi, A Antonaki, M Antonelli, S Antonelli, J Antos, F Anulli, S Aoun, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, ATH Arce, JP Archambault, S Arfaoui, JF Arguin, E Arik, M Arik, AJ Armbruster, KE Arms, SR Armstrong, O Arnaez, C Arnault, A Artamonov, G Artoni, D Arutinov, S Asai, J Silva, R Asfandiyarov, S Ask, B Åsman, L Asquith, K Assamagan, A Astbury, A Astvatsatourov, G Atoian, B Aubert, B Auerbach, E Auge, K Augsten, M Aurousseau, N Austin, R Avramidou, D Axen, C Ay, G Azuelos, Y Azuma, MA Baak, G Baccaglioni, C Bacci, AM Bach, H Bachacou, K Bachas, G Bachy, M Backes, E Badescu, P Bagnaia, S Bahinipati, Y Bai, DC Bailey, T Bain, JT Baines, OK Baker, S Baker, F Baltasar Dos Santos Pedrosa, E Banas, P Banerjee, Sw Banerjee, D Banfi, A Bangert, V Bansal, HS Bansil, L Barak, SP Baranov, A Barashkou, A Barbaro Galtieri, T Barber', '전체 인용횟수': '1410회 인용20112012201320142015201620172018201920202021202220239717115414114415310511111454486055', '페이지': '252303', '학술 문서': 'Observation of a centrality-dependent dijet asymmetry in lead-lead collisions at s NN= 2.76 TeV with the ATLAS detector at the LHCG Aad, B Abbott, J Abdallah, AA Abdelalim…\\xa0- Physical review letters, 20101407회 인용 관련 학술자료 전체 104개의 버전 Observation of a centrality-dependent dijet asymmetry in lead-lead collisions at vsNN= 2.76 Tev with the ATLASO detector at the LHCG Aad, A De Santo, F Salvatore, C ATLAS\\xa0- Physical Review Letters, 20104회 인용 관련 학술자료 전체 2개의 버전 Observation of a centrality-dependent dijet asymmetry in lead-lead collisions at√ sNN= 2.76 Tev with the ATLASO detector at the LHCATLAS Collaboration - 2010관련 학술자료 전체 19개의 버전 Observation of a Centrality‐Dependent Dijet Asymmetry in Lead‐Lead Collisions at√ SNN= 2.76 TeV with the ATLAS Detector at LHCJ Salt Cairols, JA Valls Ferrer, S Cabrera Urbán…\\xa0- Physical Review Letters, 2010, vol. 105, p. 252303-1\\xa0…, 2010관련 학술자료 전체 2개의 버전 ', '호': '25'}, title='Observation of a Centrality-Dependent Dijet Asymmetry in Lead-Lead Collisions at  with the ATLAS Detector at the LHC', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Observation of a centrality-dependent dijet asymmetry in lead-lead collisions at √sNN=2.76 Tev with the ATLASO detector at the LHC': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010', '게시자': 'American Physical Society', '권': '105', '설명': 'By using the ATLAS detector, observations have been made of a centrality-dependent dijet asymmetry in the collisions of lead ions at the Large Hadron Collider. In a sample of lead-lead events with a per-nucleon center of mass energy of 2.76 TeV, selected with a minimum bias trigger, jets are reconstructed in fine-grained, longitudinally segmented electromagnetic and hadronic calorimeters. The transverse energies of dijets in opposite hemispheres are observed to become systematically more unbalanced with increasing event centrality leading to a large number of events which contain highly asymmetric dijets. This is the first observation of an enhancement of events with such large dijet asymmetries, not observed in proton-proton collisions, which may point to an interpretation in terms of strong jet energy loss in a hot, dense medium.', '저널': 'Physical Review Letters', '저자': \"MS Alam, J Ernst, V Rojo, S Bahinipati, NJ Buchanan, K Chan, L Chen, DM Gingrich, MS Kim, S Liu, J Lu, RW Moore, JL Pinfold, N Soni, S Subramania, O Cakir, AK Ciftci, R Ciftci, S Persembe, Yildiz H Duran, M Yilmaz, S Sultansoy, Cakir I Turk, Bella L Aperio, O Arnaez, B Aubert, M Aurousseau, N Berger, J Colas, L Di Ciaccio, TKO Doan, M El Kacimi, S Elles, P Ghez, M Gouanère, C Goy, T Guillemin, L Helary, T Hryn'ova, P Iengo, G Ionescu, A Jeremie, S Jézéquel, M Kataoka, J Labbe, R Lafaye, S Laplace, N Massol, P Perrodo, H Przysiezniak, G Sauvage, T Todorov, I Wingerter-Seez, R Zitoun, Y Zolnierowski, L Asquith, RE Blair, S Chekanov, JW Dawson, D Fellmann, GF Gieraltowski, VJ Guarino, D Hill, N Hill, K Karr, T Lecompte, D Malon, EN May, L Nodulman, A Paramonov, LE Price, J Proudfoot, Ferrando BM Salvachua, JL Schlereth, RW Stanek, DG Underwood, P Van Gemmeren, A Vaniachine, R Yoshida, J Zhang, E Cheu, KA Johns, V Kaushik, CL Lampen, W Lampl, X Lei, P Loch, P Mal, F Rühr, JP Rutherfoord, L Shaver, MA Shupe, EW Varnes, A Brandt, K De, A Farbin, H Kim, P Nilsson, N Ozturk, R Pravahan, E Sarkisyan-Grinbaum, M Sosebee, B Spurlock, AR Stradling, G Usai, A Vartapetian, A White, J Yu, A Antonaki, D Fassouliotis, V Giakoumopoulou, N Giokaris, P Ioannou, C Kourkoumelis, A Manousakis-Katsikakis, G Tzanakos, C Vellidis, T Alexopoulos, R Avramidou, M Dris, A Filippas, M Fokitis, EN Gazis, F Georgatos, G Iakovidis, E Katsoufis, S Maltezos, E Mountricha, E Panagiotopoulou, TD Papadopoulou, P Savva, C Tsarouchas, G Tsipolitis, S Vlachos, L Xaplanteris, M Aliyev, N Huseynov, F Khalil-Zada, S Rzaeva, J Abdallah, M Bosman, MP Casado, M Cavalli-Sforza, MC Conidi, B Demirkoz, M Dosil, Curull X Espinal, L Fiorini, S Grinstein, C Helsens\", '전체 인용횟수': '1410회 인용20112012201320142015201620172018201920202021202220239717115414114415310511111454486055', '페이지': '252303-252303', '학술 문서': 'Observation of a centrality-dependent dijet asymmetry in lead-lead collisions at s NN= 2.76 TeV with the ATLAS detector at the LHCG Aad, B Abbott, J Abdallah, AA Abdelalim…\\xa0- Physical review letters, 20101407회 인용 관련 학술자료 전체 104개의 버전 Observation of a centrality-dependent dijet asymmetry in lead-lead collisions at vsNN= 2.76 Tev with the ATLASO detector at the LHCG Aad, A De Santo, F Salvatore, C ATLAS\\xa0- Physical Review Letters, 20104회 인용 관련 학술자료 전체 2개의 버전 Observation of a centrality-dependent dijet asymmetry in lead-lead collisions at√ sNN= 2.76 Tev with the ATLASO detector at the LHCATLAS Collaboration - 2010관련 학술자료 전체 19개의 버전 ', '호': '25'}, title='Observation of a centrality-dependent dijet asymmetry in lead-lead collisions at √sNN=2.76 Tev with the ATLASO detector at the LHC', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Observation of a new particle in the search for the Standard Model Higgs boson with the ATLAS detector at the LHC': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012/9/17', '게시자': 'North-Holland', '권': '716', '설명': 'A search for the Standard Model Higgs boson in proton–proton collisions with the ATLAS detector at the LHC is presented. The datasets used correspond to integrated luminosities of approximately 4.8 fb−1 collected at s=7 TeV in 2011 and 5.8 fb−1 at s=8 TeV in 2012. Individual searches in the channels H→ZZ(⁎)→4ℓ, H→γγ and H→WW(⁎)→eνμν in the 8 TeV data are combined with previously published results of searches for H→ZZ(⁎), WW(⁎), bb¯ and τ+τ− in the 7 TeV data and results from improved analyses of the H→ZZ(⁎)→4ℓ and H→γγ channels in the 7 TeV data. Clear evidence for the production of a neutral boson with a measured mass of 126.0±0.4(stat)±0.4(sys)\\xa0GeV is presented. This observation, which has a significance of 5.9 standard deviations, corresponding to a background fluctuation probability of 1.7×10−9, is compatible with the production and decay of the Standard Model Higgs boson.', '저널': 'Physics Letters B', '저자': 'Georges Aad, Tatevik Abajyan, B Abbott, J Abdallah, S Abdel Khalek, Ahmed Ali Abdelalim, R Aben, B Abi, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, BS Acharya, L Adamczyk, DL Adams, TN Addy, J Adelman, S Adomeit, P Adragna, T Adye, S Aefsky, JA Aguilar-Saavedra, M Agustoni, M Aharrouche, SP Ahlen, F Ahles, A Ahmad, M Ahsan, G Aielli, T Akdogan, TPA Åkesson, G Akimoto, AV Akimov, MS Alam, MA Alam, J Albert, S Albrand, M Aleksa, IN Aleksandrov, F Alessandria, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, M Aliev, G Alimonti, J Alison, BMM Allbrooke, PP Allport, SE Allwood-Spiers, J Almond, A Aloisio, R Alon, Alejandro Alonso, F Alonso, A Altheimer, B Alvarez Gonzalez, MG Alviggi, K Amako, C Amelung, VV Ammosov, SP Amor Dos Santos, A Amorim, N Amram, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, M-L Andrieux, XS Anduaga, S Angelidakis, P Anger, A Angerami, F Anghinolfi, A Anisenkov, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, S Aoun, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, ATH Arce, S Arfaoui, J-F Arguin, E Arik, M Arik, AJ Armbruster, O Arnaez, V Arnal, C Arnault, A Artamonov, G Artoni, D Arutinov, S Asai, S Ask, B Åsman, L Asquith, K Assamagan, A Astbury, M Atkinson, B Aubert, E Auge, K Augsten, M Aurousseau, G Avolio, R Avramidou, D Axen, G Azuelos, Y Azuma, MA Baak, G Baccaglioni, C Bacci, AM Bach, H Bachacou, K Bachas, M Backes, M Backhaus, J Backus Mayes, E Badescu, P Bagnaia, S Bahinipati, Y Bai, DC Bailey, T Bain, JT Baines, OK Baker, MD Baker, S Baker, P Balek, E Banas, P Banerjee, Sw Banerjee, D Banfi, A Bangert, V Bansal, HS Bansil', '전체 인용횟수': '21838회 인용20122013201420152016201720182019202020212022202354225002569256222752013209516931474144913831134', '페이지': '1-29', '학술 문서': 'Observation of a new particle in the search for the Standard Model Higgs boson with the ATLAS detector at the LHCG Aad, T Abajyan, B Abbott, J Abdallah, SA Khalek…\\xa0- Physics Letters B, 201212890회 인용 관련 학술자료 전체 121개의 버전 Observation of a new particle in the search for the Standard Model Higgs boson with the ATLAS detector at the LHCAtlas Collaboration\\xa0- arXiv preprint arXiv:1207.7214, 201210725회 인용 관련 학술자료 전체 13개의 버전 Search for extra dimensions using diphoton events in 7 TeV proton-proton collisions with the ATLAS detector ATLAS Collaboration*V Mitsou, JA Fuster Verdú, A Ferrer Soria…\\xa0- Physics Letters B, 2012, vol. 710, num. 4-5, p. 538-556, 20125회 인용 관련 학술자료 전체 2개의 버전 PLB 716, 1 (2012)*G Aad, ATLAS Collaboration\\xa0- arXiv preprint arXiv:1207.72143회 인용 관련 학술자료 ATLAS Collaboration*G Aad - 2012관련 학술자료 ', '호': '1'}, title='Observation of a new particle in the search for the Standard Model Higgs boson with the ATLAS detector at the LHC', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Oral rivaroxaban for symptomatic venous thromboembolism': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/12/23', '게시자': 'Massachusetts Medical Society', '권': '363', '설명': ' Background Rivaroxaban, an oral factor Xa inhibitor, may provide a simple, fixed-dose regimen for treating acute deep-vein thrombosis (DVT) and for continued treatment, without the need for laboratory monitoring. Methods We conducted an open-label, randomized, event-driven, noninferiority study that compared oral rivaroxaban alone (15 mg twice daily for 3 weeks, followed by 20 mg once daily) with subcutaneous enoxaparin followed by a vitamin K antagonist (either warfarin or acenocoumarol) for 3, 6, or 12 months in patients with acute, symptomatic DVT. In parallel, we carried out a double-blind, randomized, event-driven superiority study that compared rivaroxaban alone (20 mg once daily) with placebo for an additional 6 or 12 months in patients who had completed 6 to 12 months of treatment for venous thromboembolism. The primary efficacy outcome for both studies was recurrent venous\\xa0…', '저널': 'New England Journal of Medicine', '저자': 'Einstein Investigators', '전체 인용횟수': '3768회 인용2011201220132014201520162017201820192020202120222023122240332375397399334324263289242184160', '페이지': '2499-2510', '학술 문서': 'Oral rivaroxaban for symptomatic venous thromboembolismEinstein Investigators\\xa0- New England Journal of Medicine, 20103744회 인용 관련 학술자료 전체 16개의 버전 Oral rivaroxaban for symptomatic venous thromboembolism*GW Landman, ROB Gans\\xa0- New England Journal of Medicine, 201132회 인용 관련 학술자료 전체 3개의 버전 ', '호': '26'}, title='Oral rivaroxaban for symptomatic venous thromboembolism', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Observation of electron-antineutrino disappearance at Daya Bay': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012/4/23', '게시자': 'American Physical Society', '권': '108', '설명': 'The Daya Bay Reactor Neutrino Experiment has measured a nonzero value for the neutrino mixing angle θ 13 with a significance of 5.2 standard deviations. Antineutrinos from six 2.9 G W th reactors were detected in six antineutrino detectors deployed in two near (flux-weighted baseline 470 m and 576 m) and one far (1648 m) underground experimental halls. With a 43 000 ton–G W th–day live-time exposure in 55 days, 10 416 (80 376) electron-antineutrino candidates were detected at the far hall (near halls). The ratio of the observed to expected number of antineutrinos at the far hall is R= 0.940±0.011 (stat.)±0.004 (syst.). A rate-only analysis finds sin\\ufeff 2 2 θ 13= 0.092±0.016 (stat.)±0.005 (syst.) in a three-neutrino framework.', '저널': 'Physical Review Letters', '저자': 'FP An, JZ Bai, AB Balantekin, HR Band, D Beavis, W Beriguete, M Bishai, S Blyth, K Boddy, RL Brown, B Cai, GF Cao, J Cao, R Carr, WT Chan, JF Chang, Y Chang, C Chasman, HS Chen, HY Chen, SJ Chen, SM Chen, XC Chen, XH Chen, XS Chen, Y Chen, YX Chen, JJ Cherwinka, MC Chu, JP Cummings, ZY Deng, YY Ding, MV Diwan, L Dong, E Draeger, XF Du, DA Dwyer, WR Edwards, SR Ely, SD Fang, JY Fu, ZW Fu, LQ Ge, V Ghazikhanian, RL Gill, J Goett, M Gonchar, GH Gong, H Gong, YA Gornushkin, LS Greenler, WQ Gu, MY Guan, XH Guo, RW Hackenburg, RL Hahn, S Hans, M He, Q He, WS He, KM Heeger, YK Heng, P Hinrichs, TH Ho, YK Hor, YB Hsiung, BZ Hu, T Hu, HX Huang, HZ Huang, PW Huang, X Huang, XT Huang, Patrick Huber, Z Isvan, DE Jaffe, S Jetter, XL Ji, XP Ji, HJ Jiang, WQ Jiang, JB Jiao, RA Johnson, L Kang, SH Kettell, M Kramer, KK Kwan, MW Kwok, T Kwok, CY Lai, WC Lai, WH Lai, K Lau, L Lebanowski, J Lee, MKP Lee, R Leitner, JKC Leung, KY Leung, CA Lewis, B Li, F Li, GS Li, J Li, QJ Li, SF Li, WD Li, XB Li, XN Li, XQ Li, Y Li, ZB Li, H Liang, J Liang, CJ Lin, Guey-Lin Lin, SK Lin, SX Lin, YC Lin, JJ Ling, Jonathan M Link, L Littenberg, BR Littlejohn, BJ Liu, C Liu, DW Liu, H Liu, JC Liu, JL Liu, S Liu, X Liu, YB Liu, C Lu, HQ Lu, A Luk, KB Luk, T Luo, XL Luo, LH Ma, QM Ma, XB Ma, XY Ma, YQ Ma, B Mayes, KT McDonald, MC McFarlane, RD McKeown, Y Meng, D Mohapatra, JE Morgan', '전체 인용횟수': '3296회 인용201220132014201520162017201820192020202120222023366573429374321230225176172142133126', '페이지': '171803', '학술 문서': 'Observation of electron-antineutrino disappearance at Daya BayFP An, JZ Bai, AB Balantekin, HR Band, D Beavis…\\xa0- Physical Review Letters, 20123296회 인용 관련 학술자료 전체 31개의 버전 ', '호': '17'}, title='Observation of electron-antineutrino disappearance at Daya Bay', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The BABAR detector': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2002/2/21', '게시자': 'North-Holland', '권': '479', '설명': 'BABAR, the detector for the SLAC PEP-II asymmetric e+e− B Factory operating at the ϒ(4S) resonance, was designed to allow comprehensive studies of CP-violation in B-meson decays. Charged particle tracks are measured in a multi-layer silicon vertex tracker surrounded by a cylindrical wire drift chamber. Electromagnetic showers from electrons and photons are detected in an array of CsI crystals located just inside the solenoidal coil of a superconducting magnet. Muons and neutral hadrons are identified by arrays of resistive plate chambers inserted into gaps in the steel flux return of the magnet. Charged hadrons are identified by dE/dx measurements in the tracking detectors and by a ring-imaging Cherenkov detector surrounding the drift chamber. The trigger, data acquisition and data-monitoring systems, VME- and network-based, are controlled by custom-designed online software. Details of the layout and\\xa0…', '저자': \"Bernard Aubert, A Bazan, A Boucham, D Boutigny, I De Bonis, J Favier, J-M Gaillard, A Jeremie, Y Karyotakis, T Le Flour, JP Lees, S Lieunard, P Petitpas, P Robbe, V Tisserand, K Zachariadou, Antimo Palano, GP Chen, JC Chen, ND Qi, G Rong, P Wang, YS Zhu, G Eigen, PL Reinertsen, B Stugu, B Abbott, GS Abrams, L Amerman, AW Borgland, AB Breon, DN Brown, J Button-Shafer, AR Clark, S Dardin, C Day, SF Dow, Q Fan, I Gaponenko, MS Gill, FR Goozen, SJ Gowdy, A Gritsan, Y Groysman, C Hernikl, RG Jacobsen, RC Jared, RW Kadel, J Kadyk, A Karcher, LT Kerth, I Kipnis, S Kluth, JF Kral, R Lafever, C LeClerc, ME Levi, SA Lewis, C Lionberger, T Liu, M Long, L Luo, G Lynch, P Luft, E Mandelli, M Marino, K Marks, C Matuk, AB Meyer, R Minor, A Mokhtarani, M Momayezi, M Nyman, PJ Oddone, J Ohnemus, D Oshatz, S Patton, M Pedrali-Noy, A Perazzo, C Peters, W Pope, M Pripstein, DR Quarrie, JE Rasson, NA Roe, A Romosan, MT Ronan, VG Shelkov, R Stone, PD Strother, AV Telnov, H von der Lippe, TF Weber, WA Wenzel, G Zizka, PG Bright-Thomas, CM Hawkes, A Kirk, DJ Knowles, SW O'Neale, AT Watson, NK Watson, T Deppermann, H Koch, J Krug, M Kunze, B Lewandowski, K Peters, H Schmuecker, M Steinke, JC Andress, NR Barlow, W Bhimji, N Chevalier, PJ Clark, WN Cottingham, N De Groot, N Dyce, B Foster, A Mass, JD McFall, D Wallom, FF Wilson, K Abe, C Hearty, JA McKenna, D Thiessen, B Camanzi, TJ Harrison, AK McKemey, J Tinslay, EI Antohin, VE Blinov, AD Bukin, DA Bukin, AR Buzykaev, MS Dubrovin, VB Golubev, VN Ivanchenko, GM Kolachev, AA Korol, EA Kravchenko, SF Mikhailov, AP Onuchin, AA Salnikov, SI Serednyakov, Yu I Skovpen, VI Telnov, AN Yushkov, J Booth\", '전체 인용횟수': '3249회 인용200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220233110719326126427125026028921818215815982886454655348303360', '출처': 'Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment', '페이지': '1-116', '학술 문서': 'The BABAR detectorB Aubert, A Bazan, A Boucham, D Boutigny, I De Bonis…\\xa0- Nuclear Instruments and Methods in Physics Research\\xa0…, 20023249회 인용 관련 학술자료 전체 50개의 버전 ', '호': '1'}, title='The BABAR detector', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The belle detector': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2002/2/21', '게시자': 'North-Holland', '권': '479', '설명': 'The Belle detector was designed and constructed to carry out quantitative studies of rare B-meson decay modes with very small branching fractions using an asymmetric e+e− collider operating at the ϒ(4S) resonance, the KEK-B-factory. Such studies require data samples containing ∼107 B-meson decays. The Belle detector is configured around a 1.5 T  superconducting solenoid and iron structure surrounding the KEK-B beams at the Tsukuba interaction region. B-meson decay vertices are measured by a silicon vertex detector situated just outside of a cylindrical beryllium beam pipe. Charged particle tracking is performed by a wire drift chamber (CDC). Particle identification is provided by dE/dx measurements in CDC, aerogel threshold Cherenkov counter and time-of-flight counter placed radially outside of CDC. Electromagnetic showers are detected in an array of CsI(Tl) crystals located inside the solenoid coil\\xa0…', '저자': 'A Abashian, K Gotow, N Morgan, L Piilonen, S Schrenk, K Abe, I Adachi, JP Alexander, K Aoki, S Behari, Y Doi, R Enomoto, H Fujii, Y Fujita, Y Funahashi, J Haba, H Hamasaki, T Haruyama, K Hayashi, Y Higashi, N Hitomi, S Igarashi, Y Igarashi, T Iijima, Hirokazu Ikeda, Hitomi Ikeda, R Itoh, M Iwai, H Iwasaki, Y Iwasaki, KK Joo, K Kasami, N Katayama, M Kawai, H Kichimi, T Kobayashi, S Koike, Y Kondo, MH Lee, Y Makida, A Manabe, T Matsuda, T Murakami, S Nagayama, M Nakao, T Nozaki, K Ogawa, R Ohkubo, Y Ohnishi, H Ozaki, H Sagawa, M Saito, Y Sakai, T Sasaki, N Sato, T Sumiyoshi, J Suzuki, JI Suzuki, S Suzuki, F Takasaki, K Tamai, M Tanaka, T Tatomi, T Tsuboyama, K Tsukada, T Tsukamoto, S Uehara, N Ujiie, S Uno, B Yabsley, Y Yamada, H Yamaguchi, H Yamaoka, Y Yamaoka, M Yamauchi, Y Yoshimura, H Zhao, R Abe, G Iwai, T Kawasaki, H Miyata, K Shimada, S Takahashi, N Tamura, H Hanada, T Nagamine, M Nakajima, T Nakajima, S Narita, M Sanpei, T Takayama, M Ueki, M Yamaga, A Yamaguchi, BS Ahn, JS Kang, Hyunwoo Kim, CW Park, H Park, HS Ahn, HK Jang, CH Kim, SK Kim, SH Lee, CS Park, E Won, H Aihara, T Higuchi, H Kawai, T Matsubara, T Nakadaira, H Tajima, J Tanaka, T Tomura, M Yokoyama, M Akatsu, K Fujimoto, M Hirose, K Inami, A Ishikawa, S Itami, T Kani, T Matsumoto, I Nagai, T Okabe, T Oshima, K Senyo, A Sugi, A Sugiyama, S Suitoh, M Tomoto, K Yoshida, R Akhmetshin, P Chang, Y Chao, YQ Chen, WS Hou, SC Hsu, HC Huang, TJ Huang, MC Lee, RS Lu, JC Peng, KC Peng, S Sahu, HF Sung, KL Tsai, K Ueno, CC Wang, MZ Wang', '전체 인용횟수': '2291회 인용200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220239428299133113135153128108102102118911141201151258761928765', '출처': 'Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment', '페이지': '117-232', '학술 문서': 'The belle detectorA Abashian, K Gotow, N Morgan, L Piilonen, S Schrenk…\\xa0- Nuclear Instruments and Methods in Physics Research\\xa0…, 20022291회 인용 관련 학술자료 전체 12개의 버전 ', '호': '1'}, title='The belle detector', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Observation of  Resonances Consistent with Pentaquark States in  Decays': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/8/12', '게시자': 'American Physical Society', '권': '115', '설명': 'Observations of exotic structures in the J/ψ p channel, which we refer to as charmonium-pentaquark states, in Λ b 0→ J/ψ K− p decays are presented. The data sample corresponds to an integrated luminosity of 3 fb− 1 acquired with the LHCb detector from 7 and 8 TeV p p collisions. An amplitude analysis of the three-body final state reproduces the two-body mass and angular distributions. To obtain a satisfactory fit of the structures seen in the J/ψ p mass spectrum, it is necessary to include two Breit-Wigner amplitudes that each describe a resonant state. The significance of each of these resonances is more than 9 standard deviations. One has a mass of 4380±8±29 MeV and a width of 205±18±86 MeV, while the second is narrower, with a mass of 4449.8±1.7±2.5 MeV and a width of 39±5±19 MeV. The preferred J P assignments are of opposite parity, with one state having spin 3/2 and the other 5/2.', '저널': 'Physical review letters', '저자': 'Roel Aaij, B Adeva, M Adinolfi, A Affolder, Z Ajaltouni, S Akar, J Albrecht, F Alessio, M Alexander, S Ali, G Alkhazov, P Alvarez Cartelle, AA Alves Jr, S Amato, S Amerio, Y Amhis, L An, L Anderlini, J Anderson, G Andreassi, M Andreotti, JE Andrews, RB Appleby, O Aquines Gutierrez, F Archilli, P d’Argent, A Artamonov, M Artuso, E Aslanides, G Auriemma, M Baalouch, S Bachmann, JJ Back, A Badalov, C Baesso, W Baldini, RJ Barlow, C Barschel, S Barsuk, W Barter, V Batozskaya, V Battista, A Bay, L Beaucourt, J Beddow, F Bedeschi, I Bediaga, LJ Bel, V Bellee, N Belloli, I Belyaev, E Ben-Haim, G Bencivenni, S Benson, J Benton, A Berezhnoy, R Bernet, A Bertolin, M-O Bettler, M Van Beuzekom, A Bien, S Bifani, P Billoir, T Bird, A Birnkraut, A Bizzeti, T Blake, F Blanc, J Blouw, S Blusk, V Bocci, A Bondar, N Bondar, W Bonivento, S Borghi, M Borsato, TJV Bowcock, E Bowen, C Bozzi, S Braun, M Britsch, T Britton, J Brodzicka, NH Brook, A Bursche, J Buytaert, S Cadeddu, R Calabrese, M Calvi, M Calvo Gomez, P Campana, D Campora Perez, L Capriotti, Angelo Carbone, G Carboni, R Cardinale, A Cardini, P Carniti, L Carson, K Carvalho Akiba, G Casse, L Cassina, L Castillo Garcia, M Cattaneo, Ch Cauet, G Cavallero, R Cenci, M Charles, Ph Charpentier, M Chefdeville, S Chen, S-F Cheung, N Chiapolini, M Chrzaszcz, X Cid Vidal, G Ciezarek, PEL Clarke, M Clemencic, HV Cliff, J Closier, V Coco, J Cogan, E Cogneras, V Cogoni, L Cojocariu, G Collazuol, P Collins, A Comerma-Montells, A Contu, A Cook, M Coombes, S Coquereau, G Corti, M Corvo, B Couturier, GA Cowan, DC Craik, A Crocombe, M Cruz Torres, S Cunliffe, R Currie, C D’Ambrosio, E Dall’Occo, J Dalseno, PNY David, A Davis, K De Bruyn, S De Capua, M De Cian, JM De Miranda', '전체 인용횟수': '2132회 인용20152016201720182019202020212022202388341302267284225245192170', '페이지': '072001', '학술 문서': 'Observation of J/ψ p Resonances Consistent with Pentaquark States in Λ b 0→ J/ψ K− p DecaysR Aaij, B Adeva, M Adinolfi, A Affolder, Z Ajaltouni…\\xa0- Physical review letters, 20152132회 인용 관련 학술자료 전체 71개의 버전 ', '호': '7'}, title='Observation of  Resonances Consistent with Pentaquark States in  Decays', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Jet energy scale and resolution in the CMS experiment in pp collisions at 8 TeV': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '게시자': 'IOP Publishing', '설명': 'The state-of-the-art techniques used in the CMS experiment at the CERN LHC for jet energy scale (JES) and jet energy resolution (JER) calibration are presented, based on a data sample corresponding to an integrated luminosity of 19.7 fb− 1 collected in proton-proton collisions at a center-of-mass energy of 8 TeV. Jets are the experimental signatures of energetic quarks and gluons produced in high-energy processes. Like all experimentally-reconstructed objects, jets need to be calibrated in order to have the correct energy scale: this is the aim of the jet energy corrections (JEC). The detailed understanding of both the energy scale and the transverse momentum resolution of the jets is of crucial importance for many physics analyses, and a leading component of their associated systematic uncertainties. Improvements made in understanding the JES in the recent years have resulted in very precise measurements\\xa0…', '저자': \"Vardan Khachatryan, Albert M Sirunyan, Armen Tumasyan, Wolfgang Adam, E Asilar, Thomas Bergauer, Johannes Brandstetter, Erica Brondolin, Marko Dragicevic, J Ero, Martin Flechl, G Gomez, C Silkworth, Y Iiyama, E Laird, S Undleeb, B Stieger, P Turner, S Jindariani, H Brun, N Varelas, J Varela, P Giacomelli, J Tuominiemi, MC Fouz, Y Eshaq, M Trovato, Z Wu, S Thuer, P Petkov, M Zakaria, B Bilki, S Basegmez, MF Sevilla, F Ball, E Tuovinen, I Volobouev, R Dell'Orso, W Clarida, N Heracleous, S Carrillo Moreno, S Wasserbaech, K Dilsiz, G Landsberg, S Brandt, I Ahmed, S Durgut, L Wendland, L Beck, RP Gandrajula, JW Gary, M Haytmyradov, G Rolandi, M Naimuddin, JG Bian, MABM Ali, V Palichik, S Liu, S Lacaprara, E Kwon, AT Serban, H Stadie, V Khristenko, S Sarkar, JP Merlo, H Mermerkaya, B Dahmes, M Ahmad, A Dierlamm, A Safonov, Z Mao, G Quast, A Mestvirishvili, A Escalante Del Valle, JJ Brooke, V Cherepanov, A Grebenyuk, D Sabes, P Spagnolo, M Velasco, Y Choi, L Uvarov, S Jabeen, S Wilbur, F Pauss, G Masetti, RJ Wang, O Bouhali, A Custodio, A Brinkerhoff, N Dev, M Hildreth, C Jessop, DJ Karmgard, L Mundim, J Zhang, S Schael, N Tosi, F Romeo, A Lopez Virto, N Kellams, N Mccoll, Y Erdogan, G Majumder, J Wetzel, N De Filippis, M Stoye, M Kirakosyan, K Lannon, S Lynch, J Marco, N Marinelli, Y Mao, S Fink, M Bluj, E Halkiadakis, M Friedl, M Kovac, M Johnson, A Belloni, F Meng, M Finger, S Elgammal, C Mueller, M Narain, W Waltenberger, G Flugge, Y Takahashi, PR Dudero, F Frensch, Y Musienko, T Pearson, JR Vlimant, R Marco, GSF Stephans, M Planer, A Meyer, M De Palma, M Sharan, E Appelt, K Yi, E Clement, A Reinsvold, M Giffels, C Martinez Rivero, A Levine, J Talvitie, R Ruchti, G Smith, S Taroni\", '전체 인용횟수': '2013회 인용2016201720182019202020212022202318240443385260252238171', '학술 문서': 'Jet energy scale and resolution in the CMS experiment in pp collisions at 8 TeVV Khachatryan, AM Sirunyan, A Tumasyan, W Adam… - 20171852회 인용 관련 학술자료 전체 93개의 버전 The CMS trigger system*V Khachatryan, AM Sirunyan, A Tumasyan, W Adam…\\xa0- Journal of Instrumentation, 2017277회 인용 관련 학술자료 전체 50개의 버전 Jet energy scale and resolution in the CMS experiment in pp collisions at 8 TeVA Apyan, RA Barbieri, AA Baty, K Bierwagen… - 2017관련 학술자료 '}, title='Jet energy scale and resolution in the CMS experiment in pp collisions at 8 TeV', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Measurement of the Z/γ * boson transverse momentum distribution in pp collisions at s = 7  TeV with the ATLAS detector': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/9', '게시자': 'Springer Berlin Heidelberg', '권': '2014', '설명': 'This paper describes a measurement of the Z/γ* boson transverse momentum spectrum using ATLAS proton-proton collision data at a centre-of-mass energy of TeV at the LHC. The measurement is performed in the Z/γ*→ e+ e− and Z/γ*→ μ+ μ− channels, using data corresponding to an integrated luminosity of 4.7 fb− 1. Normalized differential cross sections as a function of the Z/γ* boson transverse momentum are measured for transverse momenta up to 800 GeV. The measurement is performed inclusively for Z/γ* rapidities up to 2.4, as well as in three rapidity bins. The channel results are combined, compared to perturbative and resummed QCD calculations and used to constrain the parton shower parameters of Monte Carlo generators.', '저널': 'Journal of High Energy Physics', '저자': 'Georges Aad, Brad Abbott, Jalal Abdallah, Samah Abdel Khalek, R Aben, B Abi, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, J Adelman, Stefanie Adomeit, T Adye, T Agatonovic-Jovin, JA Aguilar-Saavedra, M Agustoni, SP Ahlen, F Ahmadov, G Aielli, H Akerstedt, TPA Åkesson, G Akimoto, AV Akimov, GIAN LUIGI Alberghi, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, G Alimonti, L Alio, J Alison, BMM Allbrooke, LJ Allison, PP Allport, J Almond, A Aloisio, A Alonso, F Alonso, C Alpigiani, A Altheimer, B Alvarez Gonzalez, MG Alviggi, K Amako, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, S Angelidakis, I Angelozzi, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, JP Araque, ATH Arce, JF Arguin, S Argyropoulos, M Arik, AJ Armbruster, O Arnaez, V Arnal, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, A Ashkenazi, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, B Auerbach, K Augsten, M Aurousseau, G Avolio, G Azuelos, Y Azuma, MA Baak, C Bacci, H Bachacou, K Bachas, M Backes, M Backhaus, J Backus Mayes, E Badescu, P Bagiacchi, P Bagnaia, Y Bai, T Bain, JT Baines, OK Baker, S Baker, P Balek, F Balli, E Banas, Sw Banerjee, AAE Bannoura, V Bansal, HS Bansil, L Barak, SP Baranov, EL Barberio, D Barberis, M Barbero, T Barillari, M Barisonzi, T Barklow, N Barlow', '전체 인용횟수': '1704회 인용20142015201620172018201920202021202220231364154246328247187192106158', '페이지': '1-47', '학술 문서': 'Measurement of the Z/γ* boson transverse momentum distribution in pp collisions at s= 7$$\\\\sqrt {s}= 7$$ TeV with the ATLAS detectorG Aad, B Abbott, J Abdallah, S Abdel Khalek, R Aben…\\xa0- Journal of High Energy Physics, 20141692회 인용 관련 학술자료 전체 87개의 버전 Measurement of the Z/γ* boson transverse momentum distribution in pp collisions at√ s= 7 TeV with the ATLAS detector*G Aad, B Abbott, J Abdallah, S Abdel Khalek, R Aben… - 201420회 인용 관련 학술자료 ', '호': '9'}, title='Measurement of the Z/γ * boson transverse momentum distribution in pp collisions at s = 7  TeV with the ATLAS detector', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Design and construction of the BESIII detector': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/3/11', '게시자': 'North-Holland', '권': '614', '설명': 'This paper will discuss the design and construction of BESIII, which is designed to study physics in the τ-charm energy region utilizing the new high luminosity BEPCII double ring e+e− collider. The expected performance will be given based on Monte Carlo simulations and results of cosmic ray and beam tests. In BESIII, tracking and momentum measurements for charged particles are made by a cylindrical multilayer drift chamber in a 1T superconducting solenoid. Charged particles are identified with a time-of-flight system based on plastic scintillators in conjunction with dE/dx (energy loss per unit pathlength) measurements in the drift chamber. Energies of electromagnetic showers are measured by a CsI(Tl) crystal calorimeter located inside the solenoid magnet. Muons are identified by arrays of resistive plate chambers in a steel magnetic yoke for the flux return. The level 1 trigger system, data acquisition system\\xa0…', '저널': 'Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment', '저자': 'Medina Ablikim, ZH An, JZ Bai, Niklaus Berger, JM Bian, X Cai, GF Cao, XX Cao, JF Chang, C Chen, G Chen, HC Chen, HX Chen, J Chen, JC Chen, LP Chen, P Chen, XH Chen, YB Chen, ML Chen, YP Chu, XZ Cui, HL Dai, ZY Deng, MY Dong, SX Du, ZZ Du, J Fang, CD Fu, CS Gao, MY Gong, WX Gong, SD Gu, BJ Guan, J Guan, YN Guo, JF Han, KL He, M He, X He, YK Heng, ZL Hou, HM Hu, T Hu, B Huang, J Huang, SK Huang, YP Huang, Q Ji, XB Ji, XL Ji, LK Jia, LL Jiang, XS Jiang, DP Jin, S Jin, Y Jin, YF Lai, GK Lei, F Li, G Li, HB Li, HS Li, J Li, JC Li, QJ Li, L Li, RB Li, RY Li, WD Li, WG Li, XN Li, XP Li, XR Li, YR Li, W Li, DX Lin, BJ Liu, CX Liu, F Liu, GM Liu, H Liu, HM Liu, HW Liu, JB Liu, LF Liu, Q Liu, QG Liu, SD Liu, WJ Liu, X Liu, XZ Liu, Y Liu, YJ Liu, ZA Liu, ZQ Liu, ZX Liu, JG Lu, T Lu, YP Lu, XL Luo, HL Ma, QM Ma, X Ma, XY Ma, ZP Mao, J Min, XH Mo, J Nie, ZD Nie, RG Ping, S Qian, Q Qiao, G Qin, ZH Qin, JF Qiu, RG Liu, ZY Ren, G Rong, L Shang, DL Shen, XY Shen, HY Sheng, YF Shi, LW Song, WY Song, DH Sun, GX Sun, HS Sun, LJ Sun, SS Sun, XD Sun, YZ Sun, ZJ Sun, JP Tan, SQ Tang, X Tang, N Tao, HL Tian, YR Tian, X Wan, DY Wang, JK Wang, JZ Wang, K Wang, KX Wang, L Wang, LJ Wang, LS Wang, M Wang', '전체 인용횟수': '1697회 인용2010201120122013201420152016201720182019202020212022202322417896106150142194128152101147173160', '페이지': '345-399', '학술 문서': 'Design and construction of the BESIII detectorM Ablikim, ZH An, JZ Bai, N Berger, JM Bian, X Cai…\\xa0- Nuclear Instruments and Methods in Physics Research\\xa0…, 20101697회 인용 관련 학술자료 전체 18개의 버전 ', '호': '3'}, title='Design and construction of the BESIII detector', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Observation of Large  Violation in the Neutral  Meson System': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2001/8/14', '게시자': 'American Physical Society', '권': '87', '설명': 'We present a measurement of the standard model CP violation parameter sin 2 φ 1 based on a 29.1 fb− 1 data sample collected at the ϒ (4 S) resonance with the Belle detector at the KEKB asymmetric-energy e+ e− collider. One neutral B meson is fully reconstructed as a J/ψ K S, ψ (2 S) K S, χ c 1 K S, η c K S, J/ψ K L, or J/ψ K* 0 decay and the flavor of the accompanying B meson is identified from its decay products. From the asymmetry in the distribution of the time intervals between the two B meson decay points, we determine sin 2 φ 1= 0.99±0.14 (stat)±0.06 (syst). We conclude that we have observed CP violation in the neutral B meson system.', '저널': 'Physical review letters', '저자': 'Kazuo Abe, R Abe, I Adachi, Byoung Sup Ahn, H Aihara, M Akatsu, G Alimonti, K Asai, M Asai, Y Asano, T Aso, V Aulchenko, T Aushev, AM Bakich, E Banas, S Behari, PK Behera, D Beiline, A Bondar, A Bozek, TE Browder, BCK Casey, P Chang, Y Chao, K-F Chen, BG Cheon, R Chistov, S-K Choi, Y Choi, LY Dong, J Dragic, A Drutskoy, S Eidelman, V Eiges, Y Enari, R Enomoto, CW Everton, F Fang, H Fujii, C Fukunaga, M Fukushima, N Gabyshev, A Garmash, TJ Gershon, A Gordon, K Gotow, H Guler, R Guo, J Haba, H Hamasaki, K Hanagaki, F Handa, K Hara, T Hara, NC Hastings, H Hayashii, M Hazumi, EM Heenan, Y Higasino, I Higuchi, T Higuchi, T Hirai, H Hirano, T Hojo, T Hokuue, Y Hoshi, K Hoshina, SR Hou, W-S Hou, S-C Hsu, H-C Huang, Y Igarashi, T Iijima, H Ikeda, K Ikeda, K Inami, A Ishikawa, H Ishino, R Itoh, G Iwai, H Iwasaki, Y Iwasaki, DJ Jackson, P Jalocha, HK Jang, M Jones, R Kagan, H Kakuno, J Kaneko, JH Kang, JS Kang, P Kapusta, N Katayama, H Kawai, Y Kawakami, N Kawamura, T Kawasaki, H Kichimi, DW Kim, Heejong Kim, HJ Kim, Hyunwoo Kim, SK Kim, TH Kim, K Kinoshita, S Kobayashi, S Koishi, H Konishi, K Korotushenko, P Krokovny, R Kulasiri, S Kumar, T Kuniya, E Kurihara, A Kuzmin, Y-J Kwon, JS Lange, G Leder, MH Lee, SH Lee, C Leonidopoulos, Y-S Lin, D Liventsev, R-S Lu, J MacNaughton, D Marlow, T Matsubara, S Matsui, S Matsumoto, T Matsumoto, Y Mikami, K Misono, K Miyabayashi, H Miyake, H Miyata, LC Moffitt, GR Moloney, GF Moorhead, S Mori, T Mori, A Murakami, T Nagamine, Y Nagasaka, Y Nagashima, T Nakadaira, T Nakamura, E Nakano, M Nakao, H Nakazawa, JW Nam', '전체 인용횟수': '1640회 인용20012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023551981429258705770513739576168566367615667516260', '페이지': '091802', '학술 문서': 'Observation of large CP violation in the neutral B meson systemK Abe, R Abe, I Adachi, BS Ahn, H Aihara, M Akatsu…\\xa0- Physical review letters, 20011640회 인용 관련 학술자료 전체 24개의 버전 ', '호': '9'}, title='Observation of Large  Violation in the Neutral  Meson System', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Observation of  Violation in the  Meson System': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2001/8/14', '게시자': 'American Physical Society', '권': '87', '설명': 'We present an updated measurement of time-dependent CP-violating asymmetries in neutral B decays with the BABAR detector at the PEP-II asymmetric B Factory at SLAC. This result uses an additional sample of ϒ (4 S) decays collected in 2001, bringing the data available to 32× 10 6 B B pairs. We select events in which one neutral B meson is fully reconstructed in a final state containing charmonium and the flavor of the other neutral B meson is determined from its decay products. The amplitude of the CP-violating asymmetry, which in the standard model is proportional to sin 2 β, is derived from the decay time distributions in such events. The result sin 2 β= 0.59±0.14 (stat)±0.05 (syst) establishes CP violation in the B 0 meson system. We also determine| λ|= 0.93±0.09 (stat)±0.03 (syst), consistent with no direct CP violation.', '저널': 'Physical review letters', '저자': \"Bernard Aubert, D Boutigny, J-M Gaillard, A Hicheur, Y Karyotakis, JP Lees, P Robbe, V Tisserand, Antimo Palano, GP Chen, JC Chen, ND Qi, G Rong, P Wang, YS Zhu, G Eigen, PL Reinertsen, B Stugu, B Abbott, GS Abrams, AW Borgland, AB Breon, DN Brown, J Button-Shafer, RN Cahn, AR Clark, MS Gill, AV Gritsan, Y Groysman, RG Jacobsen, RW Kadel, J Kadyk, LT Kerth, S Kluth, Yu G Kolomensky, JF Kral, C LeClerc, ME Levi, T Liu, G Lynch, AB Meyer, M Momayezi, PJ Oddone, A Perazzo, M Pripstein, NA Roe, A Romosan, MT Ronan, VG Shelkov, AV Telnov, WA Wenzel, MS Zisman, PG Bright-Thomas, TJ Harrison, CM Hawkes, DJ Knowles, SW O'Neale, RC Penny, AT Watson, NK Watson, T Deppermann, K Goetzen, H Koch, J Krug, M Kunze, B Lewandowski, K Peters, H Schmuecker, M Steinke, JC Andress, NR Barlow, W Bhimji, N Chevalier, PJ Clark, WN Cottingham, N De Groot, N Dyce, B Foster, JD McFall, D Wallom, FF Wilson, K Abe, C Hearty, TS Mattison, JA McKenna, D Thiessen, S Jolly, AK McKemey, J Tinslay, VE Blinov, AD Bukin, DA Bukin, AR Buzykaev, VB Golubev, VN Ivanchenko, AA Korol, EA Kravchenko, AP Onuchin, AA Salnikov, SI Serednyakov, Yu I Skovpen, VI Telnov, AN Yushkov, D Best, AJ Lankford, M Mandelkern, S McMahon, DP Stoker, A Ahsan, K Arisaka, C Buchanan, S Chun, JG Branson, DB MacFarlane, S Prell, Sh Rahatlou, G Raven, V Sharma, C Campagnari, B Dahmes, PA Hart, N Kuznetsova, SL Levy, O Long, A Lu, JD Richman, W Verkerke, M Witherell, S Yellin, J Beringer, DE Dorfan, AM Eisner, A Frey, AA Grillo, M Grothe, CA Heusch, RP Johnson, W Kroeger, William S Lockman, T Pulliam, H Sadrozinski, T Schalk, RE Schmitz, BA Schumm, A Seiden, M Turri, W Walkowiak, DC Williams, MG Wilson, E Chen\", '전체 인용횟수': '1483회 인용20012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023612061478456635170472833555659454947544050435544', '페이지': '091801', '학술 문서': 'Observation of CP violation in the B 0 meson systemB Aubert, D Boutigny, JM Gaillard, A Hicheur…\\xa0- Physical review letters, 20011476회 인용 관련 학술자료 전체 43개의 버전 Observation of CP violation in the B0 meson systemC BABAR, JC Andress, NR Barlow, W Bhimji…\\xa0- Physical Review Letters, 200111회 인용 관련 학술자료 ', '호': '9'}, title='Observation of  Violation in the  Meson System', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Neutrino physics with JUNO': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/2/10', '게시자': 'IOP Publishing', '권': '43', '설명': 'The Jiangmen Underground Neutrino Observatory (JUNO), a 20 kton multi-purpose underground liquid scintillator detector, was proposed with the determination of the neutrino mass hierarchy (MH) as a primary physics goal. The excellent energy resolution and the large fiducial volume anticipated for the JUNO detector offer exciting opportunities for addressing many important topics in neutrino and astro-particle physics. In this document, we present the physics motivations and the anticipated performance of the JUNO detector for various proposed measurements. Following an introduction summarizing the current status and open issues in neutrino physics, we discuss how the detection of antineutrinos generated by a cluster of nuclear power plants allows the determination of the neutrino MH at a 3–4σ significance with six years of running of JUNO. The measurement of antineutrino spectrum with excellent energy resolution will also lead to\\xa0…', '저널': 'Journal of Physics G: Nuclear and Particle Physics', '저자': 'Fengpeng An, Guangpeng An, Qi An, Vito Antonelli, Eric Baussan, John Beacom, Leonid Bezrukov, Simon Blyth, Riccardo Brugnera, Margherita Buizza Avanzini, Jose Busto, Anatael Cabrera, Hao Cai, Xiao Cai, Antonio Cammi, Guofu Cao, Jun Cao, Yun Chang, Shaomin Chen, Shenjian Chen, Yixue Chen, Davide Chiesa, Massimiliano Clemenza, Barbara Clerbaux, Janet Conrad, Davide D’Angelo, Herve De Kerret, Zhi Deng, Ziyan Deng, Yayun Ding, Zelimir Djurcic, Damien Dornic, Marcos Dracos, Olivier Drapier, Stefano Dusini, Stephen Dye, Timo Enqvist, Donghua Fan, Jian Fang, Laurent Favart, Richard Ford, Marianne Goeger-Neff, Haonan Gan, Alberto Garfagnini, Marco Giammarchi, Maxim Gonchar, Guanghua Gong, Hui Gong, Michel Gonin, Marco Grassi, Christian Grewing, Mengyun Guan, Vic Guarino, Gang Guo, Wanlei Guo, Xin-Heng Guo, Caren Hagner, Ran Han, Miao He, Yuekun Heng, Yee Hsiung, Jun Hu, Shouyang Hu, Tao Hu, Hanxiong Huang, Xingtao Huang, Lei Huo, Ara Ioannisian, Manfred Jeitler, Xiangdong Ji, Xiaoshan Jiang, Cecile Jollet, Li Kang, Michael Karagounis, Narine Kazarian, Zinovy Krumshteyn, Andre Kruth, Pasi Kuusiniemi, Tobias Lachenmaier, Rupert Leitner, Chao Li, Jiaxing Li, Weidong Li, Weiguo Li, Xiaomei Li, Xiaonan Li, Yi Li, Yufeng Li, Zhi-Bing Li, Hao Liang, Guey-Lin Lin, Tao Lin, Yen-Hsun Lin, Jiajie Ling, Ivano Lippi, Dawei Liu, Hongbang Liu, Hu Liu, Jianglai Liu, Jianli Liu, Jinchang Liu, Qian Liu, Shubin Liu, Shulin Liu, Paolo Lombardi, Yongbing Long, Haoqi Lu, Jiashu Lu, Jingbin Lu, Junguang Lu, Bayarto Lubsandorzhiev, Livia Ludhova, Shu Luo, Vladimir Lyashuk, Randolph Moellenberg, Xubo Ma, Fabio Mantovani, Yajun Mao, Stefano M Mari, William F McDonough, Guang Meng, Anselmo Meregaglia, Emanuela Meroni, Mauro Mezzetto, Lino Miramonti, Thomas Mueller, Dmitry Naumov, Lothar Oberauer, Juan Pedro Ochoa-Ricoux, Alexander Olshevskiy, Fausto Ortica, Alessandro Paoloni, Haiping Peng, Jen-Chieh Peng, Ezio Previtali, Ming Qi, Sen Qian, Xin Qian, Yongzhong Qian, Zhonghua Qin, Georg Raffelt, Gioacchino Ranucci, Barbara Ricci, Markus Robens, Aldo Romani, Xiangdong Ruan, Xichao Ruan, Giuseppe Salamanna, Mike Shaevitz, Valery Sinev', '전체 인용횟수': '1336회 인용2015201620172018201920202021202220231380130171147185210209182', '페이지': '030401', '학술 문서': 'Neutrino physics with JUNOF An, G An, Q An, V Antonelli, E Baussan, J Beacom…\\xa0- Journal of Physics G: Nuclear and Particle Physics, 20161336회 인용 관련 학술자료 전체 29개의 버전 ', '호': '3'}, title='Neutrino physics with JUNO', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Observation of a Broad Structure in the  Mass Spectrum around ': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2005/9/28', '게시자': 'American Physical Society', '권': '95', '설명': 'We study initial-state radiation events, e+ e−→ γ ISR π+ π− J/ψ, with data collected with the BABAR detector. We observe an accumulation of events near 4.26 GeV/c 2 in the invariant-mass spectrum of π+ π− J/ψ. Fits to the mass spectrum indicate that a broad resonance with a mass of about 4.26 GeV/c 2 is required to describe the observed structure. The presence of additional narrow resonances cannot be excluded. The fitted width of the broad resonance is 50 to 90 MeV/c 2, depending on the fit hypothesis.', '저널': 'Physical review letters', '저자': 'Bernard Aubert, R Barate, D Boutigny, F Couderc, Y Karyotakis, JP Lees, V Poireau, V Tisserand, A Zghiche, E Grauges, A Palano, M Pappagallo, A Pompili, JC Chen, ND Qi, G Rong, P Wang, YS Zhu, G Eigen, I Ofte, B Stugu, GS Abrams, M Battaglia, AB Breon, DN Brown, J Button-Shafer, RN Cahn, E Charles, CT Day, MS Gill, AV Gritsan, Y Groysman, RG Jacobsen, RW Kadel, J Kadyk, LT Kerth, Yu G Kolomensky, G Kukartsev, G Lynch, LM Mir, PJ Oddone, TJ Orimoto, M Pripstein, NA Roe, MT Ronan, WA Wenzel, M Barrett, KE Ford, TJ Harrison, AJ Hart, CM Hawkes, SE Morgan, AT Watson, M Fritsch, K Goetzen, T Held, H Koch, B Lewandowski, M Pelizaeus, K Peters, T Schroeder, M Steinke, JT Boyd, JP Burke, N Chevalier, WN Cottingham, T Cuhadar-Donszelmann, BG Fulsom, C Hearty, NS Knecht, TS Mattison, JA McKenna, A Khan, P Kyberd, M Saleem, L Teodorescu, AE Blinov, VE Blinov, AD Bukin, VP Druzhinin, VB Golubev, EA Kravchenko, AP Onuchin, SI Serednyakov, Yu I Skovpen, EP Solodov, AN Yushkov, D Best, M Bondioli, M Bruinsma, M Chao, S Curry, I Eschrich, D Kirkby, AJ Lankford, P Lund, M Mandelkern, RK Mommsen, W Roethel, DP Stoker, C Buchanan, BL Hartfiel, AJR Weinstein, SD Foulkes, JW Gary, O Long, BC Shen, K Wang, L Zhang, D Del Re, HK Hadavand, EJ Hill, DB MacFarlane, HP Paar, S Rahatlou, V Sharma, JW Berryhill, C Campagnari, A Cunha, B Dahmes, TM Hong, MA Mazur, JD Richman, W Verkerke, TW Beck, AM Eisner, CJ Flacco, CA Heusch, J Kroseberg, WS Lockman, G Nesom, T Schalk, BA Schumm, A Seiden, P Spradlin, DC Williams, MG Wilson, J Albert, E Chen, GP Dubois-Felsmann, A Dvoretskii, DG Hitlin, I Narsky, T Piatenko, FC Porter, A Ryd, A Samuel, R Andreassen, S Jayatilleke, G Mancinelli', '전체 인용횟수': '1233회 인용2005200620072008200920102011201220132014201520162017201820192020202120222023107676901067747607065845283586458584948', '페이지': '142001', '학술 문서': 'Observation of a broad structure in the π+ π− J/ψ mass spectrum around 4.26 GeV/c 2B Aubert, R Barate, D Boutigny, F Couderc…\\xa0- Physical review letters, 20051233회 인용 관련 학술자료 전체 39개의 버전 ', '호': '14'}, title='Observation of a Broad Structure in the  Mass Spectrum around ', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Observation of a Charged Charmoniumlike Structure in  at ': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/6/17', '게시자': 'American Physical Society', '권': '110', '설명': 'We study the process e+ e−→ π+ π− J/ψ at a center-of-mass energy of 4.260 GeV using a 525 pb− 1 data sample collected with the BESIII detector operating at the Beijing Electron Positron Collider. The Born cross section is measured to be (62.9±1.9±3.7) pb, consistent with the production of the Y (4260). We observe a structure at around 3.9 GeV/c 2 in the π±J/ψ mass spectrum, which we refer to as the Z c (3900). If interpreted as a new particle, it is unusual in that it carries an electric charge and couples to charmonium. A fit to the π±J/ψ invariant mass spectrum, neglecting interference, results in a mass of (3899.0±3.6±4.9) MeV/c 2 and a width of (46±10±20) MeV. Its production ratio is measured to be R=(σ (e+ e−→ π±Z c (3900)∓→ π+ π− J/ψ)/σ (e+ e−→ π+ π− J/ψ))=(21.5±3.3±7.5)%. In all measurements the first errors are statistical and the second are systematic.', '저널': 'Physical review letters', '저자': 'M Ablikim, MN Achasov, XC Ai, O Albayrak, DJ Ambrose, FF An, Q An, JZ Bai, R Baldini Ferroli, Y Ban, J Becker, JV Bennett, M Bertani, JM Bian, E Boger, O Bondarenko, I Boyko, RA Briere, V Bytev, H Cai, X Cai, O Cakir, A Calcaterra, GF Cao, SA Cetin, JF Chang, G Chen, HS Chen, JC Chen, ML Chen, SJ Chen, X Chen, YB Chen, HP Cheng, YP Chu, D Cronin-Hennessy, HL Dai, JP Dai, D Dedovich, ZY Deng, A Denig, I Denysenko, M Destefanis, WM Ding, Y Ding, LY Dong, MY Dong, SX Du, J Fang, SS Fang, L Fava, CQ Feng, P Friedel, CD Fu, JL Fu, O Fuks, Q Gao, Y Gao, C Geng, K Goetzen, WX Gong, W Gradl, Michela Greco, MH Gu, YT Gu, YH Guan, AQ Guo, LB Guo, T Guo, YP Guo, YL Han, FA Harris, KL He, M He, ZY He, T Held, YK Heng, ZL Hou, C Hu, HM Hu, JF Hu, T Hu, GM Huang, GS Huang, JS Huang, L Huang, XT Huang, Y Huang, YP Huang, T Hussain, CS Ji, Q Ji, QP Ji, XB Ji, XL Ji, LL Jiang, XS Jiang, JB Jiao, Z Jiao, DP Jin, S Jin, FF Jing, N Kalantar-Nayestanaki, M Kavatsyuk, B Kopf, M Kornicer, W Kühn, W Lai, JS Lange, M Lara, P Larin, M Leyhe, CH Li, Cheng Li, Cui Li, DM Li, F Li, G Li, HB Li, JC Li, K Li, Lei Li, QJ Li, SL Li, WD Li, WG Li, XL Li, XN Li, XQ Li, XR Li, ZB Li, H Liang, YF Liang, YT Liang, GR Liao, XT Liao, D Lin, BJ Liu, CL Liu, CX Liu, FH Liu, Fang Liu, Feng Liu, H Liu, HB Liu, HH Liu, HM Liu, HW Liu, JP Liu, K Liu', '전체 인용횟수': '1226회 인용201320142015201620172018201920202021202220238015116713911211799861039272', '페이지': '252001', '학술 문서': 'Observation of a Charged Charmoniumlike Structure in e+ e−→ π+ π− J/ψ at s= 4.26 GeVM Ablikim, MN Achasov, XC Ai, O Albayrak…\\xa0- Physical review letters, 20131215회 인용 관련 학술자료 전체 19개의 버전 Observation of a Charged Charmoniumlike Structure in [e. sup.+][e. sup.-][right arrow][[pi]. sup.+][[pi]. sup.-] J/[psi] at [square root of s]= 4.26 GeV*M Ablikim\\xa0- Phys. Rev. Lett, 201321회 인용 관련 학술자료 ', '호': '25'}, title='Observation of a Charged Charmoniumlike Structure in  at ', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Observation of a Narrow Meson State Decaying to  at a Mass of ': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2003/6/17', '게시자': 'American Physical Society', '권': '90', '설명': 'We have observed a narrow state near 2.32 G e V/c 2 in the inclusive D s+ π 0 invariant mass distribution from e+ e− annihilation data at energies near 10.6 GeV. The observed width is consistent with the experimental resolution. The small intrinsic width and the quantum numbers of the final state indicate that the decay violates isospin conservation. The state has natural spin-parity and the low mass suggests a J P= 0+ assignment. The data sample corresponds to an integrated luminosity of 91 f b− 1 recorded by the BABAR detector at the SLAC PEP-II asymmetric-energy e+ e− storage ring.', '저널': 'Physical Review Letters', '저자': 'Bernard Aubert, R Barate, D Boutigny, J-M Gaillard, A Hicheur, Y Karyotakis, JP Lees, P Robbe, V Tisserand, A Zghiche, Antimo Palano, A Pompili, JC Chen, ND Qi, G Rong, P Wang, YS Zhu, G Eigen, I Ofte, B Stugu, GS Abrams, AW Borgland, AB Breon, DN Brown, J Button-Shafer, RN Cahn, E Charles, CT Day, MS Gill, AV Gritsan, Y Groysman, RG Jacobsen, RW Kadel, J Kadyk, LT Kerth, Yu G Kolomensky, JF Kral, G Kukartsev, C LeClerc, ME Levi, G Lynch, LM Mir, PJ Oddone, TJ Orimoto, M Pripstein, NA Roe, A Romosan, MT Ronan, VG Shelkov, AV Telnov, WA Wenzel, K Ford, TJ Harrison, CM Hawkes, DJ Knowles, SE Morgan, RC Penny, AT Watson, NK Watson, T Deppermann, K Goetzen, H Koch, B Lewandowski, M Pelizaeus, K Peters, H Schmuecker, M Steinke, NR Barlow, JT Boyd, N Chevalier, WN Cottingham, MP Kelly, TE Latham, C Mackay, FF Wilson, K Abe, T Cuhadar-Donszelmann, C Hearty, TS Mattison, JA McKenna, D Thiessen, P Kyberd, AK McKemey, VE Blinov, AD Bukin, VB Golubev, VN Ivanchenko, EA Kravchenko, AP Onuchin, SI Serednyakov, Yu I Skovpen, EP Solodov, AN Yushkov, D Best, M Chao, D Kirkby, AJ Lankford, M Mandelkern, S McMahon, RK Mommsen, W Roethel, DP Stoker, C Buchanan, Daniele del Re, HK Hadavand, EJ Hill, DB MacFarlane, HP Paar, Sh Rahatlou, U Schwanke, V Sharma, JW Berryhill, C Campagnari, B Dahmes, N Kuznetsova, SL Levy, O Long, A Lu, MA Mazur, JD Richman, W Verkerke, TW Beck, J Beringer, AM Eisner, CA Heusch, WS Lockman, T Schalk, RE Schmitz, BA Schumm, A Seiden, M Turri, W Walkowiak, DC Williams, MG Wilson, J Albert, E Chen, GP Dubois-Felsmann, A Dvoretskii, DG Hitlin, I Narsky, FC Porter, A Ryd, A Samuel, S Yang, S Jayatilleke, G Mancinelli, BT Meadows, MD Sokoloff, T Abe, T Barillari', '전체 인용횟수': '1103회 인용20032004200520062007200820092010201120122013201420152016201720182019202020212022202362142110768366474638442336325037392925353239', '페이지': '242001', '학술 문서': 'Observation of a Narrow Meson State Decaying to D s+ π 0 at a Mass of 2.32 G e V/c 2B Aubert, R Barate, D Boutigny, JM Gaillard, A Hicheur…\\xa0- Physical Review Letters, 20031103회 인용 관련 학술자료 전체 43개의 버전 ', '호': '24'}, title='Observation of a Narrow Meson State Decaying to  at a Mass of ', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Measurement of the ratio of branching fractions B (B¯ 0→ D*+ τ− ν¯ τ)/B (B¯ 0→ D*+ μ− ν¯ μ)': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/9/9', '게시자': 'American Physical Society', '권': '115', '설명': 'The branching fraction ratio R (D*)≡ B (B 0→ D*+ τ− ν τ)/B (B 0→ D*+ μ− ν μ) is measured using a sample of proton-proton collision data corresponding to 3.0 fb− 1 of integrated luminosity recorded by the LHCb experiment during 2011 and 2012. The tau lepton is identified in the decay mode τ−→ μ− ν μ ν τ. The semitauonic decay is sensitive to contributions from non-standard-model particles that preferentially couple to the third generation of fermions, in particular, Higgs-like charged scalars. A multidimensional fit to kinematic distributions of the candidate B 0 decays gives R (D*)= 0.336±0.027 (stat)±0.030 (syst). This result, which is the first measurement of this quantity at a hadron collider, is 2.1 standard deviations larger than the value expected from lepton universality in the standard model.', '저널': 'Physical review letters', '저자': 'Roel Aaij, B Adeva, M Adinolfi, A Affolder, Z Ajaltouni, S Akar, J Albrecht, F Alessio, M Alexander, S Ali, G Alkhazov, P Alvarez Cartelle, AA Alves Jr, S Amato, S Amerio, Y Amhis, L An, L Anderlini, J Anderson, G Andreassi, M Andreotti, JE Andrews, RB Appleby, O Aquines Gutierrez, F Archilli, P d’Argent, A Artamonov, M Artuso, E Aslanides, Giulio Auriemma, M Baalouch, S Bachmann, JJ Back, A Badalov, C Baesso, W Baldini, RJ Barlow, C Barschel, S Barsuk, W Barter, V Batozskaya, V Battista, A Bay, L Beaucourt, J Beddow, F Bedeschi, I Bediaga, LJ Bel, V Bellee, I Belyaev, E Ben-Haim, G Bencivenni, S Benson, J Benton, A Berezhnoy, R Bernet, A Bertolin, M-O Bettler, Martin Van Beuzekom, A Bien, S Bifani, T Bird, A Birnkraut, A Bizzeti, T Blake, F Blanc, Johan Blouw, S Blusk, V Bocci, A Bondar, N Bondar, W Bonivento, S Borghi, M Borsato, TJV Bowcock, E Bowen, C Bozzi, S Braun, D Brett, Markward Britsch, T Britton, J Brodzicka, NH Brook, A Bursche, J Buytaert, S Cadeddu, R Calabrese, M Calvi, M Calvo Gomez, P Campana, D Campora Perez, L Capriotti, A Carbone, G Carboni, R Cardinale, A Cardini, P Carniti, L Carson, K Carvalho Akiba, G Casse, L Cassina, L Castillo Garcia, M Cattaneo, Ch Cauet, G Cavallero, R Cenci, M Charles, Ph Charpentier, M Chefdeville, S Chen, S-F Cheung, N Chiapolini, M Chrzaszcz, X Cid Vidal, G Ciezarek, PEL Clarke, M Clemencic, HV Cliff, J Closier, V Coco, J Cogan, E Cogneras, V Cogoni, L Cojocariu, G Collazuol, P Collins, A Comerma-Montells, A Contu, A Cook, M Coombes, S Coquereau, G Corti, M Corvo, B Couturier, GA Cowan, DC Craik, A Crocombe, M Cruz Torres, S Cunliffe, R Currie, C D’Ambrosio, E Dall’Occo, J Dalseno, PNY David, A Davis, K De Bruyn, S De Capua, M De Cian, JM De Miranda, L De Paula', '전체 인용횟수': '1099회 인용2015201620172018201920202021202220232110616818318012010312585', '페이지': '111803', '학술 문서': 'Measurement of the ratio of branching fractions B (B¯ 0→ D*+ τ− ν¯ τ)/B (B¯ 0→ D*+ μ− ν¯ μ)R Aaij, B Adeva, M Adinolfi, A Affolder, Z Ajaltouni…\\xa0- Physical review letters, 20151097회 인용 관련 학술자료 전체 41개의 버전 Measurement of the Ratio of Branching Fractions B (anti-B0→ D*+ τ− anti-ντ)/B (anti-B0→ D*+ μ− anti-νμ)*R Aaij, B Adeva, M Adinolfi, A Affolder, Z Ajaltouni…\\xa0- PHYSICAL REVIEW LETTERS, 20152회 인용 관련 학술자료 전체 2개의 버전 ', '호': '11'}, title='Measurement of the ratio of branching fractions B (B¯ 0→ D*+ τ− ν¯ τ)/B (B¯ 0→ D*+ μ− ν¯ μ)', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Measurement of the Inelastic Proton-Proton Cross Section at  with the ATLAS Detector at the LHC': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/10/26', '게시자': 'American Physical Society', '권': '117', '설명': 'This Letter presents a measurement of the inelastic proton-proton cross section using 60 μ b− 1 of p p collisions at a center-of-mass energy s of 13 TeV with the ATLAS detector at the LHC. Inelastic interactions are selected using rings of plastic scintillators in the forward region (2.07<| η|< 3.86) of the detector. A cross section of 68.1±1.4 mb is measured in the fiducial region ξ= M X 2/s> 10− 6, where M X is the larger invariant mass of the two hadronic systems separated by the largest rapidity gap in the event. In this ξ range the scintillators are highly efficient. For diffractive events this corresponds to cases where at least one proton dissociates to a system with M X> 13 GeV. The measured cross section is compared with a range of theoretical predictions. When extrapolated to the full phase space, a cross section of 78.1±2.9 mb is measured, consistent with the inelastic cross section increasing with center-of-mass energy.', '저널': 'Physical review letters', '저자': 'Morad Aaboud, G Aad, B Abbott, J Abdallah, Baptiste Abeloos, Rosemarie Aben, OS AbouZeid, NL Abraham, Halina Abramowicz, Henso Abreu, Ricardo Abreu, Yiming Abulaiti, Bobby Sami Acharya, Leszek Adamczyk, DL Adams, Jahred Adelman, Stefanie Adomeit, Tim Adye, AA Affolder, Tatjana Agatonovic-Jovin, Juan Anton Aguilar-Saavedra, SP Ahlen, Faig Ahmadov, Giulio Aielli, Henrik Akerstedt, TPA Åkesson, AV Akimov, Gian Luigi Alberghi, Justin Albert, Solveig Albrand, MJ Alconada Verzini, Martin Aleksa, IN Aleksandrov, Calin Alexa, Gideon Alexander, Theodoros Alexopoulos, Muhammad Alhroob, Babar Ali, Malik Aliev, Gianluca Alimonti, John Alison, Steven Pat Alkire, BMM Allbrooke, Benjamin W Allen, PP Allport, Alberto Aloisio, Alejandro Alonso, Francisco Alonso, Cristiano Alpigiani, Mahmoud Alstaty, B Alvarez Gonzalez, D Álvarez Piqueras, MG Alviggi, Brian Thom Amadio, Katsuya Amako, Y Amaral Coutinho, Christoph Amelung, Dante Amidei, SP Amor Dos Santos, Antonio Amorim, Simone Amoroso, Glenn Amundsen, Christos Anastopoulos, Lucian Ste Ancu, Nansi Andari, Timothy Andeen, CF Anders, Gabriel Anders, John Kenne Anders, KJ Anderson, Attilio Andreazza, V Andrei, Stylianos Angelidakis, Ivan Angelozzi, Philipp Anger, Aaron Angerami, Francis Anghinolfi, AV Anisenkov, Nuno Anjos, Alberto Annovi, Claire Antel, Mario Antonelli, Alexey Antonov, Fabio Anulli, Masato Aoki, L Aperio Bella, Giorgi Arabidze, Yasuo Arai, Juan Pedro Araque, ATH Arce, FA Arduh, Jean-Franc Arguin, Spyridon Argyropoulos, Metin Arik, Aaron Jame Armbruster, Lewis Jame Armitage, Olivier Arnaez, Hannah Arnold, Miguel Arratia, Ozan Arslan, Andrei Artamonov, Giacomo Artoni, Sebastian Artz, Shoji Asai, Nedaa Asbah, Adi Ashkenazi, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, K Augsten, G Avolio, B Axen, MK Ayoub, G Azuelos, MA Baak, AE Baas, MJ Baca, H Bachacou, K Bachas, M Backes, M Backhaus, P Bagiacchi, Paolo Bagnaia, Y Bai, JT Baines, OK Baker, EM Baldin, P Balek, T Balestri, F Balli, WK Balunas, E Banas, Sw Banerjee, AAE Bannoura, L Barak, EL Barberio, D Barberis, M Barbero, T Barillari, MS Barisits, T Barklow, N Barlow, SL Barnes, BM Barnett, RM Barnett, Z Barnovska, A Baroncelli', '전체 인용횟수': '1060회 인용20162017201820192020202120222023221282501771711356898', '페이지': '182002', '학술 문서': 'Measurement of the Inelastic Proton-Proton Cross Section at s= 13 TeV with the ATLAS Detector at the LHCM Aaboud, G Aad, B Abbott, J Abdallah, B Abeloos…\\xa0- Physical review letters, 20161060회 인용 관련 학술자료 전체 95개의 버전 Measurement of the Inelastic Proton-Proton Cross Section at√ s= 13 TeV with the ATLAS Detector at the LHCMJ Alconada Verzini, F Alonso, FA Arduh, MT Dova…\\xa0- Physical Review Letters, 2016전체 5개의 버전 ', '호': '18'}, title='Measurement of the Inelastic Proton-Proton Cross Section at  with the ATLAS Detector at the LHC', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Study of the  decay and measurement of the  branching fraction': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2005/4/22', '게시자': 'American Physical Society', '권': '71', '설명': 'We study the decay B−→ J/ψ K− π+ π− using 117× 10 6 B B events collected at the Y (4 S) resonance with the BABAR detector at the PEP-II e+ e− asymmetric-energy storage ring. We measure the branching fractions B (B−→ J/ψ K− π+ π−)=(116±7 (stat.)±9 (syst.))× 10− 5 and B (B−→ X (3872) K−)× B (X (3872)→ J/ψ π+ π−)=(1.28±0.41)× 10− 5 and find the mass of the X (3872) to be 3873.4±1.4 MeV/c 2. We search for the h c narrow state in the decay B−→ h c K−, h c→ J/ψ π+ π− and for the decay B−→ J/ψ D 0 π−, with D 0→ K− π+. We set the 90% CL limits B (B−→ h c K−)× B (h c→ J/ψ π+ π−)< 3.4× 10− 6 and B (B−→ J/ψ D 0 π−)< 5.2× 10− 5.', '저널': 'Physical Review D', '저자': 'Bernard Aubert, R Barate, D Boutigny, F Couderc, J-M Gaillard, A Hicheur, Y Karyotakis, JP Lees, V Tisserand, A Zghiche, A Palano, A Pompili, JC Chen, ND Qi, G Rong, P Wang, YS Zhu, G Eigen, I Ofte, B Stugu, GS Abrams, AW Borgland, AB Breon, DN Brown, J Button-Shafer, RN Cahn, E Charles, CT Day, MS Gill, AV Gritsan, Y Groysman, RG Jacobsen, RW Kadel, J Kadyk, LT Kerth, Yu G Kolomensky, G Kukartsev, C LeClerc, G Lynch, AM Merchant, LM Mir, PJ Oddone, TJ Orimoto, M Pripstein, NA Roe, MT Ronan, VG Shelkov, WA Wenzel, K Ford, TJ Harrison, CM Hawkes, SE Morgan, AT Watson, M Fritsch, K Goetzen, T Held, H Koch, B Lewandowski, M Pelizaeus, M Steinke, JT Boyd, N Chevalier, WN Cottingham, MP Kelly, TE Latham, FF Wilson, T Cuhadar-Donszelmann, C Hearty, NS Knecht, TS Mattison, JA McKenna, D Thiessen, A Khan, P Kyberd, L Teodorescu, VE Blinov, AD Bukin, VP Druzhinin, VB Golubev, VN Ivanchenko, EA Kravchenko, AP Onuchin, SI Serednyakov, Yu I Skovpen, EP Solodov, AN Yushkov, D Best, M Bruinsma, M Chao, I Eschrich, D Kirkby, AJ Lankford, M Mandelkern, RK Mommsen, W Roethel, DP Stoker, C Buchanan, BL Hartfiel, JW Gary, BC Shen, K Wang, Daniele del Re, HK Hadavand, EJ Hill, DB MacFarlane, HP Paar, Sh Rahatlou, V Sharma, JW Berryhill, C Campagnari, B Dahmes, SL Levy, O Long, A Lu, MA Mazur, JD Richman, W Verkerke, TW Beck, AM Eisner, CA Heusch, WS Lockman, T Schalk, RE Schmitz, BA Schumm, A Seiden, P Spradlin, DC Williams, MG Wilson, J Albert, E Chen, GP Dubois-Felsmann, A Dvoretskii, DG Hitlin, I Narsky, T Piatenko, FC Porter, A Ryd, A Samuel, S Yang, S Jayatilleke, G Mancinelli, BT Meadows, MD Sokoloff, T Abe, F Blanc, P Bloom, S Chen, WT Ford, U Nauenberg, A Olivas', '전체 인용횟수': '896회 인용200520062007200820092010201120122013201420152016201720182019202020212022202318606759596649386366594750343623403225', '페이지': '071103', '학술 문서': 'Study of the B−→ J/ψ K− π+ π− decay and measurement of the B−→ X (3872) K− branching fractionB Aubert, R Barate, D Boutigny, F Couderc, JM Gaillard…\\xa0- Physical Review D, 2005896회 인용 관련 학술자료 전체 46개의 버전 ', '호': '7'}, title='Study of the  decay and measurement of the  branching fraction', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Measurement of Higgs boson production in the diphoton decay channel in  collisions at center-of-mass energies of 7 and 8\\xa0TeV with the ATLAS detector': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/12/24', '게시자': 'American Physical Society', '권': '90', '설명': 'A measurement of the production processes of the recently discovered Higgs boson is performed in the two-photon final state using 4.5 fb− 1 of proton-proton collisions data at s= 7 TeV and 20.3 fb− 1 at s= 8 TeV collected by the ATLAS detector at the Large Hadron Collider. The number of observed Higgs boson decays to diphotons divided by the corresponding Standard Model prediction, called the signal strength, is found to be μ= 1.17±0.27 at the value of the Higgs boson mass measured by ATLAS, m H= 125.4 GeV. The analysis is optimized to measure the signal strengths for individual Higgs boson production processes at this value of m H. They are found to be μ ggF= 1.32±0.38, μ VBF= 0.8±0.7, μ W H= 1.0±1.6, μ Z H= 0.1− 0.1+ 3.7, and μ t t H= 1.6− 1.8+ 2.7, for Higgs boson production through gluon fusion, vector-boson fusion, and in association with a W or Z boson or a top-quark pair, respectively\\xa0…', '저널': 'Physical Review D', '저자': 'Georges Aad, Brad Abbott, Jalal Abdallah, S Abdel Khalek, R Aben, B Abi, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, J Adelman, S Adomeit, T Adye, T Agatonovic-Jovin, JA Aguilar-Saavedra, M Agustoni, SP Ahlen, F Ahmadov, G Aielli, H Akerstedt, TPA Åkesson, G Akimoto, AV Akimov, GL Alberghi, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, G Alimonti, L Alio, J Alison, BMM Allbrooke, LJ Allison, PP Allport, A Aloisio, A Alonso, F Alonso, C Alpigiani, A Altheimer, B Alvarez Gonzalez, MG Alviggi, K Amako, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, S Angelidakis, I Angelozzi, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, JP Araque, ATH Arce, JF Arguin, S Argyropoulos, M Arik, AJ Armbruster, O Arnaez, V Arnal, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, A Ashkenazi, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, B Auerbach, K Augsten, M Aurousseau, G Avolio, G Azuelos, Y Azuma, MA Baak, AE Baas, C Bacci, H Bachacou, K Bachas, M Backes, M Backhaus, J Backus Mayes, E Badescu, P Bagiacchi, Paolo Bagnaia, Y Bai, T Bain, JT Baines, OK Baker, P Balek, F Balli, E Banas, Sw Banerjee, AAE Bannoura, V Bansal, HS Bansil, L Barak, SP Baranov, EL Barberio, D Barberis, M Barbero, T Barillari, M Barisonzi, T Barklow, N Barlow, BM Barnett', '전체 인용횟수': '872회 인용201420152016201720182019202020212022202323237178123865345393547', '페이지': '112015', '학술 문서': 'Measurement of Higgs boson production in the diphoton decay channel in p p collisions at center-of-mass energies of 7 and 8 TeV with the ATLAS detectorG Aad, B Abbott, J Abdallah, SA Khalek, R Aben, B Abi…\\xa0- Physical Review D, 2014868회 인용 관련 학술자료 전체 92개의 버전 Measurement of Higgs boson production in the diphoton decay channel in pp collisions at center-of-mass energies of 7 and 8 TeV with the ATLAS detectorG Aad, B Abbott, J Abdallah, S Abdel Khalek, R Aben… - 20145회 인용 관련 학술자료 Measurement of Higgs boson production in the diphoton decay channel in pp collisions at center-of-mass energies of 7 and 8 TeV with the ATLAS detectorATLAS Collaboration - 2014관련 학술자료 전체 2개의 버전 ', '호': '11'}, title='Measurement of Higgs boson production in the diphoton decay channel in  collisions at center-of-mass energies of 7 and 8\\xa0TeV with the ATLAS detector', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Observation of  Production': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/6/4', '게시자': 'American Physical Society', '권': '120', '설명': 'The observation of Higgs boson production in association with a top quark-antiquark pair is reported, based on a combined analysis of proton-proton collision data at center-of-mass energies of s= 7, 8, and 13 TeV, corresponding to integrated luminosities of up to 5.1, 19.7, and 35.9 fb− 1, respectively. The data were collected with the CMS detector at the CERN LHC. The results of statistically independent searches for Higgs bosons produced in conjunction with a top quark-antiquark pair and decaying to pairs of W bosons, Z bosons, photons, τ leptons, or bottom quark jets are combined to maximize sensitivity. An excess of events is observed, with a significance of 5.2 standard deviations, over the expectation from the background-only hypothesis. The corresponding expected significance from the standard model for a Higgs boson mass of 125.09 GeV is 4.2 standard deviations. The combined best fit signal strength\\xa0…', '저널': 'Physical review letters', '저자': 'Albert M Sirunyan, Armen Tumasyan, Wolfgang Adam, Federico Ambrogi, Ece Asilar, Thomas Bergauer, Johannes Brandstetter, Marko Dragicevic, Janos Erö, A Escalante Del Valle, Martin Flechl, Rudolf Fruehwirth, Vasile Mihai Ghete, Josef Hrubec, Manfred Jeitler, Natascha Krammer, Ilse Krätschmer, Dietrich Liko, Thomas Madlener, Ivan Mikulec, Navid Rad, Herbert Rohringer, J Schieck, R Schoefbeck, M Spanring, D Spitzbart, A Taurok, W Waltenberger, J Wittmann, C-E Wulz, M Zarucki, J Suarez Gonzalez, EA De Wolf, D Di Croce, X Janssen, J Lauwers, M Pieters, M Van De Klundert, H Van Haevermaet, P Van Mechelen, S Abu Zeid, F Blekman, J D’Hondt, I De Bruyn, J De Clercq, K Deroover, G Flouris, D Lontkovskyi, S Lowette, I Marchesini, S Moortgat, L Moreels, Q Python, K Skovpen, S Tavernier, W Van Doninck, P Van Mulders, I Van Parijs, D Beghin, B Bilin, H Brun, B Clerbaux, G De Lentdecker, H Delannoy, B Dorney, G Fasanella, L Favart, R Goldouzian, A Grebenyuk, AK Kalsi, T Lenzi, J Luetic, N Postiau, E Starling, L Thomas, C Vander Velde, Pascal Vanlaer, David Vannerom, Qun Wang, Tom Cornelis, Didar Dobur, Alexis Fagot, Muhammad Gul, Illia Khvastunov, Deniz Poyraz, Christos Roskas, Daniele Trocino, Michael Tytgat, Willem Verbeke, Basile Vermassen, Martina Vit, Nikolaos Zaganidis, Hamed Bakhshiansohi, Olivier Bondu, Sébastien Brochet, Giacomo Bruno, Claudio Caputo, Pieter David, Christophe Delaere, Martin Delcourt, Brieuc Francois, Andrea Giammanco, Georgios Krintiras, Vincent Lemaitre, Alessio Magitteri, Alexandre Mertens, Marco Musich, Krzysztof Piotrzkowski, Alessia Saggio, M Vidal Marono, Sébastien Wertz, Joze Zobec, Fábio Lúcio Alves, GA Alves, Lucas Brito, M Correa Martins Junior, G Correia Silva, Carsten Hensel, Arthur Moraes, Maria Elena Pol, P Rebello Teles, E Belchior Batista Das Chagas, Wagner Carvalho, Jose Chinellato, Eduardo Coelho, EM Da Costa, Gustavo Gil Da Silveira, D De Jesus Damiao, C De Oliveira Martins, S Fonseca De Souza, Helena Malbouisson, D Matos Figueiredo, M Melo De Almeida, C Mora Herrera, Luiz Mundim, Helio Nogima, WL Prado Da Silva, LJ Sanchez Rosas, Alberto Santoro, Andre Sznajder, Mauricio Thiel, EJ Tonelli Manganote, F Torres Da Silva De Araujo, A Vilela Pereira, Sudha Ahuja, Cesar Augusto Bernardes, Luigi Calligaris, TR Fernandez Perez Tomei, EM Gregores, Pedro G Mercadante', '전체 인용횟수': '859회 인용201720182019202020212022202331352571581459463', '페이지': '231801', '학술 문서': \"Observation of t t¯ H productionAM Sirunyan, A Tumasyan, W Adam, F Ambrogi…\\xa0- Physical review letters, 2018856회 인용 관련 학술자료 전체 111개의 버전 Observation of (tt) over-barH productionAM Sirunyan, A Tumasyan, W Adam, F Ambrogi… - 20183회 인용 관련 학술자료 Observation of t¯ t H Production*SA Zeid, F Blekman, J D'Hondt, IH De Bruyn…\\xa0- Phys. Rev. Lett., 2018\", '호': '23'}, title='Observation of  Production', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Glove: Global vectors for word representation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/10', '설명': 'Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.', '저자': 'Jeffrey Pennington, Richard Socher, Christopher D Manning', '전체 인용횟수': '37826회 인용20152016201720182019202020212022202339510872053371455966469673663585071', '컨퍼런스': 'Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)', '페이지': '1532-1543', '학술 문서': 'Glove: Global vectors for word representationJ Pennington, R Socher, CD Manning\\xa0- Proceedings of the 2014 conference on empirical\\xa0…, 201437726회 인용 관련 학술자료 전체 27개의 버전 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)*K Cho, B van Merrienboer, C Gulcehre, D Bahdanau…\\xa0- Association for Computational Linguistics. https://doi\\xa0…, 2014104회 인용 관련 학술자료 '}, title='Glove: Global vectors for word representation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013', '설명': 'Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.', '저자': 'Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, Christopher Potts Potts', '전체 인용횟수': '8191회 인용20142015201620172018201920202021202220231944375857338709441021108611611056', '컨퍼런스': 'EMNLP', '학술 문서': 'Recursive deep models for semantic compositionality over a sentiment treebankR Socher, A Perelygin, J Wu, J Chuang, CD Manning…\\xa0- Proceedings of the 2013 conference on empirical\\xa0…, 20138191회 인용 관련 학술자료 전체 16개의 버전 '}, title='Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Improved semantic representations from tree-structured long short-term memory networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/2/28', '설명': 'Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).', '저널': 'arXiv preprint arXiv:1503.00075', '저자': 'Kai Sheng Tai, Richard Socher, Christopher D Manning', '전체 인용횟수': '3788회 인용20152016201720182019202020212022202364192326523629636556465362', '학술 문서': 'Improved semantic representations from tree-structured long short-term memory networksKS Tai, R Socher, CD Manning\\xa0- arXiv preprint arXiv:1503.00075, 20153788회 인용 관련 학술자료 전체 20개의 버전 '}, title='Improved semantic representations from tree-structured long short-term memory networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Reasoning with neural tensor networks for knowledge base completion': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013', '권': '26', '설명': 'A common problem in knowledge representation and related fields is reasoning over a large joint knowledge graph, represented as triples of a relation between two entities. The goal of this paper is to develop a more powerful neural network model suitable for inference over these relationships. Previous models suffer from weak interaction between entities or simple linear projection of the vector space. We address these problems by introducing a neural tensor network (NTN) model which allow the entities and relations to interact multiplicatively. Additionally, we observe that such knowledge base models can be further improved by representing each entity as the average of vectors for the words in the entity name, giving an additional dimension of similarity by which entities can share statistical strength. We assess the model by considering the problem of predicting additional true relations between entities given a partial knowledge base. Our model outperforms previous models and can classify unseen relationships in WordNet and FreeBase with an accuracy of 86.2% and 90.0%, respectively.', '저널': 'Advances in neural information processing systems', '저자': 'Richard Socher, Danqi Chen, Christopher D Manning, Andrew Ng', '전체 인용횟수': '2303회 인용201420152016201720182019202020212022202336116176213272312361341270194', '학술 문서': 'Reasoning with neural tensor networks for knowledge base completionR Socher, D Chen, CD Manning, A Ng\\xa0- Advances in neural information processing systems, 20132303회 인용 관련 학술자료 전체 17개의 버전 '}, title='Reasoning with neural tensor networks for knowledge base completion', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Parsing natural scenes and natural language with recursive neural networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011', '설명': 'Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algorithm can be used both to provide a competitive syntactic parser for natural language sentences from the Penn Treebank and to outperform alternative approaches for semantic scene segmentation, annotation and classification. For segmentation and annotation our algorithm obtains a new level of state-of-theart performance on the Stanford background dataset (78.1%). The features from the image parse tree outperform Gist descriptors for scene classification by 4%.', '저자': 'Richard Socher, Cliff Chiung-Yu Lin, Andrew Y Ng, Christopher D Manning', '전체 인용횟수': '1779회 인용2011201220132014201520162017201820192020202120222023833639917819219524822117014112276', '학술 문서': 'Parsing natural scenes and natural language with recursive neural networksR Socher, CC Lin, C Manning, AY Ng\\xa0- Proceedings of the 28th international conference on\\xa0…, 20111779회 인용 관련 학술자료 전체 22개의 버전 '}, title='Parsing natural scenes and natural language with recursive neural networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Semantic compositionality through recursive matrix-vector spaces': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012/7', '설명': 'Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.', '저자': 'Richard Socher, Brody Huval, Christopher D Manning, Andrew Y Ng', '전체 인용횟수': '1737회 인용201220132014201520162017201820192020202120222023670121173194200223197179138110100', '컨퍼런스': 'Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning', '페이지': '1201-1211', '학술 문서': 'Semantic compositionality through recursive matrix-vector spacesR Socher, B Huval, CD Manning, AY Ng\\xa0- Proceedings of the 2012 joint conference on empirical\\xa0…, 20121737회 인용 관련 학술자료 전체 19개의 버전 '}, title='Semantic compositionality through recursive matrix-vector spaces', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A deep reinforced model for abstractive summarization': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '저널': 'International Conference on Learning Representations', '저자': 'Paulus Romain, Xiong Caiming, Socher Richard', '전체 인용횟수': '1716회 인용201720182019202020212022202319220280336329295227', '학술 문서': 'A deep reinforced model for abstractive summarization*R Paulus, C Xiong, R Socher\\xa0- arXiv preprint arXiv:1705.04304, 20171683회 인용 관련 학술자료 전체 4개의 버전 A deep reinforced model for abstractive summarizationP Romain, X Caiming, S Richard\\xa0- International Conference on Learning Representations, 201832회 인용 관련 학술자료 A deep reinforced model for abstractive summarization. arXiv 2017*R Paulus, C Xiong, R Socher\\xa0- arXiv preprint arXiv:1705.0430419회 인용 관련 학술자료 A deep reinforced model for abstractive summarization (2017)*R Paulus, C Xiong, R Socher\\xa0- arXiv preprint arXiv:1705.04304, 201718회 인용 관련 학술자료 A deep reinforced model for abstractive summarization. CoRR abs/1705.04304 (2017)*R Paulus, C Xiong, R Socher\\xa0- arXiv preprint arXiv:1705.04304, 20177회 인용 관련 학술자료 '}, title='A deep reinforced model for abstractive summarization', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Semi-supervised recursive autoencoders for predicting sentiment distributions': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011/7', '설명': 'We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model’s ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.', '저자': 'Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, Christopher D Manning', '전체 인용횟수': '1695회 인용2011201220132014201520162017201820192020202120222023527591221871912302412021411179056', '컨퍼런스': 'Proceedings of the 2011 conference on empirical methods in natural language processing', '페이지': '151-161', '학술 문서': 'Semi-supervised recursive autoencoders for predicting sentiment distributionsR Socher, J Pennington, EH Huang, AY Ng…\\xa0- Proceedings of the 2011 conference on empirical\\xa0…, 20111695회 인용 관련 학술자료 전체 21개의 버전 '}, title='Semi-supervised recursive autoencoders for predicting sentiment distributions', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Pointer sentinel mixture models': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/9/26', '설명': 'Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.', '저널': 'arXiv preprint arXiv:1609.07843', '저자': 'Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher', '전체 인용횟수': '1690회 인용20162017201820192020202120222023958135219255302338369', '학술 문서': 'Pointer sentinel mixture modelsS Merity, C Xiong, J Bradbury, R Socher\\xa0- arXiv preprint arXiv:1609.07843, 20161690회 인용 관련 학술자료 전체 4개의 버전 '}, title='Pointer sentinel mixture models', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Zero-shot learning through cross-modal transfer': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013', '권': '26', '설명': \"This work introduces a model that can recognize objects in images even if no training data is available for the object class. The only necessary knowledge about unseen categories comes from unsupervised text corpora. Unlike previous zero-shot learning models, which can only differentiate between unseen classes, our model can operate on a mixture of objects, simultaneously obtaining state of the art performance on classes with thousands of training images and reasonable performance on unseen classes. This is achieved by seeing the distributions of words in texts as a semantic space for understanding what objects look like. Our deep learning model does not require any manually defined semantic or visual features for either words or images. Images are mapped to be close to semantic word vectors corresponding to their classes, and the resulting image embeddings can be used to distinguish whether an image is of a seen or unseen class. Then, a separate recognition model can be employed for each type. We demonstrate two strategies, the first gives high accuracy on unseen classes, while the second is conservative in its prediction of novelty and keeps the seen classes' accuracy high.\", '저널': 'Advances in neural information processing systems', '저자': 'Richard Socher, Milind Ganjoo, Christopher D Manning, Andrew Ng', '전체 인용횟수': '1632회 인용20142015201620172018201920202021202220233371109140192234255242189134', '학술 문서': 'Zero-shot learning through cross-modal transferR Socher, M Ganjoo, CD Manning, A Ng\\xa0- Advances in neural information processing systems, 20131632회 인용 관련 학술자료 전체 19개의 버전 '}, title='Zero-shot learning through cross-modal transfer', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Knowing when to look: Adaptive attention via a visual sentinel for image captioning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'Attention-based neural encoder-decoder frameworks have been widely adopted for image captioning. Most methods force visual attention to be active for every generated word. However, the decoder likely requires little to no visual information from the image to predict non-visual words such as\" the\" and\" of\". Other words that may seem visual can often be predicted reliably just from the language model eg,\" sign\" after\" behind a red stop\" or\" phone\" following\" talking on a cell\". In this paper, we propose a novel adaptive attention model with a visual sentinel. At each time step, our model decides whether to attend to the image (and if so, to which regions) or to the visual sentinel. The model decides whether to attend to the image and where, in order to extract meaningful information for sequential word generation. We test our method on the COCO image captioning 2015 challenge dataset and Flickr30K. Our approach sets the new state-of-the-art by a significant margin.', '저자': 'Jiasen Lu, Caiming Xiong, Devi Parikh, Richard Socher', '전체 인용횟수': '1608회 인용201720182019202020212022202349174268264298314232', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '375-383', '학술 문서': 'Knowing when to look: Adaptive attention via a visual sentinel for image captioningJ Lu, C Xiong, D Parikh, R Socher\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20171608회 인용 관련 학술자료 전체 11개의 버전 '}, title='Knowing when to look: Adaptive attention via a visual sentinel for image captioning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Improving word representations via global context and multiple word prototypes': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012/7', '설명': 'Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models. 1', '저자': 'Eric H Huang, Richard Socher, Christopher D Manning, Andrew Y Ng', '전체 인용횟수': '1586회 인용2012201320142015201620172018201920202021202220237361061892452061951671571187664', '컨퍼런스': 'Proceedings of the 50th annual meeting of the association for computational linguistics (Volume 1: Long papers)', '페이지': '873-882', '학술 문서': 'Improving word representations via global context and multiple word prototypesEH Huang, R Socher, CD Manning, AY Ng\\xa0- Proceedings of the 50th annual meeting of the\\xa0…, 20121586회 인용 관련 학술자료 전체 16개의 버전 '}, title='Improving word representations via global context and multiple word prototypes', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Ask me anything: Dynamic memory networks for natural language processing': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/6/24', '설명': 'Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook’s bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.', '저널': 'arXiv preprint arXiv:1506.07285', '저자': 'Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, Richard Socher', '전체 인용횟수': '1476회 인용20152016201720182019202020212022202318108190225248228184136118', '학술 문서': 'Ask me anything: Dynamic memory networks for natural language processingA Kumar, O Irsoy, P Ondruska, M Iyyer, J Bradbury…\\xa0- International conference on machine learning, 20161476회 인용 관련 학술자료 전체 15개의 버전 Ask Me Anything: Dynamic Memory Networks for Natural Language ProcessingAKOIP Ondruska, MIJBI Gulrajani, R Socher\\xa0- arXiv preprint arXiv:1506.07285, 2015관련 학술자료 Ask Me Anything: Dynamic Memory Networks for Natural Language ProcessingAKOIJ Su, JBR English, BPPOM Iyyer, IGR Socher\\xa0- arXiv preprint arXiv:1506.07285, 2015관련 학술자료 '}, title='Ask me anything: Dynamic memory networks for natural language processing', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Regularizing and optimizing LSTM language models': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/8/7', '설명': 'Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.', '저널': 'arXiv preprint arXiv:1708.02182', '저자': 'Stephen Merity, Nitish Shirish Keskar, Richard Socher', '전체 인용횟수': '1208회 인용201720182019202020212022202312127240309227182106', '학술 문서': 'Regularizing and optimizing LSTM language modelsS Merity, NS Keskar, R Socher\\xa0- arXiv preprint arXiv:1708.02182, 20171208회 인용 관련 학술자료 전체 5개의 버전 '}, title='Regularizing and optimizing LSTM language models', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Parsing with compositional vector grammars': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/8', '설명': 'Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness. Instead, we introduce a Compositional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations. The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%. It is fast to train and implemented approximately as an efficient reranker it is about 20% faster than the current Stanford factored parser. The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments.', '저자': 'Richard Socher, John Bauer, Christopher D Manning, Andrew Y Ng', '전체 인용횟수': '1195회 인용201220132014201520162017201820192020202120222023322116187165156133112103686942', '컨퍼런스': 'Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)', '페이지': '455-465', '학술 문서': 'Parsing with compositional vector grammarsR Socher, J Bauer, CD Manning, AY Ng\\xa0- Proceedings of the 51st Annual Meeting of the\\xa0…, 20131195회 인용 관련 학술자료 전체 4개의 버전 '}, title='Parsing with compositional vector grammars', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learned in translation: Contextualized word vectors': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '권': '30', '설명': 'Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.', '저널': 'Advances in neural information processing systems', '저자': 'Bryan McCann, James Bradbury, Caiming Xiong, Richard Socher', '전체 인용횟수': '1155회 인용201720182019202020212022202311123256228233177119', '학술 문서': 'Learned in translation: Contextualized word vectorsB McCann, J Bradbury, C Xiong, R Socher\\xa0- Advances in neural information processing systems, 20171155회 인용 관련 학술자료 전체 9개의 버전 '}, title='Learned in translation: Contextualized word vectors', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Dynamic pooling and unfolding recursive autoencoders for paraphrase detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011', '권': '24', '설명': 'Paraphrase detection is the task of examining two sentences and determining whether they have the same meaning. In order to obtain high accuracy on this task, thorough syntactic and semantic analysis of the two statements is needed. We introduce a method for paraphrase detection based on recursive autoencoders (RAE). Our unsupervised RAEs are based on a novel unfolding objective and learn feature vectors for phrases in syntactic trees. These features are used to measure the word-and phrase-wise similarity between two sentences. Since sentences may be of arbitrary length, the resulting matrix of similarity measures is of variable size. We introduce a novel dynamic pooling layer which computes a fixed-sized representation from the variable-sized matrices. The pooled representation is then used as input to a classifier. Our method outperforms other state-of-the-art approaches on the challenging MSRP paraphrase corpus.', '저널': 'Advances in neural information processing systems', '저자': 'Richard Socher, Eric Huang, Jeffrey Pennin, Christopher D Manning, Andrew Ng', '전체 인용횟수': '1128회 인용2011201220132014201520162017201820192020202120222023420547913914313714712194727033', '학술 문서': 'Dynamic pooling and unfolding recursive autoencoders for paraphrase detectionR Socher, E Huang, J Pennin, CD Manning, A Ng\\xa0- Advances in neural information processing systems, 20111128회 인용 관련 학술자료 전체 23개의 버전 '}, title='Dynamic pooling and unfolding recursive autoencoders for paraphrase detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Better Word Representations with Recursive Neural Networks for Morphology': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013', '저자': 'Minh-Thang Luong, Richard Socher, Christopher D Manning', '전체 인용횟수': '1061회 인용20132014201520162017201820192020202120222023539681071341451801351247638', '학술 문서': 'The Arms, morphology*AH Toha, M Dailami - 20131061회 인용 관련 학술자료 전체 14개의 버전 '}, title='Better Word Representations with Recursive Neural Networks for Morphology', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Grounded compositional semantics for finding and describing images with sentences': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/4/1', '게시자': 'MIT Press', '권': '2', '설명': ' Previous work on Recursive Neural Networks (RNNs) shows that these models can                     produce compositional feature vectors for accurately representing and                     classifying sentences or images. However, the sentence vectors of previous                     models cannot accurately represent visually grounded meaning. We introduce the                     DT-RNN model which uses dependency trees to embed sentences into a vector space                     in order to retrieve images that are described by those sentences. Unlike                     previous RNN-based models which use constituency trees, DT-RNNs naturally focus                     on the action and agents in a sentence. They are better able to abstract from                     the details of word order and syntactic expression. DT-RNNs outperform other                     recursive and recurrent neural networks, kernelized CCA and a bag-of-words                     baseline on\\xa0…', '저널': 'Transactions of the Association for Computational Linguistics', '저자': 'Richard Socher, Andrej Karpathy, Quoc V Le, Christopher D Manning, Andrew Y Ng', '전체 인용횟수': '1005회 인용201420152016201720182019202020212022202338110122125140123108997154', '페이지': '207-218', '학술 문서': 'Grounded compositional semantics for finding and describing images with sentencesR Socher, A Karpathy, QV Le, CD Manning, AY Ng\\xa0- Transactions of the Association for Computational\\xa0…, 20141005회 인용 관련 학술자료 전체 16개의 버전 '}, title='Grounded compositional semantics for finding and describing images with sentences', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Image super-resolution via sparse representation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/5/18', '게시자': 'IEEE', '권': '19', '설명': 'This paper presents a new approach to single-image superresolution, based upon sparse signal representation. Research on image statistics suggests that image patches can be well-represented as a sparse linear combination of elements from an appropriately chosen over-complete dictionary. Inspired by this observation, we seek a sparse representation for each patch of the low-resolution input, and then use the coefficients of this representation to generate the high-resolution output. Theoretical results from compressed sensing suggest that under mild conditions, the sparse representation can be correctly recovered from the downsampled signals. By jointly training two dictionaries for the low- and high-resolution image patches, we can enforce the similarity of sparse representations between the low-resolution and high-resolution image patch pair with respect to their own dictionaries. Therefore, the sparse\\xa0…', '저널': 'IEEE transactions on image processing', '저자': 'Jianchao Yang, John Wright, Thomas S Huang, Yi Ma', '전체 인용횟수': '6071회 인용201020112012201320142015201620172018201920202021202220232494209310398496600609702672622501425326', '페이지': '2861-2873', '학술 문서': 'Image super-resolution via sparse representationJ Yang, J Wright, TS Huang, Y Ma\\xa0- IEEE transactions on image processing, 20106071회 인용 관련 학술자료 전체 28개의 버전 ', '호': '11'}, title='Image super-resolution via sparse representation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Least-squares fitting of two 3-D point sets': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1987/9', '게시자': 'IEEE', '설명': \"Two point sets {pi} and {p'i}; i = 1, 2,..., N are related by p'i = Rpi + T + Ni, where R is a rotation matrix, T a translation vector, and Ni a noise vector. Given {pi} and {p'i}, we present an algorithm for finding the least-squares solution of R and T, which is based on the singular value decomposition (SVD) of a 3 × 3 matrix. This new algorithm is compared to two earlier algorithms with respect to computer time requirements.\", '저널': 'IEEE Transactions on pattern analysis and machine intelligence', '저자': 'K Somani Arun, Thomas S Huang, Steven D Blostein', '전체 인용횟수': '5430회 인용1989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202320212929443962626578778910580122130152159197187155202191226231249277228234249201254296326318', '페이지': '698-700', '학술 문서': 'Least-squares fitting of two 3-D point setsKS Arun, TS Huang, SD Blostein\\xa0- IEEE Transactions on pattern analysis and machine\\xa0…, 19875430회 인용 관련 학술자료 전체 23개의 버전 ', '호': '5'}, title='Least-squares fitting of two 3-D point sets', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Locality-constrained linear coding for image classification': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/6/13', '게시자': 'IEEE', '설명': 'The traditional SPM approach based on bag-of-features (BoF) requires nonlinear classifiers to achieve good image classification performance. This paper presents a simple but effective coding scheme called Locality-constrained Linear Coding (LLC) in place of the VQ coding in traditional SPM. LLC utilizes the locality constraints to project each descriptor into its local-coordinate system, and the projected coordinates are integrated by max pooling to generate the final representation. With linear classifier, the proposed approach performs remarkably better than the traditional nonlinear SPM, achieving state-of-the-art performance on several benchmarks. Compared with the sparse coding strategy [22], the objective function used by LLC has an analytical solution. In addition, the paper proposes a fast approximated LLC method by first performing a K-nearest-neighbor search and then solving a constrained least\\xa0…', '저자': 'Jinjun Wang, Jianchao Yang, Kai Yu, Fengjun Lv, Thomas Huang, Yihong Gong', '전체 인용횟수': '4023회 인용20092010201120122013201420152016201720182019202020212022202313171092224044985775554683472532041337957', '컨퍼런스': '2010 IEEE computer society conference on computer vision and pattern recognition', '페이지': '3360-3367', '학술 문서': 'Locality-constrained linear coding for image classificationJ Wang, J Yang, K Yu, F Lv, T Huang, Y Gong\\xa0- 2010 IEEE computer society conference on computer\\xa0…, 20104023회 인용 관련 학술자료 전체 25개의 버전 '}, title='Locality-constrained linear coding for image classification', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Linear spatial pyramid matching using sparse coding for image classification': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009/6/20', '게시자': 'IEEE', '설명': 'Recently SVMs using spatial pyramid matching (SPM) kernel have been highly successful in image classification. Despite its popularity, these nonlinear SVMs have a complexity O(n 2  ∼ n 3 ) in training and O(n) in testing, where n is the training size, implying that it is nontrivial to scaleup the algorithms to handlemore than thousands of training images. In this paper we develop an extension of the SPM method, by generalizing vector quantization to sparse coding followed by multi-scale spatial max pooling, and propose a linear SPM kernel based on SIFT sparse codes. This new approach remarkably reduces the complexity of SVMs to O(n) in training and a constant in testing. In a number of image categorization experiments, we find that, in terms of classification accuracy, the suggested linear SPM based on sparse coding of SIFT descriptors always significantly outperforms the linear SPM kernel on histograms, and\\xa0…', '저자': 'Jianchao Yang, Kai Yu, Yihong Gong, Thomas Huang', '전체 인용횟수': '3955회 인용20092010201120122013201420152016201720182019202020212022202314791532854275195285374152872171641307946', '컨퍼런스': '2009 IEEE Conference on computer vision and pattern recognition', '페이지': '1794-1801', '학술 문서': 'Linear spatial pyramid matching using sparse coding for image classificationJ Yang, K Yu, Y Gong, T Huang\\xa0- 2009 IEEE Conference on computer vision and pattern\\xa0…, 20093955회 인용 관련 학술자료 전체 21개의 버전 '}, title='Linear spatial pyramid matching using sparse coding for image classification', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Image retrieval: Current techniques, promising directions, and open issues': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1999/3/1', '게시자': 'Academic Press', '권': '10', '설명': 'This paper provides a comprehensive survey of the technical achievements in the research area of image retrieval, especially content-based image retrieval, an area that has been so active and prosperous in the past few years. The survey includes 100+ papers covering the research aspects of image feature representation and extraction, multidimensional indexing, and system design, three of the fundamental bases of content-based image retrieval. Furthermore, based on the state-of-the-art technology available now and the demand from real-world applications, open research issues are identified and future promising research directions are suggested.', '저널': 'Journal of visual communication and image representation', '저자': 'Yong Rui, Thomas S Huang, Shih-Fu Chang', '전체 인용횟수': '3724회 인용19992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220233179137180210231224213211206165174192189179177160152120959675635240', '페이지': '39-62', '학술 문서': 'Image retrieval: Current techniques, promising directions, and open issuesY Rui, TS Huang, SF Chang\\xa0- Journal of visual communication and image\\xa0…, 19993212회 인용 관련 학술자료 전체 22개의 버전 Image retrieval: Past, present, and future*Y Rui, TS Huang, SF Chang\\xa0- Journal of Visual Communication and Image\\xa0…, 1999564회 인용 관련 학술자료 전체 5개의 버전 ', '호': '1'}, title='Image retrieval: Current techniques, promising directions, and open issues', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A survey of affect recognition methods: audio, visual and spontaneous expressions': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2007/11/12', '설명': 'Automated analysis of human affective behavior has attracted increasing attention from researchers in psychology, computer science, linguistics, neuroscience, and related disciplines. Promising approaches have been reported, including automatic methods for facial and vocal affect recognition. However, the existing methods typically handle only deliberately displayed and exaggerated expressions of prototypical emotions-despite the fact that deliberate behavior differs in visual and audio expressions from spontaneously occurring behavior. Recently efforts to develop algorithms that can process naturally occurring human affective behavior have emerged. This paper surveys these efforts. We first discuss human emotion perception from a psychological perspective. Next, we examine the available approaches to solving the problem of machine understanding of human affective behavior occurring in real-world\\xa0…', '저자': 'Zhihong Zeng, Maja Pantic, Glenn I Roisman, Thomas S Huang', '전체 인용횟수': '3551회 인용20082009201020112012201320142015201620172018201920202021202220233786190228245316327342302311263204198182156104', '출처': 'Proceedings of the 9th international conference on Multimodal interfaces', '페이지': '126-133', '학술 문서': 'A survey of affect recognition methods: audio, visual and spontaneous expressionsZ Zeng, M Pantic, GI Roisman, TS Huang\\xa0- Proceedings of the 9th international conference on\\xa0…, 20073551회 인용 관련 학술자료 전체 28개의 버전 '}, title='A survey of affect recognition methods: audio, visual and spontaneous expressions', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Visual interpretation of hand gestures for human-computer interaction: A review': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1997/7', '게시자': 'IEEE', '권': '19', '설명': 'The use of hand gestures provides an attractive alternative to cumbersome interface devices for human-computer interaction (HCI). In particular, visual interpretation of hand gestures can help in achieving the ease and naturalness desired for HCI. This has motivated a very active research area concerned with computer vision-based analysis and interpretation of hand gestures. We survey the literature on visual interpretation of hand gestures in the context of its role in HCI. This discussion is organized on the basis of the method used for modeling, analyzing, and recognizing gestures. Important differences in the gesture interpretation approaches arise depending on whether a 3D model of the human hand or an image appearance model of the human hand is used. 3D hand models offer a way of more elaborate modeling of hand gestures but lead to computational hurdles that have not been overcome given the real\\xa0…', '저자': 'Vladimir I Pavlovic, Rajeev Sharma, Thomas S.  Huang', '전체 인용횟수': '2899회 인용1998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202333427376751041211141291251261211331571671951811471591318211280546747', '출처': 'IEEE Transactions on pattern analysis and machine intelligence', '페이지': '677-695', '학술 문서': 'Visual interpretation of hand gestures for human-computer interaction: A reviewVI Pavlovic, R Sharma, TS Huang\\xa0- IEEE Transactions on pattern analysis and machine\\xa0…, 19972899회 인용 관련 학술자료 전체 10개의 버전 ', '호': '7'}, title='Visual interpretation of hand gestures for human-computer interaction: A review', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Relevance feedback: a power tool for interactive content-based image retrieval': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1998/9', '게시자': 'IEEE', '권': '8', '설명': \"Content-based image retrieval (CBIR) has become one of the most active research areas in the past few years. Many visual feature representations have been explored and many systems built. While these research efforts establish the basis of CBIR, the usefulness of the proposed approaches is limited. Specifically, these efforts have relatively ignored two distinct characteristics of CBIR systems: (1) the gap between high-level concepts and low-level features, and (2) the subjectivity of human perception of visual content. This paper proposes a relevance feedback based interactive retrieval approach, which effectively takes into account the above two characteristics in CBIR. During the retrieval process, the user's high-level query and perception subjectivity are captured by dynamically updated weights based on the user's feedback. The experimental results over more than 70000 images show that the proposed\\xa0…\", '저널': 'IEEE Transactions on circuits and systems for video technology', '저자': 'Yong Rui, Thomas S Huang, Michael Ortega, Sharad Mehrotra', '전체 인용횟수': '2746회 인용19992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220233168105101161159190173177167159123124148122104939590706147613231', '페이지': '644-655', '학술 문서': 'Relevance feedback: a power tool for interactive content-based image retrievalY Rui, TS Huang, M Ortega, S Mehrotra\\xa0- IEEE Transactions on circuits and systems for video\\xa0…, 19982746회 인용 관련 학술자료 전체 16개의 버전 ', '호': '5'}, title='Relevance feedback: a power tool for interactive content-based image retrieval', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Multiframe image restoration and registration': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1984', '권': '1', '설명': 'A new method for multiframe image restoration and registration is presented. The observations are sequences of low-resolution, undersampled, discrete frames. The result is a restored high-resolution image. The restoration part is suitable for real-time implementation since the computation consists of only a few complex operations.', '저널': 'Multiframe image restoration and registration', '저자': 'Roger Y Tsai, Thomas S Huang', '전체 인용횟수': '2562회 인용1992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202376109121519152726425660861161351001481351351401421471331401421346992868463', '페이지': '317-339', '학술 문서': 'Multiframe image restoration and registrationRY Tsai, TS Huang\\xa0- Multiframe image restoration and registration, 19842562회 인용 관련 학술자료 전체 3개의 버전 '}, title='Multiframe image restoration and registration', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Graph regularized nonnegative matrix factorization for data representation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/12/23', '게시자': 'IEEE', '권': '33', '설명': 'Matrix factorization techniques have been frequently applied in information retrieval, computer vision, and pattern recognition. Among them, Nonnegative Matrix Factorization (NMF) has received considerable attention due to its psychological and physiological interpretation of naturally occurring data whose representation may be parts based in the human brain. On the other hand, from the geometric perspective, the data is usually sampled from a low-dimensional manifold embedded in a high-dimensional ambient space. One then hopes to find a compact representation,which uncovers the hidden semantics and simultaneously respects the intrinsic geometric structure. In this paper, we propose a novel algorithm, called Graph Regularized Nonnegative Matrix Factorization (GNMF), for this purpose. In GNMF, an affinity graph is constructed to encode the geometrical information and we seek a matrix factorization\\xa0…', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Deng Cai, Xiaofei He, Jiawei Han, Thomas S Huang', '전체 인용횟수': '2485회 인용2011201220132014201520162017201820192020202120222023166198123174218238277248279265235226', '페이지': '1548-1560', '학술 문서': 'Graph regularized nonnegative matrix factorization for data representationD Cai, X He, J Han, TS Huang\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20102485회 인용 관련 학술자료 전체 21개의 버전 ', '호': '8'}, title='Graph regularized nonnegative matrix factorization for data representation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Ccnet: Criss-cross attention for semantic segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020', '설명': 'Full-image dependencies provide useful contextual information to benefit visual understanding problems. In this work, we propose a Criss-Cross Network (CCNet) for obtaining such contextual information in a more effective and efficient way. Concretely, for each pixel, a novel criss-cross attention module in CCNet harvests the contextual information of all the pixels on its criss-cross path. By taking a further recurrent operation, each pixel can finally capture the full-image dependencies from all pixels. Overall, CCNet is with the following merits: 1) GPU memory friendly. Compared with the non-local block, the proposed recurrent criss-cross attention module requires 11x less GPU memory usage. 2) High computational efficiency. The recurrent criss-cross attention significantly reduces FLOPs by about 85% of the non-local block in computing full-image dependencies. 3) The state-of-the-art performance. We conduct extensive experiments on popular semantic segmentation benchmarks including Cityscapes, ADE20K, and instance segmentation benchmark COCO. In particular, our CCNet achieves the mIoU score of 81.4 and 45.22 on Cityscapes test set and ADE20K validation set, respectively, which are the new state-of-the-art results. The source code is available at https://github. com/speedinghzl/CCNet.', '저자': 'Zilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi, Wenyu Liu, Thomas S. Huang', '전체 인용횟수': '2443회 인용2019202020212022202340248565775805', '컨퍼런스': 'IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)', '학술 문서': 'Ccnet: Criss-cross attention for semantic segmentationZ Huang, X Wang, L Huang, C Huang, Y Wei, W Liu\\xa0- Proceedings of the IEEE/CVF international conference\\xa0…, 20192443회 인용 관련 학술자료 전체 14개의 버전 '}, title='Ccnet: Criss-cross attention for semantic segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Generative image inpainting with contextual attention': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'Recent deep learning based approaches have shown promising results for the challenging task of inpainting large missing regions in an image. These methods can generate visually plausible image structures and textures, but often create distorted structures or blurry textures inconsistent with surrounding areas. This is mainly due to ineffectiveness of convolutional neural networks in explicitly borrowing or copying information from distant spatial locations. On the other hand, traditional texture and patch synthesis approaches are particularly suitable when it needs to borrow textures from the surrounding regions. Motivated by these observations, we propose a new deep generative model-based approach which can not only synthesize novel image structures but also explicitly utilize surrounding image features as references during network training to make better predictions. The model is a feed-forward, fully convolutional neural network which can process images with multiple holes at arbitrary locations and with variable sizes during the test time. Experiments on multiple datasets including faces (CelebA, CelebA-HQ), textures (DTD) and natural images (ImageNet, Places2) demonstrate that our proposed approach generates higher-quality inpainting results than existing ones. Code, demo and models are available at: https://github. com/JiahuiYu/generative_inpainting.', '저자': 'Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, Thomas S Huang', '전체 인용횟수': '2366회 인용20182019202020212022202339260442573557480', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '5505-5514', '학술 문서': 'Generative image inpainting with contextual attentionJ Yu, Z Lin, J Yang, X Shen, X Lu, TS Huang\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20182366회 인용 관련 학술자료 전체 15개의 버전 '}, title='Generative image inpainting with contextual attention', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Sparse representation for computer vision and pattern recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/4/29', '게시자': 'IEEE', '권': '98', '설명': 'Techniques from sparse signal representation are beginning to see significant impact in computer vision, often on nontraditional applications where the goal is not just to obtain a compact high-fidelity representation of the observed signal, but also to extract semantic information. The choice of dictionary plays a key role in bridging this gap: unconventional dictionaries consisting of, or learned from, the training samples themselves provide the key to obtaining state-of-the-art results and to attaching semantic meaning to sparse signal representations. Understanding the good performance of such unconventional dictionaries in turn demands new algorithmic and analytical techniques. This review paper highlights a few representative examples of how the interaction between sparse signal representation and computer vision can enrich both fields, and raises a number of open questions for further study.', '저자': 'John Wright, Yi Ma, Julien Mairal, Guillermo Sapiro, Thomas S Huang, Shuicheng Yan', '전체 인용횟수': '2249회 인용2010201120122013201420152016201720182019202020212022202332771451912482692482362061641151148882', '출처': 'Proceedings of the IEEE', '페이지': '1031-1044', '학술 문서': 'Sparse representation for computer vision and pattern recognitionJ Wright, Y Ma, J Mairal, G Sapiro, TS Huang, S Yan\\xa0- Proceedings of the IEEE, 20102249회 인용 관련 학술자료 전체 15개의 버전 ', '호': '6'}, title='Sparse representation for computer vision and pattern recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Image super-resolution as sparse representation of raw image patches': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2008/6/23', '게시자': 'IEEE', '설명': 'This paper addresses the problem of generating a super-resolution (SR) image from a single low-resolution input image. We approach this problem from the perspective of compressed sensing. The low-resolution image is viewed as downsampled version of a high-resolution image, whose patches are assumed to have a sparse representation with respect to an over-complete dictionary of prototype signal-atoms. The principle of compressed sensing ensures that under mild conditions, the sparse representation can be correctly recovered from the downsampled signal. We will demonstrate the effectiveness of sparsity as a prior for regularizing the otherwise ill-posed super-resolution problem. We further show that a small set of randomly chosen raw patches from training images of similar statistical nature to the input image generally serve as a good dictionary, in the sense that the computed representation is sparse\\xa0…', '저자': 'Jianchao Yang, John Wright, Thomas Huang, Yi Ma', '전체 인용횟수': '2125회 인용20092010201120122013201420152016201720182019202020212022202323689713714017820218720219216615615410986', '컨퍼런스': '2008 IEEE conference on computer vision and pattern recognition', '페이지': '1-8', '학술 문서': 'Image super-resolution as sparse representation of raw image patchesJ Yang, J Wright, T Huang, Y Ma\\xa0- 2008 IEEE conference on computer vision and pattern\\xa0…, 20082125회 인용 관련 학술자료 전체 22개의 버전 '}, title='Image super-resolution as sparse representation of raw image patches', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A fast two-dimensional median filtering algorithm': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1979/2', '게시자': 'IEEE', '권': '27', '설명': 'We present a fast algorithm for two-dimensional median filtering. It is based on storing and updating the gray level histogram of the picture elements in the window. The algorithm is much faster than conventional sorting methods. For a window size of m × n, the computer time required is 0(n).', '저널': 'IEEE transactions on acoustics, speech, and signal processing', '저자': 'Thomas Huang, GJTGY Yang, Greory Tang', '전체 인용횟수': '1838회 인용19841985198619871988198919901991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220231613122227272721222021162624219211923212619313932414560645877988211397102919810681', '페이지': '13-18', '학술 문서': 'A fast two-dimensional median filtering algorithmT Huang, G Yang, G Tang\\xa0- IEEE transactions on acoustics, speech, and signal\\xa0…, 19791838회 인용 관련 학술자료 전체 6개의 버전 ', '호': '1'}, title='A fast two-dimensional median filtering algorithm', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Free-form image inpainting with gated convolution': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': 'We present a generative image inpainting system to complete images with free-form mask and guidance. The system is based on gated convolutions learned from millions of images without additional labelling efforts. The proposed gated convolution solves the issue of vanilla convolution that treats all input pixels as valid ones, generalizes partial convolution by providing a learnable dynamic feature selection mechanism for each channel at each spatial location across all layers. Moreover, as free-form masks may appear anywhere in images with any shape, global and local GANs designed for a single rectangular mask are not applicable. Thus, we also present a patch-based GAN loss, named SN-PatchGAN, by applying spectral-normalized discriminator on dense image patches. SN-PatchGAN is simple in formulation, fast and stable in training. Results on automatic image inpainting and user-guided extension demonstrate that our system generates higher-quality and more flexible results than previous methods. Our system helps user quickly remove distracting objects, modify image layouts, clear watermarks and edit faces. Code, demo and models are available at: https://github. com/JiahuiYu/generative_inpainting.', '저자': 'Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, Thomas S Huang', '전체 인용횟수': '1652회 인용201820192020202120222023786222405481440', '컨퍼런스': 'Proceedings of the IEEE/CVF international conference on computer vision', '페이지': '4471-4480', '학술 문서': 'Free-form image inpainting with gated convolutionJ Yu, Z Lin, J Yang, X Shen, X Lu, TS Huang\\xa0- Proceedings of the IEEE/CVF international conference\\xa0…, 20191652회 인용 관련 학술자료 전체 7개의 버전 '}, title='Free-form image inpainting with gated convolution', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Human face detection in a complex background': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1994/1/1', '게시자': 'Pergamon', '권': '27', '설명': 'The human face is a complex pattern. Finding human faces automatically in a scene is a difficult yet significant problem. It is the first important step in a fully automatic human face recognition system. In this paper a new method to locate human faces in a complex background is proposed. This system utilizes a hierarchical knowledge-based method and consists of three levels. The higher two levels are based on mosaic images at different resolutions. In the lower level, an improved edge detection method is proposed. In this research the problem of scale is dealt with, so that the system can locate unknown human faces spanning a wide range of sizes in a complex black-and-white picture. Some experimental results are given.', '저널': 'Pattern recognition', '저자': 'Guangzheng Yang, Thomas S Huang', '전체 인용횟수': '1368회 인용1994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220234825303031554464666882666449696762765849374931323227361911', '페이지': '53-63', '학술 문서': 'Human face detection in a complex backgroundG Yang, TS Huang\\xa0- Pattern recognition, 19941368회 인용 관련 학술자료 전체 7개의 버전 ', '호': '1'}, title='Human face detection in a complex background', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Uniqueness and estimation of three-dimensional motion parameters of rigid objects with curved surfaces': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1984/1', '게시자': 'IEEE', '설명': \"Two main results are established in this paper. First, we show that seven point correspondences are sufficient to uniquely determine from two perspective views the three-dimensional motion parameters (within a scale factor for the translations) of a rigid object with curved surfaces. The seven points should not be traversed by two planes with one plane containing the origin, nor by a cone containing the origin. Second, a set of ``essential parameters'' are introduced which uniquely determine the motion parameters up to a scale factor for the translations, and can be estimated by solving a set of linear equations which are derived from the correspondences of eight image points. The actual motion parameters can subsequently be determined by computing the singular value decomposition (SVD) of a 3×3 matrix containing the essential parameters.\", '저널': 'IEEE Transactions on pattern analysis and machine intelligence', '저자': 'Roger Y Tsai, Thomas S Huang', '전체 인용횟수': '1310회 인용198419851986198719881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202316324248556385656768515051575453643422292324272311161581619151569455545', '페이지': '13-27', '학술 문서': 'Uniqueness and estimation of three-dimensional motion parameters of rigid objects with curved surfacesRY Tsai, TS Huang\\xa0- IEEE Transactions on pattern analysis and machine\\xa0…, 19841310회 인용 관련 학술자료 전체 23개의 버전 ', '호': '1'}, title='Uniqueness and estimation of three-dimensional motion parameters of rigid objects with curved surfaces', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Relevance feedback in image retrieval: A comprehensive review': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2003/4', '게시자': 'Springer-Verlag', '권': '8', '설명': '  We analyze the nature of the relevance feedback problem in a continuous representation space in the context of content-based image retrieval. Emphasis is put on exploring the uniqueness of the problem and comparing the assumptions, implementations, and merits of various solutions in the literature. An attempt is made to compile a list of critical issues to consider when designing a relevance feedback algorithm. With a comprehensive review as the main portion, this paper also offers some novel solutions and perspectives throughout the discussion.', '저자': 'Xiang Sean Zhou, Thomas S Huang', '전체 인용횟수': '1234회 인용20032004200520062007200820092010201120122013201420152016201720182019202020212022202312325641851027795105829082686650553529271010', '출처': 'Multimedia systems', '페이지': '536-544', '학술 문서': 'Relevance feedback in image retrieval: A comprehensive reviewXS Zhou, TS Huang\\xa0- Multimedia systems, 20031234회 인용 관련 학술자료 전체 15개의 버전 '}, title='Relevance feedback in image retrieval: A comprehensive review', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Going deeper with convolutions': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation of this architecture, GoogLeNet, a 22 layers deep network, was used to assess its quality in the context of object detection and classification.', '저자': 'Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich', '전체 인용횟수': '55025회 인용20152016201720182019202020212022202363819013836576475138254904890637094', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '1-9', '학술 문서': 'Going deeper with convolutionsC Szegedy, W Liu, Y Jia, P Sermanet, S Reed…\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 201555025회 인용 관련 학술자료 전체 57개의 버전 '}, title='Going deeper with convolutions', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Rethinking the inception architecture for computer vision': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': 'Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible. We benchmark our methods on the ILSVRC 2012 classification challenge validation set and demonstrate substantial gains over the state of the art via to carefully factorized convolutions and aggressive regularization: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters.', '저자': 'Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna', '전체 인용횟수': '29851회 인용20162017201820192020202120222023164810215936764910598264115476', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '2818-2826', '학술 문서': 'Rethinking the inception architecture for computer visionC Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 201629851회 인용 관련 학술자료 전체 28개의 버전 '}, title='Rethinking the inception architecture for computer vision', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Tensorflow: Large-scale machine learning on heterogeneous distributed systems': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/3/14', '설명': 'TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.', '저널': 'arXiv preprint arXiv:1603.04467', '저자': 'Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Józefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, Xiaoqiang Zheng', '전체 인용횟수': '29330회 인용201620172018201920202021202220236182286382541084603503547873745', '학술 문서': \"Tensorflow: Large-scale machine learning on heterogeneous distributed systemsM Abadi, A Agarwal, P Barham, E Brevdo, Z Chen…\\xa0- arXiv preprint arXiv:1603.04467, 201628299회 인용 관련 학술자료 전체 4개의 버전 TensorFlow: Large-scale machine learning on heterogeneous systems, software available from tensorflow. org (2015)*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen…\\xa0- URL https://www. tensorflow. org, 2015723회 인용 관련 학술자료 TensorFlow: large-scale machine learning on heterogeneous distributed systems. 2015*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen…\\xa0- URL http://download. tensorflow. org/paper\\xa0…, 2015373회 인용 관련 학술자료 TensorFlow: Large-scale machine learning on heterogeneous systems (2015), software available from tensorflow. org*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen…\\xa0- URL https://www. tensorflow. org, 2019252회 인용 관련 학술자료 Tensor ow: Large-scale machine learning on heterogeneous distributed systemsM Abadi, A Agarwal, P Barham, E Brevdo, Z Chen…\\xa0- arXiv preprint arXiv:1603.04467, 2016156회 인용 관련 학술자료 TensorFlow: Large-scale machine learning on heterogeneous systems. tensorflow. org*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen…\\xa0- Accessed May, 201533회 인용 관련 학술자료 i Xiaoqiang Zheng*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen…\\xa0- TensorFlow: Large-scale machine learning on\\xa0…, 201533회 인용 관련 학술자료 TensorFlow: Large-scale machine learning on heterogeneous systems [Internet]. 2015*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen…\\xa0- Available from: tensorflow. org, 201932회 인용 관련 학술자료 & Zheng, X.(2015)*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen…\\xa0- TensorFlow: Large-scale machine learning on\\xa0…, 201723회 인용 관련 학술자료 Martin wa enberg, martin wicke, yuan yu, and xiaoqiang zheng. 2015. tensorflow: Large-scale machine learning on heterogeneous systems.(2015). hp*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen…\\xa0- tensor ow. org/So ware available from tensor ow. org, 201519회 인용 관련 학술자료 TensorFlow: large-scale machine learning on heterogeneous distributed systems. arXiv [cs. DC]M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen…\\xa0- arXiv preprint arXiv:1603.04467, 201616회 인용 관련 학술자료 Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint: 160304467*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen…\\xa0- Retrieved from arxiv. org/abs/1603.04467, 201611회 인용 관련 학술자료 Tensorflow: Large-scale machine learning on heterogeneous systems, 2015. url h ttp. tensorflow. org*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen…\\xa0- Software available from tensorflow. org11회 인용 관련 학술자료 TensorFlow: Large-Scale machine learning on heterogeneous distributed systems. ArXiv e-prints, March 2016M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen…\\xa0- arXiv preprint arXiv:1603.04467, 20169회 인용 관련 학술자료 TensorFlow: Large-scale machine learning on heterogeneous systems, 2015*AA Mart'in Abadi, P Barham, E Brevdo, Z Chen, C Citro…\\xa0- Software available from tensorflow. org, 20159회 인용 관련 학술자료 Google Research*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen…\\xa0- TensorFlow: Large-Scale Machine Learning on\\xa0…, 20158회 인용 관련 학술자료 see tensorflow. org for “*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen…\\xa0- TensorFlow: Large-scale machine learning on\\xa0…, 20158회 인용 관련 학술자료 TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. 2015*R JOZEFOWICZ, L KAISER, M KUDLUR…\\xa0- Available also from: https://www. tensorflow. org\\xa0…7회 인용 관련 학술자료 TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems.(2015). tensorflow. org*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen… - 20155회 인용 관련 학술자료 TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL http://tensorflow. org/. Software available from tensorflow. org. in electricity markets: The Australian experience*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen…\\xa0- 28th USENIX Security Symposium (USENIX Security\\xa0…, 20055회 인용 관련 학술자료 Yuan Yu i Xiaoqiang Zheng, TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems, 2015*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen…4회 인용 관련 학술자료 TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems. arXiv: 160304467. 2016*M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen…3회 인용 관련 학술자료 TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems. CoRR Vol. abs/1603.04467 (2016)*AA Mart'ın Abadi, P Barham, E Brevdo, Z Chen, C Citro… - 20162회 인용 관련 학술자료 \"}, title='Tensorflow: Large-scale machine learning on heterogeneous distributed systems', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Inception-v4, inception-resnet and the impact of residual connections on learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/2/12', '권': '31', '설명': 'Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge.', '저널': 'Proceedings of the AAAI conference on artificial intelligence', '저자': 'Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alexander Alemi', '전체 인용횟수': '15599회 인용2016201720182019202020212022202397505128021302731311731112476', '학술 문서': 'Inception-v4, inception-resnet and the impact of residual connections on learningC Szegedy, S Ioffe, V Vanhoucke, A Alemi\\xa0- Proceedings of the AAAI conference on artificial\\xa0…, 201715599회 인용 관련 학술자료 전체 22개의 버전 ', '호': '1'}, title='Inception-v4, inception-resnet and the impact of residual connections on learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012/10/18', '게시자': 'IEEE', '권': '29', '설명': 'Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.', '저널': 'IEEE Signal processing magazine', '저자': 'Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, Brian Kingsbury', '전체 인용횟수': '12870회 인용201320142015201620172018201920202021202220232034097551088130717211919173114901216909', '페이지': '82-97', '학술 문서': 'Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groupsG Hinton, L Deng, D Yu, GE Dahl, A Mohamed, N Jaitly…\\xa0- IEEE Signal processing magazine, 201212800회 인용 관련 학술자료 전체 42개의 버전 Deep neural networks for acoustic modeling in speech recognition*A Senior, V Vanhoucke, P Nguyen, T Sainath\\xa0- IEEE Signal processing magazine, 2012105회 인용 관련 학술자료 ', '호': '6'}, title='Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Scalable deep reinforcement learning for vision-based robotic manipulation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/6/27', '게시자': 'PMLR', '설명': 'In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2 M parameters to perform closed-loop, real-world grasping that generalizes to 96% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.', '저자': 'Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, Sergey Levine', '전체 인용횟수': '1301회 인용20182019202020212022202316113210315358278', '컨퍼런스': '2nd Conference on Robot Learning (CoRL)', '페이지': '651-673', '학술 문서': 'Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation*D Kalashnikov, A Irpan, P Pastor, J Ibarz, A Herzog…\\xa0- arXiv preprint arXiv:1806.10293, 2018780회 인용 관련 학술자료 전체 6개의 버전 Scalable deep reinforcement learning for vision-based robotic manipulationD Kalashnikov, A Irpan, P Pastor, J Ibarz, A Herzog…\\xa0- Conference on Robot Learning, 2018559회 인용 관련 학술자료 전체 3개의 버전 ', '호': '87'}, title='Scalable deep reinforcement learning for vision-based robotic manipulation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Improving the speed of neural networks on CPUs': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011/12', '설명': 'Recent advances in deep learning have made the use of large, deep neural networks with tens of millions of parameters suitable for a number of applications that require real-time processing. The sheer size of these networks can represent a challenging computational burden, even for modern CPUs. For this reason, GPUs are routinely used instead to train and run such networks. This paper is a tutorial for students and researchers on some of the techniques that can be used to reduce this computational cost considerably on modern x86 CPUs. We emphasize data layout, batching of the computation, the use of SSE2 instructions, and particularly leverage SSSE3 and SSE4 fixed-point instructions which provide a 3× improvement over an optimized floating-point baseline. We use speech recognition as an example task, and show that a real-time hybrid hidden Markov model/neural network (HMM/NN) large vocabulary system can be built with a 10× speedup over an unoptimized baseline and a 4× speedup over an aggressively optimized floating-point baseline at no cost in accuracy. The techniques described extend readily to neural network training and provide an effective alternative to the use of specialized hardware.', '저자': 'Vincent Vanhoucke, Andrew Senior, Mark Z Mao', '전체 인용횟수': '1020회 인용201220132014201520162017201820192020202120222023752541809913716513111812180', '컨퍼런스': 'Advances in Neural Information Processing Systems', '학술 문서': 'Improving the speed of neural networks on CPUsV Vanhoucke, A Senior, MZ Mao - 20111020회 인용 관련 학술자료 전체 12개의 버전 '}, title='Improving the speed of neural networks on CPUs', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Sim-to-real: Learning agile locomotion for quadruped robots': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/4/27', '설명': 'Designing agile locomotion for quadruped robots often requires extensive expertise and tedious manual tuning. In this paper, we present a system to automate this process by leveraging deep reinforcement learning techniques. Our system can learn quadruped locomotion from scratch using simple reward signals. In addition, users can provide an open loop reference to guide the learning process when more control over the learned gait is needed. The control policies are learned in a physics simulator and then deployed on real robots. In robotics, policies trained in simulation often do not transfer to the real world. We narrow this reality gap by improving the physics simulator and learning robust policies. We improve the simulation using system identification, developing an accurate actuator model and simulating latency. We learn robust controllers by randomizing the physical environments, adding perturbations and designing a compact observation space. We evaluate our system on two agile locomotion gaits: trotting and galloping. After learning in simulation, a quadruped robot can successfully perform both gaits in the real world.', '저자': 'Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, Vincent Vanhoucke', '전체 인용횟수': '734회 인용20182019202020212022202320100132163166147', '컨퍼런스': 'Robotics: Science and Systems (RSS) XIV', '학술 문서': 'Sim-to-real: Learning agile locomotion for quadruped robotsJ Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner…\\xa0- arXiv preprint arXiv:1804.10332, 2018734회 인용 관련 학술자료 전체 9개의 버전 '}, title='Sim-to-real: Learning agile locomotion for quadruped robots', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'System and method for enabling the use of captured images through recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'전체 인용횟수': '721회 인용20082009201020112012201320142015201620172018201920202021202220239113938656967745846385143414424', '특허 번호': '20060251339', '특허청': 'US', '학술 문서': 'System and method for enabling the use of captured images through recognition*SB Gokturk, D Anguelov, V Vanhoucke, K Lee, D Vu…\\xa0- US Patent 7,519,200, 2009597회 인용 관련 학술자료 전체 4개의 버전 System and method for enabling the use of captured images through recognition*SB Gokturk, D Anguelov, V Vanhoucke, K Lee, D Vu…\\xa0- US Patent 8,649,572, 2014145회 인용 관련 학술자료 전체 4개의 버전 System and method for enabling the use of captured images through recognition*SB Gokturk, D Anguelov, V Vanhoucke, K Lee, D Vu…\\xa0- US Patent 8,897,505, 201434회 인용 관련 학술자료 전체 4개의 버전 '}, title='System and method for enabling the use of captured images through recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'On rectified linear units for speech processing': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/5/26', '게시자': 'IEEE', '설명': 'Deep neural networks have recently become the gold standard for acoustic modeling in speech recognition systems. The key computational unit of a deep network is a linear projection followed by a point-wise non-linearity, which is typically a logistic function. In this work, we show that we can improve generalization and make training of deep networks faster and simpler by substituting the logistic units with rectified linear units. These units are linear when their input is positive and zero otherwise. In a supervised setting, we can successfully train very deep nets from random initialization on a large vocabulary speech recognition task achieving lower word error rates than using a logistic network with the same topology. Similarly in an unsupervised setting, we show how we can learn sparse features that can be useful for discriminative tasks. All our experiments are executed in a distributed environment using several\\xa0…', '저자': 'Matthew D Zeiler, M Ranzato, Rajat Monga, Min Mao, Kun Yang, Quoc Viet Le, Patrick Nguyen, Alan Senior, Vincent Vanhoucke, Jeffrey Dean, Geoffrey E Hinton', '전체 인용횟수': '713회 인용201320142015201620172018201920202021202220231045797885766871746950', '컨퍼런스': '2013 IEEE International Conference on Acoustics, Speech and Signal Processing', '페이지': '3517-3521', '학술 문서': 'On rectified linear units for speech processingMD Zeiler, M Ranzato, R Monga, M Mao, K Yang…\\xa0- 2013 IEEE International Conference on Acoustics\\xa0…, 2013713회 인용 관련 학술자료 전체 21개의 버전 '}, title='On rectified linear units for speech processing', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Using simulation and domain adaptation to improve efficiency of deep robotic grasping': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/5/21', '설명': 'Instrumenting and collecting annotated visual grasping datasets to train modern machine learning algorithms can be extremely time-consuming and expensive. An appealing alternative is to use off-the-shelf simulators to render synthetic data for which ground-truth annotations are generated automatically. Unfortunately, models trained purely on simulated data often fail to generalize to the real world. We study how randomized simulated environments and domain adaptation methods can be extended to train a grasping system to grasp novel objects from raw monocular RGB images. We extensively evaluate our approaches with a total of more than 25,000 physical test grasps, studying a range of simulation conditions and domain adaptation methods, including a novel extension of pixel-level domain adaptation that we term the GraspGAN. We show that, by using synthetic data and domain adaptation, we are able to\\xa0…', '저자': 'Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mrinal Kalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, Sergey Levine, Vincent Vanhoucke', '전체 인용횟수': '677회 인용201720182019202020212022202376011714313312785', '컨퍼런스': '2018 IEEE International Conference on Robotics and Automation (ICRA)', '페이지': '4243-4250', '학술 문서': 'Using simulation and domain adaptation to improve efficiency of deep robotic graspingK Bousmalis, A Irpan, P Wohlhart, Y Bai, M Kelcey…\\xa0- 2018 IEEE international conference on robotics and\\xa0…, 2018677회 인용 관련 학술자료 전체 8개의 버전 '}, title='Using simulation and domain adaptation to improve efficiency of deep robotic grasping', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Do as I can, not as I say: Grounding language in robotic affordances': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2022/4/4', '설명': 'Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model\\'s \"hands and eyes,\" while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project\\'s website and the video can be found at https\\xa0…', '저널': 'arXiv preprint arXiv:2204.01691', '저자': 'Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan', '전체 인용횟수': '632회 인용2022202397532', '학술 문서': 'Do as i can, not as i say: Grounding language in robotic affordancesM Ahn, A Brohan, N Brown, Y Chebotar, O Cortes…\\xa0- arXiv preprint arXiv:2204.01691, 2022560회 인용 관련 학술자료 전체 2개의 버전 Do as i can, not as i say: Grounding language in robotic affordances*A Brohan, Y Chebotar, C Finn, K Hausman, A Herzog…\\xa0- Conference on Robot Learning, 202376회 인용 관련 학술자료 전체 3개의 버전 '}, title='Do as I can, not as I say: Grounding language in robotic affordances', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'YouTube-BoundingBoxes: A large high-precision human-annotated data set for object detection in video': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'We introduce a new large-scale data set of video URLs with densely-sampled object bounding box annotations called YouTube-BoundingBoxes (YT-BB). The data set consists of approximately 380,000 video segments about 19s long, automatically selected to feature objects in natural settings without editing or post-processing, with a recording quality often akin to that of a hand-held cell phone camera. The objects represent a subset of the COCO label set. All video segments were human-annotated with high-precision classification labels and bounding boxes at 1 frame per second. The use of a cascade of increasingly precise human annotations ensures a label accuracy above 95% for every class and tight bounding boxes. Finally, we train and evaluate well-known deep network architectures and report baseline figures for per-frame classification and localization. We also demonstrate how the temporal contiguity of video can potentially be used to improve such inferences. The data set can be found at https://research. google. com/youtube-bb. We hope the availability of such large curated corpus will spur new advances in video object detection and tracking.', '저자': 'Esteban Real, Jonathon Shlens, Stefano Mazzocchi, Xin Pan, Vincent Vanhoucke', '전체 인용횟수': '611회 인용201720182019202020212022202313266799145138119', '컨퍼런스': 'IEEE Conference on Computer Vision and Pattern Recognition (CVPR)', '페이지': '5296-5305', '학술 문서': 'Youtube-boundingboxes: A large high-precision human-annotated data set for object detection in videoE Real, J Shlens, S Mazzocchi, X Pan, V Vanhoucke\\xa0- proceedings of the IEEE Conference on Computer\\xa0…, 2017611회 인용 관련 학술자료 전체 11개의 버전 '}, title='YouTube-BoundingBoxes: A large high-precision human-annotated data set for object detection in video', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'System and method for enabling search and retrieval from image files based on recognized information': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/10/5', '발명자': 'Salih Burak Gokturk, Dragomir Anguelov, Vincent Vanhoucke, Kuang-chih Lee, Diem Vu, Danny Yang, Munjal Shah, Azhar Khan', '설명': 'An embodiment provides for enabling retrieval of a collection of captured images that form at least a portion of a library of images. For each image in the collection, a captured image may be analyzed to recognize information from image data contained in the captured image, and an index may be generated, where the index data is based on the recognized information. Using the index, functionality such as search and retrieval is enabled. Various recognition techniques, including those that use the face, clothing, apparel, and combinations of characteristics may be utilized. Recognition may be performed on, among other things, persons and text carried on objects.', '전체 인용횟수': '514회 인용20092010201120122013201420152016201720182019202020212022202371632474557694941333325201614', '출원번호': '11246741', '특허 번호': '7809722', '특허청': 'US', '학술 문서': 'System and method for enabling search and retrieval from image files based on recognized informationSB Gokturk, D Anguelov, V Vanhoucke, K Lee, D Vu…\\xa0- US Patent 7,809,722, 2010514회 인용 관련 학술자료 전체 4개의 버전 '}, title='System and method for enabling search and retrieval from image files based on recognized information', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'System and method for recognizing objects from images and identifying relevancy amongst images and information': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/10/5', '발명자': 'Salih Burak Gokturk, Dragomir Anguelov, Vincent Vanhoucke, Kuang-chih Lee, Diem Vu, Danny Yang, Munjal Shah, Azhar Khan', '설명': 'An embodiment provides for enabling retrieval of a collection of captured images that form at least a portion of a library of images. For each image in the collection, a captured image may be analyzed to recognize information from image data contained in the captured image, and an index may be generated, where the index data is based on the recognized information. Using the index, functionality such as search and retrieval is enabled. Various recognition techniques, including those that use the face, clothing, apparel, and combinations of characteristics may be utilized. Recognition may be performed on, among other things, persons and text carried on objects.', '전체 인용횟수': '455회 인용20092010201120122013201420152016201720182019202020212022202391424353845514731223931272212', '출원번호': '11246589', '특허 번호': '7809192', '특허청': 'US', '학술 문서': 'System and method for recognizing objects from images and identifying relevancy amongst images and informationSB Gokturk, D Anguelov, V Vanhoucke, K Lee, D Vu…\\xa0- US Patent 7,809,192, 2010455회 인용 관련 학술자료 전체 4개의 버전 '}, title='System and method for recognizing objects from images and identifying relevancy amongst images and information', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'System and method for providing objectified image renderings using recognition information from images': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/8/24', '발명자': 'Salih Burak Gokturk, Dragomir Anguelov, Vincent Vanhoucke, Kuang-chih Lee, Diem Vu, Danny Yang, Munjal Shah, Azhar Khan', '설명': 'An embodiment provides for enabling retrieval of a collection of captured images that form at least a portion of a library of images. For each image in the collection, a captured image may be analyzed to recognize information from image data contained in the captured image, and an index may be generated, where the index data is based on the recognized information. Using the index, functionality such as search and retrieval is enabled. Various recognition techniques, including those that use the face, clothing, apparel, and combinations of characteristics may be utilized. Recognition may be performed on, among other things, persons and text carried on objects.', '전체 인용횟수': '453회 인용20092010201120122013201420152016201720182019202020212022202361611221835413842372949512922', '출원번호': '11246434', '특허 번호': '7783135', '특허청': 'US', '학술 문서': 'System and method for providing objectified image renderings using recognition information from imagesSB Gokturk, D Anguelov, V Vanhoucke, K Lee, D Vu…\\xa0- US Patent 7,783,135, 2010366회 인용 관련 학술자료 전체 4개의 버전 System and method for providing objectified image renderings using recognition information from imagesSB Gokturk, D Anguelov, V Vanhoucke, K Lee, D Vu…\\xa0- US Patent 8,139,900, 201259회 인용 관련 학술자료 전체 4개의 버전 System and method for providing objectified image renderings using recognition information from images*SB Gokturk, D Anguelov, V Vanhoucke, K Lee, D Vu…\\xa0- US Patent 8,630,513, 201438회 인용 관련 학술자료 전체 4개의 버전 System and method for providing objectified image renderings using recognition information from images*SB Gokturk, D Anguelov, VO Vanhoucke, K Lee, DT Vu…\\xa0- US Patent 9,430,719, 201618회 인용 관련 학술자료 전체 4개의 버전 System and method for providing objectified image renderings using recognition information from images*SB Gokturk, D Anguelov, V Vanhoucke, K Lee, D Vu…\\xa0- US Patent 9,171,013, 201515회 인용 관련 학술자료 전체 4개의 버전 '}, title='System and method for providing objectified image renderings using recognition information from images', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'System and method for enabling image recognition and searching of images': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/7/1', '발명자': 'Salih Burak Gokturk, Baris Sumengen, Diem Vu, Navneet Dalal, Danny Yang, Xiaofan Lin, Azhar Khan, Munjal Shah, Dragomir Anguelov, Lorenzo Torresani, Vincent Vanhoucke', '설명': 'Embodiments programmatically analyze each of a plurality of images in order to determine one or more visual characteristics about an item shown in each of the plurality of images. Data is stored corresponding to the one or more visual characteristics. An interface in is provided for which a user is able to specify one or more search criteria. In response to receiving the one or more search criteria, a search operation is performed to identify one or more items that have a visual characteristic that satisfies at least some of the one or more search criteria.', '전체 인용횟수': '91회 인용20112012201320142015201620172018201920202021202220231372210512876441', '출원번호': '12648245', '특허청': 'US', '학술 문서': 'System and method for enabling image recognition and searching of imagesSB Gokturk, B Sumengen, D Vu, N Dalal, D Yang, X Lin…\\xa0- US Patent App. 12/648,245, 201091회 인용 관련 학술자료 전체 2개의 버전 '}, title='System and method for enabling image recognition and searching of images', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'System and method for search portions of objects in images and features thereof': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/4/14', '발명자': 'Salih Burak Gokturk, Baris Sumengen, Diem Vu, Navneet Dalal, Danny Yang, Xiaofan Lin, Azhar Khan, Munjal Shah, Dragomir Anguelov, Lorenzo Torresani, Vincent Vanhoucke', '설명': 'Embodiments enable searching of portions of objects in images, including programmatically analyzing each image in a collection in order to determine image data that, for individual images in the collection, represents one or more visual characteristics of a portion of an object shown in that image. A user is enabled to specify one or more search criteria that includes image data, and a search result may be determined based on one or more images in the collection that show a corresponding object that has a portion that satisfies a threshold. The threshold is defined at least in part by the one or more search criteria.', '전체 인용횟수': '58회 인용201420152016201720182019202020212022202312184971277', '출원번호': '13619939', '특허 번호': '9008435', '특허청': 'US', '학술 문서': 'System and method for search portions of objects in images and features thereofSB Gokturk, B Sumengen, D Vu, N Dalal, D Yang, X Lin…\\xa0- US Patent 9,008,435, 201558회 인용 관련 학술자료 전체 4개의 버전 '}, title='System and method for search portions of objects in images and features thereof', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Multilingual acoustic models using distributed deep neural networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/5/26', '설명': \"Today's speech recognition technology is mature enough to be useful for many practical applications. In this context, it is of paramount importance to train accurate acoustic models for many languages within given resource constraints such as data, processing power, and time. Multilingual training has the potential to solve the data issue and close the performance gap between resource-rich and resource-scarce languages. Neural networks lend themselves naturally to parameter sharing across languages, and distributed implementations have made it feasible to train large networks. In this paper, we present experimental results for cross- and multi-lingual network training of eleven Romance languages on 10k hours of data in total. The average relative gains over the monolingual baselines are 4%/2% (data-scarce/data-rich languages) for cross- and 7%/2% for multi-lingual training. However, the additional gain\\xa0…\", '저자': 'Georg Heigold, Vincent Vanhoucke, Alan Senior, Patrick Nguyen, Marc’Aurelio Ranzato, Matthieu Devin, Jeffrey Dean', '전체 인용횟수': '375회 인용20132014201520162017201820192020202120222023936414839434136382121', '컨퍼런스': '2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)', '페이지': '8619-8623', '학술 문서': 'Multilingual acoustic models using distributed deep neural networksG Heigold, V Vanhoucke, A Senior, P Nguyen…\\xa0- 2013 IEEE international conference on acoustics\\xa0…, 2013375회 인용 관련 학술자료 전체 11개의 버전 '}, title='Multilingual acoustic models using distributed deep neural networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'System and method for enabling image recognition and searching of remote content on display': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/5/20', '발명자': 'Salih Burak Gokturk, Dan Chiao, Jacquie Phillips, Mark Moran, Vincent Vanhoucke, Azhar Khan, Xiaofan Lin, Munjal Shah, Andrew Miller, Navneet Dalal, Diem Vu', '설명': 'Images are analyzed by programmatic mechanisms for assessing one or more remote web pages to retrieve content on display at remote web pages. The retrieved images may be analyzed to determine information about an object shown in a corresponding images of the content on display. At least a portion of the object shown in the corresponding image of the content on display may be made selectable and associated with the determined information. This determined information may subsequently be used, in for example, search applications.', '전체 인용횟수': '321회 인용200820092010201120122013201420152016201720182019202020212022202318141016243038242717182437249', '출원번호': '11777894', '특허 번호': '8732025', '특허청': 'US', '학술 문서': 'System and method for enabling image recognition and searching of remote content on displaySB Gokturk, D Chiao, J Phillips, M Moran, V Vanhoucke…\\xa0- US Patent 8,732,025, 2014321회 인용 관련 학술자료 전체 4개의 버전 '}, title='System and method for enabling image recognition and searching of remote content on display', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Arbitrary style transfer in real-time with adaptive instance normalization': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition, our approach allows flexible user controls such as content-style trade-off, style interpolation, color & spatial controls, all using a single feed-forward neural network.', '저자': 'Xun Huang, Serge Belongie', '전체 인용횟수': '3895회 인용20172018201920202021202220231812129454684110321026', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '1501-1510', '학술 문서': 'Arbitrary style transfer in real-time with adaptive instance normalizationX Huang, S Belongie\\xa0- Proceedings of the IEEE international conference on\\xa0…, 20173895회 인용 관련 학술자료 전체 10개의 버전 '}, title='Arbitrary style transfer in real-time with adaptive instance normalization', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The caltech-ucsd birds-200-2011 dataset': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011', '게시자': 'California Institute of Technology', '설명': 'CUB-200-2011 is an extended version of CUB-200 [7], a challenging dataset of 200 bird species. The extended version roughly doubles the number of images per category and adds new part localization annotations. All images are annotated with bounding boxes, part locations, and at- tribute labels. Images and annotations were filtered by mul- tiple users of Mechanical Turk. We introduce benchmarks and baseline experiments for multi-class categorization and part localization.', '저자': 'Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, Serge Belongie', '전체 인용횟수': '3646회 인용20132014201520162017201820192020202120222023204075107168269390440690732658', '학술 문서': 'The caltech-ucsd birds-200-2011 datasetC Wah, S Branson, P Welinder, P Perona, S Belongie - 20113646회 인용 관련 학술자료 전체 8개의 버전 ', '호': '2010-001'}, title='The caltech-ucsd birds-200-2011 dataset', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Robust object tracking with online multiple instance learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/12/23', '게시자': 'IEEE', '권': '33', '설명': 'In this paper, we address the problem of tracking an object in a video given its location in the first frame and no other information. Recently, a class of tracking techniques called “tracking by detection” has been shown to give promising results at real-time speeds. These methods train a discriminative classifier in an online manner to separate the object from the background. This classifier bootstraps itself by using the current tracker state to extract positive and negative examples from the current frame. Slight inaccuracies in the tracker can therefore lead to incorrectly labeled training examples, which degrade the classifier and can cause drift. In this paper, we show that using Multiple Instance Learning (MIL) instead of traditional supervised learning avoids these problems and can therefore lead to a more robust tracker with fewer parameter tweaks. We propose a novel online MIL algorithm for object tracking that\\xa0…', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Boris Babenko, Ming-Hsuan Yang, Serge Belongie', '전체 인용횟수': '2650회 인용201120122013201420152016201720182019202020212022202313541852903873523163162521721249149', '페이지': '1619-1632', '학술 문서': 'Robust object tracking with online multiple instance learningB Babenko, MH Yang, S Belongie\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20102650회 인용 관련 학술자료 전체 13개의 버전 ', '호': '8'}, title='Robust object tracking with online multiple instance learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Visual tracking with online multiple instance learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009', '설명': 'In this paper, we address the problem of learning an adaptive appearance model for object tracking. In particular, a class of tracking techniques called “tracking by detection” have been shown to give promising results at real-time speeds. These methods train a discriminative classifier in an online manner to separate the object from the background. This classifier bootstraps itself by using the current tracker state to extract positive and negative examples from the current frame. Slight inaccuracies in the tracker can therefore lead to incorrectly labeled training examples, which degrades the classifier and can cause further drift. In this paper we show that using Multiple Instance Learning (MIL) instead of traditional supervised learning avoids these problems, and can therefore lead to a more robust tracker with fewer parameter tweaks. We present a novel online MIL algorithm for object tracking that achieves superior\\xa0…', '저자': 'B. Babenko, M.-H. Yang, S. Belongie', '전체 인용횟수': '2539회 인용2009201020112012201320142015201620172018201920202021202220237781321741942562963242412181911311068248', '컨퍼런스': 'IEEE Conference on Computer Vision and Pattern Recognition', '페이지': '983-990', '학술 문서': 'Visual tracking with online multiple instance learningB Babenko, MH Yang, S Belongie\\xa0- 2009 IEEE Conference on computer vision and Pattern\\xa0…, 20092537회 인용 관련 학술자료 전체 25개의 버전 Visual tracking with online multiple instance learning in CVPR*B Babenko, MH Yang, S Belongi - 20095회 인용 관련 학술자료 '}, title='Visual tracking with online multiple instance learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Multimodal unsupervised image-to-image translation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'Unsupervised image-to-image translation is an important and challenging problem in computer vision. Given an image in the source domain, the goal is to learn the conditional distribution of corresponding images in the target domain, without seeing any pairs of corresponding images. While this conditional distribution is inherently multimodal, existing approaches make an overly simplified assumption, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse outputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework. We assume that the image representation can be decomposed into a content code that is domain-invariant, and a style code that captures domain-specific properties. To translate an image to another domain, we recombine its content code with a random style code sampled from the style space of the target domain. We analyze the proposed framework and establish several theoretical results. Extensive experiments with comparisons to the state-of-the-art approaches further demonstrates the advantage of the proposed framework. Moreover, our framework allows users to control the style of translation outputs by providing an example style image.', '저자': 'Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz', '전체 인용횟수': '2512회 인용20182019202020212022202348274473615588501', '컨퍼런스': 'Proceedings of the European Conference on Computer Vision (ECCV)', '페이지': '172-189', '학술 문서': 'Multimodal unsupervised image-to-image translationX Huang, MY Liu, S Belongie, J Kautz\\xa0- Proceedings of the European conference on computer\\xa0…, 20182512회 인용 관련 학술자료 전체 10개의 버전 '}, title='Multimodal unsupervised image-to-image translation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'DOTA: A large-scale dataset for object detection in aerial images': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': \"Object detection is an important and challenging problem in computer vision. Although the past decade has witnessed major advances in object detection in natural scenes, such successes have been slow to aerial imagery, not only because of the huge variation in the scale, orientation and shape of the object instances on the earth's surface, but also due to the scarcity of well-annotated datasets of objects in aerial scenes. To advance object detection research in Earth Vision, also known as Earth Observation and Remote Sensing, we introduce a large-scale Dataset for Object deTection in Aerial images (DOTA). To this end, we collect 2806 aerial images from different sensors and platforms. Each image is of the size about 4000-by-4000 pixels and contains objects exhibiting a wide variety of scales, orientations, and shapes. These DOTA images are then annotated by experts in aerial image interpretation using 15 common object categories. The fully annotated DOTA images contains 188,282 instances, each of which is labeled by an arbitrary (8 dof) quadrilateral. To build a baseline for object detection in Earth Vision, we evaluate state-of-the-art object detection algorithms on DOTA. Experiments demonstrate that DOTA well represents real Earth Vision applications and are quite challenging.\", '저자': 'Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, Liangpei Zhang', '전체 인용횟수': '2014회 인용20182019202020212022202334132266460568545', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '3974-3983', '학술 문서': 'DOTA: A large-scale dataset for object detection in aerial imagesGS Xia, X Bai, J Ding, Z Zhu, S Belongie, J Luo…\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20182014회 인용 관련 학술자료 전체 20개의 버전 '}, title='DOTA: A large-scale dataset for object detection in aerial images', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Class-balanced loss based on effective number of samples': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': 'With the rapid increase of large-scale, real-world datasets, it becomes critical to address the problem of long-tailed data distribution (ie, a few classes account for most of the data, while most classes are under-represented). Existing solutions typically adopt class re-balancing strategies such as re-sampling and re-weighting based on the number of observations for each class. In this work, we argue that as the number of samples increases, the additional benefit of a newly added data point will diminish. We introduce a novel theoretical framework to measure data overlap by associating with each sample a small neighboring region rather than a single point. The effective number of samples is defined as the volume of samples and can be calculated by a simple formula (1-b^ n)/(1-b), where n is the number of samples and b\\\\in [0, 1) is a hyperparameter. We design a re-weighting scheme that uses the effective number of samples for each class to re-balance the loss, thereby yielding a class-balanced loss. Comprehensive experiments are conducted on artificially induced long-tailed CIFAR datasets and large-scale datasets including ImageNet and iNaturalist. Our results show that when trained with the proposed class-balanced loss, the network is able to achieve significant performance gains on long-tailed datasets.', '저자': 'Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, Serge Belongie', '전체 인용횟수': '1980회 인용2019202020212022202333187446623682', '컨퍼런스': 'Proceedings of the IEEE/CVF conference on computer vision and pattern recognition', '페이지': '9268-9277', '학술 문서': 'Class-balanced loss based on effective number of samplesY Cui, M Jia, TY Lin, Y Song, S Belongie\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 20191980회 인용 관련 학술자료 전체 13개의 버전 '}, title='Class-balanced loss based on effective number of samples', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Contour and texture analysis for image segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2001/6', '게시자': 'Kluwer Academic Publishers', '권': '43', '설명': ' This paper provides an algorithm for partitioning grayscale images into disjoint regions of coherent brightness and texture. Natural images contain both textured and untextured regions, so the cues of contour and texture differences are exploited simultaneously. Contours are treated in the intervening contour framework, while texture is analyzed using textons. Each of these cues has a domain of applicability, so to facilitate cue combination we introduce a gating operator based on the texturedness of the neighborhood at a pixel. Having obtained a local measure of how likely two nearby pixels are to belong to the same region, we use the spectral graph theoretic framework of normalized cuts to find partitions of the image into regions of coherent texture and brightness. Experimental results on a wide range of images are shown.', '저널': 'International journal of computer vision', '저자': 'Jitendra Malik, Serge Belongie, Thomas Leung, Jianbo Shi', '전체 인용횟수': '1671회 인용200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220232024575567858670751008911310113511710680585351324423', '페이지': '7-27', '학술 문서': 'Contour and texture analysis for image segmentationJ Malik, S Belongie, T Leung, J Shi\\xa0- International journal of computer vision, 20011671회 인용 관련 학술자료 전체 24개의 버전 '}, title='Contour and texture analysis for image segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Caltech-UCSD birds 200': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/9/29', '게시자': 'California Institute of Technology', '설명': 'Caltech-UCSD Birds 200 (CUB-200) is a challenging image dataset annotated with 200 bird species. It was created to enable the study of subordinate categorization, which is not possible with other popular datasets that focus on basic level categories (such as PASCAL VOC, Caltech-101, etc). The images were downloaded from the website Flickr and filtered by workers on Amazon Mechanical Turk. Each image is annotated with a bounding box, a rough bird segmentation, and a set of attribute labels.', '저자': 'Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, Pietro Perona', '전체 인용횟수': '1537회 인용20112012201320142015201620172018201920202021202220232122314149675996125204283280242', '학술 문서': 'Caltech-UCSD birds 200P Welinder, S Branson, T Mita, C Wah, F Schroff… - 20101537회 인용 관련 학술자료 전체 9개의 버전 ', '호': '2010-001'}, title='Caltech-UCSD birds 200', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'End-to-end scene text recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011/11/6', '게시자': 'IEEE', '설명': 'This paper focuses on the problem of word detection and recognition in natural images. The problem is significantly more challenging than reading text in scanned documents, and has only recently gained attention from the computer vision community. Sub-components of the problem, such as text detection and cropped image word recognition, have been studied in isolation [7, 4, 20]. However, what is unclear is how these recent approaches contribute to solving the end-to-end problem of word recognition. We fill this gap by constructing and evaluating two systems. The first, representing the de facto state-of-the-art, is a two stage pipeline consisting of text detection followed by a leading OCR engine. The second is a system rooted in generic object recognition, an extension of our previous work in [20]. We show that the latter approach achieves superior performance. While scene text recognition has generally been\\xa0…', '저자': 'Kai Wang, Boris Babenko, Serge Belongie', '전체 인용횟수': '1378회 인용20122013201420152016201720182019202020212022202332527999116104113142134174158127', '컨퍼런스': '2011 International conference on computer vision', '페이지': '1457-1464', '학술 문서': 'End-to-end scene text recognitionK Wang, B Babenko, S Belongie\\xa0- 2011 International conference on computer vision, 20111372회 인용 관련 학술자료 전체 19개의 버전 , 2013 IEEE Int. Conf. on Computer Vision (ICCV)*K Wang, B Babenko, S Belongie - 20115회 인용 관련 학술자료 IEEE Int. Conf. on Computer Vision (ICCV)*K Wang, B Babenko, S Belongie\\xa0- IEEE, Piscataway, NJ,(2011), 20113회 인용 관련 학술자료 ICCV 2011*K Wang, B Babenko, S Belongie - 20112회 인용 관련 학술자료 '}, title='End-to-end scene text recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Residual networks behave like ensembles of relatively shallow networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '권': '29', '설명': 'In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.', '저널': 'Advances in neural information processing systems', '저자': 'Andreas Veit, Michael J Wilber, Serge Belongie', '전체 인용횟수': '1262회 인용201620172018201920202021202220232199159194207207209149', '학술 문서': 'Residual networks behave like ensembles of relatively shallow networksA Veit, MJ Wilber, S Belongie\\xa0- Advances in neural information processing systems, 20161153회 인용 관련 학술자료 전체 12개의 버전 Residual networks are exponential ensembles of relatively shallow networks*A Veit, M Wilber, S Belongie\\xa0- arXiv preprint arXiv:1605.06431, 2016120회 인용 관련 학술자료 '}, title='Residual networks behave like ensembles of relatively shallow networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Blobworld: A system for region-based image indexing and retrieval': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1999', '게시자': 'Springer Berlin Heidelberg', '설명': ' Blobworld is a system for image retrieval based on finding coherent image regions which roughly correspond to objects. Each image is automatically segmented into regions (“blobs”) with associated color and texture descriptors. Queryingi s based on the attributes of one or two regions of interest, rather than a description of the entire image. In order to make large-scale retrieval feasible, we index the blob descriptions using a tree. Because indexing in the high-dimensional feature space is computationally prohibitive, we use a lower-rank approximation to the high-dimensional distance. Experiments show encouraging results for both querying and indexing.', '저자': 'Chad Carson, Megan Thomas, Serge Belongie, Joseph M Hellerstein, Jitendra Malik', '전체 인용횟수': '1259회 인용199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202311316288103105105969266706751583833313727171914583', '컨퍼런스': 'Visual Information and Information Systems: Third International Conference, VISUAL’99 Amsterdam, The Netherlands, June 2–4, 1999 Proceedings 3', '페이지': '509-517', '학술 문서': 'Blobworld: A system for region-based image indexing and retrievalC Carson, M Thomas, S Belongie, JM Hellerstein…\\xa0- Visual Information and Information Systems: Third\\xa0…, 19991257회 인용 관련 학술자료 전체 17개의 버전 Blobworld: A System for Region-Based Image Indexing and Retrieval (long version)?*C Carson, M Thomas, S Belongie, JM Hellerstein…\\xa0- Retrieved from the Internet Archive3회 인용 관련 학술자료 전체 9개의 버전 '}, title='Blobworld: A system for region-based image indexing and retrieval', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'An iterative image registration technique with an application to stereo vision': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1981/8/24', '권': '2', '설명': 'Image registration finds a variety of applications in computer vision. Unfortunately, traditional image registration techniques tend to be costly. We present a new image registration technique that makes use of the spatial intensity gradient of the images to find a good match using a type of Newton-Raphson iteration. Our technique is faster because it examines far fewer potential matches between the images than existing techniques. Furthermore, this registration technique can be generalized to handle rotation, scaling and shearing. We show show our technique can be adapted for use in a stereo vision system.', '저자': 'Bruce D Lucas, Takeo Kanade', '전체 인용횟수': '19046회 인용19941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202358497576111991281581972743664375527188468779681009100110951157115212191061949879861872878617', '컨퍼런스': \"IJCAI'81: 7th international joint conference on Artificial intelligence\", '페이지': '674-679', '학술 문서': \"An iterative image registration technique with an application to stereo visionBD Lucas, T Kanade\\xa0- IJCAI'81: 7th international joint conference on Artificial\\xa0…, 198119046회 인용 관련 학술자료 전체 37개의 버전 \"}, title='An iterative image registration technique with an application to stereo vision', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Neural network-based face detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1998/1', '게시자': 'IEEE', '권': '20', '설명': 'We present a neural network-based upright frontal face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We present a straightforward procedure for aligning positive face examples for training. To collect negative examples, we use a bootstrap algorithm, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting nonface training examples, which must be chosen to span the entire space of nonface images. Simple heuristics, such as using the fact that faces rarely overlap in images, can further improve the accuracy. Comparisons with several other state-of-the-art face detection systems are presented, showing that our system has comparable performance in\\xa0…', '저널': 'IEEE Transactions on pattern analysis and machine intelligence', '저자': 'Henry A Rowley, Shumeet Baluja, Takeo Kanade', '전체 인용횟수': '6343회 인용199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023245310816920726928832239936032134731132431630425630324621624620817315514312255', '페이지': '23-38', '학술 문서': 'Neural network-based face detectionHA Rowley, S Baluja, T Kanade\\xa0- IEEE Transactions on pattern analysis and machine\\xa0…, 19986343회 인용 관련 학술자료 전체 54개의 버전 ', '호': '1'}, title='Neural network-based face detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/6/13', '게시자': 'IEEE', '설명': 'In 2000, the Cohn-Kanade (CK) database was released for the purpose of promoting research into automatically detecting individual facial expressions. Since then, the CK database has become one of the most widely used test-beds for algorithm development and evaluation. During this period, three limitations have become apparent: 1) While AU codes are well validated, emotion labels are not, as they refer to what was requested rather than what was actually performed, 2) The lack of a common performance metric against which to evaluate new algorithms, and 3) Standard protocols for common databases have not emerged. As a consequence, the CK database has been used for both AU and emotion detection (even though labels for the latter have not been validated), comparison with benchmark algorithms is missing, and use of random subsets of the original database makes meta-analyses difficult. To\\xa0…', '저자': 'Patrick Lucey, Jeffrey F Cohn, Takeo Kanade, Jason Saragih, Zara Ambadar, Iain Matthews', '전체 인용횟수': '4648회 인용20112012201320142015201620172018201920202021202220235275143200258325376461508538570564496', '컨퍼런스': '2010 ieee computer society conference on computer vision and pattern recognition-workshops', '페이지': '94-101', '학술 문서': 'The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expressionP Lucey, JF Cohn, T Kanade, J Saragih, Z Ambadar…\\xa0- 2010 ieee computer society conference on computer\\xa0…, 20104648회 인용 관련 학술자료 전체 17개의 버전 '}, title='The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Shape and motion from image streams under orthography: a factorization method': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1992/11', '게시자': 'Kluwer Academic Publishers', '권': '9', '설명': ' Inferring scene geometry and camera motion from a stream of images is possible in principle, but is an ill-conditioned problem when the objects are distant with respect to their size. We have developed a factorization method that can overcome this difficulty by recovering shape and motion under orthography without computing depth as an intermediate step. An image stream can be represented by the 2F×P measurement matrix of the image coordinates of P points tracked through F frames. We show that under orthographic projection this matrix is of rank 3. Based on this observation, the factorization method uses the singular-value decomposition technique to factor the measurement matrix into two matrices which represent object shape and camera rotation respectively. Two of the three translation components are computed in a preprocessing stage. The method can also handle and obtain a\\xa0…', '저널': 'International journal of computer vision', '저자': 'Carlo Tomasi, Takeo Kanade', '전체 인용횟수': '4394회 인용19921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023112144476292108135131147156176184201211204188176178191166165177158210178132124124917470', '페이지': '137-154', '학술 문서': 'Shape and motion from image streams under orthography: a factorization methodC Tomasi, T Kanade\\xa0- International journal of computer vision, 19924093회 인용 관련 학술자료 전체 51개의 버전 Shape and motion from image streams: a factorization method.*C Tomasi, T Kanade\\xa0- Proceedings of the National Academy of Sciences, 1993231회 인용 관련 학술자료 전체 26개의 버전 Shape and motion from image streams: a factorization method: full report on the orthographic case*C Tomasi, T Kanade - 199273회 인용 관련 학술자료 Shape and motion from image stream: A factrization method*C Tomasi\\xa0- Technical Report in CMU, 199231회 인용 관련 학술자료 Shape and Motion from Image Streams: A Factorization Method. Detection and Tracking of Point Features*C Tomasi, T Kanade - 19916회 인용 관련 학술자료 '}, title='Shape and motion from image streams under orthography: a factorization method', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Detection and tracking of point': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1991/4', '권': '9', '설명': 'The factorization method described in this series of reports requires an algorithm to track the motion of features in an image stream. Given the small inter-frame displacement made possible by the factorization approach, the best tracking method turns out to be the one proposed by Lucas and Kanade in 1981.The method defines the measure of match between fixed-size feature windows in the past and current frame as the sum of squared intensity di erences over the windows. The displacement is then defined as the one that minimizes this sum. For small motions, a linearization of the image intensities leads to a Newton-Raphson style minimization. In this report, after rederiving the method in a physically intuitive way, we answer the crucial question of how to choose the feature windows that are best suited for tracking. Our selection criterion is based directly on the definition of the tracking algorithm, and expresses how well a feature can be tracked. As a result, the criterion is optimal by construction. We show by experiment that the performance of both the selection and the tracking algorithm are adequate for our factorization method, and we address the issue of how to detect occlusions. In the conclusion, we point out specific open questions for future research.', '저널': 'Int J Comput Vis', '저자': 'Carlo Tomasi, Takeo Kanade', '전체 인용횟수': '3889회 인용199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220231311818162144658811711615918120520222720717921218823724320620414714013316394', '페이지': '3', '학술 문서': 'Detection and tracking of pointC Tomasi, T Kanade\\xa0- Int J Comput Vis, 19913889회 인용 관련 학술자료 전체 30개의 버전 ', '호': '137-154'}, title='Detection and tracking of point', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Comprehensive database for facial expression analysis': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2000/3/28', '게시자': 'IEEE', '설명': 'Within the past decade, significant effort has occurred in developing methods of facial expression analysis. Because most investigators have used relatively limited data sets, the generalizability of these various methods remains unknown. We describe the problem space for facial expression analysis, which includes level of description, transitions among expressions, eliciting conditions, reliability and validity of training and test data, individual differences in subjects, head orientation and scene complexity image characteristics, and relation to non-verbal behavior. We then present the CMU-Pittsburgh AU-Coded Face Expression Image Database, which currently includes 2105 digitized image sequences from 182 adult subjects of varying ethnicity, performing multiple tokens of most primary FACS action units. This database is the most comprehensive testbed to date for comparative studies of facial expression analysis.', '저자': 'Takeo Kanade, Jeffrey F Cohn, Yingli Tian', '전체 인용횟수': '3525회 인용2000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220239192134668698121130164208222192210233223237232246204161151117107', '컨퍼런스': 'Proceedings fourth IEEE international conference on automatic face and gesture recognition (cat. No. PR00580)', '페이지': '46-53', '학술 문서': 'Comprehensive database for facial expression analysisT Kanade, JF Cohn, Y Tian\\xa0- Proceedings fourth IEEE international conference on\\xa0…, 20003525회 인용 관련 학술자료 전체 23개의 버전 '}, title='Comprehensive database for facial expression analysis', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Convolutional pose machines': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': 'Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.', '저자': 'Shih-En Wei, Varun Ramakrishna, Takeo Kanade, Yaser Sheikh', '전체 인용횟수': '3390회 인용2016201720182019202020212022202337206477594579552517376', '컨퍼런스': 'Proceedings of the IEEE conference on Computer Vision and Pattern Recognition', '페이지': '4724-4732', '학술 문서': 'Convolutional pose machinesSE Wei, V Ramakrishna, T Kanade, Y Sheikh\\xa0- Proceedings of the IEEE conference on Computer\\xa0…, 20163390회 인용 관련 학술자료 전체 17개의 버전 '}, title='Convolutional pose machines', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Multi-pie': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/5/1', '게시자': 'Elsevier', '권': '28', '설명': 'A close relationship exists between the advancement of face recognition algorithms and the availability of face databases varying factors that affect facial appearance in a controlled manner. The CMU PIE database has been very influential in advancing research in face recognition across pose and illumination. Despite its success the PIE database has several shortcomings: a limited number of subjects, a single recording session and only few expressions captured. To address these issues we collected the CMU Multi-PIE database. It contains 337 subjects, imaged under 15 view points and 19 illumination conditions in up to four recording sessions. In this paper we introduce the database and describe the recording procedure. We furthermore present results from baseline experiments using PCA and LDA classifiers to highlight similarities and differences between PIE and Multi-PIE.', '저널': 'Image and vision computing', '저자': 'Ralph Gross, Iain Matthews, Jeffrey Cohn, Takeo Kanade, Simon Baker', '전체 인용횟수': '2566회 인용200920102011201220132014201520162017201820192020202120222023193464101115166221245228265254231237176147', '페이지': '807-813', '학술 문서': 'Multi-pieR Gross, I Matthews, J Cohn, T Kanade, S Baker\\xa0- Image and vision computing, 20102566회 인용 관련 학술자료 전체 16개의 버전 ', '호': '5'}, title='Multi-pie', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Recognizing action units for facial expression analysis': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2001/2', '게시자': 'IEEE', '권': '23', '설명': 'Most automatic expression analysis systems attempt to recognize a small set of prototypic expressions, such as happiness, anger, surprise, and fear. Such prototypic expressions, however, occur rather infrequently. Human emotions and intentions are more often communicated by changes in one or a few discrete facial features. In this paper, we develop an automatic face analysis (AFA) system to analyze facial expressions based on both permanent facial features (brows, eyes, mouth) and transient facial features (deepening of facial furrows) in a nearly frontal-view face image sequence. The AFA system recognizes fine-grained changes in facial expression into action units (AU) of the Facial Action Coding System (FACS), instead of a few prototypic expressions. Multistate face and facial component models are proposed for tracking and modeling the various facial features, including lips, eyes, brows, cheeks, and\\xa0…', '저널': 'IEEE Transactions on pattern analysis and machine intelligence', '저자': 'Y-I Tian, Takeo Kanade, Jeffrey F Cohn', '전체 인용횟수': '2295회 인용20012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023297560748891871041038899931161121281331119913710312810992', '페이지': '97-115', '학술 문서': 'Recognizing action units for facial expression analysisYI Tian, T Kanade, JF Cohn\\xa0- IEEE Transactions on pattern analysis and machine\\xa0…, 20012285회 인용 관련 학술자료 전체 16개의 버전 Recognizing action units for facial expression analysisYL Tian, T Kanade, JF Colin\\xa0- Multimodal interface for human-machine\\xa0…, 200212회 인용 관련 학술자료 전체 10개의 버전 ', '호': '2'}, title='Recognizing action units for facial expression analysis', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A statistical method for 3D object detection applied to faces and cars': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2000/6/15', '게시자': 'IEEE', '권': '1', '설명': 'In this paper, we describe a statistical method for 3D object detection. We represent the statistics of both object appearance and \"non-object\" appearance using a product of histograms. Each histogram represents the joint statistics of a subset of wavelet coefficients and their position on the object. Our approach is to use many such histograms representing a wide variety of visual attributes. Using this method, we have developed the first algorithm that can reliably detect human faces with out-of-plane rotation and the first algorithm that can reliably detect passenger cars over a wide range of viewpoints.', '저자': 'Henry Schneiderman, Takeo Kanade', '전체 인용횟수': '1924회 인용2000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220236616410215415515313514013112110096968260655030212421205', '컨퍼런스': 'Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No. PR00662)', '페이지': '746-751', '학술 문서': 'A statistical method for 3D object detection applied to faces and carsH Schneiderman, T Kanade\\xa0- Proceedings IEEE Conference on Computer Vision\\xa0…, 20001909회 인용 관련 학술자료 전체 25개의 버전 A statistical approcah to 3d object detection applied to faces and cars*H Schneiderman, T Kanade\\xa0- Proceedings of the Eighth IEEE International\\xa0…, 200018회 인용 관련 학술자료 '}, title='A statistical method for 3D object detection applied to faces and cars', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A stereo matching algorithm with an adaptive window: Theory and experiment': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1994/9', '게시자': 'IEEE', '권': '16', '설명': 'A central problem in stereo matching by computing correlation or sum of squared differences (SSD) lies in selecting an appropriate window size. The window size must be large enough to include enough intensity variation for reliable matching, but small enough to avoid the effects of projective distortion. If the window is too small and does not cover enough intensity variation, it gives a poor disparity estimate, because the signal (intensity variation) to noise ratio is low. If, on the other hand, the window is too large and covers a region in which the depth of scene points (i.e., disparity) varies, then the position of maximum correlation or minimum SSD may not represent correct matching due to different projective distortions in the left and right images. For this reason, a window size must be selected adaptively depending on local variations of intensity and disparity. The authors present a method to select an appropriate\\xa0…', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Takeo Kanade, Masatoshi Okutomi', '전체 인용횟수': '1919회 인용199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202371218263347525570769210911197857397901111148468565552484729253116', '페이지': '920-932', '학술 문서': 'A stereo matching algorithm with an adaptive window: Theory and experimentT Kanade, M Okutomi\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 19941919회 인용 관련 학술자료 전체 17개의 버전 ', '호': '9'}, title='A stereo matching algorithm with an adaptive window: Theory and experiment', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Limits on super-resolution and how to break them': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2002/9', '게시자': 'IEEE', '권': '24', '설명': 'Nearly all super-resolution algorithms are based on the fundamental constraints that the super-resolution image should generate low resolution input images when appropriately warped and down-sampled to model the image formation process. (These reconstruction constraints are normally combined with some form of smoothness prior to regularize their solution.) We derive a sequence of analytical results which show that the reconstruction constraints provide less and less useful information as the magnification factor increases. We also validate these results empirically and show that, for large enough magnification factors, any smoothness prior leads to overly smooth results with very little high-frequency content. Next, we propose a super-resolution algorithm that uses a different kind of constraint in addition to the reconstruction constraints. The algorithm attempts to recognize local features in the low-resolution\\xa0…', '저널': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', '저자': 'Simon Baker, Takeo Kanade', '전체 인용횟수': '1916회 인용2002200320042005200620072008200920102011201220132014201520162017201820192020202120222023541677092949213910412115112813810711191719456494630', '페이지': '1167-1183', '학술 문서': 'Limits on super-resolution and how to break themS Baker, T Kanade\\xa0- IEEE Transactions on Pattern Analysis and Machine\\xa0…, 20021916회 인용 관련 학술자료 전체 7개의 버전 ', '호': '9'}, title='Limits on super-resolution and how to break them', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A system for video surveillance and monitoring': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2000/5', '게시자': 'CMU-RI-TR-00-12, Carnegie Melon University, Pittsburgh, Peen, America', '권': '2000', '설명': 'Under the three-year Video Surveillance and Monitoring (VSAM) project (1997–1999), the Robotics Institute at Carnegie Mellon University (CMU) and the Sarnoff Corporation developed a system for autonomous Video Surveillance and Monitoring. The technical approach uses multiple, cooperative video sensors to provide continuous coverage of people and vehicles in a cluttered environment. This final report presents an overview of the system, and of the technical accomplishments that have been achieved. c', '저널': 'VSAM final report', '저자': 'Robert T Collins, Alan J Lipton, Takeo Kanade, Hironobu Fujiyoshi, David Duggins, Yanghai Tsin, David Tolliver, Nobuyoshi Enomoto, Osamu Hasegawa, Peter Burt, Lambert Wixson', '전체 인용횟수': '1773회 인용20002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202361636396711310211810212713412211811693646762565442423522', '페이지': '1', '학술 문서': 'A system for video surveillance and monitoringRT Collins, AJ Lipton, T Kanade, H Fujiyoshi…\\xa0- VSAM final report, 20001773회 인용 관련 학술자료 전체 15개의 버전 ', '호': '1-68'}, title='A system for video surveillance and monitoring', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Stereo by intra-and inter-scanline search using dynamic programming': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1985/3', '게시자': 'IEEE', '설명': 'This paper presents a stereo matching algorithm using the dynamic programming technique. The stereo matching problem, that is, obtaining a correspondence between right and left images, can be cast as a search problem. When a pair of stereo images is rectified, pairs of corresponding points can be searched for within the same scanlines. We call this search intra-scanline search. This intra-scanline search can be treated as the problem of finding a matching path on a two-dimensional (2D) search plane whose axes are the right and left scanlines. Vertically connected edges in the images provide consistency constraints across the 2D search planes. Inter-scanline search in a three-dimensional (3D) search space, which is a stack of the 2D search planes, is needed to utilize this constraint. Our stereo matching algorithm uses edge-delimited intervals as elements to be matched, and employs the above mentioned\\xa0…', '저널': 'IEEE Transactions on pattern analysis and machine intelligence', '저자': 'Yuichi Ohta, Takeo Kanade', '전체 인용횟수': '1581회 인용198419851986198719881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202341020323839413737393844384253595047514165786269645950475245473225241614129206', '페이지': '139-154', '학술 문서': 'Stereo by intra-and inter-scanline search using dynamic programmingY Ohta, T Kanade\\xa0- IEEE Transactions on pattern analysis and machine\\xa0…, 19851581회 인용 관련 학술자료 전체 17개의 버전 ', '호': '2'}, title='Stereo by intra-and inter-scanline search using dynamic programming', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Color information for region segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1980/7/1', '게시자': 'Academic Press', '권': '13', '설명': 'In color image processing various kinds of color features can be calculated from the tristimuli R, G, and B. We attempt to derive a set of effective color features by systematic experiments of region segmentation. An Ohlander-type segmentation algorithm by recursive thresholding is employed as a tool for the experiment. At each step of segmenting a region, new color features are calculated for the pixels in that region by the Karhunen Loeve transformation of R, G, and B data. By analyzing more than 100 color features which are thus obtained during segmenting eight kinds of color pictures, we have found that a set of color features,(R+ G+ B) 3, R− B, and (2G− R− B) 2, are effective. These three features are significant in this order and in many cases a good segmentation can be achieved by using only the first two. The effectiveness of our color feature set is discussed by a comparative study with various other sets of\\xa0…', '저널': 'Computer graphics and image processing', '저자': 'Yu-Ichi Ohta, Takeo Kanade, Toshiyuki Sakai', '전체 인용횟수': '1548회 인용19851986198719881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202383715112692437273038414727515150506882456053826476585644514732383217201711', '페이지': '222-241', '학술 문서': 'Color information for region segmentationYI Ohta, T Kanade, T Sakai\\xa0- Computer graphics and image processing, 19801548회 인용 관련 학술자료 전체 5개의 버전 ', '호': '3'}, title='Color information for region segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A multiple-baseline stereo': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1993/4', '게시자': 'IEEE', '권': '15', '설명': 'A stereo matching method that uses multiple stereo pairs with various baselines generated by a lateral displacement of a camera to obtain precise distance estimates without suffering from ambiguity is presented. Matching is performed simply by computing the sum of squared-difference (SSD) values. The SSD functions for individual stereo pairs are represented with respect to the inverse distance and are then added to produce the sum of SSDs. This resulting function is called the SSSD-in-inverse-distance. It is shown that the SSSD-in-inverse-distance function exhibits a unique and clear minimum at the correct matching position, even when the underlying intensity patterns of the scene include ambiguities or repetitive patterns. The authors first define a stereo algorithm based on the SSSD-in-inverse-distance and present a mathematical analysis to show how the algorithm can remove ambiguity and increase\\xa0…', '저널': 'IEEE Transactions on pattern analysis and machine intelligence', '저자': 'Masatoshi Okutomi, Takeo Kanade', '전체 인용횟수': '1534회 인용19931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220238231828416138796673848788876669544949665542454044363228262013', '페이지': '353-363', '학술 문서': 'A multiple-baseline stereoM Okutomi, T Kanade\\xa0- IEEE Transactions on pattern analysis and machine\\xa0…, 19931528회 인용 관련 학술자료 전체 12개의 버전 A Multiple-Baseline Stereo.M OKUTOMI, T KANADE\\xa0- 電子情報通信学会論文誌 D-2, 19926회 인용 관련 학술자료 ', '호': '4'}, title='A multiple-baseline stereo', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Vision and navigation for the Carnegie-Mellon Navlab': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1988/5', '게시자': 'IEEE', '권': '10', '설명': 'A distributed architecture articulated around the CODGER (communication database with geometric reasoning) knowledge database is described for a mobile robot system that includes both perception and navigation tools. Results are described for vision and navigation tests using a mobile testbed that integrates perception and navigation capabilities that are based on two types of vision algorithms: color vision for road following, and 3-D vision for obstacle detection and avoidance. The perception modules are integrated into a system that allows the vehicle to drive continuously in an actual outdoor environment. The resulting system is able to navigate continuously on roads while avoiding obstacles.< >', '저널': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', '저자': 'Charles Thorpe, Martial H Hebert, Takeo Kanade, Steven A Shafer', '전체 인용횟수': '1126회 인용19871988198919901991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220238204049494634614843293623341426251831282927323527343229182731242628231611', '페이지': '362-373', '학술 문서': 'Vision and navigation for the Carnegie-Mellon NavlabC Thorpe, MH Hebert, T Kanade, SA Shafer\\xa0- IEEE Transactions on Pattern Analysis and Machine\\xa0…, 19881006회 인용 관련 학술자료 전체 7개의 버전 Vision and navigation for the Carnegie-Mellon NavlabC Thorpe, M Hebert, T Kanade, S Shafer\\xa0- Annual Review of Computer Science, 1987149회 인용 관련 학술자료 전체 6개의 버전 ', '호': '3'}, title='Vision and navigation for the Carnegie-Mellon Navlab', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Kalman filter-based algorithms for estimating depth from image sequences': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1989/9', '게시자': 'Kluwer Academic Publishers', '권': '3', '설명': ' Using known camera motion to estimate depth from image sequences is an important problem in robot vision. Many applications of depth-from-motion, including navigation and manipulation, require algorithms that can estimate depth in an on-line, incremental fashion. This requires a representation that records the uncertainty in depth estimates and a mechanism that integrates new measurements with existing depth estimates to reduce the uncertainty over time. Kalman filtering provides this mechanism. Previous applications of Kalman filtering to depth-from-motion have been limited to estimating depth at the location of a sparse set of features. In this paper, we introduce a new, pixel-based (iconic) algorithm that estimates depth and depth uncertainty at each pixel and incrementally refines these estimates over time. We describe the algorithm and contrast its formulation and performance to that of a feature\\xa0…', '저널': 'International Journal of Computer Vision', '저자': 'Larry Matthies, Takeo Kanade, Richard Szeliski', '전체 인용횟수': '1087회 인용198819891990199119921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023413254261416744303933253435292927222731403335374636293117162621147184', '페이지': '209-238', '학술 문서': 'Kalman filter-based algorithms for estimating depth from image sequencesL Matthies, T Kanade, R Szeliski\\xa0- International Journal of Computer Vision, 19891087회 인용 관련 학술자료 전체 26개의 버전 ', '호': '3'}, title='Kalman filter-based algorithms for estimating depth from image sequences', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Computer-assisted surgery planner and intra-operative guidance system': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2001/3/20', '발명자': \"Anthony M DiGioia III, David A Simon, Branislav Jaramaz, Michael K Blackwell, Frederick M Morgan, Robert V O'toole, Takeo Kanade\", '설명': 'An apparatus for facilitating the implantation of an artificial component in one of a hip joint, a knee joint, a hand and wrist joint, an elbow joint, a shoulder joint, and a foot and ankle joint. The apparatus includes a pre-operative geometric planner and a pre-operative kinematic biomechanical simulator in communication with the pre-operative geometric planner.', '전체 인용횟수': '998회 인용2003200420052006200720082009201020112012201320142015201620172018201920202021202220235919162417153844458667687195566952887535', '출원번호': '09189914', '특허 번호': '6205411', '특허청': 'US', '학술 문서': 'Computer-assisted surgery planner and intra-operative guidance systemAM DiGioia III, DA Simon, B Jaramaz, MK Blackwell…\\xa0- US Patent 6,205,411, 2001998회 인용 관련 학술자료 전체 2개의 버전 '}, title='Computer-assisted surgery planner and intra-operative guidance system', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Surface reflection: physical and geometrical perspectives': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1991', '권': '13', '설명': 'Machine vision can greatly benefit from the development of accurate reflectance models. There are two approaches to the study of reflection: physical and geometrical optics. While geometrical models may be construed as mere approximations to physical models, they possess simpler mathematical forms that often render them more usable than physical models. However, in general, geometrical models are applicable only when the wavelength of incident light is small compared to the dimensions of the surface imperfections. Therefore, it is incorrect to use these models to interpret or predict reflections from smooth surfaces; only physical models are capable of describing the underlying reflection mechanism.In this paper, reflectance models based on physical optics and geometrical optics are studied in detail. More specifically, we consider the Beckmann-Spizzichino (physical optics) model and the Torrance-Sparrow (geometrical optics) model. We have chosen these two particular models as they have been reported to fit experimental data well. Each model is described in detail, and the conditions that determine the validity of the model are clearly stated. By studying reflectance curves predicted by the two models, we propose a reflectance framework comprising three components: the diffuse lobe, the specular lobe, and the specular spike. The effects of surface roughness on the three primary components are analyzed in detail.', '저자': 'Shree K Nayar, Katsushi Ikeuchi, Takeo Kanade', '전체 인용횟수': '969회 인용199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220231614322132222345233645382853474534313231224736302625182215252115', '페이지': '611-634', '학술 문서': 'Surface reflection: physical and geometrical perspectivesSK Nayar, K Ikeuchi, T Kanade - 1991969회 인용 관련 학술자료 전체 13개의 버전 ', '호': '7'}, title='Surface reflection: physical and geometrical perspectives', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Graph-based visual saliency': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2006', '권': '19', '설명': 'A new bottom-up visual saliency model, Graph-Based Visual Saliency (GBVS), is proposed. It consists of two steps: rst forming activation maps on certain feature channels, and then normalizing them in a way which highlights conspicuity and admits combination with other maps. The model is simple, and biologically plausible insofar as it is naturally parallelized. This model powerfully predicts human xations on 749 variations of 108 natural images, achieving 98% of the ROC area of a human-based control, whereas the classical algorithms of Itti & Koch ([2],[3],[4]) achieve only 84%.', '저널': 'Advances in neural information processing systems', '저자': 'Jonathan Harel, Christof Koch, Pietro Perona', '전체 인용횟수': '4654회 인용2008200920102011201220132014201520162017201820192020202120222023133080157223340440499550481463394297273196146', '학술 문서': 'Graph-based visual saliencyJ Harel, C Koch, P Perona\\xa0- Advances in neural information processing systems, 20064654회 인용 관련 학술자료 전체 19개의 버전 '}, title='Graph-based visual saliency', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Caltech-256 object category dataset': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2007/3/10', '게시자': 'California Institute of Technology', '설명': 'We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 [1] was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) artifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, then benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching [2] algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions.', '저자': 'Gregory Griffin, Alex Holub, Pietro Perona', '전체 인용횟수': '2789회 인용20072008200920102011201220132014201520162017201820192020202120222023216274120135178189204208207190212223225193146146', '학술 문서': 'Caltech-256 object category datasetG Griffin, A Holub, P Perona - 20072789회 인용 관련 학술자료 전체 3개의 버전 '}, title='Caltech-256 object category dataset', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Self-tuning spectral clustering': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2004', '권': '17', '설명': 'We study a number of open issues in spectral clustering:(i) Selecting the appropriate scale of analysis,(ii) Handling multi-scale data,(iii) Clustering with irregular background clutter, and,(iv) Finding automatically the number of groups. We first propose that a ‘local’scale should be used to compute the affinity between each pair of points. This local scaling leads to better clustering especially when the data includes multiple scales and when the clusters are placed within a cluttered background. We further suggest exploiting the structure of the eigenvectors to infer automatically the number of groups. This leads to a new algorithm in which the final randomly initialized k-means stage is eliminated.', '저널': 'Advances in neural information processing systems', '저자': 'Lihi Zelnik-Manor, Pietro Perona', '전체 인용횟수': '2767회 인용20052006200720082009201020112012201320142015201620172018201920202021202220231636528598140169179191162198194165195201183183150137', '학술 문서': 'Self-tuning spectral clusteringL Zelnik-Manor, P Perona\\xa0- Advances in neural information processing systems, 20042767회 인용 관련 학술자료 전체 17개의 버전 '}, title='Self-tuning spectral clustering', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Preattentive texture discrimination with early vision mechanisms': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1990/5/1', '게시자': 'Optica Publishing Group', '권': '7', '설명': 'We present a model of human preattentive texture perception. This model consists of three stages: (1) convolution of the image with a bank of even-symmetric linear filters followed by half-wave rectification to give a set of responses modeling outputs of V1 simple cells, (2) inhibition, localized in space, within and among the neural-response profiles that results in the suppression of weak responses when there are strong responses at the same or nearby locations, and (3) texture-boundary detection by using wide odd-symmetric mechanisms. Our model can predict the salience of texture boundaries in any arbitrary gray-scale image. A computer implementation of this model has been tested on many of the classic stimuli from psychophysical literature. Quantitative predictions of the degree of discriminability of different texture pairs match well with experimental measurements of discriminability in human observers.', '저널': 'JOSA A', '저자': 'Jitendra Malik, Pietro Perona', '전체 인용횟수': '1368회 인용19901991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220234183938514952444657544943575860515461514333394251373824211519261317', '페이지': '923-932', '학술 문서': 'Preattentive texture discrimination with early vision mechanismsJ Malik, P Perona\\xa0- JOSA A, 19901368회 인용 관련 학술자료 전체 23개의 버전 ', '호': '5'}, title='Preattentive texture discrimination with early vision mechanisms', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The multidimensional wisdom of crowds': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010', '설명': 'Distributing labeling tasks among hundreds or thousands of annotators is an increasingly important method for annotating large datasets. We present a method for estimating the underlying value (eg the class) of each image from (noisy) annotations provided by multiple annotators. Our method is based on a model of the image formation and annotation process. Each image has different characteristics that are represented in an abstract Euclidean space. Each annotator is modeled as a multidimensional entity with variables representing competence, expertise and bias. This allows the model to discover and represent groups of annotators that have different sets of skills and knowledge, as well as groups of images that differ qualitatively. We find that our model predicts ground truth labels on both synthetic and real data more accurately than state of the art methods. Experiments also show that our model, starting from a set of binary labels, may discover rich information, such as different\" schools of thought\" amongst the annotators, and can group together images belonging to separate categories.', '저자': 'Peter Welinder, Steve Branson, Serge J. Belongie, Pietro Perona', '전체 인용횟수': '1104회 인용20112012201320142015201620172018201920202021202220233342821021081019694871051087360', '컨퍼런스': 'Advances in Neural Information Processing Systems (NIPS)', '페이지': '2424-2432', '학술 문서': 'The multidimensional wisdom of crowdsP Welinder, S Branson, P Perona, S Belongie\\xa0- Advances in neural information processing systems, 20101104회 인용 관련 학술자료 전체 16개의 버전 '}, title='The multidimensional wisdom of crowds', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " \"Learning object categories from google's image search\": Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2005/10/17', '게시자': 'IEEE', '권': '2', '설명': 'Current approaches to object category recognition require datasets of training images to be manually prepared, with varying degrees of supervision. We present an approach that can learn an object category from just its name, by utilizing the raw output of image search engines available on the Internet. We develop a new model, TSI-pLSA, which extends pLSA (as applied to visual words) to include spatial information in a translation and scale invariant manner. Our approach can handle the high intra-class variability and large proportion of unrelated images returned by search engines. We evaluate tire models on standard test sets, showing performance competitive with existing methods trained on hand prepared datasets', '저자': 'Robert Fergus, Li Fei-Fei, Pietro Perona, Andrew Zisserman', '전체 인용횟수': '961회 인용2005200620072008200920102011201220132014201520162017201820192020202120222023829608910296868775896241332791014175', '컨퍼런스': \"Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1\", '페이지': '1816-1823', '학술 문서': \"Learning object categories from google's image searchR Fergus, L Fei-Fei, P Perona, A Zisserman\\xa0- Tenth IEEE International Conference on Computer\\xa0…, 2005961회 인용 관련 학술자료 전체 28개의 버전 \"}, title=\"Learning object categories from google's image search\", authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Unsupervised learning of models for recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2000', '게시자': 'Springer Berlin Heidelberg', '설명': ' We present a method to learn object class models from unlabeled and unsegmented cluttered scenes for the purpose of visual object recognition. We focus on a particular type of model where objects are represented as flexible constellations of rigid parts (features). The variability within a class is represented by a joint probability density function (pdf) on the shape of the constellation and the output of part detectors. In a first stage, the method automatically identifies distinctive parts in the training set by applying a clustering algorithm to patterns selected by an interest operator. It then learns the statistical shape model using expectation maximization. The method achieves very good classification results on human faces and rear views of cars.', '저자': 'Markus Weber, Max Welling, Pietro Perona', '전체 인용횟수': '960회 인용200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023419262373758473736772734042362726171823189128', '컨퍼런스': 'Computer Vision-ECCV 2000: 6th European Conference on Computer Vision Dublin, Ireland, June 26–July 1, 2000 Proceedings, Part I 6', '페이지': '18-32', '학술 문서': 'Unsupervised learning of models for recognitionM Weber, M Welling, P Perona\\xa0- Computer Vision-ECCV 2000: 6th European\\xa0…, 2000960회 인용 관련 학술자료 전체 21개의 버전 '}, title='Unsupervised learning of models for recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A performance evaluation of local descriptors': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2003/6/18', '게시자': 'IEEE', '권': '2', '설명': 'In this paper, we compare the performance of descriptors computed for local interest regions, as, for example, extracted by the Harris-Affine detector [Mikolajczyk, K and Schmid, C, 2004]. Many different descriptors have been proposed in the literature. It is unclear which descriptors are more appropriate and how their performance depends on the interest region detector. The descriptors should be distinctive and at the same time robust to changes in viewing conditions as well as to errors of the detector. Our evaluation uses as criterion recall with respect to precision and is carried out for different image transformations. We compare shape context [Belongie, S, et al., April 2002], steerable filters [Freeman, W and Adelson, E, Setp. 1991], PCA-SIFT [Ke, Y and Sukthankar, R, 2004], differential invariants [Koenderink, J and van Doorn, A, 1987], spin images [Lazebnik, S, et al., 2003], SIFT [Lowe, D. G., 1999], complex filters\\xa0…', '저자': 'Krystian Mikolajczyk, Cordelia Schmid', '전체 인용횟수': '10804회 인용2004200520062007200820092010201120122013201420152016201720182019202020212022202381146339392535691835829894959849793776588473400319310249185', '컨퍼런스': 'Computer Vision and Pattern Recognition, 2003. Proceedings. 2003 IEEE Computer Society Conference on', '페이지': 'II-257-II-263 vol. 2', '학술 문서': 'A performance evaluation of local descriptorsK Mikolajczyk, C Schmid\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 200510799회 인용 관련 학술자료 전체 50개의 버전 Local feature evaluation dataset*K Mikolajczyk - 200911회 인용 관련 학술자료 Feature detection evaluation sequences*K Mikolajczyk - 20014회 인용 관련 학술자료 A Performance Evaluation of Local Descriptors. 27 (10)*K Mikolajczyk, C Schmid - 20053회 인용 관련 학술자료 '}, title='A performance evaluation of local descriptors', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2006/6/17', '게시자': 'IEEE', '권': '2', '설명': 'This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting \"spatial pyramid\" is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralba’s \"gist\" and Lowe’s SIFT descriptors.', '저자': 'Svetlana Lazebnik, Cordelia Schmid, Jean Ponce', '전체 인용횟수': '10607회 인용20062007200820092010201120122013201420152016201720182019202020212022202345127213359501642783993115311831039863653561460346285219', '컨퍼런스': \"2006 IEEE computer society conference on computer vision and pattern recognition (CVPR'06)\", '페이지': '2169-2178', '학술 문서': 'Beyond bags of features: Spatial pyramid matching for recognizing natural scene categoriesS Lazebnik, C Schmid, J Ponce\\xa0- 2006 IEEE computer society conference on computer\\xa0…, 200610607회 인용 관련 학술자료 전체 41개의 버전 '}, title='Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Scale & affine invariant interest point detectors': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2004/10/1', '게시자': 'Springer Netherlands', '권': '60', '설명': ' In this paper we propose a novel approach for detecting interest points invariant to scale and affine transformations. Our scale and affine invariant detectors are based on the following recent results: (1) Interest points extracted with the Harris detector can be adapted to affine transformations and give repeatable results (geometrically stable). (2) The characteristic scale of a local structure is indicated by a local extremum over scale of normalized derivatives (the Laplacian). (3) The affine shape of a point neighborhood is estimated based on the second moment matrix. Our scale invariant detector computes a multi-scale representation for the Harris interest point detector and then selects points at which a local measure (the Laplacian) is maximal over scales. This provides a set of distinctive points which are invariant to scale, rotation and translation as well as robust to illumination changes and limited\\xa0…', '저널': 'International journal of computer vision', '저자': 'Krystian Mikolajczyk, Cordelia Schmid', '전체 인용횟수': '5689회 인용20052006200720082009201020112012201320142015201620172018201920202021202220237420327734641347049149748143644035125822019013213310489', '페이지': '63-86', '학술 문서': 'Scale & affine invariant interest point detectorsK Mikolajczyk, C Schmid\\xa0- International journal of computer vision, 20045689회 인용 관련 학술자료 전체 50개의 버전 ', '호': '1'}, title='Scale & affine invariant interest point detectors', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning realistic human actions from movies': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2008/6/23', '게시자': 'IEEE', '설명': 'The aim of this paper is to address recognition of natural human actions in diverse and realistic video settings. This challenging but important subject has mostly been ignored in the past due to several problems one of which is the lack of realistic and annotated video datasets. Our first contribution is to address this limitation and to investigate the use of movie scripts for automatic annotation of human actions in videos. We evaluate alternative methods for action retrieval from scripts and show benefits of a text-based classifier. Using the retrieved action samples for visual learning, we next turn to the problem of action classification in video. We present a new method for video classification that builds upon and extends several recent ideas including local space-time features, space-time pyramids and multi-channel non-linear SVMs. The method is shown to improve state-of-the-art results on the standard KTH action\\xa0…', '저자': 'Ivan Laptev, Marcin Marszalek, Cordelia Schmid, Benjamin Rozenfeld', '전체 인용횟수': '4536회 인용200820092010201120122013201420152016201720182019202020212022202317111159225353449438459469414356276247193173103', '컨퍼런스': '2008 IEEE Conference on Computer Vision and Pattern Recognition', '페이지': '1-8', '학술 문서': 'Learning realistic human actions from moviesI Laptev, M Marszalek, C Schmid, B Rozenfeld\\xa0- 2008 IEEE Conference on Computer Vision and\\xa0…, 20084536회 인용 관련 학술자료 전체 33개의 버전 '}, title='Learning realistic human actions from movies', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Product quantization for nearest neighbor search': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011/1', '게시자': 'IEEE', '권': '33', '설명': 'This paper introduces a product quantization-based approach for approximate nearest neighbor search. The idea is to decompose the space into a Cartesian product of low-dimensional subspaces and to quantize each subspace separately. A vector is represented by a short code composed of its subspace quantization indices. The euclidean distance between two vectors can be efficiently estimated from their codes. An asymmetric version increases precision, as it computes the approximate distance between a vector and a code. Experimental results show that our approach searches for nearest neighbors efficiently, in particular in combination with an inverted file system. Results for SIFT and GIST image descriptors show excellent search accuracy, outperforming three state-of-the-art approaches. The scalability of our approach is validated on a data set of two billion vectors.', '저널': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', '저자': 'Herve Jegou, Matthijs Douze, Cordelia Schmid', '전체 인용횟수': '4154회 인용1988198919901991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220231834383335437156807798821078647171413161324314880130188222247254249259257233291321289', '페이지': '117-128', '학술 문서': 'Product quantization for nearest neighbor searchH Jegou, M Douze, C Schmid\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20104137회 인용 관련 학술자료 전체 35개의 버전 Searching with quantization: approximate nearest neighbor search using short codes and distance estimators*H Jégou, M Douze, C Schmid - 200926회 인용 관련 학술자료 전체 9개의 버전 ', '호': '1'}, title='Product quantization for nearest neighbor search', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Action recognition with improved trajectories': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013', '설명': 'Recently dense trajectories were shown to be an efficient video representation for action recognition and achieved state-of-the-art results on a variety of datasets. This paper improves their performance by taking into account camera motion to correct them. To estimate camera motion, we match feature points between frames using SURF descriptors and dense optical flow, which are shown to be complementary. These matches are, then, used to robustly estimate a homography with RANSAC. Human motion is in general different from camera motion and generates inconsistent matches. To improve the estimation, a human detector is employed to remove these matches. Given the estimated camera motion, we remove trajectories consistent with it. We also use this estimation to cancel out camera motion from the optical flow. This significantly improves motion-based descriptors, such as HOF and MBH. Experimental results on four challenging action datasets (ie, Hollywood2, HMDB51, Olympic Sports and UCF50) significantly outperform the current state of the art.', '저자': 'Heng Wang, Cordelia Schmid', '전체 인용횟수': '3902회 인용2013201420152016201720182019202020212022202310113295410491524493474443313257', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '3551-3558', '학술 문서': 'Action recognition with improved trajectoriesH Wang, C Schmid\\xa0- Proceedings of the IEEE international conference on\\xa0…, 20133902회 인용 관련 학술자료 전체 33개의 버전 '}, title='Action recognition with improved trajectories', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Aggregating local descriptors into a compact image representation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/6/13', '게시자': 'IEEE', '설명': 'We address the problem of image search on a very large scale, where three constraints have to be considered jointly: the accuracy of the search, its efficiency, and the memory usage of the representation. We first propose a simple yet efficient way of aggregating local image descriptors into a vector of limited dimension, which can be viewed as a simplification of the Fisher kernel representation. We then show how to jointly optimize the dimension reduction and the indexing algorithm, so that it best preserves the quality of vector comparison. The evaluation shows that our approach significantly outperforms the state of the art: the search accuracy is comparable to the bag-of-features approach for an image representation that fits in 20 bytes. Searching a 10 million image dataset takes about 50ms.', '저자': 'Hervé Jégou, Matthijs Douze, Cordelia Schmid, Patrick Pérez', '전체 인용횟수': '3102회 인용2009201020112012201320142015201620172018201920202021202220238218288128197260340326327309271262257186', '컨퍼런스': 'Conf. on Computer Vision and Pattern Recognition (CVPR 2010)', '학술 문서': 'Aggregating local descriptors into a compact image representationH Jégou, M Douze, C Schmid, P Pérez\\xa0- 2010 IEEE computer society conference on computer\\xa0…, 20103029회 인용 관련 학술자료 전체 16개의 버전 Aggregating local descriptors into a compact representation*H Jégou, M Douze, C Schmid, P Pérez\\xa0- IEEE Conf. Comput. Vis. Pattern Recognit, 2010127회 인용 관련 학술자료 전체 7개의 버전 '}, title='Aggregating local descriptors into a compact image representation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Aggregating local image descriptors into compact codes': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012/9', '게시자': 'IEEE', '권': '34', '설명': 'This paper addresses the problem of large-scale image search. Three constraints have to be taken into account: search accuracy, efficiency, and memory usage. We first present and evaluate different ways of aggregating local image descriptors into a vector and show that the Fisher kernel achieves better performance than the reference bag-of-visual words approach for any given vector dimension. We then jointly optimize dimensionality reduction and indexing in order to obtain a precise vector comparison as well as a compact representation. The evaluation shows that the image representation can be reduced to a few dozen bytes while preserving high accuracy. Searching a 100 million image data set takes about 250 ms on one processor core.', '저널': 'IEEE Trans. Pattern Analysis and Machine Intelligence', '저자': 'Hervé Jégou, Florent Perronnin, Matthijs Douze, Jose Sánchez, Patrick Pérez, Cordelia Schmid', '전체 인용횟수': '2924회 인용198819891990199119921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023183435353642695480769682105884917159159223128317013116620223520718417413611910378', '페이지': '1704-1716', '학술 문서': 'Aggregating local image descriptors into compact codesH Jégou, F Perronnin, M Douze, J Sánchez, P Pérez…\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20112924회 인용 관련 학술자료 전체 27개의 버전 ', '호': '9'}, title='Aggregating local image descriptors into compact codes', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Local features and kernels for classification of texture and object categories: A comprehensive study': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2007/6', '게시자': 'Kluwer Academic Publishers', '권': '73', '설명': ' Recently, methods based on local image features have shown promise for texture and object recognition tasks. This paper presents a large-scale evaluation of an approach that represents images as distributions (signatures or histograms) of features extracted from a sparse set of keypoint locations and learns a Support Vector Machine classifier with kernels based on two effective measures for comparing distributions, the Earth Mover’s Distance and the χ2 distance. We first evaluate the performance of our approach with different keypoint detectors and descriptors, as well as different kernels and classifiers. We then conduct a comparative evaluation with several state-of-the-art recognition methods on four texture and five object databases. On most of these databases, our implementation exceeds the best reported results and achieves comparable performance on the rest. Finally, we investigate the\\xa0…', '저널': 'International journal of computer vision', '저자': 'Jianguo Zhang, Marcin Marszałek, Svetlana Lazebnik, Cordelia Schmid', '전체 인용횟수': '2610회 인용20062007200820092010201120122013201420152016201720182019202020212022202312411051772562442632492952221891161118958605529', '페이지': '213-238', '학술 문서': 'Local features and kernels for classification of texture and object categories: A comprehensive studyJ Zhang, M Marszałek, S Lazebnik, C Schmid\\xa0- International journal of computer vision, 20072610회 인용 관련 학술자료 전체 44개의 버전 '}, title='Local features and kernels for classification of texture and object categories: A comprehensive study', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Human detection using oriented histograms of flow and appearance': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2006', '게시자': 'Springer Berlin Heidelberg', '설명': ' Detecting humans in films and videos is a challenging problem owing to the motion of the subjects, the camera and the background and to variations in pose, appearance, clothing, illumination and background clutter. We develop a detector for standing and moving people in videos with possibly moving cameras and backgrounds, testing several different motion coding schemes and showing empirically that orientated histograms of differential optical flow give the best overall performance. These motion-based descriptors are combined with our Histogram of Oriented Gradient appearance descriptors. The resulting detector is tested on several databases including a challenging test set taken from feature films and containing wide ranges of pose, motion and background variations, including moving cameras and backgrounds. We validate our results on two challenging test sets containing more than 4400\\xa0…', '저자': 'Navneet Dalal, Bill Triggs, Cordelia Schmid', '전체 인용횟수': '2425회 인용2006200720082009201020112012201320142015201620172018201920202021202220237264167889812618018423022920221717417416010178', '컨퍼런스': 'Computer Vision–ECCV 2006: 9th European Conference on Computer Vision, Graz, Austria, May 7-13, 2006. Proceedings, Part II 9', '페이지': '428-441', '학술 문서': 'Human detection using oriented histograms of flow and appearanceN Dalal, B Triggs, C Schmid\\xa0- Computer Vision–ECCV 2006: 9th European\\xa0…, 20062425회 인용 관련 학술자료 전체 18개의 버전 '}, title='Human detection using oriented histograms of flow and appearance', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Evaluation of interest point detectors': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2000/6', '게시자': 'Kluwer Academic Publishers', '권': '37', '설명': \" Many different low-level feature detectors exist and it is widely agreed that the evaluation of detectors is important. In this paper we introduce two evaluation criteria for interest points' repeatability rate and information content. Repeatability rate evaluates the geometric stability under different transformations. Information content measures the distinctiveness of features. Different interest point detectors are compared using these two criteria. We determine which detector gives the best results and show that it satisfies the criteria well.\", '저널': 'International Journal of computer vision', '저자': 'Cordelia Schmid, Roger Mohr, Christian Bauckhage', '전체 인용횟수': '2415회 인용200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220232031608910412614516715018817816118114311913091716156414326', '페이지': '151-172', '학술 문서': 'Evaluation of interest point detectorsC Schmid, R Mohr, C Bauckhage\\xa0- International Journal of computer vision, 20002415회 인용 관련 학술자료 전체 32개의 버전 ', '호': '2'}, title='Evaluation of interest point detectors', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Local grayvalue invariants for image retrieval': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1997/5', '게시자': 'IEEE', '권': '19', '설명': 'This paper addresses the problem of retrieving images from large image databases. The method is based on local grayvalue invariants which are computed at automatically detected interest points. A voting algorithm and semilocal constraints make retrieval possible. Indexing allows for efficient retrieval from a database of more than 1,000 images. Experimental results show correct retrieval in the case of partial visibility, similarity transformations, extraneous features, and small perspective deformations.', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Cordelia Schmid, Roger Mohr', '전체 인용횟수': '2382회 인용1998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202312346062699712715717518116816115713713610091838952423033322314', '페이지': '530-535', '학술 문서': 'Local grayvalue invariants for image retrievalC Schmid, R Mohr\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 19972382회 인용 관련 학술자료 전체 32개의 버전 ', '호': '5'}, title='Local grayvalue invariants for image retrieval', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Hamming embedding and weak geometric consistency for large scale image search': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2008/10', '게시자': 'Springer Berlin/Heidelberg', '권': '5302', '설명': ' This paper improves recent methods for large scale image search. State-of-the-art methods build on the bag-of-features image representation. We, first, analyze bag-of-features in the framework of approximate nearest neighbor search. This shows the sub-optimality of such a representation for matching descriptors and leads us to derive a more precise representation based on 1) Hamming embedding (HE) and 2) weak geometric consistency constraints (WGC). HE provides binary signatures that refine the matching based on visual words. WGC filters matching descriptors that are not consistent in terms of angle and scale. HE and WGC are integrated within the inverted file and are efficiently exploited for all images, even in the case of very large datasets. Experiments performed on a dataset of one million of images show a significant improvement due to the binary signature and the weak geometric\\xa0…', '저자': 'Herve Jegou, Matthijs Douze, Cordelia Schmid', '전체 인용횟수': '2379회 인용20082009201020112012201320142015201620172018201920202021202220236511201161371842312512452182131681431058766', '컨퍼런스': 'European Conference on Computer Vision (ECCV)', '페이지': '304-317', '학술 문서': 'Hamming embedding and weak geometric consistency for large scale image searchH Jegou, M Douze, C Schmid\\xa0- Computer Vision–ECCV 2008: 10th European\\xa0…, 20082359회 인용 관련 학술자료 전체 24개의 버전 INRIA Holidays dataset*H Jegou, M Douze, C Schmid - 200824회 인용 관련 학술자료 Recent advances in large scale image search*H Jegou, M Douze, C Schmid\\xa0- LIX Fall Colloquium on Emerging Trends in Visual\\xa0…, 200810회 인용 관련 학술자료 전체 15개의 버전 M. Douze, C. Schmid Hamming Embedding and Weak geometry consistency for large scale image search*H Jegou\\xa0- Proceedings of the 10th European conference on\\xa0…, 20081회 인용 관련 학술자료 Assistance device for image recognition*H Jegou, C Schmidt, M Douze\\xa0- US Patent 8,687,899, 2014관련 학술자료 전체 4개의 버전 ', '호': 'Part I'}, title='Hamming embedding and weak geometric consistency for large scale image search', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A spatio-temporal descriptor based on 3d-gradients': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2008/9/1', '게시자': 'British Machine Vision Association', '설명': 'In this work, we present a novel local descriptor for video sequences. The proposed descriptor is based on histograms of oriented 3D spatio-temporal gradients. Our contribution is four-fold. (i) To compute 3D gradients for arbitrary scales, we develop a memory-efficient algorithm based on integral videos. (ii) We propose a generic 3D orientation quantization which is based on regular polyhedrons. (iii) We perform an in-depth evaluation of all descriptor parameters and optimize them for action recognition. (iv) We apply our descriptor to various action datasets (KTH, Weizmann, Hollywood) and show that we outperform the state-of-the-art.', '저자': 'Alexander Klaser, Marcin Marszałek, Cordelia Schmid', '전체 인용횟수': '2320회 인용2009201020112012201320142015201620172018201920202021202220232858661351841822192562432162241681409769', '컨퍼런스': 'BMVC 2008-19th British Machine Vision Conference', '페이지': '275: 1-10', '학술 문서': 'A spatio-temporal descriptor based on 3d-gradientsA Klaser, M Marszałek, C Schmid\\xa0- BMVC 2008-19th British Machine Vision Conference, 20082320회 인용 관련 학술자료 전체 18개의 버전 '}, title='A spatio-temporal descriptor based on 3d-gradients', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'An affine invariant interest point detector': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2002', '게시자': 'Springer Berlin/Heidelberg', '설명': ' This paper presents a novel approach for detecting affine invariant interest points. Our method can deal with significant affine transformations including large scale changes. Such transformations introduce significant changes in the point location as well as in the scale and the shape of the neighbourhood of an interest point. Our approach allows to solve for these problems simultaneously. It is based on three key ideas: 1) The second moment matrix computed in a point can be used to normalize a region in an affine invariant way (skew and stretch). 2) The scale of the local structure is indicated by local extrema of normalized derivatives over scale. 3) An affine-adapted Harris detector determines the location of interest points. A multi-scale version of this detector is used for initialization. An iterative algorithm then modifies location, scale and neighbourhood of each point and converges to affine invariant points\\xa0…', '저널': 'Computer Vision—ECCV 2002', '저자': 'Krystian Mikolajczyk, Cordelia Schmid', '전체 인용횟수': '2229회 인용2003200420052006200720082009201020112012201320142015201620172018201920202021202220233110712013311513813915115316215113213411297758652492629', '페이지': '128-142', '학술 문서': 'An affine invariant interest point detectorK Mikolajczyk, C Schmid\\xa0- Computer Vision—ECCV 2002: 7th European\\xa0…, 20022229회 인용 관련 학술자료 전체 30개의 버전 '}, title='An affine invariant interest point detector', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The visual object tracking vot2015 challenge results': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'The Visual Object Tracking challenge 2015, VOT2015, aims at comparing short-term single-object visual trackers that do not apply pre-learned models of object appearance. Results of 62 trackers are presented. The number of tested trackers makes VOT 2015 the largest benchmark on short-term tracking to date. For each participating tracker, a short description is provided in the appendix. Features of the VOT2015 challenge that go beyond its VOT2014 predecessor are:(i) a new VOT2015 dataset twice as large as in VOT2014 with full annotation of targets by rotated bounding boxes and per-frame attribute,(ii) extensions of the VOT2014 evaluation methodology by introduction of a new performance measure. The dataset, the evaluation kit as well as the results are publicly available at the challenge website.', '저자': 'Matej Kristan, Jiri Matas, Ales Leonardis, Michael Felsberg, Luka Cehovin, Gustavo Fernandez, Tomas Vojir, Gustav Hager, Georg Nebehay, Roman Pflugfelder', '전체 인용횟수': '2103회 인용20142015201620172018201920202021202220231762147233332381325263190117', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision workshops', '페이지': '1-23', '학술 문서': 'The visual object tracking vot2015 challenge resultsM Kristan, J Matas, A Leonardis, M Felsberg, L Cehovin…\\xa0- Proceedings of the IEEE international conference on\\xa0…, 20152102회 인용 관련 학술자료 전체 102개의 버전 The VOT2015 challenge results*M Kristan, J Matas, A Leonardis, M Felsberg, L Cehovin…\\xa0- ICCV Workshop, 20153회 인용 관련 학술자료 '}, title='The visual object tracking vot2015 challenge results', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Description of interest regions with local binary patterns': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009/3/1', '게시자': 'Pergamon', '권': '42', '설명': 'This paper presents a novel method for interest region description. We adopted the idea that the appearance of an interest region can be well characterized by the distribution of its local features. The most well-known descriptor built on this idea is the SIFT descriptor that uses gradient as the local feature. Thus far, existing texture features are not widely utilized in the context of region description. In this paper, we introduce a new texture feature called center-symmetric local binary pattern (CS-LBP) that is a modified version of the well-known local binary pattern (LBP) feature. To combine the strengths of the SIFT and LBP, we use the CS-LBP as the local feature in the SIFT algorithm. The resulting descriptor is called the CS-LBP descriptor. In the matching and object category classification experiments, our descriptor performs favorably compared to the SIFT. Furthermore, the CS-LBP descriptor is computationally\\xa0…', '저널': 'Pattern Recognition', '저자': 'Marko Heikkilä, Matti Pietikäinen, Cordelia Schmid', '전체 인용횟수': '1579회 인용20092010201120122013201420152016201720182019202020212022202314639010714314917216013013411187946344', '페이지': '425-436', '학술 문서': 'Description of interest regions with local binary patternsM Heikkilä, M Pietikäinen, C Schmid\\xa0- Pattern recognition, 20091579회 인용 관련 학술자료 전체 16개의 버전 ', '호': '3'}, title='Description of interest regions with local binary patterns', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Dense trajectories and motion boundary descriptors for action recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/5', '게시자': 'Springer US', '권': '103', '설명': ' This paper introduces a video representation based on dense trajectories and motion boundary descriptors. Trajectories capture the local motion information of the video. A dense representation guarantees a good coverage of foreground motion as well as of the surrounding context. A state-of-the-art optical flow algorithm enables a robust and efficient extraction of dense trajectories. As descriptors we extract features aligned with the trajectories to characterize shape (point coordinates), appearance (histograms of oriented gradients) and motion (histograms of optical flow). Additionally, we introduce a descriptor based on motion boundary histograms (MBH) which rely on differential optical flow. The MBH descriptor shows to consistently outperform other state-of-the-art descriptors, in particular on real-world videos that contain a significant amount of camera motion. We evaluate our video representation in the\\xa0…', '저널': 'International journal of computer vision', '저자': 'Heng Wang, Alexander Kläser, Cordelia Schmid, Cheng-Lin Liu', '전체 인용횟수': '1990회 인용201320142015201620172018201920202021202220234715120526525824520817817212790', '페이지': '60-79', '학술 문서': 'Dense trajectories and motion boundary descriptors for action recognitionH Wang, A Kläser, C Schmid, CL Liu\\xa0- International journal of computer vision, 20131990회 인용 관련 학술자료 전체 31개의 버전 '}, title='Dense trajectories and motion boundary descriptors for action recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Indexing based on scale invariant interest points': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2001', '게시자': 'IEEE', '권': '1', '설명': 'This paper presents a new method for detecting scale invariant interest points. The method is based on two recent results on scale space: (1) Interest points can be adapted to scale and give repeatable results (geometrically stable). (2) Local extrema over scale of normalized derivatives indicate the presence of characteristic local structures. Our method first computes a multi-scale representation for the Harris interest point detector. We then select points at which a local measure (the Laplacian) is maximal over scales. This allows a selection of distinctive points for which the characteristic scale is known. These points are invariant to scale, rotation and translation as well as robust to illumination changes and limited changes of viewpoint. For indexing, the image is characterized by a set of scale invariant points; the scale associated with each point allows the computation of a scale invariant descriptor. Our descriptors\\xa0…', '저자': 'Krystian Mikolajczyk, Cordelia Schmid', '전체 인용횟수': '1863회 인용200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220239214072831071011261491171261221111289710673654646371922', '컨퍼런스': 'Computer Vision, 2001. ICCV 2001. Proceedings. Eighth IEEE International Conference on', '페이지': '525-531 vol. 1', '학술 문서': 'Indexing based on scale invariant interest pointsK Mikolajczyk, C Schmid\\xa0- Proceedings Eighth IEEE International Conference on\\xa0…, 20011862회 인용 관련 학술자료 전체 28개의 버전 Selection of scale-invariant parts for object class recognition*K Mikolajczyk, C Schmid\\xa0- Proceedings of the International Conference on\\xa0…, 20012회 인용 관련 학술자료 '}, title='Indexing based on scale invariant interest points', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Pattern classification': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2000', '게시자': 'Wiley', '설명': 'Pattern Classification Page 1 Pattern Classification All materials in these slides were taken \\nfrom Pattern Classification (2nd ed) by RO Duda, PE Hart and DG Stork, John Wiley & Sons, \\n2000 with the permission of the authors and the publisher Page 2 Pattern Classification, \\nChapter 2 (Part 1) 1 Page 3 Pattern Classification, Chapter 2 (Part 1) 2 • Posterior, likelihood, \\nevidence • P(ωj | x) = P(x | ωj ) . P (ωj ) / P(x) • Where in case of two categories • Posterior = (Likelihood. \\nPrior) / Evidence ∑ = = = 2j 1j j j )(P)|x(P )x(P ω ω Page 4 Pattern Classification, Chapter 2 (Part \\n1) 3 Page 5 Pattern Classification, Chapter 2 (Part 1) 4 • Decision given the posterior \\nprobabilities X is an observation for which: if P(ω1 | x) > P(ω2 | x) True state of nature = ω1 if \\nP(ω1 | x) < P(ω2 | x) True state of nature = ω2 Therefore: whenever we observe a particular x, \\nthe probability of error is : P(error | x) = P(ω1 | x) if we decide ω2 P(error | x) = P(ω2 | x) if …', '저자': 'Peter E Hart, David G Stork, Richard O Duda', '전체 인용횟수': '1070회 인용200220032004200520062007200820092010201120122013201420152016201720182019202020212022202392630614455394255544964544749472926314310994', '학술 문서': 'Pattern classificationPE Hart, DG Stork, RO Duda - 20001070회 인용 관련 학술자료 전체 3개의 버전 '}, title='Pattern classification', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Pattern classification and scene analysis': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1973/1', '게시자': 'Wiley', '권': '3', '설명': 'Until now we have assumed that the training samples used to design a classifier were labelled by their category membership. Procedures that use labelled samples are said to be supervised. Now we shall investigate a number of unsupervised procedures, which use unlabelled samples. That is, we shall see what can be done when all one has is a collection of samples without being told their category. One might wonder why anyone is interested in such an unpromising problem, and whether or not it is even possible in principle to learn anything of value from unlabelled samples. There are at least five basic reasons for interest in unsupervised procedures. First, the collection and labelling of a large set of sample patterns can be surprisingly costly. For instance, recorded speech is virtually free, but labelling the speech| marking what word or phoneme is being uttered at each instant| can be very expensive and time consuming. If a classifier can be crudely designed on a small, labelled set of samples, and then\\\\tuned up\" by allowing it to run without supervision on a large, unlabelled set, much time and trouble can be saved. Second, one might wish to proceed in the reverse direction: train with large amounts of (less expensive) unlabelled data, and only then use supervision to label the groupings found. This may be appropriate for large\\\\data mining\" applications where the contents of a large database are not known beforehand. Third, in many applications the characteristics of the patterns can change slowly with time, for example in automated food classification as the seasons change. If these changes can be tracked by a classifier running in an\\xa0…', '저자': 'Richard O Duda, Peter E Hart', '전체 인용횟수': '26829회 인용198419851986198719881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202315916617425027833443948355958870870777986497197010099899109381005989949965865813744703739679643591601502442444407371371258', '페이지': '731-739', '학술 문서': 'Pattern classification and scene analysisRO Duda, PE Hart - 197326814회 인용 관련 학술자료 전체 6개의 버전 Pattern classification ans scene analysisRO Duda, PE Hart - 197315회 인용 관련 학술자료 '}, title='Pattern classification and scene analysis', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Nearest neighbor pattern classification': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1967/1', '게시자': 'IEEE', '권': '13', '설명': 'The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of error  of such a rule must be at least as great as the Bayes probability of error  --the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in the  -category case that  , where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in\\xa0…', '저널': 'IEEE transactions on information theory', '저자': 'Thomas Cover, Peter Hart', '전체 인용횟수': '17805회 인용19931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220235381737293109135118125137187241250299346363415529586671742824882917975108312341354134114551362', '페이지': '21-27', '학술 문서': \"Nearest neighbor pattern classificationT Cover, P Hart\\xa0- IEEE transactions on information theory, 196717801회 인용 관련 학술자료 전체 5개의 버전 Nearest neighbour classification*T Cover, P Hart\\xa0- IEEE Transactions IT-134회 인용 관련 학술자료 Nearest Pattern Classification*TM Cover, PE Hart\\xa0- IEEE Trans. on Infor'mation Theory, vol. IT4회 인용 관련 학술자료 \", '호': '1'}, title='Nearest neighbor pattern classification', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A formal basis for the heuristic determination of minimum cost paths': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1968/7', '게시자': 'IEEE', '권': '4', '설명': 'Although the problem of determining the minimum cost path through a graph arises naturally in a number of interesting applications, there has been no underlying theory to guide the development of efficient search procedures. Moreover, there is no adequate conceptual framework within which the various ad hoc search strategies proposed to date can be compared. This paper describes how heuristic information from the problem domain can be incorporated into a formal mathematical theory of graph searching and demonstrates an optimality property of a class of search strategies.', '저널': 'IEEE transactions on Systems Science and Cybernetics', '저자': 'Peter E Hart, Nils J Nilsson, Bertram Raphael', '전체 인용횟수': '14444회 인용198819891990199119921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023403942624856586337435755607370831071152012322923864185106067116797408138338659651054126313061121', '페이지': '100-107', '학술 문서': 'A formal basis for the heuristic determination of minimum cost pathsPE Hart, NJ Nilsson, B Raphael\\xa0- IEEE transactions on Systems Science and\\xa0…, 196814444회 인용 관련 학술자료 전체 5개의 버전 ', '호': '2'}, title='A formal basis for the heuristic determination of minimum cost paths', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Use of the Hough transformation to detect lines and curves in pictures': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1972/1/1', '게시자': 'ACM', '권': '15', '설명': 'Hough has proposed an interesting and computationally efficient procedure for detecting lines in pictures. This paper points out that the use of angle-radius rather than slope-intercept parameters simplifies the computation further. It also shows how the method can be used for more general curve fitting, and gives alternative interpretations that explain the source of its efficiency.', '저널': 'Communications of the ACM', '저자': 'Richard O Duda, Peter E Hart', '전체 인용횟수': '10123회 인용1985198619871988198919901991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220233436588468987210285116114128989399105111107107109136201260315354379444505551583587510543504482454477446333', '페이지': '11-15', '학술 문서': 'Use of the Hough transformation to detect lines and curves in picturesRO Duda, PE Hart\\xa0- Communications of the ACM, 197210105회 인용 관련 학술자료 전체 7개의 버전 Communs Ass. comput*RO Duda, PE Hart\\xa0- Mach, 197532회 인용 관련 학술자료 Use of Hough transformation to detect line and curve in picturesRO Dude, PE Hart\\xa0- CACM, 19724회 인용 관련 학술자료 ', '호': '1'}, title='Use of the Hough transformation to detect lines and curves in pictures', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The condensed nearest neighbor rule (corresp.)': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1968/5', '게시자': 'IEEE', '권': '14', '설명': 'The purpose of this note is to introduce the condensed nearest neighbor decision rule (CNN rule) and to pose some unsolved theoretical questions which it raises. The CNN rule, one of a class of ad hoc decision rules which have appeared in the literature in the past few years, was motivated by statistical considerations', '저널': 'IEEE transactions on information theory', '저자': 'Peter Hart', '전체 인용횟수': '2868회 인용198919901991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220238917172637272342273064585664658071108859010695122108118112128110122143140134166124', '페이지': '515-516', '학술 문서': 'The condensed nearest neighbor rule (corresp.)P Hart\\xa0- IEEE transactions on information theory, 19682868회 인용 관련 학술자료 전체 7개의 버전 ', '호': '3'}, title='The condensed nearest neighbor rule (corresp.)', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Pattern classification, chapter nonparametric techniques': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2000', '게시자': 'Wiley-Interscience Publication,', '설명': 'In most pattern recognition applications, the common parametric forms of the underlying density function rarely fit the densities actually encountered in practice.', '저자': 'RO Duda, PE Hart, DG Stork, Alexandru Ionescu', '전체 인용횟수': '2659회 인용20022003200420052006200720082009201020112012201320142015201620172018201920202021202220231929479191117199161176176212202181138981331181161011088338', '페이지': '177-178', '학술 문서': 'Pattern classification, chapter nonparametric techniquesRO Duda, PE Hart, DG Stork, A Ionescu - 20002574회 인용 관련 학술자료 Pattern Classification, chapter 10*RO Duda, PE Hart, DG Stork - 200186회 인용 관련 학술자료 '}, title='Pattern classification, chapter nonparametric techniques', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning and executing generalized robot plans': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1972/1/1', '게시자': 'Elsevier', '권': '3', '설명': 'In this paper we describe some major new additions to the STRIPS robot problem-solving system. The first addition is a process for generalizing a plan produced by STRIPS so that problem-specific constants appearing in the plan are replaced by problem-independent parameters.The generalized plan, stored in a convenient format called a triangle table, has two important functions. The more obvious function is as a single macro action that can be used by STRIPS— either in whole or in part—during the solution of a subsequent problem. Perhaps less obviously, the generalized plan also plays a central part in the process that monitors the real-world execution of a plan, and allows the robot to react “intelligently” to unexpected consequences of actions.We conclude with a discussion of experiments with the system on several example problems.', '저널': 'Artificial intelligence', '저자': 'Richard E Fikes, Peter E Hart, Nils J Nilsson', '전체 인용횟수': '1695회 인용198419851986198719881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202325313644726172586776566249354034253333373032384735392921273334202127242722222521', '페이지': '251-288', '학술 문서': 'Learning and executing generalized robot plansRE Fikes, PE Hart, NJ Nilsson\\xa0- Artificial intelligence, 19721695회 인용 관련 학술자료 전체 12개의 버전 '}, title='Learning and executing generalized robot plans', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Subjective Bayesian methods for rule-based inference systems': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1976/6/7', '도서': 'Proceedings of the June 7-10, 1976, national computer conference and exposition', '설명': 'The general problem of drawing inferences from uncertain or incomplete evidence has invited a variety of technical approaches, some mathematically rigorous and some largely informal and intuitive. Most current inference systems in artificial intelligence have emphasized intuitive methods, because the absence of adequate statistical samples forces a reliance on the subjective judgment of human experts. We describe in this paper a subjective Bayesian inference method that realizes some of the advantages of both formal and informal approaches. Of particular interest are the modifications needed to deal with the inconsistencies usually found in collections of subjective statements.', '저자': 'Richard O Duda, Peter E Hart, Nils J Nilsson', '전체 인용횟수': '1002회 인용1984198519861987198819891990199119921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023193756497151505044263131212517131315177101518249161123184223108126663105', '페이지': '1075-1082', '학술 문서': 'Subjective Bayesian methods for rule-based inference systemsRO Duda, PE Hart, NJ Nilsson\\xa0- Proceedings of the June 7-10, 1976, national computer\\xa0…, 19761002회 인용 관련 학술자료 전체 8개의 버전 '}, title='Subjective Bayesian methods for rule-based inference systems', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Model Design in the Prospector Consultant System for Mineral Exploration,': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1970', '게시자': 'Edinburgh University Press, Edinburgh', '도서': 'Expert Systems in the Micro Electronic Age', '저자': 'J. Gaschnig and P. E. Hart Duda, R. O.', '전체 인용횟수': '855회 인용1984198519861987198819891990199119921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023415667606150432630323219238121015136107105513201231913149336108865', '페이지': '153-167', '학술 문서': 'Model design in the PROSPECTOR consultant system for mineral exploration*R Duda, J Gaschnig, P Hart\\xa0- Readings in Artificial Intelligence, 1981855회 인용 관련 학술자료 전체 2개의 버전 '}, title='Model Design in the Prospector Consultant System for Mineral Exploration,', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Correction to \"A Formal Basis for the Heuristic Determination of Minimum Cost Paths\"': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1972/12/1', '게시자': 'ACM', '설명': 'Our paper on the use of heuristic information in graph searching defined a path-finding algorithm, A*, and proved that it had two important properties. In the notation of the paper, we proved that if the heuristic function ñ (n) is a lower bound on the true minimal cost from node n to a goal node, then A* is admissible; i.e., it would find a minimal cost path if any path to a goal node existed. Further, we proved that if the heuristic function also satisfied something called the consistency assumption, then A* was optimal; i.e., it expanded no more nodes than any other admissible algorithm A no more informed than A*. These results were summarized in a book by one of us.', '저널': 'ACM SIGART Bulletin', '저자': 'Peter E Hart, Nils J Nilsson, Bertram Raphael', '전체 인용횟수': '571회 인용1986198719881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202333566764455143469108333101627283345413220202024272521922', '페이지': '28-29', '학술 문서': 'Correction to\" a formal basis for the heuristic determination of minimum cost paths\"PE Hart, NJ Nilsson, B Raphael\\xa0- ACM SIGART Bulletin, 1972571회 인용 관련 학술자료 전체 12개의 버전 ', '호': '37'}, title='Correction to \"A Formal Basis for the Heuristic Determination of Minimum Cost Paths\"', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Pattern Classification. JohnWiley & Sons': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2001', '저널': 'Inc.,', '저자': 'Richard O Duda, Peter E Hart, David G Stork', '전체 인용횟수': '545회 인용20002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202333916223334253036401629292730342226202014911', '학술 문서': 'Pattern Classification. JohnWiley & SonsRO Duda, PE Hart, DG Stork\\xa0- Inc.,, 2001416회 인용 관련 학술자료 Pattern Classification, John Willey & SonsRO Duda, PE Hart, DG Stork\\xa0- Inc., second edition edition, 2001129회 인용 관련 학술자료 '}, title='Pattern Classification. JohnWiley & Sons', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Image matching and retrieval by multi-access redundant hashing': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1995/11/7', '발명자': 'Jonathan J Hull, Peter E Hart', '설명': 'An improved document matching and retrieval system is disclosed where an input document is matched against a database of documents, using a descriptor database which lists descriptors and points to a list of documents containing features from which the descriptor is derived document. The descriptors are selected to be invariant to distortions caused by digitizing the documents or differences between the input document and its match in the document database. An array of accumulators is used to accumulate votes for each document in the document database as the descriptor base is scanned, wherein a vote is added to an accumulator for a document if the document is on the list as having a descriptor which is also found in the input document. The document which accumulates the most votes is returned as the matching document, or the documents with more than a threshold number of votes are returned.', '전체 인용횟수': '433회 인용1998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202391121187871611122223363642352221231816161172', '출원번호': '08222281', '특허 번호': '5465353', '특허청': 'US', '학술 문서': 'Image matching and retrieval by multi-access redundant hashingJJ Hull, PE Hart\\xa0- US Patent 5,465,353, 1995433회 인용 관련 학술자료 전체 2개의 버전 '}, title='Image matching and retrieval by multi-access redundant hashing', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Image database browsing and query using texture analysis': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1999/8/3', '발명자': 'John Cullen, Jonathan Hull, Peter Hart', '설명': \"Method and apparatus for querying a document image database based on texture, ie, analytically discernable patterns in the document images of the database. According to the invention, a document image database could be browsed for documents with a particular texture in a variety of ways. For example, a user could input an example document image with a similar appearance to the desired document. Alternatively, the user could employ a simple interface to define a synthetic document based on selection of a few categories. The synthetic document would then serve as an example for search. Or the user may employ a graphical interface to more precisely define an example for search. Thus, the user's knowledge of the general appearance of the desired document or documents provides the basis for the search.\", '전체 인용횟수': '308회 인용2001200220032004200520062007200820092010201120122013201420152016201720182019202046751111816101692233314527217118', '출원번호': '08609641', '특허 번호': '5933823', '특허청': 'US', '학술 문서': 'Image database browsing and query using texture analysisJ Cullen, J Hull, P Hart\\xa0- US Patent 5,933,823, 1999308회 인용 관련 학술자료 전체 2개의 버전 '}, title='Image database browsing and query using texture analysis', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Triggering applications based on a captured text in a mixed media environment': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/3/2', '발명자': 'Jonathan J Hull, Berna Erol, Jamey Graham, Peter E Hart, Dar-Shyang Lee, Kurt Piersol', '설명': 'A Mixed Media Reality (MMR) system and associated techniques are disclosed. The MMR system provides mechanisms for forming a mixed media document that includes media of at least two types (eg, printed paper as a first medium and digital content and/or web link as a second medium). In one particular embodiment, the MMR system includes an action processor and method, and MMR documents with an associated action. The MMR document structure is particularly advantageous because the ability to specify different actions for different MMR documents, combined with the ability to create any number of MMR documents for a particular location on any media, allows the MMR architecture to serve as a universal trigger or initiator for additional processing. In other words, addition processing or actions can be triggered or initiated based on MMR recognition. The action processor receives the output of the MMR\\xa0…', '전체 인용횟수': '282회 인용20092010201120122013201420152016201720182019202020212022202398192234343629161613101265', '출원번호': '11461032', '특허 번호': '7672543', '특허청': 'US', '학술 문서': 'Triggering applications based on a captured text in a mixed media environmentJJ Hull, B Erol, J Graham, PE Hart, DS Lee, K Piersol\\xa0- US Patent 7,672,543, 2010282회 인용 관련 학술자료 전체 4개의 버전 '}, title='Triggering applications based on a captured text in a mixed media environment', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Tokens Usable in Value-Based Transactions': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2008/10/2', '발명자': 'Peter E Hart, John W Barrus, Jamey Graham', '설명': 'Techniques for generating a token that can be used to transfer value. The token may be used to transfer value in a value-based transaction with a vendor in a way that is secure and safe and maintains anonymity of the source of the value and preserves secrecy of information that should preferably not be disclosed to an untrusted third party such as a vendor. The token comprises sufficient information that enables value to be transferred from an account associated with the token to a vendor during a value-based transaction. Such a token may be presented by a user to a vendor in a value-based transaction with the purpose of transferring value involved in the transaction to the vendor in order to complete the transaction.', '전체 인용횟수': '268회 인용2012201320142015201620172018201920202021202220232465122031563549369', '출원번호': '11694076', '특허청': 'US', '학술 문서': 'Tokens Usable in Value-Based TransactionsPE Hart, JW Barrus, J Graham\\xa0- US Patent App. 11/694,076, 2008268회 인용 관련 학술자료 전체 2개의 버전 '}, title='Tokens Usable in Value-Based Transactions', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Automatic adaptive document help for paper documents': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2002/4/9', '발명자': 'Jamey Graham, Peter E Hart', '설명': \"A reader's annotation assistant application for documents in paper form is provided by virtue of the present invention. In certain embodiments, an elongated thumbnail image of all or part of an electronically stored document is imprinted on each page of the printed form of the document. Sections of the document of interest to the reader are emphasized in the elongated thumbnail image. The emphasized area in the elongated thumbnail image assists the user with the selection of sections or pages of the document having particular interest to the user. The operation of the assistant is personalized for a particular user by setting of a sensitivity level and selection of relevant topics of interest. Some embodiments of the assistant are also capable of improved performance over time by both automatic and manual feedback. The assistant is usable with many popular electronic document formats.\", '전체 인용횟수': '229회 인용200220032004200520062007200820092010201120122013201420152016201720182019202020212022202315711141820333011111816137234221', '출원번호': '09149921', '특허 번호': '6369811', '특허청': 'US', '학술 문서': 'Automatic adaptive document help for paper documentsJ Graham, PE Hart\\xa0- US Patent 6,369,811, 2002229회 인용 관련 학술자료 전체 2개의 버전 '}, title='Automatic adaptive document help for paper documents', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'How the Hough transform was invented [DSP History]': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009/10/23', '게시자': 'IEEE', '권': '26', '설명': 'Where did this transform come from? You may vaguely recall learning that it goes back to a 1962 patent by P.V.C. Hough, though the author suspect very few readers have actually looked at that patent, the title page. If you do, you may be surprised to find that the popular transform used today is not described there. Indeed, today\\'s transform was not a single-step invention but instead took several steps that resulted in Hough\\'s initial idea being combined with an idea from an obscure branch of late 19th century mathematics to produce the familiar sinusoidal transform. The previously untold history of how this came about illustrates how important advances sometime come from combining not-obviously-related ideas. The history perhaps also illustrates that the observation of Louis Pasteur, \"Chance favors the prepared mind,\" remains as apt in the 20th and 21st centuries as it was in the 19th.', '저널': 'IEEE Signal Processing Magazine', '저자': 'Peter E Hart', '전체 인용횟수': '206회 인용2010201120122013201420152016201720182019202020212022202352112152219241216101015914', '페이지': '18-22', '학술 문서': 'How the Hough transform was invented [DSP History]PE Hart\\xa0- IEEE Signal Processing Magazine, 2009206회 인용 관련 학술자료 전체 6개의 버전 ', '호': '6'}, title='How the Hough transform was invented [DSP History]', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Pattern classification 2nd edition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2001', '권': '35', '저널': 'New York, USA: John Wiley&Sons', '저자': 'Richard O Duda, Peter E Hart, David G Stork', '전체 인용횟수': '206회 인용200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220231393713101713151412111311551458662', '학술 문서': 'Pattern classification 2nd editionRO Duda, PE Hart, DG Stork\\xa0- New York, USA: John Wiley&Sons, 2001206회 인용 관련 학술자료 '}, title='Pattern classification 2nd edition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Ssd: Single shot multibox detector': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '게시자': 'Springer International Publishing', '설명': ' We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component\\xa0…', '저자': 'Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C Berg', '전체 인용횟수': '35400회 인용2017201820192020202120222023782238444855647722579386585', '컨퍼런스': 'Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part I 14', '페이지': '21-37', '학술 문서': 'Ssd: Single shot multibox detectorW Liu, D Anguelov, D Erhan, C Szegedy, S Reed…\\xa0- Computer Vision–ECCV 2016: 14th European\\xa0…, 201634148회 인용 관련 학술자료 전체 34개의 버전 European conference on computer vision*W Liu, D Anguelov, D Erhan, C Szegedy, S Reed… - 20161518회 인용 관련 학술자료 SSD: Single shot multibox detector. arXiv 2015W Liu, D Anguelov, D Erhan, C Szegedy, S Reed…\\xa0- arXiv preprint arXiv:1512.02325, 2020132회 인용 관련 학술자료 SSD: single shot multibox detector. CoRR abs/1512.02325 (2015)*W Liu, DE Dragomir Anguelov, C Szegedy, SE Reed…\\xa0- arXiv preprint arXiv:1512.02325, 201559회 인용 관련 학술자료 SSD: Single shot multibox detector, European Conf*W Liu, D Anguelov, D Erhan, C Szegedy, S Reed…\\xa0- Computer Vision (Springer, Cham, 2016)10회 인용 관련 학술자료 SSD: Single Shot MultiBox Detector.(dec 2015)*W Liu, DE Dragomir Anguelov, C Szegedy, S Reed…\\xa0- arXiv preprint arXiv:1512.02325, 20158회 인용 관련 학술자료 SSD ARCHITECTURE*W Liu, D Anguelov, D Erhan, C Szegedy, S Reed…\\xa0- image관련 학술자료 '}, title='Ssd: Single shot multibox detector', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Intriguing properties of neural networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/12/21', '설명': \"Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.\", '저널': 'arXiv preprint arXiv:1312.6199', '저자': 'Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus', '전체 인용횟수': '14723회 인용201520162017201820192020202120222023111210436111917332424282830762680', '학술 문서': 'Intriguing properties of neural networksC Szegedy, W Zaremba, I Sutskever, J Bruna, D Erhan…\\xa0- arXiv preprint arXiv:1312.6199, 201314723회 인용 관련 학술자료 전체 20개의 버전 '}, title='Intriguing properties of neural networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Show and tell: A neural image caption generator': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.', '저자': 'Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan', '전체 인용횟수': '7010회 인용201520162017201820192020202120222023228493680920985960919957728', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '3156-3164', '학술 문서': 'Show and tell: A neural image caption generatorO Vinyals, A Toshev, S Bengio, D Erhan\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20156982회 인용 관련 학술자료 전체 27개의 버전 Show and tell: A neural image caption generator. CoRR abs/1411.4555 (2014)*O Vinyals, A Toshev, S Bengio, D Erhan\\xa0- arXiv preprint arXiv:1411.4555, 201425회 인용 관련 학술자료 Show and tell: A neural image caption generator. CoRR abs/1411.4555*O Vinyals, A Toshev, S Bengio, D Erhan\\xa0- arXiv preprint arXiv:1411.4555, 20148회 인용 관련 학술자료 '}, title='Show and tell: A neural image caption generator', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Why does unsupervised pre-training help deep learning?': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/3/31', '게시자': 'JMLR Workshop and Conference Proceedings', '설명': 'Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants with impressive results being obtained in several areas, mostly on vision and language datasets. The best results obtained on supervised learning tasks often involve an unsupervised learning component, usually in an unsupervised pre-training phase. The main question investigated here is the following: why does unsupervised pre-training work so well? Through extensive experimentation, we explore several possible explanations discussed in the literature including its action as a regularizer (Erhan et al. 2009) and as an aid to optimization (Bengio et al. 2007). Our results build on the work of Erhan et al. 2009, showing that unsupervised pre-training appears to play predominantly a regularization role in subsequent supervised training. However our results in an online setting, with a virtually unlimited data stream, point to a somewhat more nuanced interpretation of the roles of optimization and regularization in the unsupervised pre-training effect.', '저자': 'Dumitru Erhan, Aaron Courville, Yoshua Bengio, Pascal Vincent', '전체 인용횟수': '3403회 인용2010201120122013201420152016201720182019202020212022202315336565159253339353414391384362295240', '컨퍼런스': 'Proceedings of the thirteenth international conference on artificial intelligence and statistics', '페이지': '201-208', '학술 문서': 'Why does unsupervised pre-training help deep learning?D Erhan, A Courville, Y Bengio, P Vincent\\xa0- Proceedings of the thirteenth international conference\\xa0…, 20103403회 인용 관련 학술자료 전체 41개의 버전 Why Does Unsupervised Pre-training Help Deep Discriminant Learning?*D Erhan, Y Bengio, A Courville, PA Manzagol…관련 학술자료 전체 4개의 버전 '}, title='Why does unsupervised pre-training help deep learning?', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep neural networks for object detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/12', '권': '26', '설명': 'Deep Neural Networks (DNNs) have recently shown outstanding performance on the task of whole image classification. In this paper we go one step further and address the problem of object detection--not only classifying but also precisely localizing objects of various classes using DNNs. We present a simple and yet powerful formulation of object detection as a regression to object masks. We define a multi-scale inference procedure which is able to produce a high-resolution object detection at a low cost by a few network applications. The approach achieves state-of-the-art performance on Pascal 2007 VOC.', '저널': 'Advances in neural information processing systems', '저자': 'Christian Szegedy, Alexander Toshev, Dumitru Erhan', '전체 인용횟수': '1941회 인용2013201420152016201720182019202020212022202362988152189252264260260230168', '페이지': '1-9', '학술 문서': 'Deep neural networks for object detectionC Szegedy, A Toshev, D Erhan\\xa0- Advances in neural information processing systems, 20131909회 인용 관련 학술자료 전체 15개의 버전 Deep neural networks for object detection*S Christian, T Alexander, E Dumitru\\xa0- Advances in neural information processing systems, 201345회 인용 관련 학술자료 '}, title='Deep neural networks for object detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Challenges in representation learning: A report on three machine learning contests': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013', '게시자': 'Springer berlin heidelberg', '설명': ' The ICML 2013 Workshop on Challenges in Representation Learning focused on three challenges: the black box learning challenge, the facial expression recognition challenge, and the multimodal learning challenge. We describe the datasets created for these challenges and summarize the results of the competitions. We provide suggestions for organizers of future challenges and some comments on what kind of knowledge can be gained from machine learning competitions.', '저자': 'Ian J Goodfellow, Dumitru Erhan, Pierre Luc Carrier, Aaron Courville, Mehdi Mirza, Ben Hamner, Will Cukierski, Yichuan Tang, David Thaler, Dong-Hyun Lee, Yingbo Zhou, Chetan Ramaiah, Fangxiang Feng, Ruifan Li, Xiaojie Wang, Dimitris Athanasakis, John Shawe-Taylor, Maxim Milakov, John Park, Radu Ionescu, Marius Popescu, Cristian Grozea, James Bergstra, Jingjing Xie, Lukasz Romaszko, Bing Xu, Zhang Chuang, Yoshua Bengio', '전체 인용횟수': '1731회 인용20142015201620172018201920202021202220231416356399180260322354366', '컨퍼런스': 'Neural Information Processing: 20th International Conference, ICONIP 2013, Daegu, Korea, November 3-7, 2013. Proceedings, Part III 20', '페이지': '117-124', '학술 문서': 'Challenges in representation learning: A report on three machine learning contestsIJ Goodfellow, D Erhan, PL Carrier, A Courville…\\xa0- …\\xa0: 20th International Conference, ICONIP 2013, Daegu\\xa0…, 20131731회 인용 관련 학술자료 전체 12개의 버전 '}, title='Challenges in representation learning: A report on three machine learning contests', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Unsupervised pixel-level domain adaptation with generative adversarial networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'Collecting well-annotated image datasets to train modern machine learning algorithms is prohibitively expensive for many tasks. One appealing alternative is rendering synthetic data where ground-truth annotations are generated automatically. Unfortunately, models trained purely on rendered images fail to generalize to real images. To address this shortcoming, prior work introduced unsupervised domain adaptation algorithms that have tried to either map representations between the two domains, or learn to extract features that are domain-invariant. In this work, we approach the problem in a new light by learning in an unsupervised manner a transformation in the pixel space from one domain to the other. Our generative adversarial network (GAN)-based method adapts source-domain images to appear as if drawn from the target domain. Our approach not only produces plausible samples, but also outperforms the state-of-the-art on a number of unsupervised domain adaptation scenarios by large margins. Finally, we demonstrate that the adaptation process generalizes to object classes unseen during training.', '저자': 'Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, Dilip Krishnan', '전체 인용횟수': '1727회 인용201720182019202020212022202345200304366358254192', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '3722-3731', '학술 문서': 'Unsupervised pixel-level domain adaptation with generative adversarial networksK Bousmalis, N Silberman, D Dohan, D Erhan…\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20171727회 인용 관련 학술자료 전체 15개의 버전 '}, title='Unsupervised pixel-level domain adaptation with generative adversarial networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Visualizing higher-layer features of a deep network': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009/6/9', '권': '1341', '설명': 'Deep architectures have demonstrated state-of-the-art results in a variety of settings, especially with vision datasets. Beyond the model definitions and the quantitative analyses, there is a need for qualitative comparisons of the solutions learned by various deep architectures. The goal of this paper is to find good qualitative interpretations of high level features represented by such models. To this end, we contrast and compare several techniques applied on Stacked Denoising Autoencoders and Deep Belief Networks, trained on several vision datasets. We show that, perhaps counter-intuitively, such interpretation is possible at the unit level, that it is simple to accomplish and that the results are consistent across various techniques. We hope that such techniques will allow researchers in deep architectures to understand more of how and why deep architectures work.', '저널': 'University of Montreal', '저자': 'Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pascal Vincent', '전체 인용횟수': '1574회 인용2013201420152016201720182019202020212022202312124086100166216245284228162', '페이지': '1', '학술 문서': 'Visualizing higher-layer features of a deep networkD Erhan, Y Bengio, A Courville, P Vincent\\xa0- University of Montreal, 20091574회 인용 관련 학술자료 전체 2개의 버전 ', '호': '3'}, title='Visualizing higher-layer features of a deep network', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Domain separation networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '권': '29', '설명': \"The cost of large scale data collection and annotation often makes the application of machine learning algorithms to new tasks or datasets prohibitively expensive. One approach circumventing this cost is training models on synthetic data where annotations are provided automatically. Despite their appeal, such models often fail to generalize from synthetic to real images, necessitating domain adaptation algorithms to manipulate these models before they can be successfully applied. Existing approaches focus either on mapping representations from one domain to the other, or on learning to extract features that are invariant to the domain from which they were extracted. However, by focusing only on creating a mapping or shared representation between the two domains, they ignore the individual characteristics of each domain. We hypothesize that explicitly modeling what is unique to each domain can improve a model's ability to extract domain-invariant features. Inspired by work on private-shared component analysis, we explicitly learn to extract image representations that are partitioned into two subspaces: one component which is private to each domain and one which is shared across domains. Our model is trained to not only perform the task we care about in the source domain, but also to use the partitioned representation to reconstruct the images from both domains. Our novel architecture results in a model that outperforms the state-of-the-art on a range of unsupervised domain adaptation scenarios and additionally produces visualizations of the private and shared representations enabling interpretation of the domain adaptation process.\", '저널': 'Advances in neural information processing systems', '저자': 'Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, Dumitru Erhan', '전체 인용횟수': '1497회 인용20162017201820192020202120222023660162211283272246250', '학술 문서': 'Domain separation networksK Bousmalis, G Trigeorgis, N Silberman, D Krishnan…\\xa0- Advances in neural information processing systems, 20161497회 인용 관련 학술자료 전체 12개의 버전 '}, title='Domain separation networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Scalable object detection using deep neural networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '설명': 'Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.', '저자': 'Dumitru Erhan, Christian Szegedy, Alexander Toshev, Dragomir Anguelov', '전체 인용횟수': '1497회 인용20142015201620172018201920202021202220231376135142195253183174184103', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '2147-2154', '학술 문서': 'Scalable object detection using deep neural networksD Erhan, C Szegedy, A Toshev, D Anguelov\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20141497회 인용 관련 학술자료 전체 21개의 버전 '}, title='Scalable object detection using deep neural networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'An empirical evaluation of deep architectures on problems with many factors of variation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2007/6/20', '도서': 'Proceedings of the 24th international conference on Machine learning', '설명': 'Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.', '저자': 'Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, Yoshua Bengio', '전체 인용횟수': '1353회 인용200720082009201020112012201320142015201620172018201920202021202220234172636433948679397119123147146110114109', '페이지': '473-480', '학술 문서': 'An empirical evaluation of deep architectures on problems with many factors of variationH Larochelle, D Erhan, A Courville, J Bergstra…\\xa0- Proceedings of the 24th international conference on\\xa0…, 20071353회 인용 관련 학술자료 전체 8개의 버전 '}, title='An empirical evaluation of deep architectures on problems with many factors of variation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Theano: A Python framework for fast computation of mathematical expressions': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/5', '설명': 'Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers-especially in the machine learning community-and has shown steady performance improvements. Theano is being actively and continuously developed since 2008, multiple frameworks have been built on top of it and it has been used to produce many state-of-the-art machine learning models. The present article is structured as follows. Section I provides an overview of the Theano software and its community. Section II presents the principal features of Theano and how to use them, and compares them with other similar projects. Section III focuses on recently-introduced functionalities and improvements. Section IV compares the performance of Theano against Torch7 and\\xa0…', '저널': 'arXiv e-prints', '저자': 'Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Frédéric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky, Yoshua Bengio, Arnaud Bergeron, James Bergstra, Valentin Bisson, Josh Bleecher Snyder, Nicolas Bouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, Alexandre de Brébisson, Olivier Breuleux, Pierre-Luc Carrier, Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooijmans, Marc-Alexandre Côté, Myriam Côté, Aaron Courville, Yann N Dauphin, Olivier Delalleau, Julien Demouth, Guillaume Desjardins, Sander Dieleman, Laurent Dinh, Mélanie Ducoffe, Vincent Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, Mathieu Germain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe Hamel, Iban Harlouchet, Jean-Philippe Heng, Balázs Hidasi, Sina Honari, Arjun Jain, Sébastien Jean, Kai Jia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, César Laurent, Sean Lee, Simon Lefrancois, Simon Lemieux, Nicholas Léonard, Zhouhan Lin, Jesse A Livezey, Cory Lorenz, Jeremiah Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropietro, Robert T McGibbon, Roland Memisevic, Bart van Merriënboer, Vincent Michalski, Mehdi Mirza, Alberto Orlandi, Christopher Pal, Razvan Pascanu, Mohammad Pezeshki, Colin Raffel, Daniel Renshaw, Matthew Rocklin, Adriana Romero, Markus Roth, Peter Sadowski, John Salvatier, François Savard, Jan Schlüter, John Schulman, Gabriel Schwartz, Iulian Vlad Serban, Dmitriy Serdyuk, Samira Shabanian, Étienne Simon, Sigurd Spieckermann, S Ramana Subramanyam, Jakub Sygnowski, Jérémie Tanguay, Gijs van Tulder, Joseph Turian, Sebastian Urban, Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley, Dustin J Webb, Matthew Willson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, Ying Zhang', '전체 인용횟수': '1075회 인용20162017201820192020202120222023462032322041461108142', '페이지': 'arXiv: 1605.02688', '학술 문서': 'Theano: A Python framework for fast computation of mathematical expressionsR Al-Rfou, G Alain, A Almahairi, C Angermueller…\\xa0- arXiv e-prints, 2016916회 인용 관련 학술자료 Theano: A Python framework for fast computation of mathematical expressions*TTD Team, R Al-Rfou, G Alain, A Almahairi…\\xa0- arXiv preprint arXiv:1605.02688, 2016199회 인용 관련 학술자료 전체 9개의 버전 '}, title='Theano: A Python framework for fast computation of mathematical expressions', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Training deep neural networks on noisy labels with bootstrapping': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/12/20', '설명': 'Current state-of-the-art deep learning systems for visual object recognition and detection use purely supervised training with regularization such as dropout to avoid overfitting. The performance depends critically on the amount of labeled examples, and in current practice the labels are assumed to be unambiguous and accurate. However, this assumption often does not hold; e.g. in recognition, class labels may be missing; in detection, objects in the image may not be localized; and in general, the labeling may be subjective. In this work we propose a generic way to handle noisy and incomplete labeling by augmenting the prediction objective with a notion of consistency. We consider a prediction consistent if the same prediction is made given similar percepts, where the notion of similarity is between deep network features computed from the input data. In experiments we demonstrate that our approach yields substantial robustness to label noise on several datasets. On MNIST handwritten digits, we show that our model is robust to label corruption. On the Toronto Face Database, we show that our model handles well the case of subjective labels in emotion recognition, achieving state-of-the- art results, and can also benefit from unlabeled face images with no modification to our method. On the ILSVRC2014 detection challenge data, we show that our approach extends to very deep networks, high resolution images and structured outputs, and results in improved scalable detection.', '저널': 'arXiv preprint arXiv:1412.6596', '저자': 'Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, Andrew Rabinovich', '전체 인용횟수': '1061회 인용2015201620172018201920202021202220239284479142169224196159', '학술 문서': 'Training deep neural networks on noisy labels with bootstrappingS Reed, H Lee, D Anguelov, C Szegedy, D Erhan…\\xa0- arXiv preprint arXiv:1412.6596, 20141061회 인용 관련 학술자료 전체 11개의 버전 '}, title='Training deep neural networks on noisy labels with bootstrapping', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Show and tell: Lessons learned from the 2015 mscoco image captioning challenge': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/7/7', '게시자': 'IEEE', '권': '39', '설명': 'Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. Finally, given the recent surge of interest in this task, a competition was organized in 2015 using the newly released COCO dataset. We describe and analyze the various improvements we\\xa0…', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan', '전체 인용횟수': '999회 인용20162017201820192020202120222023587143169173137143125', '페이지': '652-663', '학술 문서': 'Show and tell: Lessons learned from the 2015 mscoco image captioning challengeO Vinyals, A Toshev, S Bengio, D Erhan\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 2016999회 인용 관련 학술자료 전체 19개의 버전 ', '호': '4'}, title='Show and tell: Lessons learned from the 2015 mscoco image captioning challenge', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Model-based reinforcement learning for atari': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019/3/1', '설명': 'Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. However, this typically requires very large amounts of interaction -- substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the answer may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, we explore how video prediction models can similarly enable agents to solve Atari games with fewer interactions than model-free methods. We describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present a comparison of several model architectures, including a novel architecture that yields the best results in our setting. Our experiments evaluate SimPLe on a range of Atari games in low data regime of 100k interactions between the agent and the environment, which corresponds to two hours of real-time play. In most games SimPLe outperforms state-of-the-art model-free algorithms, in some games by over an order of magnitude.', '저널': 'arXiv preprint arXiv:1903.00374', '저자': 'Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin, Ryan Sepassi, George Tucker, Henryk Michalewski', '전체 인용횟수': '783회 인용2019202020212022202360137199212169', '학술 문서': 'Model-based reinforcement learning for atariL Kaiser, M Babaeizadeh, P Milos, B Osinski…\\xa0- arXiv preprint arXiv:1903.00374, 2019782회 인용 관련 학술자료 전체 6개의 버전 Model-based reinforcement learning for atari*B Osinski, C Finn, D Erhan, G Tucker, H Michalewski… - 20201회 인용 관련 학술자료 Model Based Reinforcement Learning for Atari*RH Campbell, K Czechowski, D Erhan, C Finn…\\xa0- Proceedings of the International Conference on\\xa0…, 20191회 인용 관련 학술자료 전체 4개의 버전 Model Based Reinforcement Learning for Atari*K Czechowski, D Erhan, C Finn, P Kozakowski…관련 학술자료 전체 4개의 버전 '}, title='Model-based reinforcement learning for atari', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The (un) reliability of saliency methods': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '게시자': 'Springer International Publishing', '설명': ' Saliency methods aim to explain the predictions of deep neural networks. These methods lack reliability when the explanation is sensitive to factors that do not contribute to the model prediction. We use a simple and common pre-processing step which can be compensated for easily—adding a constant shift to the input data—to show that a transformation with no effect on how the model makes the decision can cause numerous methods to attribute incorrectly. In order to guarantee reliability, we believe that the explanation should not change when we can guarantee that two networks process the images in identical manners. We show, through several examples, that saliency methods that do not satisfy this requirement result in misleading attribution. The approach can be seen as a type of unit test; we construct a narrow ground truth to measure one stated desirable property. As such, we hope the community will\\xa0…', '저널': 'Explainable AI: Interpreting, explaining and visualizing deep learning', '저자': 'Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T Schütt, Sven Dähne, Dumitru Erhan, Been Kim', '전체 인용횟수': '621회 인용2018201920202021202220233857101114162144', '페이지': '267-280', '학술 문서': 'The (un) reliability of saliency methodsPJ Kindermans, S Hooker, J Adebayo, M Alber…\\xa0- Explainable AI: Interpreting, explaining and visualizing\\xa0…, 2019621회 인용 관련 학술자료 전체 12개의 버전 '}, title='The (un) reliability of saliency methods', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The difficulty of training deep architectures and the effect of unsupervised pre-training': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009/4/15', '게시자': 'PMLR', '설명': 'Whereas theoretical work suggests that deep architectures might be more efficient at representing highly-varying functions, training deep architectures was unsuccessful until the recent advent of algorithms based on unsupervised pre-training. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. Answering these questions is important if learning in deep architectures is to be further improved. We attempt to shed some light on these questions through extensive simulations. The experiments confirm and clarify the advantage of unsupervised pre-training. They demonstrate the robustness of the training procedure with respect to the random initialization, the positive effect of pre-training in terms of optimization and its role as a kind of regularizer. We show the influence of architecture depth, model capacity, and number of training examples.', '저자': 'Dumitru Erhan, Pierre-Antoine Manzagol, Yoshua Bengio, Samy Bengio, Pascal Vincent', '전체 인용횟수': '616회 인용20092010201120122013201420152016201720182019202020212022202342210191839615564636548515338', '컨퍼런스': 'Artificial intelligence and statistics', '페이지': '153-160', '학술 문서': 'The difficulty of training deep architectures and the effect of unsupervised pre-trainingD Erhan, PA Manzagol, Y Bengio, S Bengio, P Vincent\\xa0- Artificial intelligence and statistics, 2009616회 인용 관련 학술자료 전체 15개의 버전 '}, title='The difficulty of training deep architectures and the effect of unsupervised pre-training', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Zero-data learning of new tasks.': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2008/7/13', '권': '1', '설명': 'We introduce the problem of zero-data learning, where a model must generalize to classes or tasks for which no training data are available and only a description of the classes or tasks are provided. Zero-data learning is useful for problems where the set of classes to distinguish or tasks to solve is very large and is not entirely covered by the training data. The main contributions of this work lie in the presentation of a general formalization of zero-data learning, in an experimental analysis of its properties and in empirical evidence showing that generalization is possible and significant in this context. The experimental work of this paper addresses two classification problems of character recognition and a multitask ranking problem in the context of drug discovery. Finally, we conclude by discussing how this new framework could lead to a novel perspective on how to extend machine learning towards AI, where an agent can be given a specification for a learning problem before attempting to solve it (with very few or even zero examples).', '저널': 'AAAI', '저자': 'Hugo Larochelle, Dumitru Erhan, Yoshua Bengio', '전체 인용횟수': '595회 인용200920102011201220132014201520162017201820192020202120222023349111710302951346360899386', '페이지': '3', '학술 문서': 'Zero-data learning of new tasks.H Larochelle, D Erhan, Y Bengio\\xa0- AAAI, 2008595회 인용 관련 학술자료 전체 12개의 버전 ', '호': '2'}, title='Zero-data learning of new tasks.', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A benchmark for interpretability methods in deep neural networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '권': '32', '설명': 'We propose an empirical measure of the approximate accuracy of feature importance estimates in deep neural networks. Our results across several large-scale image classification datasets show that many popular interpretability methods produce estimates of feature importance that are not better than a random designation of feature importance. Only certain ensemble based approaches---VarGrad and SmoothGrad-Squared---outperform such a random assignment of importance. The manner of ensembling remains critical, we show that some approaches do no better then the underlying method but carry a far higher computational burden.', '저널': 'Advances in neural information processing systems', '저자': 'Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, Been Kim', '전체 인용횟수': '561회 인용20192020202120222023480115180179', '학술 문서': 'A benchmark for interpretability methods in deep neural networksS Hooker, D Erhan, PJ Kindermans, B Kim\\xa0- Advances in neural information processing systems, 2019561회 인용 관련 학술자료 전체 11개의 버전 '}, title='A benchmark for interpretability methods in deep neural networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'U-net: Convolutional networks for biomedical image segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/10', '게시자': 'Springer', '권': '9351', '저자': 'Philipp Fischer, Thomas Brox', '전체 인용횟수': '188회 인용201620172018201920202021202220231283429444622', '컨퍼런스': 'International Conference on Medical image computing and computer-assisted intervention', '페이지': '234-241', '학술 문서': 'U-net: Convolutional networks for biomedical image segmentationP Fischer, T Brox\\xa0- International Conference on Medical image computing\\xa0…, 2015188회 인용 관련 학술자료 '}, title='U-net: Convolutional networks for biomedical image segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " '3D U-Net: learning dense volumetric segmentation from sparse annotation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '게시자': 'Springer International Publishing', '설명': ' This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup, the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup, we assume that a representative, sparsely annotated training set exists. Trained on this data set, the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch, i.e., no pre-trained network is required. We test the performance of the proposed method\\xa0…', '저자': 'Özgün Çiçek, Ahmed Abdulkadir, Soeren S Lienkamp, Thomas Brox, Olaf Ronneberger', '전체 인용횟수': '6260회 인용2017201820192020202120222023105376691944135813891318', '컨퍼런스': 'Medical Image Computing and Computer-Assisted Intervention–MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part II 19', '페이지': '424-432', '학술 문서': '3D U-Net: learning dense volumetric segmentation from sparse annotationÖ Çiçek, A Abdulkadir, SS Lienkamp, T Brox…\\xa0- Medical Image Computing and Computer-Assisted\\xa0…, 20166260회 인용 관련 학술자료 전체 10개의 버전 '}, title='3D U-Net: learning dense volumetric segmentation from sparse annotation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Striving for simplicity: The all convolutional net': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/12/21', '설명': 'Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the \"deconvolution approach\" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.', '저널': 'arXiv preprint arXiv:1412.6806', '저자': 'Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, Martin Riedmiller', '전체 인용횟수': '5300회 인용20152016201720182019202020212022202327181341600746843932860705', '학술 문서': 'Striving for simplicity: The all convolutional netJT Springenberg, A Dosovitskiy, T Brox, M Riedmiller\\xa0- arXiv preprint arXiv:1412.6806, 20145300회 인용 관련 학술자료 전체 7개의 버전 '}, title='Striving for simplicity: The all convolutional net', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Flownet: Learning optical flow with convolutional networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks CNNs succeeded at. In this paper we construct CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a large synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.', '저자': 'Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, Thomas Brox', '전체 인용횟수': '4534회 인용2015201620172018201920202021202220231695292453620739822799645', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '2758-2766', '학술 문서': 'Flownet: Learning optical flow with convolutional networksA Dosovitskiy, P Fischer, E Ilg, P Hausser, C Hazirbas…\\xa0- Proceedings of the IEEE international conference on\\xa0…, 20153908회 인용 관련 학술자료 전체 15개의 버전 Flownet: Learning optical flow with convolutional networks*P Fischer, A Dosovitskiy, E Ilg, P Häusser, C Hazırbaş…\\xa0- arXiv preprint arXiv:1504.06852, 2015681회 인용 관련 학술자료 전체 9개의 버전 Golkov,“*A Dosovitskiy, P Fischer, E Ilg, P Häusser, C Hazirbas\\xa0- FlowNet: Learning Optical Flow with Convolutional\\xa0…, 20153회 인용 관련 학술자료 '}, title='Flownet: Learning optical flow with convolutional networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'High accuracy optical flow estimation based on a theory for warping': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2004', '게시자': 'Springer Berlin Heidelberg', '설명': ' We study an energy functional for computing optical flow that combines three assumptions: a brightness constancy assumption, a gradient constancy assumption, and a discontinuity-preserving spatio-temporal smoothness constraint. In order to allow for large displacements, linearisations in the two data terms are strictly avoided. We present a consistent numerical scheme based on two nested fixed point iterations. By proving that this scheme implements a coarse-to-fine warping strategy, we give a theoretical foundation for warping which has been used on a mainly experimental basis so far. Our evaluation demonstrates that the novel method gives significantly smaller angular errors than previous techniques for optical flow estimation. We show that it is fairly insensitive to parameter variations, and we demonstrate its excellent robustness under noise.', '저자': 'Thomas Brox, Andrés Bruhn, Nils Papenberg, Joachim Weickert', '전체 인용횟수': '3659회 인용200420052006200720082009201020112012201320142015201620172018201920202021202220232142708295119141202208234233264286277304255237220173145', '컨퍼런스': 'Computer Vision-ECCV 2004: 8th European Conference on Computer Vision, Prague, Czech Republic, May 11-14, 2004. Proceedings, Part IV 8', '페이지': '25-36', '학술 문서': 'High accuracy optical flow estimation based on a theory for warpingT Brox, A Bruhn, N Papenberg, J Weickert\\xa0- Computer Vision-ECCV 2004: 8th European\\xa0…, 20043639회 인용 관련 학술자료 전체 18개의 버전 Computer Vision-ECCV 2004*T Brox, A Bruhn, N Papenberg, J Weickert\\xa0- Lecture Notes in Computer Science, 200437회 인용 관련 학술자료 '}, title='High accuracy optical flow estimation based on a theory for warping', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Flownet 2.0: Evolution of optical flow estimation with deep networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'The FlowNet demonstrated that optical flow estimation can be cast as a learning problem. However, the state of the art with regard to the quality of the flow has still been defined by traditional methods. Particularly on small displacements and real-world data, FlowNet cannot compete with variational methods. In this paper, we advance the concept of end-to-end learning of optical flow and make it work really well. The large improvements in quality and speed are caused by three major contributions: first, we focus on the training data and show that the schedule of presenting data during training is very important. Second, we develop a stacked architecture that includes warping of the second image with intermediate optical flow. Third, we elaborate on small displacements by introducing a subnetwork specializing on small motions. FlowNet 2.0 is only marginally slower than the original FlowNet but decreases the estimation error by more than 50%. It performs on par with state-of-the-art methods, while running at interactive frame rates. Moreover, we present faster variants that allow optical flow computation at up to 140fps with accuracy matching the original FlowNet.', '저자': 'Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, Thomas Brox', '전체 인용횟수': '3318회 인용201720182019202020212022202363326504620645605526', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '2462-2470', '학술 문서': 'Flownet 2.0: Evolution of optical flow estimation with deep networksE Ilg, N Mayer, T Saikia, M Keuper, A Dosovitskiy…\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20173318회 인용 관련 학술자료 전체 18개의 버전 '}, title='Flownet 2.0: Evolution of optical flow estimation with deep networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': 'Recent work has shown that optical flow estimation can be formulated as a supervised learning task and can be successfully solved with convolutional networks. Training of the so-called FlowNet was enabled by a large synthetically generated dataset. The present paper extends the concept of optical flow estimation via convolutional networks to disparity and scene flow estimation. To this end, we propose three synthetic stereo video datasets with sufficient realism, variation, and size to successfully train large networks. Our datasets are the first large-scale datasets to enable training and evaluation of scene flow methods. Besides the datasets, we present a convolutional network for real-time disparity estimation that provides state-of-the-art results. By combining a flow and disparity estimation network and training it jointly, we demonstrate the first scene flow estimation with a convolutional network.', '저자': 'Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, Thomas Brox', '전체 인용횟수': '2576회 인용2016201720182019202020212022202322126228348371492525445', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '4040-4048', '학술 문서': 'A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimationN Mayer, E Ilg, P Hausser, P Fischer, D Cremers…\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20162576회 인용 관련 학술자료 전체 21개의 버전 '}, title='A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Discriminative unsupervised feature learning with convolutional neural networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '권': '27', '설명': \"Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training. In this paper we present an approach for training a convolutional neural network using only unlabeled data. We train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled'seed'image patch. We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition. The feature representation learned by our algorithm achieves classification results matching or outperforming the current state-of-the-art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101).\", '저널': 'Advances in neural information processing systems', '저자': 'Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, Thomas Brox', '전체 인용횟수': '1828회 인용201420152016201720182019202020212022202363076102135150299368393242', '학술 문서': 'Discriminative unsupervised feature learning with convolutional neural networksA Dosovitskiy, JT Springenberg, M Riedmiller, T Brox\\xa0- Advances in neural information processing systems, 20141771회 인용 관련 학술자료 전체 21개의 버전 Unsupervised feature learning by augmenting single images*A Dosovitskiy, JT Springenberg, T Brox\\xa0- arXiv preprint arXiv:1312.5242, 201361회 인용 관련 학술자료 전체 6개의 버전 '}, title='Discriminative unsupervised feature learning with convolutional neural networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'U-Net: deep learning for cell counting, detection, and morphometry': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019/1', '게시자': 'Nature Publishing Group US', '권': '16', '설명': 'U-Net is a generic deep-learning solution for frequently occurring quantification tasks such as cell detection and shape measurements in biomedical image data. We present an ImageJ plugin that enables non-machine-learning experts to analyze their data with U-Net on either a local computer or a remote server/cloud service. The plugin comes with pretrained models for single-cell segmentation and allows for U-Net to be adapted to new tasks on the basis of a few annotated samples.', '저널': 'Nature methods', '저자': 'Thorsten Falk, Dominic Mai, Robert Bensch, Özgün Çiçek, Ahmed Abdulkadir, Yassine Marrakchi, Anton Böhm, Jan Deubner, Zoe Jäckel, Katharina Seiwald, Alexander Dovzhenko, Olaf Tietz, Cristina Dal Bosco, Sean Walsh, Deniz Saltukoglu, Tuan Leng Tay, Marco Prinz, Klaus Palme, Matias Simons, Ilka Diester, Thomas Brox, Olaf Ronneberger', '전체 인용횟수': '1443회 인용2018201920202021202220234101262361353345', '페이지': '67-70', '학술 문서': 'U-Net: deep learning for cell counting, detection, and morphometryT Falk, D Mai, R Bensch, Ö Çiçek, A Abdulkadir…\\xa0- Nature methods, 20191428회 인용 관련 학술자료 전체 7개의 버전 Author Correction: U-Net: deep learning for cell counting, detection, and morphometry*T Falk, D Mai, R Bensch, Ö Çiçek, A Abdulkadir…\\xa0- Nature Methods, 201920회 인용 관련 학술자료 전체 4개의 버전 ', '호': '1'}, title='U-Net: deep learning for cell counting, detection, and morphometry', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Generating images with perceptual similarity metrics based on deep networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '권': '29', '설명': 'We propose a class of loss functions, which we call deep perceptual similarity metrics (DeePSiM), allowing to generate sharp high resolution images from compressed abstract representations. Instead of computing distances in the image space, we compute distances between image features extracted by deep neural networks. This metric reflects perceptual similarity of images much better and, thus, leads to better results. We demonstrate two examples of use cases of the proposed loss:(1) networks that invert the AlexNet convolutional network;(2) a modified version of a variational autoencoder that generates realistic high-resolution random images.', '저널': 'Advances in neural information processing systems', '저자': 'Alexey Dosovitskiy, Thomas Brox', '전체 인용횟수': '1221회 인용201620172018201920202021202220232274171210230196169128', '학술 문서': 'Generating images with perceptual similarity metrics based on deep networksA Dosovitskiy, T Brox\\xa0- Advances in neural information processing systems, 20161221회 인용 관련 학술자료 전체 10개의 버전 '}, title='Generating images with perceptual similarity metrics based on deep networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning to generate chairs with convolutional neural networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'We train a generative convolutional neural network which is able to generate images of objects given object type, viewpoint, and color. We train the network in a supervised manner on a dataset of rendered 3D chair models. Our experiments show that the network does not merely learn all images by heart, but rather finds a meaningful representation of a 3D chair model allowing it to assess the similarity of different chairs, interpolate between given viewpoints to generate the missing ones, or invent new chair styles by interpolating between chairs from the training set. We show that the network can be used to find correspondences between different chairs from the dataset, outperforming existing approaches on this task.', '저자': 'Alexey Dosovitskiy, Jost Tobias Springenberg, Thomas Brox', '전체 인용횟수': '1021회 인용20152016201720182019202020212022202337116151184155131817262', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '1538-1546', '학술 문서': 'Learning to generate chairs with convolutional neural networksA Dosovitskiy, J Tobias Springenberg, T Brox\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20151021회 인용 관련 학술자료 전체 25개의 버전 '}, title='Learning to generate chairs with convolutional neural networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Object segmentation by long term analysis of point trajectories': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/9/5', '게시자': 'Springer Berlin Heidelberg', '도서': 'European conference on computer vision', '설명': ' Unsupervised learning requires a grouping step that defines which data belong together. A natural way of grouping in images is the segmentation of objects or parts of objects. While pure bottom-up segmentation from static cues is well known to be ambiguous at the object level, the story changes as soon as objects move. In this paper, we present a method that uses long term point trajectories based on dense optical flow. Defining pair-wise distances between these trajectories allows to cluster them, which results in temporally consistent segmentations of moving objects in a video shot. In contrast to multi-body factorization, points and even whole objects may appear or disappear during the shot. We provide a benchmark dataset and an evaluation method for this so far uncovered setting.', '저자': 'Thomas Brox, Jitendra Malik', '전체 인용횟수': '1000회 인용2009201020112012201320142015201620172018201920202021202220233326527692961151211047669604336', '페이지': '282-295', '학술 문서': 'Object segmentation by long term analysis of point trajectoriesT Brox, J Malik\\xa0- European conference on computer vision, 20101000회 인용 관련 학술자료 전체 10개의 버전 '}, title='Object segmentation by long term analysis of point trajectories', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Sparsity invariant cnns': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/10/10', '게시자': 'IEEE', '설명': 'In this paper, we consider convolutional neural networks operating on sparse inputs with an application to depth completion from sparse laser scan data. First, we show that traditional convolutional networks perform poorly when applied to sparse data even when the location of missing data is provided to the network. To overcome this problem, we propose a simple yet effective sparse convolution layer which explicitly considers the location of missing data during the convolution operation. We demonstrate the benefits of the proposed network architecture in synthetic and real experiments with respect to various baseline approaches. Compared to dense baselines, the proposed sparse convolution network generalizes well to novel datasets and is invariant to the level of sparsity in the data. For our evaluation, we derive a novel dataset from the KITTI benchmark, comprising over 94k depth annotated RGB images. Our\\xa0…', '저자': 'Jonas Uhrig, Nick Schneider, Lukas Schneider, Uwe Franke, Thomas Brox, Andreas Geiger', '전체 인용횟수': '824회 인용20182019202020212022202341107122185192173', '컨퍼런스': '2017 International Conference on 3D Vision (3DV)', '페이지': '11-20', '학술 문서': 'Sparsity invariant cnnsJ Uhrig, N Schneider, L Schneider, U Franke, T Brox…\\xa0- 2017 international conference on 3D Vision (3DV), 2017824회 인용 관련 학술자료 전체 8개의 버전 '}, title='Sparsity invariant cnns', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Inverting visual representations with convolutional networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': 'Feature representations, both hand-designed and learned ones, are often hard to analyze and interpret, even when they are extracted from visual data. We propose a new approach to study image representations by inverting them with an up-convolutional neural network. We apply the method to shallow representations (HOG, SIFT, LBP), as well as to deep networks. For shallow representations our approach provides significantly better reconstructions than existing methods, revealing that there is surprisingly rich information contained in these features. Inverting a deep network trained on ImageNet provides several insights into the properties of the feature representation learned by the network. Most strikingly, the colors and the rough contours of an image can be reconstructed from activations in higher network layers and even from the predicted class probabilities.', '저자': 'Alexey Dosovitskiy, Thomas Brox', '전체 인용횟수': '796회 인용20152016201720182019202020212022202311636610210512213810375', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '4829-4837', '학술 문서': 'Inverting visual representations with convolutional networksA Dosovitskiy, T Brox\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2016681회 인용 관련 학술자료 전체 12개의 버전 Inverting convolutional networks with convolutional networks*A Dosovitskiy, T Brox\\xa0- arXiv preprint arXiv:1506.02753, 2015124회 인용 관련 학술자료 '}, title='Inverting visual representations with convolutional networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'We present a deep convolutional decoder architecture that can generate volumetric 3D outputs in a compute-and memory-efficient manner by using an octree representation. The network learns to predict both the structure of the octree, and the occupancy values of individual cells. This makes it a particularly valuable technique for generating 3D shapes. In contrast to standard decoders acting on regular voxel grids, the architecture does not have cubic complexity. This allows representing much higher resolution outputs with a limited memory budget. We demonstrate this in several application domains, including 3D convolutional autoencoders, generation of objects and whole scenes from high-level representations, and shape from a single image.', '저자': 'Maxim Tatarchenko, Alexey Dosovitskiy, Thomas Brox', '전체 인용횟수': '786회 인용20172018201920202021202220231472119166174121112', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '2088-2096', '학술 문서': 'Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputsM Tatarchenko, A Dosovitskiy, T Brox\\xa0- Proceedings of the IEEE international conference on\\xa0…, 2017786회 인용 관련 학술자료 전체 10개의 버전 '}, title='Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning to estimate 3d hand pose from single rgb images': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'Low-cost consumer depth cameras and deep learning have enabled reasonable 3D hand pose estimation from single depth images. In this paper, we present an approach that estimates 3D hand pose from regular RGB images. This task has far more ambiguities due to the missing depth information. To this end, we propose a deep network that learns a network-implicit 3D articulation prior. Together with detected keypoints in the images, this network yields good estimates of the 3D pose. We introduce a large scale 3D hand pose dataset based on synthetic hand models for training the involved networks. Experiments on a variety of test sets, including one on sign language recognition, demonstrate the feasibility of 3D hand pose estimation on single color images.', '저자': 'Christian Zimmermann, Thomas Brox', '전체 인용횟수': '746회 인용2017201820192020202120222023762101138161147124', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '4903-4911', '학술 문서': 'Learning to estimate 3d hand pose from single rgb imagesC Zimmermann, T Brox\\xa0- Proceedings of the IEEE international conference on\\xa0…, 2017746회 인용 관련 학술자료 전체 10개의 버전 '}, title='Learning to estimate 3d hand pose from single rgb images', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Synthesizing the preferred inputs for neurons in neural networks via deep generator networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '권': '29', '설명': 'Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right---similar to why we study the human brain---and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization, which synthesizes an input (eg an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network. The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real,(2) reveals the features learned by each neuron in an interpretable way,(3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).', '저널': 'Advances in neural information processing systems', '저자': 'Anh Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, Jeff Clune', '전체 인용횟수': '747회 인용20152016201720182019202020212022202329569999113130127104', '학술 문서': 'Synthesizing the preferred inputs for neurons in neural networks via deep generator networksA Nguyen, A Dosovitskiy, J Yosinski, T Brox, J Clune\\xa0- Advances in neural information processing systems, 2016747회 인용 관련 학술자료 전체 12개의 버전 '}, title='Synthesizing the preferred inputs for neurons in neural networks via deep generator networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Demon: Depth and motion network for learning monocular stereo': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'In this paper we formulate structure from motion as a learning problem. We train a convolutional network end-to-end to compute depth and camera motion from successive, unconstrained image pairs. The architecture is composed of multiple stacked encoder-decoder networks, the core part being an iterative network that is able to improve its own predictions. The network estimates not only depth and motion, but additionally surface normals, optical flow between the images and confidence of the matching. A crucial component of the approach is a training loss based on spatial relative differences. Compared to traditional two-frame structure from motion methods, results are more accurate and more robust. In contrast to the popular depth-from-single-image networks, DeMoN learns the concept of matching and, thus, better generalizes to structures not seen during training.', '저자': 'Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, Thomas Brox', '전체 인용횟수': '745회 인용20172018201920202021202220231710513414215211575', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '5038-5047', '학술 문서': 'Demon: Depth and motion network for learning monocular stereoB Ummenhofer, H Zhou, J Uhrig, N Mayer, E Ilg…\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2017745회 인용 관련 학술자료 전체 12개의 버전 '}, title='Demon: Depth and motion network for learning monocular stereo', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Segmentation of moving objects by long term video analysis': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/12/17', '권': '33', '설명': 'Motion is a strong cue for unsupervised object-level grouping. In this paper, we demonstrate that motion will be exploited most effectively, if it is regarded over larger time windows. Opposed to classical two-frame optical flow, point trajectories that span hundreds of frames are less susceptible to short-term variations that hinder separating different objects. As a positive side effect, the resulting groupings are temporally consistent over a whole video shot, a property that requires tedious post-processing in the vast majority of existing approaches. We suggest working with a paradigm that starts with semi-dense motion cues first and that fills up textureless areas afterwards based on color. This paper also contributes the Freiburg-Berkeley motion segmentation (FBMS) dataset, a large, heterogeneous benchmark with 59 sequences and pixel-accurate ground truth annotation of moving objects.', '저널': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', '저자': 'Peter Ochs, Jitendra Malik, Thomas Brox', '전체 인용횟수': '649회 인용201420152016201720182019202020212022202315417091807960635782', '페이지': '500-513', '학술 문서': 'Segmentation of moving objects by long term video analysisP Ochs, J Malik, T Brox\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 2013649회 인용 관련 학술자료 전체 18개의 버전 ', '호': '3'}, title='Segmentation of moving objects by long term video analysis', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Image super-resolution using deep convolutional networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/6/1', '게시자': 'IEEE', '권': '38', '설명': 'We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show\\xa0…', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang', '전체 인용횟수': '8622회 인용2016201720182019202020212022202316147084412021429153415521339', '페이지': '295-307', '학술 문서': 'Image super-resolution using deep convolutional networksC Dong, CC Loy, K He, X Tang\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20158622회 인용 관련 학술자료 전체 13개의 버전 ', '호': '2'}, title='Image super-resolution using deep convolutional networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Single image haze removal using dark channel prior': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/9/9', '게시자': 'IEEE', '권': '33', '설명': 'In this paper, we propose a simple but effective image prior-dark channel prior to remove haze from a single input image. The dark channel prior is a kind of statistics of outdoor haze-free images. It is based on a key observation-most local patches in outdoor haze-free images contain some pixels whose intensity is very low in at least one color channel. Using this prior with the haze imaging model, we can directly estimate the thickness of the haze and recover a high-quality haze-free image. Results on a variety of hazy images demonstrate the power of the proposed prior. Moreover, a high-quality depth map can also be obtained as a byproduct of haze removal.', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Kaiming He, Jian Sun, Xiaoou Tang', '전체 인용횟수': '8333회 인용2010201120122013201420152016201720182019202020212022202376111158242346487581584705720882102312081142', '페이지': '2341-2353', '학술 문서': 'Single image haze removal using dark channel priorK He, J Sun, X Tang\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20108272회 인용 관련 학술자료 전체 44개의 버전 SingleimagehazeremovalusingdarkchannelpriorM HeK, TXO SUNJ\\xa0- IEEE Transactionson Pattern Analysis and Machine\\xa0…, 2011100회 인용 관련 학술자료 ', '호': '12'}, title='Single image haze removal using dark channel prior', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Guided image filtering': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012/10/2', '게시자': 'IEEE', '권': '35', '설명': 'In this paper, we propose a novel explicit image filter called guided filter. Derived from a local linear model, the guided filter computes the filtering output by considering the content of a guidance image, which can be the input image itself or another different image. The guided filter can be used as an edge-preserving smoothing operator like the popular bilateral filter [1], but it has better behaviors near edges. The guided filter is also a more generic concept beyond smoothing: It can transfer the structures of the guidance image to the filtering output, enabling new filtering applications like dehazing and guided feathering. Moreover, the guided filter naturally has a fast and nonapproximate linear time algorithm, regardless of the kernel size and the intensity range. Currently, it is one of the fastest edge-preserving filters. Experiments show that the guided filter is both effective and efficient in a great variety of computer\\xa0…', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Kaiming He, Jian Sun, Xiaoou Tang', '전체 인용횟수': '7752회 인용201120122013201420152016201720182019202020212022202343112210355581811798861829830824790644', '페이지': '1397-1409', '학술 문서': 'Guided image filteringK He, J Sun, X Tang\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20127752회 인용 관련 학술자료 전체 27개의 버전 ', '호': '6'}, title='Guided image filtering', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep learning face attributes in the wild': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation.(1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies.(2) It reveals that although the filters of LNet are fine-tuned only with image-level attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, but without face bounding boxes or landmarks, which are required by all attribute recognition works.(3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training with massive face identities, and such concepts are significantly enriched after fine-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts.', '저자': 'Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang', '전체 인용횟수': '7733회 인용201620172018201920202021202220231023226659481141140815591518', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '3730-3738', '학술 문서': 'Deep learning face attributes in the wildZ Liu, P Luo, X Wang, X Tang\\xa0- Proceedings of the IEEE international conference on\\xa0…, 20157733회 인용 관련 학술자료 전체 21개의 버전 '}, title='Deep learning face attributes in the wild', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning a deep convolutional network for image super-resolution': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '게시자': 'Springer International Publishing', '설명': ' We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) [15] that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage.', '저자': 'Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang', '전체 인용횟수': '5583회 인용20152016201720182019202020212022202396234362588746810914945822', '컨퍼런스': 'Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part IV 13', '페이지': '184-199', '학술 문서': 'Learning a deep convolutional network for image super-resolutionC Dong, CC Loy, K He, X Tang\\xa0- Computer Vision–ECCV 2014: 13th European\\xa0…, 20145583회 인용 관련 학술자료 전체 8개의 버전 '}, title='Learning a deep convolutional network for image super-resolution', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " '3d shapenets: A deep representation for volumetric shapes': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': \"3D shape is a crucial but heavily underutilized cue in today's computer vision systems, mostly due to the lack of a good generic shape representation. With the recent availability of inexpensive 2.5 D depth sensors (eg Microsoft Kinect), it is becoming increasingly important to have a powerful 3D shape representation in the loop. Apart from category recognition, recovering full 3D shapes from view-based 2.5 D depth maps is also a critical part of visual understanding. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model, 3D ShapeNets, learns the distribution of complex 3D shapes across different object categories and arbitrary poses from raw CAD data, and discovers hierarchical compositional part representation automatically. It naturally supports joint object recognition and shape completion from 2.5 D depth maps, and it enables active object recognition through view planning. To train our 3D deep learning model, we construct ModelNet-a large-scale 3D CAD model dataset. Extensive experiments show that our 3D deep representation enables significant performance improvement over the-state-of-the-arts in a variety of tasks.\", '저자': 'Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, Jianxiong Xiao', '전체 인용횟수': '5652회 인용2015201620172018201920202021202220233412925747468181298011601060', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '1912-1920', '학술 문서': '3d shapenets: A deep representation for volumetric shapesZ Wu, S Song, A Khosla, F Yu, L Zhang, X Tang, J Xiao\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20155544회 인용 관련 학술자료 전체 20개의 버전 3d shapenets for 2.5 d object recognition and next-best-view prediction*Z Wu, S Song, A Khosla, X Tang, J Xiao\\xa0- arXiv preprint arXiv:1406.5670, 2014118회 인용 관련 학술자료 전체 2개의 버전 3D-IC technologies and 3D FPGA*X Wu\\xa0- 2015 International 3D Systems Integration Conference\\xa0…, 201514회 인용 관련 학술자료 전체 10개의 버전 '}, title='3d shapenets: A deep representation for volumetric shapes', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Residual attention network for image classification': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'In this work, we propose\" Residual Attention Network\", a convolutional neural network using attention mechanism which can incorporate with state-of-art feed forward network architecture in an end-to-end training fashion. Our Residual Attention Network is built by stacking Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper. Inside each Attention Module, bottom-up top-down feedforward structure is used to unfold the feedforward and feedback attention process into a single feedforward process. Importantly, we propose attention residual learning to train very deep Residual Attention Networks which can be easily scaled up to hundreds of layers. Extensive analyses are conducted on CIFAR-10 and CIFAR-100 datasets to verify the effectiveness of every module mentioned above. Our Residual Attention Network achieves state-of-the-art object recognition performance on three benchmark datasets including CIFAR-10 (3.90% error), CIFAR-100 (20.45% error) and ImageNet (4.8% single model and single crop, top-5 error). Note that, our method achieves 0.6% top-1 accuracy improvement with 46% trunk depth and 69% forward FLOPs comparing to ResNet-200. The experiment also demonstrates that our network is robust against noisy labels.', '저자': 'Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, Xiaoou Tang', '전체 인용횟수': '3853회 인용201720182019202020212022202314147463718943904638', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '3156-3164', '학술 문서': 'Residual attention network for image classificationF Wang, M Jiang, C Qian, S Yang, C Li, H Zhang…\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20173853회 인용 관련 학술자료 전체 14개의 버전 '}, title='Residual attention network for image classification', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Action recognition with trajectory-pooled deep-convolutional descriptors': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'Visual features are of vital importance for human action understanding in videos. This paper presents a new video representation, called trajectory-pooled deep-convolutional descriptor (TDD), which shares the merits of both hand-crafted features and deep-learned features. Specifically, we utilize deep architectures to learn discriminative convolutional feature maps, and conduct trajectory-constrained pooling to aggregate these convolutional features into effective descriptors. To enhance the robustness of TDDs, we design two normalization methods to transform convolutional feature maps, namely spatiotemporal normalization and channel normalization. The advantages of our features come from (i) TDDs are automatically learned and contain high discriminative capacity compared with those hand-crafted features;(ii) TDDs take account of the intrinsic characteristics of temporal dimension and introduce the strategies of trajectory-constrained sampling and pooling for aggregating deep-learned features. We conduct experiments on two challenging datasets: HMDB51 and UCF101. Experimental results show that TDDs outperform previous hand-crafted features and deep-learned features. Our method also achieves superior performance to the state of the art on these datasets.', '저자': 'Limin Wang, Yu Qiao, Xiaoou Tang', '전체 인용횟수': '3844회 인용20112012201320142015201620172018201920202021202220232194212258313448509488411370306180141', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '4305-4314', '학술 문서': 'Action recognition with trajectory-pooled deep-convolutional descriptorsL Wang, Y Qiao, X Tang\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20153835회 인용 관련 학술자료 전체 41개의 버전 Action recognition with trajectory-pooled deep-convolutional descriptorsW Limin, Q Yu, T Xiaoou\\xa0- IEEE Conference on Computer Vision and Pattern\\xa0…, 201528회 인용 관련 학술자료 Action recognition with trajectory-pooled deep-convolutional descriptors, 4305–4314*L Wang, Y Qiao, X Tang\\xa0- IEEE Conference on Computer Vision and Pattern\\xa0…, 20153회 인용 관련 학술자료 '}, title='Action recognition with trajectory-pooled deep-convolutional descriptors', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Esrgan: Enhanced super-resolution generative adversarial networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN–network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge (region 3) with the best perceptual index. The code is available at https://github. com/xinntao/ESRGAN.', '저자': 'Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, Chen Change Loy', '전체 인용횟수': '3469회 인용20192020202120222023189525789937998', '컨퍼런스': 'Proceedings of the European conference on computer vision (ECCV) workshops', '페이지': '0-0', '학술 문서': 'Esrgan: Enhanced super-resolution generative adversarial networksX Wang, K Yu, S Wu, J Gu, Y Liu, C Dong, Y Qiao…\\xa0- Proceedings of the European conference on computer\\xa0…, 20183469회 인용 관련 학술자료 전체 9개의 버전 '}, title='Esrgan: Enhanced super-resolution generative adversarial networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Accelerating the super-resolution convolutional neural network': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '게시자': 'Springer International Publishing', '설명': ' As a successful deep model applied in image super-resolution (SR), the Super-Resolution Convolutional Neural Network (SRCNN) [1, 2] has demonstrated superior performance to the previous hand-crafted models either in speed and restoration quality. However, the high computational cost still hinders it from practical usage that demands real-time performance (24 fps). In this paper, we aim at accelerating the current SRCNN, and propose a compact hourglass-shape CNN structure for faster and better SR. We re-design the SRCNN structure mainly in three aspects. First, we introduce a deconvolution layer at the end of the network, then the mapping is learned directly from the original low-resolution image (without interpolation) to the high-resolution one. Second, we reformulate the mapping layer by shrinking the input feature dimension before mapping and expanding back afterwards. Third, we adopt\\xa0…', '저자': 'Chao Dong, Chen Change Loy, Xiaoou Tang', '전체 인용횟수': '3267회 인용2017201820192020202120222023101282378549667672589', '컨퍼런스': 'Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14', '페이지': '391-407', '학술 문서': 'Accelerating the super-resolution convolutional neural networkC Dong, CC Loy, X Tang\\xa0- Computer Vision–ECCV 2016: 14th European\\xa0…, 20163267회 인용 관련 학술자료 전체 12개의 버전 '}, title='Accelerating the super-resolution convolutional neural network', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning to detect a salient object': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/3/18', '게시자': 'IEEE', '권': '33', '설명': 'In this paper, we study the salient object detection problem for images. We formulate this problem as a binary labeling task where we separate the salient object from the background. We propose a set of novel features, including multiscale contrast, center-surround histogram, and color spatial distribution, to describe a salient object locally, regionally, and globally. A conditional random field is learned to effectively combine these features for salient object detection. Further, we extend the proposed approach to detect a salient object from sequential images by introducing the dynamic salient features. We collected a large image database containing tens of thousands of carefully labeled images by multiple users and a video segment database, and conducted a set of experiments over them to demonstrate the effectiveness of the proposed approach.', '저널': 'IEEE Transactions on Pattern analysis and machine intelligence', '저자': 'Tie Liu, Zejian Yuan, Jian Sun, Jingdong Wang, Nanning Zheng, Xiaoou Tang, Heung-Yeung Shum', '전체 인용횟수': '3185회 인용2009201020112012201320142015201620172018201920202021202220234578135169249264307329327313279184182142100', '페이지': '353-367', '학술 문서': 'Learning to detect a salient objectT Liu, Z Yuan, J Sun, J Wang, N Zheng, X Tang…\\xa0- IEEE Transactions on Pattern analysis and machine\\xa0…, 20103185회 인용 관련 학술자료 전체 24개의 버전 ', '호': '2'}, title='Learning to detect a salient object', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep learning face representation by joint identification-verification': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '권': '27', '설명': 'The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 features extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 features extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset, 99.15% face verification accuracy is achieved. Compared with the best previous deep learning result on LFW, the error rate has been significantly reduced by 67%.', '저널': 'Advances in neural information processing systems', '저자': 'Yi Sun, Yuheng Chen, Xiaogang Wang, Xiaoou Tang', '전체 인용횟수': '2732회 인용20142015201620172018201920202021202220237100192335398430448322268182', '학술 문서': 'Deep learning face representation by joint identification-verificationY Sun, Y Chen, X Wang, X Tang\\xa0- Advances in neural information processing systems, 20142732회 인용 관련 학술자료 전체 16개의 버전 '}, title='Deep learning face representation by joint identification-verification', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep learning face representation from predicting 10,000 classes': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '설명': 'This paper proposes to learn a set of high-level feature representations through deep learning, referred to as Deep hidden IDentity features (DeepID), for face verification. We argue that DeepID can be effectively learned through challenging multi-class face identification tasks, whilst they can be generalized to other tasks (such as verification) and new identities unseen in the training set. Moreover, the generalization capability of DeepID increases as more face classes are to be predicted at training. DeepID features are taken from the last hidden layer neuron activations of deep convolutional networks (ConvNets). When learned as classifiers to recognize about 10,000 face identities in the training set and configured to keep reducing the neuron numbers along the feature extraction hierarchy, these deep ConvNets gradually form compact identity-related features in the top layers with only a small number of hidden neurons. The proposed features are extracted from various face regions to form complementary and over-complete representations. Any state-of-the-art classifiers can be learned based on these high-level representations for face verification. 97.45% verification accuracy on LFW is achieved with only weakly aligned faces.', '저자': 'Yi Sun, Xiaogang Wang, Xiaoou Tang', '전체 인용횟수': '2466회 인용201420152016201720182019202020212022202321113240324404340312278227156', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '1891-1898', '학술 문서': 'Deep learning face representation from predicting 10,000 classesY Sun, X Wang, X Tang\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20142466회 인용 관련 학술자료 전체 22개의 버전 '}, title='Deep learning face representation from predicting 10,000 classes', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Wider face: A face detection benchmark': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': 'Face detection is one of the most studied topics in the computer vision community. Much of the progresses have been made by the availability of face detection benchmark datasets. We show that there is a gap between current face detection performance and the real world requirements. To facilitate future face detection research, we introduce the WIDER FACE dataset, which is 10 times larger than existing datasets. The dataset contains rich annotations, including occlusions, poses, event categories, and face bounding boxes. Faces in the proposed dataset are extremely challenging due to large variations in scale, pose and occlusion, as shown in Fig. 1. Furthermore, we show that WIDER FACE dataset is an effective training source for face detection. We benchmark several representative detection systems, providing an overview of state-of-the-art performance and propose a solution to deal with large scale variation. Finally, we discuss common failure cases that worth to be further investigated.', '저자': 'Shuo Yang, Ping Luo, Chen-Change Loy, Xiaoou Tang', '전체 인용횟수': '1904회 인용201620172018201920202021202220231591148235303397395301', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '5525-5533', '학술 문서': 'Wider face: A face detection benchmarkS Yang, P Luo, CC Loy, X Tang\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20161904회 인용 관련 학술자료 전체 18개의 버전 '}, title='Wider face: A face detection benchmark', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deepfashion: Powering robust clothes recognition and retrieval with rich annotations': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': 'Recent advances in clothes recognition have been driven by the construction of clothes datasets. Existing datasets are limited in the amount of annotations and are difficult to cope with the various challenges in real-world applications. In this work, we introduce DeepFashion, a large-scale clothes dataset with comprehensive annotations. It contains over 800,000 images, which are richly annotated with massive attributes, clothing landmarks, and correspondence of images taken under different scenarios including store, street snapshot, and consumer. Such rich annotations enable the development of powerful algorithms in clothes recognition and facilitating future researches. To demonstrate the advantages of DeepFashion, we propose a new deep model, namely FashionNet, which learns clothing features by jointly predicting clothing attributes and landmarks. The estimated landmarks are then employed to pool or gate the learned features. It is optimized in an iterative manner. Extensive experiments demonstrate the effectiveness of FashionNet and the usefulness of DeepFashion.', '저자': 'Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, Xiaoou Tang', '전체 인용횟수': '1829회 인용20162017201820192020202120222023876158255295338361324', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '1096-1104', '학술 문서': 'Deepfashion: Powering robust clothes recognition and retrieval with rich annotationsZ Liu, P Luo, S Qiu, X Wang, X Tang\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20161829회 인용 관련 학술자료 전체 18개의 버전 '}, title='Deepfashion: Powering robust clothes recognition and retrieval with rich annotations', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep convolutional network cascade for facial point detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013', '설명': 'We propose a new approach for estimation of the positions of facial keypoints with three-level carefully designed convolutional networks. At each level, the outputs of multiple networks are fused for robust and accurate estimation. Thanks to the deep structures of convolutional networks, global high-level features are extracted over the whole face region at the initialization stage, which help to locate high accuracy keypoints. There are two folds of advantage for this. First, the texture context information over the entire face is utilized to locate each keypoint. Second, since the networks are trained to predict all the keypoints simultaneously, the geometric constraints among keypoints are implicitly encoded. The method therefore can avoid local minimum caused by ambiguity and data corruption in difficult image samples due to occlusions, large pose variations, and extreme lightings. The networks at the following two levels are trained to locally refine initial predictions and their inputs are limited to small regions around the initial predictions. Several network structures critical for accurate and robust facial point detection are investigated. Extensive experiments show that our approach outperforms state-ofthe-art methods in both detection accuracy and reliability 1.', '저자': 'Yi Sun, Xiaogang Wang, Xiaoou Tang', '전체 인용횟수': '1749회 인용20132014201520162017201820192020202120222023115413421221523323619816416183', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '3476-3483', '학술 문서': 'Deep convolutional network cascade for facial point detectionY Sun, X Wang, X Tang\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20131749회 인용 관련 학술자료 전체 14개의 버전 '}, title='Deep convolutional network cascade for facial point detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Facial landmark detection by deep multi-task learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '게시자': 'Springer International Publishing', '설명': ' Facial landmark detection has long been impeded by the problems of occlusion and pose variation. Instead of treating the detection task as a single and independent problem, we investigate the possibility of improving detection robustness through multi-task learning. Specifically, we wish to optimize facial landmark detection together with heterogeneous but subtly correlated tasks, e.g. head pose estimation and facial attribute inference. This is non-trivial since different tasks have different learning difficulties and convergence rates. To address this problem, we formulate a novel tasks-constrained deep model, with task-wise early stopping to facilitate learning convergence. Extensive evaluations show that the proposed task-constrained learning (i) outperforms existing methods, especially in dealing with faces with severe occlusion and pose variation, and (ii) reduces model complexity drastically compared to\\xa0…', '저자': 'Zhanpeng Zhang, Ping Luo, Chen Change Loy, Xiaoou Tang', '전체 인용횟수': '1669회 인용20152016201720182019202020212022202348120186240254255210172146', '컨퍼런스': 'Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13', '페이지': '94-108', '학술 문서': 'Facial landmark detection by deep multi-task learningZ Zhang, P Luo, CC Loy, X Tang\\xa0- Computer Vision–ECCV 2014: 13th European\\xa0…, 20141669회 인용 관련 학술자료 전체 14개의 버전 '}, title='Facial landmark detection by deep multi-task learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deepid3: Face recognition with very deep neural networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/2/3', '설명': 'The state-of-the-art of face recognition has been significantly advanced by the emergence of deep learning. Very deep neural networks recently achieved great success on general object recognition because of their superb learning capacity. This motivates us to investigate their effectiveness on face recognition. This paper proposes two very deep neural network architectures, referred to as DeepID3, for face recognition. These two architectures are rebuilt from stacked convolution and inception layers proposed in VGG net and GoogLeNet to make them suitable to face recognition. Joint face identification-verification supervisory signals are added to both intermediate and final feature extraction layers during training. An ensemble of the proposed two architectures achieves 99.53% LFW face verification accuracy and 96.0% LFW rank-1 face identification accuracy, respectively. A further discussion of LFW face verification result is given in the end.', '저널': 'arXiv preprint arXiv:1502.00873', '저자': 'Yi Sun, Ding Liang, Xiaogang Wang, Xiaoou Tang', '전체 인용횟수': '1170회 인용201520162017201820192020202120222023147614018917520017111274', '학술 문서': 'Deepid3: Face recognition with very deep neural networksY Sun, D Liang, X Wang, X Tang\\xa0- arXiv preprint arXiv:1502.00873, 20151170회 인용 관련 학술자료 전체 4개의 버전 '}, title='Deepid3: Face recognition with very deep neural networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deeply learned face representations are sparse, selective, and robust': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'This paper designs a high-performance deep convolutional network (DeepID2+) for face recognition. It is learned with the identification-verification supervisory signal. By increasing the dimension of hidden representations and adding supervision to early convolutional layers, DeepID2+ achieves new state-of-the-art on LFW and YouTube Faces benchmarks. Through empirical studies, we have discovered three properties of its deep neural activations critical for the high performance: sparsity, selectiveness and robustness.(1) It is observed that neural activations are moderately sparse. Moderate sparsity maximizes the discriminative power of the deep net as well as the distance between images. It is surprising that DeepID2+ still can achieve high recognition accuracy even after the neural responses are binarized.(2) Its neurons in higher layers are highly selective to identities and identity-related attributes. We can identify different subsets of neurons which are either constantly excited or inhibited when different identities or attributes are present. Although DeepID2+ is not taught to distinguish attributes during training, it has implicitly learned such high-level concepts.(3) It is much more robust to occlusions, although occlusion patterns are not included in the training set.', '저자': 'Yi Sun, Xiaogang Wang, Xiaoou Tang', '전체 인용횟수': '1146회 인용201520162017201820192020202120222023351051741741781491509958', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '2892-2900', '학술 문서': 'Deeply learned face representations are sparse, selective, and robustY Sun, X Wang, X Tang\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20151146회 인용 관련 학술자료 전체 18개의 버전 '}, title='Deeply learned face representations are sparse, selective, and robust', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Distinctive image features from scale-invariant keypoints': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2004/11/1', '게시자': 'Springer Netherlands', '권': '60', '설명': ' This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through\\xa0…', '저널': 'International journal of computer vision', '저자': 'David G Lowe', '전체 인용횟수': '72715회 인용20042005200620072008200920102011201220132014201520162017201820192020202120222023190505101615802242301038294212463250765408567453304960460645204098416138142971', '페이지': '91-110', '학술 문서': 'Distinctive image features from scale-invariant keypointsDG Lowe\\xa0- International journal of computer vision, 200472715회 인용 관련 학술자료 전체 141개의 버전 ', '호': '2'}, title='Distinctive image features from scale-invariant keypoints', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Object recognition from local scale-invariant features': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1999', '게시자': 'IEEE', '설명': 'An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object\\xa0…', '저널': 'International Conference on Computer Vision, 1999', '저자': 'David G Lowe', '전체 인용횟수': '24872회 인용200320042005200620072008200920102011201220132014201520162017201820192020202120222023832233053934976518981114133414761663178718541886176616651652147314741307993', '페이지': '1150-1157', '학술 문서': 'Object recognition from local scale-invariant featuresDG Lowe\\xa0- Proceedings of the seventh IEEE international\\xa0…, 199924872회 인용 관련 학술자료 전체 56개의 버전 '}, title='Object recognition from local scale-invariant features', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Fast Approximate Nearest Neighbors with Automatic Algorithm Configuration.': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009/2/5', '권': '2', '설명': 'For many computer vision problems, the most time consuming component consists of nearest neighbor matching in high-dimensional spaces. There are no known exact algorithms for solving these high-dimensional problems that are faster than linear search. Approximate algorithms are known to provide large speedups with only minor loss in accuracy, but many such algorithms have been published with only minimal guidance on selecting an algorithm and its parameters for any given problem. In this paper, we describe a system that answers the question,“What is the fastest approximate nearest-neighbor algorithm for my data?” Our system will take any given dataset and desired degree of precision and use these to automatically determine the best algorithm and parameter values. We also describe a new algorithm that applies priority search on hierarchical k-means trees, which we have found to provide the best known performance on many datasets. After testing a range of alternatives, we have found that multiple randomized kd trees provide the best performance for other datasets. We are releasing public domain code that implements these approaches. This library provides about one order of magnitude improvement in query time over the best previously available software and provides fully automated parameter selection.', '저널': 'VISAPP (1)', '저자': 'Marius Muja, David G Lowe', '전체 인용횟수': '4077회 인용20092010201120122013201420152016201720182019202020212022202340100184224354399441375357331280246240245183', '페이지': '331-340', '학술 문서': 'Fast approximate nearest neighbors with automatic algorithm configuration.M Muja, DG Lowe\\xa0- VISAPP (1), 20093980회 인용 관련 학술자료 전체 14개의 버전 Flann-fast library for approximate nearest neighbors user manual*M Muja, D Lowe\\xa0- Computer Science Department, University of British\\xa0…, 2009117회 인용 관련 학술자료 전체 4개의 버전 '}, title='Fast Approximate Nearest Neighbors with Automatic Algorithm Configuration.', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Automatic panoramic image stitching using invariant features': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2007/8/1', '게시자': 'Springer Netherlands', '권': '74', '설명': ' This paper concerns the problem of fully automated panoramic image stitching. Though the 1D problem (single axis of rotation) is well studied, 2D or multi-row stitching is more difficult. Previous approaches have used human input or restrictions on the image sequence in order to establish matching images. In this work, we formulate stitching as a multi-image matching problem, and use invariant local features to find matches between all of the images. Because of this our method is insensitive to the ordering, orientation, scale and illumination of the input images. It is also insensitive to noise images that are not part of a panorama, and can recognise multiple panoramas in an unordered image dataset. In addition to providing more detail, this paper extends our previous work in the area (Brown and Lowe, 2003) by introducing gain compensation and automatic straightening steps.', '저널': 'International Journal of Computer Vision', '저자': 'Matthew Brown, David G Lowe', '전체 인용횟수': '3485회 인용20072008200920102011201220132014201520162017201820192020202120222023174782154161188251250245295302294333255246182136', '페이지': '59-73', '학술 문서': 'Automatic panoramic image stitching using invariant featuresM Brown, DG Lowe\\xa0- International journal of computer vision, 20073485회 인용 관련 학술자료 전체 27개의 버전 ', '호': '1'}, title='Automatic panoramic image stitching using invariant features', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Unsupervised learning of depth and ego-motion from video': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'We present an unsupervised learning framework for the task of dense 3D geometry and camera motion estimation from unstructured video sequences. In common with recent work, we use an end-to-end learning approach with view synthesis as the supervisory signal. In contrast to these works, our method is completely unsupervised, requiring only a sequence of images as input. We achieve this with a network that estimates the 6-DoF camera pose parameters of the input set, along with dense depth for a reference view using single-view inference. Our loss is constructed by projecting the nearby posed views into the reference view via the depth map. Results using the KITTI dataset demonstrate the effectiveness of our approach, which performs on par with another deep learning approach that assumes ground-truth pose information at training time.', '저자': 'Tinghui Zhou, Matthew Brown, Noah Snavely, David G Lowe', '전체 인용횟수': '2605회 인용201720182019202020212022202331227392466532509432', '컨퍼런스': 'Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition', '페이지': '1851-1858', '학술 문서': 'Unsupervised learning of depth and ego-motion from videoT Zhou, M Brown, N Snavely, DG Lowe\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20172605회 인용 관련 학술자료 전체 14개의 버전 '}, title='Unsupervised learning of depth and ego-motion from video', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Perceptual Organization and Visual Recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1985', '게시자': 'Kluwer Academic Publishers, Boston', '저자': 'David G Lowe', '전체 인용횟수': '2167회 인용19861987198819891990199119921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023729549181871251291081061059884737173697037445256454640394550292534222021161689', '페이지': '162', '학술 문서': 'Perceptual organization and visual recognition*D Lowe - 20122167회 인용 관련 학술자료 전체 8개의 버전 '}, title='Perceptual Organization and Visual Recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Three-dimensional object recognition from single two-dimensional images': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1987/3/31', '게시자': 'Elsevier', '권': '31', '설명': 'A computer vision system has been implemented that can recognize three-dimensional objects from unknown viewpoints in single gray-scale images. Unlike most other approaches, the recognition is accomplished without any attempt to reconstruct depth information bottom-up from the visual input. Instead, three other mechanisms are used that can bridge the gap between the two-dimensional image and knowledge of three-dimensional objects. First, a process of perceptual organization is used to form groupings and structures in the image that are likely to be invariant over a wide range of viewpoints. Second, a probabilistic ranking method is used to reduce the size of the search space during model-based matching. Finally, a process of spatial correspondence brings the projections of three-dimensional models into direct correspondence with the image by solving for unknown viewpoint and model parameters. A\\xa0…', '저널': 'Artificial intelligence', '저자': 'David G Lowe', '전체 인용횟수': '2090회 인용19871988198919901991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220237385552659178867582646773534875487152527167706261618149475034323426221922', '페이지': '355-395', '학술 문서': 'Three-dimensional object recognition from single two-dimensional imagesDG Lowe\\xa0- Artificial intelligence, 19872090회 인용 관련 학술자료 전체 17개의 버전 ', '호': '3'}, title='Three-dimensional object recognition from single two-dimensional images', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Scalable Nearest Neighbor Algorithms for High Dimensional Data': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '게시자': 'IEEE', '권': '36', '설명': 'For many computer vision and machine learning problems, large training sets are key for good performance. However, the most computationally expensive part of many computer vision and machine learning algorithms consists of finding nearest neighbor matches to high dimensional vectors that represent the training data. We propose new algorithms for approximate nearest neighbor matching and evaluate and compare them with previous algorithms. For matching high dimensional features, we find two algorithms to be the most efficient: the randomized k-d forest and a new algorithm proposed in this paper, the priority search k-means tree. We also propose a new algorithm for matching binary features by searching multiple hierarchical clustering trees and show it outperforms methods typically used in the literature. We show that the optimal nearest neighbor algorithm and its parameters depend on the data set\\xa0…', '저널': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', '저자': 'Marius Muja, David G. Lowe', '전체 인용횟수': '1647회 인용20142015201620172018201920202021202220232113617425726323915816511195', '페이지': '2227-40', '학술 문서': 'Scalable nearest neighbor algorithms for high dimensional dataM Muja, DG Lowe\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20141647회 인용 관련 학술자료 전체 9개의 버전 ', '호': '11'}, title='Scalable Nearest Neighbor Algorithms for High Dimensional Data', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Recognising panoramas': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2003', '게시자': 'IEEE', '설명': 'The problem considered in this paper is the fully automatic construction of panoramas. Fundamentally, this problem requires recognition, as we need to know which parts of the panorama join up. Previous approaches have used human input or restrictions on the image sequence for the matching step. In this work we use object recognition techniques based on invariant local features to select matching images, and a probabilistic model for verification. Because of this our method is insensitive to the ordering, orientation, scale and illumination of the images. It is also insensitive to ‘noise’images which are not part of the panorama at all, that is, it recognises panoramas. This suggests a useful application for photographers: the system takes as input the images on an entire flash card or film, recognises images that form part of a panorama, and stitches them with no user input whatsoever.', '저자': 'Matthew Brown, David G Lowe', '전체 인용횟수': '1615회 인용200320042005200620072008200920102011201220132014201520162017201820192020202120222023615397911010112413813313312897907165666143382623', '컨퍼런스': 'International Conference on Computer Vision, 2003', '페이지': '1218-1225', '학술 문서': 'Recognising panoramas.M Brown, DG Lowe\\xa0- ICCV, 20031615회 인용 관련 학술자료 전체 27개의 버전 '}, title='Recognising panoramas', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Shape indexing using approximate nearest-neighbour search in high-dimensional spaces': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1997/6/17', '게시자': 'IEEE', '설명': 'Shape indexing is a way of making rapid associations between features detected in an image and object models that could have produced them. When model databases are large, the use of high-dimensional features is critical, due to the improved level of discrimination they can provide. Unfortunately, finding the nearest neighbour to a query point rapidly becomes inefficient as the dimensionality of the feature space increases. Past indexing methods have used hash tables for hypothesis recovery, but only in low-dimensional situations. In this paper we show that a new variant of the k-d tree search algorithm makes indexing in higher-dimensional spaces practical. This Best Bin First, or BBF search is an approximate algorithm which finds the nearest neighbour for a large fraction of the queries, and a very close neighbour in the remaining cases. The technique has been integrated into a fully developed recognition\\xa0…', '저자': 'Jeffrey S Beis, David G Lowe', '전체 인용횟수': '1565회 인용1998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202357465917314262881091431091211501191119567655232512917', '컨퍼런스': 'Computer Vision and Pattern Recognition, 1997. Proceedings., 1997 IEEE Computer Society Conference on', '페이지': '1000-1006', '학술 문서': 'Shape indexing using approximate nearest-neighbour search in high-dimensional spacesJS Beis, DG Lowe\\xa0- Proceedings of IEEE computer society conference on\\xa0…, 19971565회 인용 관련 학술자료 전체 20개의 버전 '}, title='Shape indexing using approximate nearest-neighbour search in high-dimensional spaces', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A boosted particle filter: Multitarget detection and tracking': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2004', '게시자': 'Springer Berlin/Heidelberg', '설명': ' The problem of tracking a varying number of non-rigid objects has two major difficulties. First, the observation models and target distributions can be highly non-linear and non-Gaussian. Second, the presence of a large, varying number of objects creates complex interactions with overlap and ambiguities. To surmount these difficulties, we introduce a vision system that is capable of learning, detecting and tracking the objects of interest. The system is demonstrated in the context of tracking hockey players using video sequences. Our approach combines the strengths of two successful algorithms: mixture particle filters and Adaboost. The mixture particle filter\\xa0[17] is ideally suited to multi-target tracking as it assigns a mixture component to each player. The crucial design issues in mixture particle filters are the choice of the proposal distribution and the treatment of objects leaving and entering the scene. Here\\xa0…', '저널': 'Computer Vision-ECCV 2004', '저자': 'Kenji Okuma, Ali Taleghani, Nando de Freitas, James J Little, David G Lowe', '전체 인용횟수': '1516회 인용20032004200520062007200820092010201120122013201420152016201720182019202020212022202341053878411911610211497104107998980574946421816', '페이지': '28-39', '학술 문서': 'A boosted particle filter: Multitarget detection and trackingK Okuma, A Taleghani, N De Freitas, JJ Little, DG Lowe\\xa0- Computer Vision-ECCV 2004: 8th European\\xa0…, 20041516회 인용 관련 학술자료 전체 15개의 버전 '}, title='A boosted particle filter: Multitarget detection and tracking', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Fitting parameterized three-dimensional models to images': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1991/5/1', '권': '13', '설명': 'Model-based recognition and motion tracking depends upon the ability to solve for projection and model parameters that will best fit a 3-D model to matching 2-D image features. This paper extends current methods of parameter solving to handle objects with arbitrary curved surfaces and with any number of internal parameters representing articulation, variable dimensions, or surface deformations. Numerical stabilization methods are developed that take account of inherent inaccuracies in the image measurements and allow useful solutions to be determined even when there are fewer matches than unknown parameters. The Levenberg-Marquardt method is used to always ensure convergence of the solution. These techniques allow model-based vision to be used for a much wider class of problems than was possible with previous methods. Their application is demonstrated for tracking the motion of curved\\xa0…', '저널': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', '저자': 'David G.  Lowe', '전체 인용횟수': '1420회 인용19911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202362121405255443940455053546765647557585264655334373524212331201411', '페이지': '441-450', '학술 문서': 'Fitting parameterized three-dimensional models to imagesDG Lowe\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 19911420회 인용 관련 학술자료 전체 20개의 버전 ', '호': '5'}, title='Fitting parameterized three-dimensional models to images', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Invariant features from interest point groups.': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2002/9/2', '권': '4', '설명': 'This paper approaches the problem of finding correspondences between images in which there are large changes in viewpoint, scale and illumination. Recent work has shown that scale-space ‘interest points’ may be found with good repeatability in spite of such changes. Furthermore, the high entropy of the surrounding image regions means that local descriptors are highly discriminative for matching. For descriptors at interest points to be robustly matched between images, they must be as far as possible invariant to the imaging process. In this work we introduce a family of features which use groups of interest points to form geometrically invariant descriptors of image regions. Feature descriptors are formed by resampling the image relative to canonical frames defined by the points. In addition to robust matching, a key advantage of this approach is that each match implies a hypothesis of the local 2D (projective) transformation. This allows us to immediately reject most of the false matches using a Hough transform. We reject remaining outliers using RANSAC and the epipolar constraint. Results show that dense feature matching can be achieved in a few seconds of computation on 1GHz Pentium III machines.', '저자': 'Matthew Brown, David G Lowe', '전체 인용횟수': '1198회 인용2003200420052006200720082009201020112012201320142015201620172018201920202021202220237252447396983991041009881917359503229362412', '컨퍼런스': 'BMVC', '페이지': '398-410', '학술 문서': 'Invariant features from interest point groups.M Brown, DG Lowe\\xa0- Bmvc, 20021198회 인용 관련 학술자료 전체 18개의 버전 '}, title='Invariant features from interest point groups.', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Mobile robot localization and mapping with uncertainty using scale-invariant visual landmarks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2002/8/1', '게시자': 'SAGE Publications', '권': '21', '설명': 'A key component of a mobile robot system is the ability to localize itself accurately and, simultaneously, to build a map of the environment. Most of the existing algorithms are based on laser range finders, sonar sensors or artificial landmarks. In this paper, we describe a vision-based mobile robot localization and mapping algorithm, which uses scale-invariant image features as natural landmarks in unmodified environments. The invariance of these features to image translation, scaling and rotation makes them suitable landmarks for mobile robot localization and map building. With our Triclops stereo vision system, these landmarks are localized and robot ego-motion is estimated by least-squares minimization of the matched landmarks. Feature viewpoint variation and occlusion are taken into account by maintaining a view direction for each landmark. Experiments show that these visual landmarks are robustly\\xa0…', '저널': 'The international Journal of robotics Research', '저자': 'Stephen Se, David Lowe, Jim Little', '전체 인용횟수': '1198회 인용20032004200520062007200820092010201120122013201420152016201720182019202020212022202374062103113100827881798050414440393828292818', '페이지': '735-758', '학술 문서': 'Mobile robot localization and mapping with uncertainty using scale-invariant visual landmarksS Se, D Lowe, J Little\\xa0- The international Journal of robotics Research, 20021198회 인용 관련 학술자료 전체 18개의 버전 ', '호': '8'}, title='Mobile robot localization and mapping with uncertainty using scale-invariant visual landmarks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Local feature view clustering for 3D object recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2001', '게시자': 'IEEE', '권': '1', '설명': 'There have been important recent advances in object recognition through the matching of invariant local image features. However, the existing approaches are based on matching to individual training images. This paper presents a method for combining multiple images of a 3D object into a single model representation. This provides for recognition of 3D objects from any viewpoint, the generalization of models to non-rigid changes, and improved robustness through the combination of features acquired under a range of imaging conditions. The decision of whether to cluster a training image into an existing view representation or to treat it as a new view is based on the geometric accuracy of the match to previous model views. A new probabilistic model is developed to reduce the false positive matches that would otherwise arise due to loosened geometric constraints on matching 3D and non-rigid models. A system\\xa0…', '저자': 'David G Lowe', '전체 인용횟수': '876회 인용200220032004200520062007200820092010201120122013201420152016201720182019202020212022202358252545405255695852615658434940273531198', '컨퍼런스': 'Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on', '페이지': 'I-682-I-688 vol. 1', '학술 문서': 'Local feature view clustering for 3D object recognitionDG Lowe\\xa0- Proceedings of the 2001 IEEE Computer Society\\xa0…, 2001876회 인용 관련 학술자료 전체 16개의 버전 '}, title='Local feature view clustering for 3D object recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Vision-based mobile robot localization and mapping using scale-invariant features': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2001', '게시자': 'IEEE', '권': '2', '설명': 'A key component of a mobile robot system is the ability to localize itself accurately and build a map of the environment simultaneously. In this paper, a vision-based mobile robot localization and mapping algorithm is described which uses scale-invariant image features as landmarks in unmodified dynamic environments. These 3D landmarks are localized and robot ego-motion is estimated by matching them, taking into account the feature viewpoint variation. With our Triclops stereo vision system, experiments show that these features are robustly matched between views, 3D landmarks are tracked, robot pose is estimated and a 3D map is built.', '저자': 'Stephen Se, David Lowe, Jim Little', '전체 인용횟수': '797회 인용200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220233161831273853606776484536505143312817221466', '컨퍼런스': 'Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No. 01CH37164)', '페이지': '2051-2058', '학술 문서': 'Vision-based mobile robot localization and mapping using scale-invariant featuresS Se, D Lowe, J Little\\xa0- …\\xa02001 ICRA. IEEE International Conference on\\xa0…, 2001797회 인용 관련 학술자료 전체 16개의 버전 '}, title='Vision-based mobile robot localization and mapping using scale-invariant features', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Method and apparatus for identifying scale invariant features in an image and use of same for locating an object in an image': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2004/3/23', '발명자': 'David G Lowe', '설명': 'A method and apparatus for identifying scale invariant features in an image and a further method and apparatus for using such scale invariant features to locate an object in an image are disclosed. The method and apparatus for identifying scale invariant features may involve the use of a processor circuit for producing a plurality of component subregion descriptors for each subregion of a pixel region about pixel amplitude extrema in a plurality of difference images produced from the image. This may involve producing a plurality of difference images by blurring an initial image to produce a blurred image and by subtracting the blurred image from the initial image to produce the difference image. For each difference image, pixel amplitude extrema are located and a corresponding pixel region is defined about each pixel amplitude extremum. Each pixel region is divided into subregions and a plurality of component\\xa0…', '전체 인용횟수': '789회 인용20062007200820092010201120122013201420152016201720182019202020212022202368571521284053797986886479634221', '출원번호': '09519893', '특허 번호': '6711293', '특허청': 'US', '학술 문서': 'Method and apparatus for identifying scale invariant features in an image and use of same for locating an object in an imageDG Lowe\\xa0- US Patent 6,711,293, 2004789회 인용 관련 학술자료 전체 2개의 버전 '}, title='Method and apparatus for identifying scale invariant features in an image and use of same for locating an object in an image', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Vision-based global localization and mapping for mobile robots': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2005/6', '게시자': 'IEEE', '권': '21', '설명': 'We have previously developed a mobile robot system which uses scale-invariant visual landmarks to localize and simultaneously build three-dimensional (3-D) maps of unmodified environments. In this paper, we examine global localization, where the robot localizes itself globally, without any prior location estimate. This is achieved by matching distinctive visual landmarks in the current frame to a database map. A Hough transform approach and a RANSAC approach for global localization are compared, showing that RANSAC is much more efficient for matching specific features, but much worse for matching nonspecific features. Moreover, robust global localization can be achieved by matching a small submap of the local region built from multiple frames. This submap alignment algorithm for global localization can be applied to map building, which can be regarded as alignment of multiple 3-D submaps. A global\\xa0…', '저널': 'Robotics, IEEE Transactions on', '저자': 'Stephen Se, David G Lowe, James J Little', '전체 인용횟수': '769회 인용20062007200820092010201120122013201420152016201720182019202020212022202326717770724761576044344129171915174', '페이지': '364-375', '학술 문서': 'Vision-based global localization and mapping for mobile robotsS Se, DG Lowe, JJ Little\\xa0- IEEE Transactions on robotics, 2005769회 인용 관련 학술자료 전체 16개의 버전 ', '호': '3'}, title='Vision-based global localization and mapping for mobile robots', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Multiclass object recognition with sparse, localized features': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2006/6/17', '게시자': 'IEEE', '권': '1', '설명': 'We apply a biologically inspired model of visual object recognition to the multiclass object categorization problem. Our model modifies that of Serre, Wolf, and Poggio. As in that work, we first apply Gabor filters at all positions and scales; feature complexity and position/scale invariance are then built up by alternating template matching and max pooling operations. We refine the approach in several biologically plausible ways, using simple versions of sparsification and lateral inhibition. We demonstrate the value of retaining some position and scale information above the intermediate feature level. Using feature selection we arrive at a model that performs better with fewer features. Our final model is tested on the Caltech 101 object categories and the UIUC car localization task, in both cases achieving state-of-the-art performance. The results strengthen the case for using this class of model in computer vision.', '저자': 'Jim Mutch, David G Lowe', '전체 인용횟수': '658회 인용20052006200720082009201020112012201320142015201620172018201920202021202220232126068646851495161333816241971253', '컨퍼런스': \"2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)\", '페이지': '11-18', '학술 문서': 'Multiclass object recognition with sparse, localized featuresJ Mutch, DG Lowe\\xa0- 2006 IEEE Computer Society Conference on Computer\\xa0…, 2006658회 인용 관련 학술자료 전체 14개의 버전 '}, title='Multiclass object recognition with sparse, localized features', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Robust model-based motion tracking through the integration of search and estimation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1992/8/1', '게시자': 'Springer Netherlands', '권': '8', '설명': ' A computer vision system has been developed for real-time motion tracking of 3-D objects, including those with variable internal parameters. This system provides for the integrated treatment of matching and measurement errors that arise during motion tracking. These two sources of error have very different distributions and are best handled by separate computational mechanisms. These errors can be treated in an integrated way by using the computation of variance in predicted feature measurements to determine the probability of correctness for each potential matching feature. In return, a best-first search procedure uses these probabilities to find consistent sets of matches, which eliminates the need to treat outliers during the analysis of measurement errors. The most reliable initial matches are used to reduce the parameter variance on further iterations, minimizing the amount of search required for\\xa0…', '저널': 'International Journal of Computer Vision', '저자': 'David G Lowe', '전체 인용횟수': '561회 인용199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202371222352029322539312822382724241415171715108313442625', '페이지': '113-122', '학술 문서': 'Robust model-based motion tracking through the integration of search and estimationDG Lowe\\xa0- International Journal of Computer Vision, 1992561회 인용 관련 학술자료 전체 16개의 버전 ', '호': '2'}, title='Robust model-based motion tracking through the integration of search and estimation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Visualizing and understanding convolutional networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '게시자': 'Springer International Publishing', '설명': ' Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.', '저자': 'Matthew D Zeiler, Rob Fergus', '전체 인용횟수': '20756회 인용20132014201520162017201820192020202120222023631705409941656248527802927296226902091', '컨퍼런스': 'Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13', '페이지': '818-833', '학술 문서': 'Visualizing and understanding convolutional networksMD Zeiler, R Fergus\\xa0- Computer Vision–ECCV 2014: 13th European\\xa0…, 201420756회 인용 관련 학술자료 전체 23개의 버전 '}, title='Visualizing and understanding convolutional networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning spatiotemporal features with 3d convolutional networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets; 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets; and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8% accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use.', '저자': 'Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, Manohar Paluri', '전체 인용횟수': '9048회 인용2016201720182019202020212022202316954392812501403164315821401', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '4489-4497', '학술 문서': 'Learning spatiotemporal features with 3d convolutional networksD Tran, L Bourdev, R Fergus, L Torresani, M Paluri\\xa0- Proceedings of the IEEE international conference on\\xa0…, 20159048회 인용 관련 학술자료 전체 15개의 버전 '}, title='Learning spatiotemporal features with 3d convolutional networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Overfeat: Integrated recognition, localization and detection using convolutional networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/12/21', '게시자': 'CBLS', '설명': 'We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.', '저자': 'Pierre Sermanet, David Eigen, Xiang Zhang, Michaël Mathieu, Rob Fergus, Yann LeCun', '전체 인용횟수': '7493회 인용19881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202319353634384471568380998210688501418122415243329325047175467673789892885765657526355', '컨퍼런스': '[ICLR 2014] International Conference on Learning Representations', '페이지': '16', '학술 문서': 'Overfeat: Integrated recognition, localization and detection using convolutional networksP Sermanet, D Eigen, X Zhang, M Mathieu, R Fergus…\\xa0- arXiv preprint arXiv:1312.6229, 20137324회 인용 관련 학술자료 전체 30개의 버전 OverFeat: Integrated Recognition*P Sermanet, D Eigen, X Zhang, M Mathieu, R Fergus…\\xa0- Localization and Detection using Convolutional\\xa0…, 2013233회 인용 관련 학술자료 Overfeat: Integrated recognition, localization and detection using convolutional networks. arXiv 2013*P Sermanet, D Eigen, X Zhang, M Mathieu, R Fergus…\\xa0- arXiv preprint arXiv:1312.6229177회 인용 관련 학술자료 Overfeat: Integrated Recognition, Localization and Detection Using Convolutional Networks. arxiv. Org*P Sermanet, D Eigen, X Zhang, M Mathieu, R Fergus… - 20134회 인용 관련 학술자료 '}, title='Overfeat: Integrated recognition, localization and detection using convolutional networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Indoor segmentation and support inference from rgbd images': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012', '게시자': 'Springer Berlin Heidelberg', '설명': ' We present an approach to interpret the major surfaces, objects, and support relations of an indoor scene from an RGBD image. Most existing work ignores physical interactions or is applied only to tidy rooms and hallways. Our goal is to parse typical, often messy, indoor scenes into floor, walls, supporting surfaces, and object regions, and to recover support relationships. One of our main interests is to better understand how 3D cues can best inform a structured 3D interpretation. We also contribute a novel integer programming formulation to infer physical support relations. We offer a new dataset of 1449 RGBD images, capturing 464 diverse indoor scenes, with detailed annotations. Our experiments demonstrate our ability to infer support relations in complex scenes and verify that our 3D scene cues and inferred support lead to better object segmentation.', '저자': 'Nathan Silberman, Derek Hoiem, Pushmeet Kohli, Rob Fergus', '전체 인용횟수': '5603회 인용2013201420152016201720182019202020212022202388145239309386502596667830887843', '컨퍼런스': 'Computer Vision–ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part V 12', '페이지': '746-760', '학술 문서': 'Indoor segmentation and support inference from rgbd imagesN Silberman, D Hoiem, P Kohli, R Fergus\\xa0- Computer Vision–ECCV 2012: 12th European\\xa0…, 20125603회 인용 관련 학술자료 전체 22개의 버전 '}, title='Indoor segmentation and support inference from rgbd images', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Depth map prediction from a single image using a multi-scale deep network': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '권': '27', '설명': 'Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation.', '저널': 'Advances in neural information processing systems', '저자': 'David Eigen, Christian Puhrsch, Rob Fergus', '전체 인용횟수': '3981회 인용20152016201720182019202020212022202367138219384564601676682591', '학술 문서': 'Depth map prediction from a single image using a multi-scale deep networkD Eigen, C Puhrsch, R Fergus\\xa0- Advances in neural information processing systems, 20143981회 인용 관련 학술자료 전체 15개의 버전 '}, title='Depth map prediction from a single image using a multi-scale deep network', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Spectral hashing': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2008', '권': '21', '설명': 'Semantic hashing seeks compact binary codes of datapoints so that the Hamming distance between codewords correlates with semantic similarity. Hinton et al. used a clever implementation of autoencoders to find such codes. In this paper, we show that the problem of finding a best code for a given dataset is closely related to the problem of graph partitioning and can be shown to be NP hard. By relaxing the original problem, we obtain a spectral method whose solutions are simply a subset of thresh-olded eigenvectors of the graph Laplacian. By utilizing recent results on convergence of graph Laplacian eigenvectors to the Laplace-Beltrami eigen-functions of manifolds, we show how to efficiently calculate the code of a novel datapoint. Taken together, both learning the code and applying it to a novel point are extremely simple. Our experiments show that our codes significantly outperform the state-of-the art.', '저널': 'Advances in neural information processing systems', '저자': 'Yair Weiss, Antonio Torralba, Rob Fergus', '전체 인용횟수': '3167회 인용200920102011201220132014201520162017201820192020202120222023275287139184235274342352309294267241185130', '학술 문서': 'Spectral hashingY Weiss, A Torralba, R Fergus\\xa0- Advances in neural information processing systems, 20083167회 인용 관련 학술자료 전체 16개의 버전 '}, title='Spectral hashing', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'In this paper we address three different computer vision tasks using a single basic architecture: depth prediction, surface normal estimation, and semantic labeling. We use a multiscale convolutional network that is able to adapt easily to each task using only small modifications, regressing from the input image to the output map directly. Our method progressively refines predictions using a sequence of scales, and captures many image details without any superpixels or low-level segmentation. We achieve state-of-the-art performance on benchmarks for all three tasks.', '저자': 'David Eigen, Rob Fergus', '전체 인용횟수': '3029회 인용20152016201720182019202020212022202324124257414475495463426293', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '2650-2658', '학술 문서': 'Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architectureD Eigen, R Fergus\\xa0- Proceedings of the IEEE international conference on\\xa0…, 20153029회 인용 관련 학술자료 전체 18개의 버전 '}, title='Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'End-to-end memory networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '권': '28', '설명': 'We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.', '저널': 'Advances in neural information processing systems', '저자': 'Sainbayar Sukhbaatar, Jason Weston, Rob Fergus', '전체 인용횟수': '2942회 인용20152016201720182019202020212022202333173303441488498393349234', '학술 문서': 'End-to-end memory networksS Sukhbaatar, J Weston, R Fergus\\xa0- Advances in neural information processing systems, 20152942회 인용 관련 학술자료 전체 14개의 버전 '}, title='End-to-end memory networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep generative image models using a￼ laplacian pyramid of adversarial networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '권': '28', '설명': 'In this paper we introduce a generative model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks (convnets) within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach. Samples drawn from our model are of significantly higher quality than existing models. In a quantitive assessment by human evaluators our CIFAR10 samples were mistaken for real images around 40% of the time, compared to 10% for GAN samples. We also show samples from more diverse datasets such as STL10 and LSUN.', '저널': 'Advances in neural information processing systems', '저자': 'Emily L Denton, Soumith Chintala, Rob Fergus', '전체 인용횟수': '2802회 인용20152016201720182019202020212022202320129296417441446372357266', '학술 문서': 'Deep generative image models using a￼ laplacian pyramid of adversarial networksEL Denton, S Chintala, R Fergus\\xa0- Advances in neural information processing systems, 20152802회 인용 관련 학술자료 전체 8개의 버전 '}, title='Deep generative image models using a￼ laplacian pyramid of adversarial networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Removing camera shake from a single photograph': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2006/7/1', '도서': 'Acm Siggraph 2006 Papers', '설명': 'Camera shake during exposure leads to objectionable image blur and ruins many photographs. Conventional blind deconvolution methods typically assume frequency-domain constraints on images, or overly simplified parametric forms for the motion path during camera shake. Real camera motions can follow convoluted paths, and a spatial domain prior can better maintain visually salient image characteristics. We introduce a method to remove the effects of camera shake from seriously blurred images. The method assumes a uniform camera blur over the image and negligible in-plane camera rotation. In order to estimate the blur from the camera shake, the user must specify an image region without saturation effects. We show results for a variety of digital photographs taken from personal photo collections.', '저자': 'Rob Fergus, Barun Singh, Aaron Hertzmann, Sam T Roweis, William T Freeman', '전체 인용횟수': '2619회 인용2006200720082009201020112012201320142015201620172018201920202021202220238285197130131186184219199225182172174169150147120', '페이지': '787-794', '학술 문서': 'Removing camera shake from a single photographR Fergus, B Singh, A Hertzmann, ST Roweis…\\xa0- Acm Siggraph 2006 Papers, 20062619회 인용 관련 학술자료 전체 13개의 버전 '}, title='Removing camera shake from a single photograph', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " '80 million tiny images: A large data set for nonparametric object and scene recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2008/5/30', '게시자': 'IEEE', '권': '30', '설명': 'With the advent of the Internet, billions of images are now freely available online and constitute a dense sampling of the visual world. Using a variety of non-parametric methods, we explore this world with the aid of a large dataset of 79,302,017 images collected from the Internet. Motivated by psychophysical results showing the remarkable tolerance of the human visual system to degradations in image resolution, the images in the dataset are stored as 32 x 32 color images. Each image is loosely labeled with one of the 75,062 non-abstract nouns in English, as listed in the Wordnet lexical database. Hence the image database gives a comprehensive coverage of all object categories and scenes. The semantic information from Wordnet can be used in conjunction with nearest-neighbor methods to perform object classification over a range of semantic levels minimizing the effects of labeling noise. For certain classes\\xa0…', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Antonio Torralba, Rob Fergus, William T Freeman', '전체 인용횟수': '2319회 인용2008200920102011201220132014201520162017201820192020202120222023874113125152174177188163170158149147169165139', '페이지': '1958-1970', '학술 문서': '80 million tiny images: A large data set for nonparametric object and scene recognitionA Torralba, R Fergus, WT Freeman\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20082319회 인용 관련 학술자료 전체 19개의 버전 ', '호': '11'}, title='80 million tiny images: A large data set for nonparametric object and scene recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deconvolutional networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/6/13', '게시자': 'IEEE', '설명': 'Building robust low and mid-level image representations, beyond edge primitives, is a long-standing goal in vision. Many existing feature detectors spatially pool edge information which destroys cues such as edge intersections, parallelism and symmetry. We present a learning framework where features that capture these mid-level cues spontaneously emerge from image data. Our approach is based on the convolutional decomposition of images under a spar-sity constraint and is totally unsupervised. By building a hierarchy of such decompositions we can learn rich feature sets that are a robust image representation for both the analysis and synthesis of images.', '저자': 'Matthew D Zeiler, Dilip Krishnan, Graham W Taylor, Rob Fergus', '전체 인용횟수': '2132회 인용2010201120122013201420152016201720182019202020212022202392220324473118198286268276278257219', '컨퍼런스': '2010 IEEE Computer Society Conference on computer vision and pattern recognition', '페이지': '2528-2535', '학술 문서': 'Deconvolutional networksMD Zeiler, D Krishnan, GW Taylor, R Fergus\\xa0- 2010 IEEE Computer Society Conference on computer\\xa0…, 20102132회 인용 관련 학술자료 전체 15개의 버전 '}, title='Deconvolutional networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Image and depth from a conventional camera with a coded aperture': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2007/7/29', '게시자': 'ACM', '권': '26', '설명': 'A conventional camera captures blurred versions of scene information away from the plane of focus. Camera systems have been proposed that allow for recording all-focus images, or for extracting depth, but to record both simultaneously has required more extensive hardware and reduced spatial resolution. We propose a simple modification to a conventional camera that allows for the simultaneous recovery of both (a) high resolution image information and (b) depth information adequate for semi-automatic extraction of a layered depth representation of the image. Our modification is to insert a patterned occluder within the aperture of the camera lens, creating a coded aperture. We introduce a criterion for depth discriminability which we use to design the preferred aperture pattern. Using a statistical model of images, we can recover both depth information and an all-focus image from single photographs taken with\\xa0…', '저널': 'ACM transactions on graphics (TOG)', '저자': 'Anat Levin, Rob Fergus, Frédo Durand, William T Freeman', '전체 인용횟수': '1943회 인용2006200720082009201020112012201320142015201620172018201920202021202220238742861001091311581741611651461381241051008467', '페이지': '70-es', '학술 문서': 'Image and depth from a conventional camera with a coded apertureA Levin, R Fergus, F Durand, WT Freeman\\xa0- ACM transactions on graphics (TOG), 20071943회 인용 관련 학술자료 전체 16개의 버전 ', '호': '3'}, title='Image and depth from a conventional camera with a coded aperture', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Exploiting linear structure within convolutional networks for efficient evaluation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '권': '27', '설명': 'We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy, but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the redundancy present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2×, while keeping the accuracy within 1% of the original model.', '저널': 'Advances in neural information processing systems', '저자': 'Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, Rob Fergus', '전체 인용횟수': '1909회 인용201420152016201720182019202020212022202394187153245291310300238212', '학술 문서': 'Exploiting linear structure within convolutional networks for efficient evaluationEL Denton, W Zaremba, J Bruna, Y LeCun, R Fergus\\xa0- Advances in neural information processing systems, 20141909회 인용 관련 학술자료 전체 11개의 버전 '}, title='Exploiting linear structure within convolutional networks for efficient evaluation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Pointnet: Deep learning on point sets for 3d classification and segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.', '저자': 'Charles R Qi, Hao Su, Kaichun Mo, Leonidas J Guibas', '전체 인용횟수': '13223회 인용20172018201920202021202220236448712381938268934293285', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '652-660', '학술 문서': 'Pointnet: Deep learning on point sets for 3d classification and segmentationCR Qi, H Su, K Mo, LJ Guibas\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 201713223회 인용 관련 학술자료 전체 20개의 버전 '}, title='Pointnet: Deep learning on point sets for 3d classification and segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Pointnet++: Deep hierarchical feature learning on point sets in a metric space': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '권': '30', '설명': 'Few prior works study deep learning on point sets. PointNet is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.', '저널': 'Advances in neural information processing systems', '저자': 'Charles Ruizhongtai Qi, Li Yi, Hao Su, Leonidas J Guibas', '전체 인용횟수': '9238회 인용2018201920202021202220232447261226189425202565', '학술 문서': 'Pointnet++: Deep hierarchical feature learning on point sets in a metric spaceCR Qi, L Yi, H Su, LJ Guibas\\xa0- Advances in neural information processing systems, 20179238회 인용 관련 학술자료 전체 22개의 버전 '}, title='Pointnet++: Deep hierarchical feature learning on point sets in a metric space', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " \"The earth mover's distance as a metric for image retrieval\": Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2000/11', '게시자': 'Kluwer Academic Publishers', '권': '40', '설명': \" We investigate the properties of a metric between two distributions, the Earth Mover's Distance (EMD), for content-based image retrieval. The EMD is based on the minimal cost that must be paid to transform one distribution into the other, in a precise sense, and was first proposed for certain vision problems by Peleg, Werman, and Rom. For image retrieval, we combine this idea with a representation scheme for distributions that is based on vector quantization. This combination leads to an image comparison framework that often accounts for perceptual similarity better than other previously proposed methods. The EMD is based on a solution to the transportation problem from linear optimization, for which efficient algorithms are available, and also allows naturally for partial matching. It is more robust than histogram matching techniques, in that it can operate on variable-length representations of the\\xa0…\", '저널': 'International journal of computer vision', '저자': 'Yossi Rubner, Carlo Tomasi, Leonidas J Guibas', '전체 인용횟수': '6173회 인용1998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202320211225245567114156178207237253268274318403348346321358359424478450409', '페이지': '99-121', '학술 문서': \"The earth mover's distance as a metric for image retrievalY Rubner, C Tomasi, LJ Guibas\\xa0- International journal of computer vision, 20005782회 인용 관련 학술자료 전체 41개의 버전 The earth mover’s distance, multi-dimensional scaling, and color-based image retrieval*Y Rubner, LJ Guibas, C Tomasi\\xa0- Proceedings of the ARPA image understanding\\xa0…, 1997467회 인용 관련 학술자료 전체 19개의 버전 \"}, title=\"The earth mover's distance as a metric for image retrieval\", authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Shapenet: An information-rich 3d model repository': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/12/9', '설명': 'We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.', '저널': 'arXiv preprint arXiv:1512.03012', '저자': 'Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, Fisher Yu', '전체 인용횟수': '4536회 인용20162017201820192020202120222023491623304546338029761083', '학술 문서': 'Shapenet: An information-rich 3d model repositoryAX Chang, T Funkhouser, L Guibas, P Hanrahan…\\xa0- arXiv preprint arXiv:1512.03012, 20154536회 인용 관련 학술자료 전체 19개의 버전 '}, title='Shapenet: An information-rich 3d model repository', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Frustum pointnets for 3d object detection from rgb-d data': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'In this work, we study 3D object detection from RGB-D data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efficiently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efficiency as well as high recall for even small objects. Benefited from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability.', '저자': 'Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, Leonidas J Guibas', '전체 인용횟수': '2351회 인용20182019202020212022202355282429537560472', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '918-927', '학술 문서': 'Frustum pointnets for 3d object detection from rgb-d dataCR Qi, W Liu, C Wu, H Su, LJ Guibas\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20182351회 인용 관련 학술자료 전체 12개의 버전 '}, title='Frustum pointnets for 3d object detection from rgb-d data', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A metric for distributions with applications to image databases': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1998/1/7', '게시자': 'IEEE', '설명': 'We introduce a new distance between two distributions that we call the Earth Mover\\'s Distance (EMD), which reflects the minimal amount of work that must be performed to transform one distribution into the other by moving \"distribution mass\" around. This is a special case of the transportation problem from linear optimization, for which efficient algorithms are available. The EMD also allows for partial matching. When used to compare distributions that have the same overall mass, the EMD is a true metric, and has easy-to-compute lower bounds. In this paper we focus on applications to image databases, especially color and texture. We use the EMD to exhibit the structure of color-distribution and texture spaces by means of Multi-Dimensional Scaling displays. We also propose a novel approach to the problem of navigating through a collection of color images, which leads to a new paradigm for image database search.', '저자': 'Yossi Rubner, Carlo Tomasi, Leonidas J Guibas', '전체 인용횟수': '2284회 인용1998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202313271940414562827678106971107110311611111611110511611613513812495', '컨퍼런스': 'Sixth international conference on computer vision (IEEE Cat. No. 98CH36271)', '페이지': '59-66', '학술 문서': 'A metric for distributions with applications to image databasesY Rubner, C Tomasi, LJ Guibas\\xa0- Sixth international conference on computer vision\\xa0…, 19982284회 인용 관련 학술자료 전체 30개의 버전 '}, title='A metric for distributions with applications to image databases', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Primitives for the manipulation of general subdivisions and the computation of Voronoi': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1985/4/1', '게시자': 'ACM', '권': '4', '설명': 'The following problem is discussed: given n points in the plane (the sites) and an arbitrary query point q, find the site that is closest to q. This problem can be solved by constructing the Voronoi diagram of the griven sites and then locating the query point inone of its regions. Two algorithms are given, one that constructs the Voronoi diagram in O(n log n) time, and another that inserts a new sit on O(n) time. Both are based on the use of the Voronoi dual, or Delaunay triangulation, and are simple enough to be of practical value. the simplicity of both algorithms can be attributed to the separation of the geometrical and topological aspects of the problem and to the use of two simple but powerful primitives,  a geometric predicate and an operator for manipulating the topology of the diagram. The topology is represented by a new data structure for generalized diagrams, that is, embeddings of graphs in two-dimensional\\xa0…', '저널': 'ACM transactions on graphics (TOG)', '저자': 'Leonidas Guibas, Jorge Stolfi', '전체 인용횟수': '2094회 인용19851986198719881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202381122342235625441454159635047676167726982928079828157734266556646433530443725', '페이지': '74-123', '학술 문서': 'Primitives for the manipulation of general subdivisions and the computation of VoronoiL Guibas, J Stolfi\\xa0- ACM transactions on graphics (TOG), 19852094회 인용 관련 학술자료 전체 18개의 버전 ', '호': '2'}, title='Primitives for the manipulation of general subdivisions and the computation of Voronoi', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Wireless sensor networks: an information processing approach': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2004/7/6', '게시자': 'Morgan Kaufmann', '설명': 'Designing, implementing, and operating a wireless sensor network involves a wide range of disciplines and many application-specific constraints. To make sense of and take advantage of these systems, a holistic approach is neededand this is precisely what Wireless Sensor Networks delivers. Inside, two eminent researchers review the diverse technologies and techniques that interact in todays wireless sensor networks. At every step, they are guided by the high-level information-processing tasks that determine how these networks are architected and administered. Zhao and Guibas begin with the canonical problem of localizing and tracking moving objects, then systematically examine the many fundamental sensor network issues that spring from it, including network discovery, service establishment, data routing and aggregation, query processing, programming models, and system organization. The understanding gained as a resulthow different layers support the needs of different applications, and how a wireless sensor network should be built to optimize performance and economyis sure to endure as individual component technologies come and go. Features: Written for practitioners, researchers, and students and relevant to all application areas, including environmental monitoring, industrial sensing and diagnostics, automotive and transportation, security and surveillance, military and battlefield uses, and large-scale infrastructural maintenance. Skillfully integrates the many disciplines at work in wireless sensor network design: signal processing and estimation, communication theory and protocols, distributed algorithms and databases\\xa0…', '저자': 'Feng Zhao, Leonidas J Guibas', '전체 인용횟수': '2063회 인용20042005200620072008200920102011201220132014201520162017201820192020202120222023873168213169181155170147122126969269665342312015', '학술 문서': 'Wireless sensor networks: an information processing approachF Zhao, LJ Guibas - 20042063회 인용 관련 학술자료 전체 7개의 버전 '}, title='Wireless sensor networks: an information processing approach', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Kpconv: Flexible and deformable convolution for point clouds': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': 'We present Kernel Point Convolution (KPConv), a new design of point convolution, ie that operates on point clouds without any intermediate representation. The convolution weights of KPConv are located in Euclidean space by kernel points, and applied to the input points close to them. Its capacity to use any number of kernel points gives KPConv more flexibility than fixed grid convolutions. Furthermore, these locations are continuous in space and can be learned by the network. Therefore, KPConv can be extended to deformable convolutions that learn to adapt kernel points to local geometry. Thanks to a regular subsampling strategy, KPConv is also efficient and robust to varying densities. Whether they use deformable KPConv for complex tasks, or rigid KPconv for simpler tasks, our networks outperform state-of-the-art classification and segmentation approaches on several datasets. We also offer ablation studies and visualizations to provide understanding of what has been learned by KPConv and to validate the descriptive power of deformable KPConv.', '저자': 'Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, François Goulette, Leonidas J Guibas', '전체 인용횟수': '2043회 인용2019202020212022202317178439687715', '컨퍼런스': 'Proceedings of the IEEE/CVF international conference on computer vision', '페이지': '6411-6420', '학술 문서': 'Kpconv: Flexible and deformable convolution for point cloudsH Thomas, CR Qi, JE Deschaud, B Marcotegui…\\xa0- Proceedings of the IEEE/CVF international conference\\xa0…, 20192043회 인용 관련 학술자료 전체 14개의 버전 '}, title='Kpconv: Flexible and deformable convolution for point clouds', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A point set generation network for 3d object reconstruction from a single image': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'Generation of 3D data by deep neural network has been attracting increasing attention in the research community. The majority of extant works resort to regular representations such as volumetric grids or collection of images; however, these representations obscure the natural invariance of 3D shapes under geometric transformations, and also suffer from a number of other issues. In this paper we address the problem of 3D reconstruction from a single image, generating a straight-forward form of output--point cloud coordinates. Along with this problem arises a unique and interesting issue, that the groundtruth shape for an input image may be ambiguous. Driven by this unorthordox output form and the inherent ambiguity in groundtruth, we design architecture, loss function and learning paradigm that are novel and effective. Our final solution is a conditional shape sampler, capable of predicting multiple plausible 3D point clouds from an input image. In experiments not only can our system outperform state-of-the-art methods on single image based 3D reconstruction benchmarks; but it also shows strong performance for 3D shape completion and promising ability in making multiple plausible predictions.', '저자': 'Haoqiang Fan, Hao Su, Leonidas J Guibas', '전체 인용횟수': '2036회 인용201720182019202020212022202326113248324413481415', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '605-613', '학술 문서': 'A point set generation network for 3d object reconstruction from a single imageH Fan, H Su, LJ Guibas\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20172036회 인용 관련 학술자료 전체 18개의 버전 '}, title='A point set generation network for 3d object reconstruction from a single image', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A concise and provably informative multi‐scale signature based on heat diffusion': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009/7', '게시자': 'Blackwell Publishing Ltd', '권': '28', '설명': ' We propose a novel point signature based on the properties of the heat diffusion process on a shape. Our signature, called the Heat Kernel Signature (or HKS), is obtained by restricting the well‐known heat kernel to the temporal domain. Remarkably we show that under certain mild assumptions, HKS captures all of the information contained in the heat kernel, and characterizes the shape up to isometry. This means that the restriction to the temporal domain, on the one hand, makes HKS much more concise and easily commensurable, while on the other hand, it preserves all of the information about the intrinsic geometry of the shape. In addition, HKS inherits many useful properties from the heat kernel, which means, in particular, that it is stable under perturbations of the shape. Our signature also provides a natural and efficiently computable multi‐scale way to capture information about neighborhoods of a given\\xa0…', '저널': 'Computer graphics forum', '저자': 'Jian Sun, Maks Ovsjanikov, Leonidas Guibas', '전체 인용횟수': '1822회 인용200920102011201220132014201520162017201820192020202120222023124971112134139129179163144156157126110115', '페이지': '1383-1392', '학술 문서': 'A concise and provably informative multi‐scale signature based on heat diffusionJ Sun, M Ovsjanikov, L Guibas\\xa0- Computer graphics forum, 20091822회 인용 관련 학술자료 전체 19개의 버전 ', '호': '5'}, title='A concise and provably informative multi‐scale signature based on heat diffusion', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Handbook of discrete and computational geometry': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/11/22', '게시자': 'CRC press', '설명': 'The Handbook of Discrete and Computational Geometry is intended as a reference book fully accessible to nonspecialists as well as specialists, covering all major aspects of both fields. The book offers the most important results and methods in discrete and computational geometry to those who use them in their work, both in the academic world—as researchers in mathematics and computer science—and in the professional world—as practitioners in fields as diverse as operations research, molecular biology, and robotics. Discrete geometry has contributed significantly to the growth of discrete mathematics in recent years. This has been fueled partly by the advent of powerful computers and by the recent explosion of activity in the relatively young field of computational geometry. This synthesis between discrete and computational geometry lies at the heart of this Handbook. A growing list of application fields includes combinatorial optimization, computer-aided design, computer graphics, crystallography, data analysis, error-correcting codes, geographic information systems, motion planning, operations research, pattern recognition, robotics, solid modeling, and tomography.', '저자': \"Csaba D Toth, Joseph O'Rourke, Jacob E Goodman\", '전체 인용횟수': '1781회 인용19971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202352534403648456678897179859577506474607262738276999678', '학술 문서': \"Handbook of discrete and computational geometryCD Toth, J O'Rourke, JE Goodman - 20171780회 인용 관련 학술자료 전체 8개의 버전 Handbook of Discrete and Computational Geometry*G Fejes Tóth, JE Goodman, J O'Rourke - 19976회 인용 관련 학술자료 \"}, title='Handbook of discrete and computational geometry', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Volumetric and multi-view cnns for object classification on 3d data': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': '3D shape models are becoming widely available and easier to capture, making available 3D information crucial for progress in object classification. Current state-of-the-art methods rely on CNNs to address this problem. Recently, we witness two types of CNNs being developed: CNNs based upon volumetric representations versus CNNs based upon multi-view representations. Empirical results from these two types of CNNs exhibit a large gap, indicating that existing volumetric CNN architectures and approaches are unable to fully exploit the power of 3D representations. In this paper, we aim to improve both volumetric CNNs and multi-view CNNs according to extensive analysis of existing approaches. To this end, we introduce two distinct network architectures of volumetric CNNs. In addition, we examine multi-view CNNs, where we introduce multi-resolution filtering in 3D. Overall, we are able to outperform current state-of-the-art methods for both volumetric CNNs and multi-view CNNs. We provide extensive experiments designed to evaluate underlying design choices, thus providing a better understanding of the space of methods available for object classification on 3D data.', '저자': 'Charles R Qi, Hao Su, Matthias Nießner, Angela Dai, Mengyuan Yan, Leonidas J Guibas', '전체 인용횟수': '1770회 인용201620172018201920202021202220231392218303348331265176', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '5648-5656', '학술 문서': 'Volumetric and multi-view cnns for object classification on 3d dataCR Qi, H Su, M Nießner, A Dai, M Yan, LJ Guibas\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20161770회 인용 관련 학술자료 전체 22개의 버전 '}, title='Volumetric and multi-view cnns for object classification on 3d data', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep knowledge tracing': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '권': '28', '설명': 'Knowledge tracing, where a machine models the knowledge of a student as they interact with coursework, is an established and significantly unsolved problem in computer supported education. In this paper we explore the benefit of using recurrent neural networks to model student learning. This family of models have important advantages over current state of the art methods in that they do not require the explicit encoding of human domain knowledge, and have a far more flexible functional form which can capture substantially more complex student interactions. We show that these neural networks outperform the current state of the art in prediction on real student data, while allowing straightforward interpretation and discovery of structure in the curriculum. These results suggest a promising new line of research for knowledge tracing.', '저널': 'Advances in neural information processing systems', '저자': 'Chris Piech, Jonathan Bassen, Jonathan Huang, Surya Ganguli, Mehran Sahami, Leonidas J Guibas, Jascha Sohl-Dickstein', '전체 인용횟수': '1225회 인용20162017201820192020202120222023254710399171228272266', '학술 문서': 'Deep knowledge tracingC Piech, J Bassen, J Huang, S Ganguli, M Sahami…\\xa0- Advances in neural information processing systems, 20151225회 인용 관련 학술자료 전체 17개의 버전 '}, title='Deep knowledge tracing', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning representations and generative models for 3d point clouds': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/7/3', '게시자': 'PMLR', '설명': 'Three-dimensional geometric data offer an excellent domain for studying representation learning and generative modeling. In this paper, we look at geometric data represented as point clouds. We introduce a deep AutoEncoder (AE) network with state-of-the-art reconstruction quality and generalization ability. The learned representations outperform existing methods on 3D recognition tasks and enable shape editing via simple algebraic manipulations, such as semantic part editing, shape analogies and shape interpolation, as well as shape completion. We perform a thorough study of different generative models including GANs operating on the raw point clouds, significantly improved GANs trained in the fixed latent space of our AEs, and Gaussian Mixture Models (GMMs). To quantitatively evaluate generative models we introduce measures of sample fidelity and diversity based on matchings between sets of point clouds. Interestingly, our evaluation of generalization, fidelity and diversity reveals that GMMs trained in the latent space of our AEs yield the best results overall.', '저자': 'Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, Leonidas Guibas', '전체 인용횟수': '1196회 인용20182019202020212022202318113202255293312', '컨퍼런스': 'International conference on machine learning', '페이지': '40-49', '학술 문서': 'Learning representations and generative models for 3d point cloudsP Achlioptas, O Diamanti, I Mitliagkas, L Guibas\\xa0- International conference on machine learning, 20181196회 인용 관련 학술자료 전체 6개의 버전 '}, title='Learning representations and generative models for 3d point clouds', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Taskonomy: Disentangling task transfer learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'Do visual tasks have a relationship, or are they unrelated? For instance, could having surface normals simplify estimating the depth of an image? Intuition answers these questions positively, implying existence of a structure among visual tasks. Knowing this structure has notable uses; it is the concept underlying transfer learning and, for example, can provide a principled way for reusing supervision among related tasks, finding what tasks transfer well to an arbitrary target task, or solving many tasks in one system without piling up the complexity. This paper proposes a fully computational approach for finding the structure of the space of visual tasks. This is done via a sampled dictionary of twenty six 2D, 2.5 D, 3D, and semantic tasks, and modeling their (1st and higher order) transfer dependencies in a latent space. The product can be viewed as a computational taxonomic map for task transfer learning. We study the consequences of this structure, eg the nontrivial emerged relationships, and exploit them to reduce the demand for labeled data. For example, we show that the total number of labeled datapoints needed for solving a set of 10 tasks can be reduced by roughly 2/3 while keeping the performance nearly the same. Users can employ a provided Binary Integer Programming solver that leverages the taxonomy to find efficient supervision policies for their own use cases.', '저자': 'Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, Silvio Savarese', '전체 인용횟수': '1190회 인용20182019202020212022202327177221254257251', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '3712-3722', '학술 문서': 'Taskonomy: Disentangling task transfer learningAR Zamir, A Sax, W Shen, LJ Guibas, J Malik…\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20181190회 인용 관련 학술자료 전체 16개의 버전 '}, title='Taskonomy: Disentangling task transfer learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A dichromatic framework for balanced trees': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1978/10/16', '게시자': 'IEEE', '설명': 'In this paper we present a uniform framework for the implementation and study of balanced tree algorithms. We show how to imbed in this framework the best known balanced tree techniques and then use the framework to develop new algorithms which perform the update and rebalancing in one pass, on the way down towards a leaf. We conclude with a study of performance issues and concurrent updating.', '저자': 'Leo J Guibas, Robert Sedgewick', '전체 인용횟수': '1052회 인용1984198519861987198819891990199119921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023714105132021151924192924291921212811182126202729353238313331294027373039393834', '컨퍼런스': '19th Annual Symposium on Foundations of Computer Science (sfcs 1978)', '페이지': '8-21', '학술 문서': 'A dichromatic framework for balanced treesLJ Guibas, R Sedgewick\\xa0- 19th Annual Symposium on Foundations of Computer\\xa0…, 19781052회 인용 관련 학술자료 전체 9개의 버전 '}, title='A dichromatic framework for balanced trees', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep hough voting for 3d object detection in point clouds': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': \"Current 3D object detection methods are heavily influenced by 2D detectors. In order to leverage architectures in 2D detectors, they often convert 3D point clouds to regular grids (ie, to voxel grids or to bird's eye view images), or rely on detection in 2D images to propose 3D boxes. Few works have attempted to directly detect objects in point clouds. In this work, we return to first principles to construct a 3D detection pipeline for point cloud data and as generic as possible. However, due to the sparse nature of the data--samples from 2D manifolds in 3D space--we face a major challenge when directly predicting bounding box parameters from scene points: a 3D object centroid can be far from any surface point thus hard to regress accurately in one step. To address the challenge, we propose VoteNet, an end-to-end 3D object detection network based on a synergy of deep point set networks and Hough voting. Our model achieves state-of-the-art 3D detection on two large datasets of real 3D scans, ScanNet and SUN RGB-D with a simple design, compact model size and high efficiency. Remarkably, VoteNet outperforms previous methods by using purely geometric information without relying on color images.\", '저자': 'Charles R Qi, Or Litany, Kaiming He, Leonidas J Guibas', '전체 인용횟수': '1040회 인용2019202020212022202311113257344311', '컨퍼런스': 'proceedings of the IEEE/CVF International Conference on Computer Vision', '페이지': '9277-9286', '학술 문서': 'Deep hough voting for 3d object detection in point cloudsCR Qi, O Litany, K He, LJ Guibas\\xa0- proceedings of the IEEE/CVF International Conference\\xa0…, 20191040회 인용 관련 학술자료 전체 12개의 버전 '}, title='Deep hough voting for 3d object detection in point clouds', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A scalable active framework for region annotation in 3d shape collections': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/11/11', '게시자': 'ACM', '권': '35', '설명': 'Large repositories of 3D shapes provide valuable input for data-driven analysis and modeling tools. They are especially powerful once annotated with semantic information such as salient regions and functional parts. We propose a novel active learning method capable of enriching massive geometric datasets with accurate semantic region annotations. Given a shape collection and a user-specified region label our goal is to correctly demarcate the corresponding regions with minimal manual work. Our active framework achieves this goal by cycling between manually annotating the regions, automatically propagating these annotations across the rest of the shapes, manually verifying both human and automatic annotations, and learning from the verification results to improve the automatic propagation algorithm. We use a unified utility function that explicitly models the time cost of human input across all steps of our\\xa0…', '저널': 'ACM Transactions on Graphics (ToG)', '저자': 'Li Yi, Vladimir G Kim, Duygu Ceylan, I-Chao Shen, Mengyan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Sheffer, Leonidas Guibas', '전체 인용횟수': '985회 인용20172018201920202021202220232757100138181264210', '페이지': '1-12', '학술 문서': 'A scalable active framework for region annotation in 3d shape collectionsL Yi, VG Kim, D Ceylan, IC Shen, M Yan, H Su, C Lu…\\xa0- ACM Transactions on Graphics (ToG), 2016985회 인용 관련 학술자료 전체 11개의 버전 ', '호': '6'}, title='A scalable active framework for region annotation in 3d shape collections', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Locating and bypassing holes in sensor networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2006/4', '게시자': 'Kluwer Academic Publishers', '권': '11', '설명': ' In real sensor network deployments, spatial distributions of sensors are usually far from being uniform. Such networks often contain regions without enough sensor nodes, which we call holes. In this paper, we show that holes are important topological features that need to be studied. In routing, holes are communication voids that cause greedy forwarding to fail. Holes can also be defined to denote regions of interest, such as the “hot spots” created by traffic congestion or sensor power shortage. In this paper, we define holes to be the regions enclosed by a polygonal cycle which contains all the nodes where local minima can appear. We also propose simple and distributed algorithms, the Tent rule and BoundHole, to identify and build routes around holes. We show that the boundaries of holes marked using BoundHole can be used in many applications such as geographic routing, path migration, information\\xa0…', '저널': 'Mobile networks and Applications', '저자': 'Qing Fang, Jie Gao, Leonidas J Guibas', '전체 인용횟수': '928회 인용20052006200720082009201020112012201320142015201620172018201920202021202220233153737395877260655659474333201713163', '페이지': '187-200', '학술 문서': 'Locating and bypassing holes in sensor networksQ Fang, J Gao, LJ Guibas\\xa0- Mobile networks and Applications, 2006928회 인용 관련 학술자료 전체 34개의 버전 '}, title='Locating and bypassing holes in sensor networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Yolov3: An incremental improvement': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/4', '게시자': 'CoRR', '저널': 'arXiv preprint arXiv:1804.02767', '저자': 'Redmon Joseph, Farhadi Ali', '전체 인용횟수': '387회 인용201820192020202120222023839649510477', '페이지': '1-6', '학술 문서': 'Yolov3: An incremental improvementR Joseph, F Ali\\xa0- arXiv preprint arXiv:1804.02767, 2018387회 인용 관련 학술자료 '}, title='Yolov3: An incremental improvement', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'YOLO9000: Better, Faster, Stronger': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': \"We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time.\", '저널': 'CVPR', '저자': 'Joseph Redmon, Ali Farhadi', '전체 인용횟수': '19058회 인용2017201820192020202120222023349139123412986384944493534', '학술 문서': 'YOLO9000: better, faster, strongerJ Redmon, A Farhadi\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 201718997회 인용 관련 학술자료 전체 23개의 버전 ieee 2017 ieee conference on computer Vision and Pattern Recognition (cvpr)-honolulu, hi (2017.7. 21-2017.7. 26)*J Redmon, A Farhadi\\xa0- 2017 ieee conference on computer vision and pattern\\xa0…, 2017165회 인용 관련 학술자료 '}, title='YOLO9000: Better, Faster, Stronger', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/3/16', '설명': ' We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values resulting in 32 memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58 faster convolutional operations (in terms of number of the high precision operations) and 32 memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is the same as the full-precision AlexNet. We\\xa0…', '저널': 'ECCV', '저자': 'Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, Ali Farhadi', '전체 인용횟수': '5106회 인용2016201720182019202020212022202355283650805957906819588', '학술 문서': 'Xnor-net: Imagenet classification using binary convolutional neural networksM Rastegari, V Ordonez, J Redmon, A Farhadi\\xa0- European conference on computer vision, 20165106회 인용 관련 학술자료 전체 9개의 버전 '}, title='XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Unsupervised Deep Embedding for Clustering Analysis': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': 'Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.', '저널': 'ICML', '저자': 'Junyuan Xie, Ross Girshick, Ali Farhadi', '전체 인용횟수': '2864회 인용20162017201820192020202120222023961150289476586654616', '학술 문서': 'Unsupervised deep embedding for clustering analysisJ Xie, R Girshick, A Farhadi\\xa0- International conference on machine learning, 20162864회 인용 관련 학술자료 전체 15개의 버전 '}, title='Unsupervised Deep Embedding for Clustering Analysis', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Describing objects by their attributes': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009/6/20', '게시자': 'IEEE', '설명': 'We propose to shift the goal of recognition from naming to describing. Doing so allows us not only to name familiar objects, but also: to report unusual aspects of a familiar object (“spotty dog”, not just “dog”); to say something about unfamiliar objects (“hairy and four-legged”, not just “unknown”); and to learn how to recognize new objects with few or no visual examples. Rather than focusing on identity assignment, we make inferring attributes the core problem of recognition. These attributes can be semantic (“spotty”) or discriminative (“dogs have it but sheep do not”). Learning attributes presents a major new challenge: generalization across object categories, not just across instances within a category. In this paper, we also introduce a novel feature selection method for learning attributes that generalize well across categories. We support our claims by thorough evaluation that provides insights into the limitations of\\xa0…', '저자': 'Ali Farhadi, Ian Endres, Derek Hoiem, David Forsyth', '전체 인용횟수': '2374회 인용20092010201120122013201420152016201720182019202020212022202310467097146203218230210215195200203161119', '컨퍼런스': '2009 IEEE conference on computer vision and pattern recognition', '페이지': '1778-1785', '학술 문서': 'Describing objects by their attributesA Farhadi, I Endres, D Hoiem, D Forsyth\\xa0- 2009 IEEE conference on computer vision and pattern\\xa0…, 20092374회 인용 관련 학술자료 전체 25개의 버전 '}, title='Describing objects by their attributes', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Bidirectional Attention Flow for Machine Comprehension': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.', '저널': 'ICLR', '저자': 'Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi', '전체 인용횟수': '2193회 인용201620172018201920202021202220237131275472398363301227', '학술 문서': 'Bidirectional attention flow for machine comprehensionM Seo, A Kembhavi, A Farhadi, H Hajishirzi\\xa0- arXiv preprint arXiv:1611.01603, 20162193회 인용 관련 학술자료 전체 7개의 버전 '}, title='Bidirectional Attention Flow for Machine Comprehension', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Every picture tells a story: Generating sentences from images': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010', '게시자': 'Springer Berlin Heidelberg', '설명': ' Humans can prepare concise descriptions of pictures, focusing on what they find important. We demonstrate that automatic methods can do so too. We describe a system that can compute a score linking an image to a sentence. This score can be used to attach a descriptive sentence to a given image, or to obtain images that illustrate a given sentence. The score is obtained by comparing an estimate of meaning obtained from the image to one obtained from the sentence. Each estimate of meaning comes from a discriminative procedure that is learned using data. We evaluate on a novel dataset consisting of human-annotated images. While our underlying estimate of meaning is impoverished, it is sufficient to produce very good quantitative results, evaluated with a novel score that can account for synecdoche.', '저자': 'Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, David Forsyth', '전체 인용횟수': '1474회 인용20112012201320142015201620172018201920202021202220232123467698129159136170143155173112', '컨퍼런스': 'Computer Vision–ECCV 2010: 11th European Conference on Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part IV 11', '페이지': '15-29', '학술 문서': 'Every picture tells a story: Generating sentences from imagesA Farhadi, M Hejrati, MA Sadeghi, P Young…\\xa0- Computer Vision–ECCV 2010: 11th European\\xa0…, 20101474회 인용 관련 학술자료 전체 29개의 버전 '}, title='Every picture tells a story: Generating sentences from images', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/4/6', '설명': ' Computer vision has a great potential to help our daily lives by searching for lost keys, watering flowers or reminding us to take a pill. To succeed with such tasks, computer vision methods need to be trained from real and diverse examples of our daily dynamic scenes. While most of such scenes are not particularly exciting, they typically do not appear on YouTube, in movies or TV broadcasts. So how do we collect sufficiently many diverse but boring samples representing our lives? We propose a novel Hollywood in Homes approach to collect such data. Instead of shooting videos in the lab, we ensure diversity by distributing and crowdsourcing the whole process of video creation from script writing to video recording and annotation. Following this procedure we collect a new dataset, Charades, with hundreds of people recording videos in their own homes, acting out casual everyday activities. The dataset is\\xa0…', '저널': 'ECCV', '저자': 'Gunnar A Sigurdsson, Gül Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, Abhinav Gupta', '전체 인용횟수': '1163회 인용2016201720182019202020212022202333793124172248251221', '학술 문서': 'Hollywood in homes: Crowdsourcing data collection for activity understandingGA Sigurdsson, G Varol, X Wang, A Farhadi, I Laptev…\\xa0- Computer Vision–ECCV 2016: 14th European\\xa0…, 20161163회 인용 관련 학술자료 전체 12개의 버전 '}, title='Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Defending against neural fake news': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '권': '32', '설명': 'Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news.', '저널': 'Advances in neural information processing systems', '저자': 'Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi', '전체 인용횟수': '824회 인용2019202020212022202328152196192251', '학술 문서': 'Defending against neural fake newsR Zellers, A Holtzman, H Rashkin, Y Bisk, A Farhadi…\\xa0- Advances in neural information processing systems, 2019824회 인용 관련 학술자료 전체 9개의 버전 '}, title='Defending against neural fake news', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'From recognition to cognition: Visual commonsense reasoning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': \"Visual understanding goes well beyond object recognition. With one glance at an image, we can effortlessly imagine the world beyond the pixels: for instance, we can infer people's actions, goals, and mental states. While this task is easy for humans, it is tremendously difficult for today's vision systems, requiring higher-order cognition and commonsense reasoning about the world. We formalize this task as Visual Commonsense Reasoning. Given a challenging question about an image, a machine must answer correctly and then provide a rationale justifying its answer. Next, we introduce a new dataset, VCR, consisting of 290k multiple choice QA problems derived from 110k movie scenes. The key recipe for generating non-trivial and high-quality problems at scale is Adversarial Matching, a new approach to transform rich annotations into multiple choice questions with minimal bias. Experimental results show that while humans find VCR easy (over 90% accuracy), state-of-the-art vision models struggle (45%). To move towards cognition-level understanding, we present a new reasoning engine, Recognition to Cognition Networks (R2C), that models the necessary layered inferences for grounding, contextualization, and reasoning. R2C helps narrow the gap between humans and machines (65%); still, the challenge is far from solved, and we provide analysis that suggests avenues for future work.\", '저자': 'Rowan Zellers, Yonatan Bisk, Ali Farhadi, Yejin Choi', '전체 인용횟수': '729회 인용201820192020202120222023238106155218208', '컨퍼런스': 'Proceedings of the IEEE/CVF conference on computer vision and pattern recognition', '페이지': '6720-6731', '학술 문서': 'From recognition to cognition: Visual commonsense reasoningR Zellers, Y Bisk, A Farhadi, Y Choi\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 2019729회 인용 관련 학술자료 전체 7개의 버전 '}, title='From recognition to cognition: Visual commonsense reasoning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Ai2-thor: An interactive 3d environment for visual ai': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/12/14', '설명': 'We introduce The House Of inteRactions (THOR), a framework for visual AI research, available at http://ai2thor.allenai.org. AI2-THOR consists of near photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes and interact with objects to perform tasks. AI2-THOR enables research in many different domains including but not limited to deep reinforcement learning, imitation learning, learning by interaction, planning, visual question answering, unsupervised representation learning, object detection and segmentation, and learning models of cognition. The goal of AI2-THOR is to facilitate building visually intelligent models and push the research forward in this domain.', '저널': 'arXiv preprint arXiv:1712.05474', '저자': 'Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, Aniruddha Kembhavi, Abhinav Gupta, Ali Farhadi', '전체 인용횟수': '670회 인용20172018201920202021202220232335985133170184', '학술 문서': 'Ai2-thor: An interactive 3d environment for visual aiE Kolve, R Mottaghi, W Han, E VanderBilt, L Weihs…\\xa0- arXiv preprint arXiv:1712.05474, 2017670회 인용 관련 학술자료 전체 2개의 버전 '}, title='Ai2-thor: An interactive 3d environment for visual ai', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Recognition using visual phrases': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011/6/20', '게시자': 'IEEE', '설명': 'In this paper we introduce visual phrases, complex visual composites like “a person riding a horse”. Visual phrases often display significantly reduced visual complexity compared to their component objects, because the appearance of those objects can change profoundly when they participate in relations. We introduce a dataset suitable for phrasal recognition that uses familiar PASCAL object categories, and demonstrate significant experimental gains resulting from exploiting visual phrases. We show that a visual phrase detector significantly outperforms a baseline which detects component objects and reasons about relations, even though visual phrase training sets tend to be smaller than those for objects. We argue that any multi-class detection system must decode detector outputs to produce final results; this is usually done with non-maximum suppression. We describe a novel decoding procedure that can\\xa0…', '저자': 'Mohammad Amin Sadeghi, Ali Farhadi', '전체 인용횟수': '538회 인용20112012201320142015201620172018201920202021202220236433847494757525449382911', '컨퍼런스': 'CVPR 2011', '페이지': '1745-1752', '학술 문서': 'Recognition using visual phrasesMA Sadeghi, A Farhadi\\xa0- CVPR 2011, 2011538회 인용 관련 학술자료 전체 13개의 버전 '}, title='Recognition using visual phrases', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Yolov3: an incremental improvement. 2018': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1804', '권': '20', '저널': 'arXiv preprint arXiv:1804.02767', '저자': 'Joseph Redmon, Ali Farhadi', '전체 인용횟수': '534회 인용20182019202020212022202352589117176119', '학술 문서': 'Yolov3: an incremental improvement. 2018J Redmon, A Farhadi\\xa0- arXiv preprint arXiv:1804.02767, 1804534회 인용 관련 학술자료 '}, title='Yolov3: an incremental improvement. 2018', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Hellaswag: Can a machine really finish your sentence?': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019/5/19', '설명': 'Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as \"A woman sits at a piano,\" a machine must select the most likely followup: \"She sets her fingers on the keys.\" With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans (>95% accuracy), state-of-the-art models struggle (<48%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical \\'Goldilocks\\' zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.', '저널': 'arXiv preprint arXiv:1905.07830', '저자': 'Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi', '전체 인용횟수': '522회 인용20192020202120222023207182133214', '학술 문서': 'Hellaswag: Can a machine really finish your sentence?R Zellers, A Holtzman, Y Bisk, A Farhadi, Y Choi\\xa0- arXiv preprint arXiv:1905.07830, 2019522회 인용 관련 학술자료 전체 5개의 버전 '}, title='Hellaswag: Can a machine really finish your sentence?', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': ' As 3D movie viewing becomes mainstream and the Virtual Reality (VR) market emerges, the demand for 3D contents is growing rapidly. Producing 3D videos, however, remains challenging. In this paper we propose to use deep neural networks to automatically convert 2D videos and images to a stereoscopic 3D format. In contrast to previous automatic 2D-to-3D conversion algorithms, which have separate stages and need ground truth depth map as supervision, our approach is trained end-to-end directly on stereo pairs extracted from existing 3D movies. This novel training scheme makes it possible to exploit orders of magnitude more data and significantly increases performance. Indeed, Deep3D outperforms baselines in both quantitative and human subject evaluations.', '저자': 'Junyuan Xie, Ross Girshick, Ali Farhadi', '전체 인용횟수': '475회 인용20162017201820192020202120222023820599986876642', '컨퍼런스': 'ECCV', '학술 문서': 'Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networksJ Xie, R Girshick, A Farhadi\\xa0- Computer Vision–ECCV 2016: 14th European\\xa0…, 2016475회 인용 관련 학술자료 전체 6개의 버전 '}, title='Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Understanding egocentric activities': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011/11/6', '게시자': 'IEEE', '설명': 'We present a method to analyze daily activities, such as meal preparation, using video from an egocentric camera. Our method performs inference about activities, actions, hands, and objects. Daily activities are a challenging domain for activity recognition which are well-suited to an egocentric approach. In contrast to previous activity recognition methods, our approach does not require pre-trained detectors for objects and hands. Instead we demonstrate the ability to learn a hierarchical model of an activity by exploiting the consistent appearance of objects, hands, and actions that results from the egocentric context. We show that joint modeling of activities, actions, and objects leads to superior performance in comparison to the case where they are considered independently. We introduce a novel representation of actions based on object-hand interactions and experimentally demonstrate the superior performance\\xa0…', '저자': 'Alireza Fathi, Ali Farhadi, James M Rehg', '전체 인용횟수': '464회 인용2010201120122013201420152016201720182019202020212022202331141529566353503739482422', '컨퍼런스': '2011 international conference on computer vision', '페이지': '407-414', '학술 문서': 'Understanding egocentric activitiesA Fathi, A Farhadi, JM Rehg\\xa0- 2011 international conference on computer vision, 2011464회 인용 관련 학술자료 전체 18개의 버전 '}, title='Understanding egocentric activities', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020/2/15', '설명': 'Fine-tuning pretrained contextual word embedding models to supervised downstream tasks has become commonplace in natural language processing. This process, however, is often brittle: even with the same hyperparameter values, distinct random seeds can lead to substantially different results. To better understand this phenomenon, we experiment with four datasets from the GLUE benchmark, fine-tuning BERT hundreds of times on each while varying only the random seeds. We find substantial performance increases compared to previously reported results, and we quantify how the performance of the best-found model varies as a function of the number of fine-tuning trials. Further, we examine two factors influenced by the choice of random seed: weight initialization and training data order. We find that both contribute comparably to the variance of out-of-sample performance, and that some weight initializations perform well across all tasks explored. On small datasets, we observe that many fine-tuning trials diverge part of the way through training, and we offer best practices for practitioners to stop training less promising runs early. We publicly release all of our experimental data, including training and validation scores for 2,100 trials, to encourage further analysis of training dynamics during fine-tuning.', '저널': 'arXiv preprint arXiv:2002.06305', '저자': 'Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, Noah Smith', '전체 인용횟수': '442회 인용202020212022202360103136140', '학술 문서': 'Fine-tuning pretrained language models: Weight initializations, data orders, and early stoppingJ Dodge, G Ilharco, R Schwartz, A Farhadi, H Hajishirzi…\\xa0- arXiv preprint arXiv:2002.06305, 2020442회 인용 관련 학술자료 전체 2개의 버전 '}, title='Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Image-to-image translation with conditional adversarial networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without handengineering our loss functions either.', '저자': 'Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A Efros', '전체 인용횟수': '20617회 인용2017201820192020202120222023372142825873462426843413985', '컨퍼런스': 'Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)', '학술 문서': 'Image-to-image translation with conditional adversarial networksP Isola, JY Zhu, T Zhou, AA Efros\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 201720475회 인용 관련 학술자료 전체 24개의 버전 Proceedings of the IEEE conference on computer vision and pattern recognition*P Isola, JY Zhu, T Zhou, AA Efros\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2017186회 인용 관련 학술자료 Efros Alexei A*I Phillip, Z Jun-Yan, Z Tinghui\\xa0- Image-to-image translation with conditional adversarial\\xa0…, 201770회 인용 관련 학술자료 Image-to-image translation with conditional adversarial networks. arXiv preprint (2017)*P Isola, JY Zhu, T Zhou, AA Efros\\xa0- arXiv preprint arXiv:1611.07004, 201733회 인용 관련 학술자료 Image-to-image translation with conditional adversarial networks. arXiv e-prints*P Isola, JY Zhu, T Zhou, AA Efros\\xa0- arXiv preprint arXiv:1611.07004, 201629회 인용 관련 학술자료 Image-to-image translation with conditional adversarial networks arXiv preprint*P Isola, JY Zhu, T Zhou, AA Efros\\xa0- arXiv preprint arXiv:1611.07004, 201717회 인용 관련 학술자료 CVPR*P Isola, JY Zhu, T Zhou, AA Efros - 201713회 인용 관련 학술자료 BA Research,“Image-to-Image Translation with Conditional Adversarial Networks.”*P Isola, JY Zhu, T Zhou, AA Efros\\xa0- arXiv preprint ArXiv:1611.07004, 20167회 인용 관련 학술자료 Image-to-image translation with conditional adversarial networks. 2016. doi: 10.48550*P Isola, JY Zhu, T Zhou, AA Efros\\xa0- arXiv preprint ARXIV.1611.070047회 인용 관련 학술자료 Research BA. Image-to-Image Translation with Conditional Adversarial Networks*P Isola, JY Zhu, T Zhou, AA Efros\\xa0- Proceedings of the IEEE Conference on Computer\\xa0…, 20205회 인용 관련 학술자료 IEEE Conf. Computer Vision and Pattern Recognition (CVPR)*P Isola, JY Zhu, T Zhou, AA Efros - 20175회 인용 관련 학술자료 Proceedings of the IEEE conference on computer vision and pattern recognition, Honolulu, HI*P Isola, JY Zhu, T Zhou, AA Efros - 20175회 인용 관련 학술자료 Image-to-image translation with conditional adversarial networks. arXiv: 161107004*P Isola, JY Zhu, T Zhou, AA Efros - 20165회 인용 관련 학술자료 Image-to-image translation with conditional adversarial networks. CoRR, abs/1611.07004*P Isola, JY Zhu, T Zhou, AA Efros\\xa0- arXiv preprint arXiv:1611.07004, 20165회 인용 관련 학술자료 CoRR, abs/1611.07004*P Isola, J Zhu, T Zhou, AA Efros\\xa0- 2017 IEEE Conference on Computer Vision and\\xa0…, 20165회 인용 관련 학술자료 Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)*P Isola, J Zhu, T Zhou, AA Efros - 20174회 인용 관련 학술자료 Image-To-Image Translation With Conditional Adversarial Networks. 1125–1134*P Isola, JY Zhu, T Zhou, AA Efros - 20174회 인용 관련 학술자료 Image-to-image translation with conditional adversarial networks: Presented at the IEEE Conference on Computer Vision and Pattern Recognition*P Isola, JY Zhu, T Zhou, AA Efros - 20174회 인용 관련 학술자료 Proc. CVPR*P Isola, JY Zhu, T Zhou, AA Efros - 20174회 인용 관련 학술자료 Y, Zhou T, Efros AA. Image‐to‐image translation with conditional adversarial networks*P Isola, J Zhu\\xa0- ArXiv161107004 Cs, 20164회 인용 관련 학술자료 Efros Alexei A. 2016*I Phillip, Z Jun-Yan, Z Tinghui\\xa0- Image-to-image translation with conditional adversarial\\xa0…, 20164회 인용 관련 학술자료 Image-to-image translation with conditional adversarial networks. arXiv: 1611 07004v2 2017*P Isola, JY Zhu, T Zhou, AA Efros4회 인용 관련 학술자료 Image-to-image translation with conditional adversarial networks (2016). arXiv preprint*P Isola, JY Zhu, T Zhou, AA Efros\\xa0- arXiv preprint arXiv:1611.070044회 인용 관련 학술자료 Image-to-image translation with conditional adversarial networks. arXiv. org, November 2016*P Isola, JY Zhu, T Zhou, AA Efros\\xa0- arXiv preprint arXiv:1611.070044회 인용 관련 학술자료 Image-to-image translation with conditional adversarial networks. Computer Vision and Pattern Recognition (CVPR)*I Phillip, J Zhu, T Zhou, AE Alexei\\xa0- 2017 IEEE Conference, 20173회 인용 관련 학술자료 Alexei*P Isola, J Zhu, T Zhou\\xa0- A. Efros,“Image-to-Image Translation with Conditional\\xa0…, 20173회 인용 관련 학술자료 Y., Zhou T., Efros AA*P Isola, J Zhu\\xa0- Image‐to‐Image Translation with Conditional\\xa0…, 20163회 인용 관련 학술자료 Image-to-image translation with conditional 1019 adversarial networks*P Isola, JY Zhu, T Zhou, AA Efros\\xa0- Proceedings of the IEEE Conference on Computer\\xa0…3회 인용 관련 학술자료 Image-to-image Translation with Conditional Adversial NetworksP Isola, J Zhu, T Zhou, AA Efros - 20172회 인용 관련 학술자료 '}, title='Image-to-image translation with conditional adversarial networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Unpaired image-to-image translation using cycle-consistent adversarial networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/3/30', '설명': 'Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G: X-> Y such that the distribution of images from G (X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F: Y-> X and introduce a cycle consistency loss to push F (G (X))~ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.', '저널': 'Proceedings of the IEEE International Conference on Computer Vision', '저자': 'Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A Efros', '전체 인용횟수': '20345회 인용2017201820192020202120222023192115124193363436545404160', '학술 문서': 'Unpaired image-to-image translation using cycle-consistent adversarial networksJY Zhu, T Park, P Isola, AA Efros\\xa0- Proceedings of the IEEE international conference on\\xa0…, 201720132회 인용 관련 학술자료 전체 26개의 버전 Unpaired image-to-image translation using cycle-consistent adversarial networksZ Jun-Yan, P Taesung, I Phillip, AE Alexei\\xa0- Proceedings of the IEEE international conference on\\xa0…, 2017237회 인용 관련 학술자료 Proceedings of the IEEE international conference on computer vision*JY Zhu, T Park, P Isola, AA Efros - 2017181회 인용 관련 학술자료 Unpaired image-to-image translation using cycle-consistent adversarial networks. CoRR abs/1703.10593 (2017)*J Zhu, T Park, P Isola, AA Efros\\xa0- arXiv preprint arXiv:1703.10593, 201734회 인용 관련 학술자료 Research*JY Zhu, T Park, P Isola, AA Efros\\xa0- Digital Divide” and Social Integration of the Elderly in\\xa0…, 202120회 인용 관련 학술자료 Unpaired image-to-image translation using cycle-consistent adversarial networks in Proceedings of the IEEE international conference on computer vision. 2017*JY Zhu, T Park, P Isola, AA Efros\\xa0- Italy2223–2232, 201710회 인용 관련 학술자료 Cyclegan and pix2pix in pytorch*JY Zhu, T Park, P Isola, AA Efros - 20199회 인용 관련 학술자료 Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. arXiv e-prints, art*JY Zhu, T Park, P Isola, AA Efros\\xa0- arXiv preprint arXiv:1703.10593, 20179회 인용 관련 학술자료 PI, and AA Efros.“*JY Zhu, T Park\\xa0- Unpaired image-to-image translation using cycle\\xa0…, 20179회 인용 관련 학술자료 Unpaired image-to-image translation using cycle-consistent adversarial networks. CoRR abs/1703.10593*J Zhu, T Park, P Isola, AA Efros\\xa0- arXiv preprint arXiv:1703.10593, 20177회 인용 관련 학술자료 Computer Vision (ICCV), 2017 IEEE International Conference on*JY Zhu, T Park, P Isola, AA Efros - 20177회 인용 관련 학술자료 Proc. ICCV*JY Zhu, T Park, P Isola, AA Efros\\xa0- Proc. ICCV, 19957회 인용 관련 학술자료 Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. arXiv [cs. CV]. 2017*JY Zhu, T Park, P Isola, AA Efros6회 인용 관련 학술자료 Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. CoRR. 2017; abs/1703.10593*J Zhu, T Park, P Isola, AA Efros5회 인용 관련 학술자료 Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. ArXiv e-prints (March 2017)*JY Zhu, T Park, P Isola, AA Efros\\xa0- arXiv preprint arXiv:1703.10593, 20174회 인용 관련 학술자료 Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint arXiv: 170310593*JY Zhu, T Park, P Isola, AA Efros - 20174회 인용 관련 학술자료 CycleGAN*JY Zhu, T Park, P Isola, AA Efros\\xa0- URL: https://github. com/junyanz/CycleGAN# failure\\xa0…, 20174회 인용 관련 학술자료 Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv: 170310593 [cs].(2018)*JY Zhu, T Park, P Isola, AA Efros - 20193회 인용 관련 학술자료 CVPR*JY Zhu, T Park, P Isola, AA Efros - 20173회 인용 관련 학술자료 Unpaired image-to-image translation using cycle-consistent adversarial networks. CoRR*J Zhu, T Park, P Isola, AA Efros\\xa0- arXiv preprint arXiv:1703.10593, 20173회 인용 관련 학술자료 Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. arXiv: 170310593 [cs]. November 2018*JY Zhu, T Park, P Isola, AA Efros - 20192회 인용 관련 학술자료 CycleGAN failure cases*JY Zhu, T Park, P Isola, AA Efros - 20182회 인용 관련 학술자료 arXiv preprint arXiv: 1703.10593*JY Zhu, T Park, P Isola, AA Efros - 20172회 인용 관련 학술자료 Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint [Google Scholar]*JY Zhu, T Park, P Isola, AA Efros - 20172회 인용 관련 학술자료 Unpaired image-to-image translation using cycle-consistent adversarial networks. Paper presented the IEEE International Conference on Computer Vision, Venice, Italy*JY Zhu, T Park, P Isola, AA Efros - 20172회 인용 관련 학술자료 pytorch-cyclegan-and-pix2pix*JY Zhu, TW Taesung Park - 20172회 인용 관련 학술자료 Unpaired image-to-image translation using cycle-consistent adversarial networks. arXive: arXive: 1703.10593*JY Zhu, T Park, P Isola, AA Efros\\xa0- preprint, 20162회 인용 관련 학술자료 Unpaired image-to-image translation using cycle-consistent adversarial networks (2017). arXiv preprint*JY Zhu, T Park, P Isola, AA Efros\\xa0- arXiv preprint arXiv:1703.105932회 인용 관련 학술자료 '}, title='Unpaired image-to-image translation using cycle-consistent adversarial networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The unreasonable effectiveness of deep features as a perceptual metric': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called``perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.', '저자': 'Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang', '전체 인용횟수': '6879회 인용20182019202020212022202371297634127718972656', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '586-595', '학술 문서': 'The unreasonable effectiveness of deep features as a perceptual metricR Zhang, P Isola, AA Efros, E Shechtman, O Wang\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20186809회 인용 관련 학술자료 전체 16개의 버전 CVPR*R Zhang, P Isola, AA Efros, E Shechtman, O Wang - 201848회 인용 관련 학술자료 The unreasonable effectiveness of deep features as a perceptual metric. ArXiv 2018*R Zhang, P Isola, AA Efros, E Shechtman, O Wang\\xa0- arXiv preprint arXiv:1801.03924, 180114회 인용 관련 학술자료 The unreasonable effectiveness of deep features as a perceptual metric. arXiv*R Zhang, P Isola, AA Efros, E Shechtman, O Wang - 180111회 인용 관련 학술자료 Eli Shechtman and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric*R Zhang, P Isola, AA Efros\\xa0- CVPR, 20189회 인용 관련 학술자료 A. A. Efros and E. Shechtman and O. Wang, The Unreasonable Effectiveness of Deep Features as a Perceptual Metric, 2018 IEEE*R Zhang, P Isola\\xa0- CVF Conference on Computer Vision and Pattern\\xa0…, 20189회 인용 관련 학술자료 The unreasonable effectiveness of deep features as a perceptual metric. CoRR abs/1801.03924 (2018)*R Zhang, P Isola, AA Efros, E Shechtman, O Wang - 18017회 인용 관련 학술자료 The unrea-257 sonable effectiveness of deep features as a perceptual metric*R Zhang, P Isola, AA Efros, E Shechtman, O Wang\\xa0- Proceedings of the IEEE5회 인용 관련 학술자료 The unreasonable 752 effectiveness of deep features as a perceptual metric*R Zhang, P Isola, AA Efros, E Shechtman, O Wang\\xa0- Proceedings of the 753 IEEE/CVF Conference on\\xa0…, 20184회 인용 The Unreasonable Effectiveness of Deep Features as a Perceptual Metric. arXiv e-prints, art*R Zhang, P Isola, AA Efros, E Shechtman, O Wang\\xa0- arXiv preprint arXiv:1801.03924, 20183회 인용 관련 학술자료 The unreasonable effectiveness of deep features as a perceptual metric. arXiv preprint (2018)*R Zhang, P Isola, AA Efros, E Shechtman, O Wang - 20183회 인용 관련 학술자료 '}, title='The unreasonable effectiveness of deep features as a perceptual metric', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Context Encoders: Feature Learning by Inpainting': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/6', '설명': 'We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders--a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part (s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.', '저자': 'Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, Alexei A Efros', '전체 인용횟수': '5776회 인용201620172018201920202021202220232721253073189411121220977', '컨퍼런스': 'The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)', '학술 문서': 'Context encoders: Feature learning by inpaintingD Pathak, P Krahenbuhl, J Donahue, T Darrell…\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20165759회 인용 관련 학술자료 전체 14개의 버전 Context encoders: Feature learning by inpainting. CoRR abs/1604.07379 (2016)*D Pathak, P Krähenbühl, J Donahue, T Darrell…\\xa0- arXiv preprint arXiv:1604.07379, 20168회 인용 관련 학술자료 Proc. IEEE Conf. Computer Vision Pattern Recognition*D Pathak, P Krahenbuhl, J Donahue - 20166회 인용 관련 학술자료 Context encoders: Feature learning by inpainting. arXiv 2016*D Pathak, P Krahenbuhl, J Donahue, T Darrell…\\xa0- arXiv preprint arXiv:1604.073795회 인용 관련 학술자료 Context Encoders: Feature Learning by Inpainting, CoRR*D Pathak, P Krahenbuhl, J Donahue, T Darrell…\\xa0- arXiv preprint arXiv:1604.073795회 인용 관련 학술자료 Context encoders: Feature learning by inpainting: IEEE Conference on Computer Vision and Pattern Recognition*D Pathak, P Krahenbuhl, J Donahue, T Darrell… - 20163회 인용 관련 학술자료 Context Encoders: Feature Learning by Inpainting. CoRR, abs/1604.0*D Pathak, P Krahenbuhl, J Donahue, T Darrell… - 20162회 인용 관련 학술자료 Context encoders: feature learning by inpainting*J Donahue, T Darrell, D Pathak, P Krähenbühl… - 20162회 인용 관련 학술자료 l, and Alexei A*D Pathak, P Agrawal - 20162회 인용 관련 학술자료 '}, title='Context Encoders: Feature Learning by Inpainting', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Texture synthesis by non-parametric sampling': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1999', '게시자': 'Ieee', '권': '2', '설명': 'A non-parametric method for texture synthesis is proposed. The texture synthesis process grows a new image outward from an initial seed, one pixel at a time. A Markov random field model is assumed, and the conditional distribution of a pixel given all its neighbors synthesized so far is estimated by querying the sample image and finding all similar neighborhoods. The degree of randomness is controlled by a single perceptually intuitive parameter. The method aims at preserving as much local structure as possible and produces good results for a wide variety of synthetic and real-world textures.', '저자': 'Alexei A Efros, Thomas K Leung', '전체 인용횟수': '4632회 인용20002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202326457998150194204235207246237250225221261216226231227242202201173149', '컨퍼런스': 'IEEE International Conference on Computer Vision (ICCV), 1999', '페이지': '1033-1038 vol. 2', '학술 문서': \"Texture synthesis by non-parametric samplingAA Efros, TK Leung\\xa0- Proceedings of the seventh IEEE international\\xa0…, 19994624회 인용 관련 학술자료 전체 49개의 버전 Texture Synthesis by Non-parametric Sampling IEEE International Conference on Computer Vision (ICCV'99)*AA Efros, TK Leung\\xa0- Corfu, Greece, September, 19996회 인용 관련 학술자료 IEEE Comput*AA Efros, TK Leung\\xa0- Soc. Los Alamitos, CA, USA, 19996회 인용 관련 학술자료 ICCV*AA Efros, TK Leung - 19993회 인용 관련 학술자료 Th omas K Leung. Texture Synthesis by Non—paramettti Sampling*AA Efros\\xa0- Computer Science Division University of California, 19993회 인용 관련 학술자료 Computer Vision, 1999. The Proceedings of the Seventh IEEE International Conference On*AA Efros, TK Leung - 19993회 인용 관련 학술자료 Proceedings of ICCV’99*AA Efros, TK Leung - 19992회 인용 관련 학술자료 A., Leung, T., K., 1999.“Texture synthesis by non-parametric sampling”*A Efros\\xa0- IEEE International Conference on Computer Vision2회 인용 관련 학술자료 &amp; Leung, T.(1999) Texture synthesis by non-parametric sampling*A Efros\\xa0- Proceedings of the international conference on\\xa0…2회 인용 관련 학술자료 Texture Synthesis*A Efros1회 인용 관련 학술자료 \"}, title='Texture synthesis by non-parametric sampling', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Colorful Image Colorization': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/9', '게시자': 'Springer International Publishing', '설명': ' Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a “colorization Turing test,” asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32\\xa0% of the trials, significantly higher than\\xa0…', '저자': 'Richard Zhang, Phillip Isola, Alexei A Efros', '전체 인용횟수': '3777회 인용2016201720182019202020212022202329164279391573807883614', '컨퍼런스': 'European Conference on Computer Vision', '페이지': '649-666', '학술 문서': 'Colorful image colorizationR Zhang, P Isola, AA Efros\\xa0- Computer Vision–ECCV 2016: 14th European\\xa0…, 20163777회 인용 관련 학술자료 전체 15개의 버전 '}, title='Colorful Image Colorization', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Image quilting for texture synthesis and transfer': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2023/8/1', '도서': 'Seminal Graphics Papers: Pushing the Boundaries, Volume 2', '설명': 'We present a simple image-based method of generating novel visual appearance in which a new image is synthesized by stitching together small patches of existing images. We call this process image quilting. First, we use quilting as a fast and very simple texture synthesis algorithm which produces surprisingly good results for a wide range of textures. Second, we extend the algorithm to perform texture transfer - rendering an object with a texture taken from a different object. More generally, we demonstrate how an image can be re-rendered in the style of a different image. The method works directly on the images and does not require 3D information.', '저자': 'Alexei A Efros, William T Freeman', '전체 인용횟수': '3574회 인용20012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023125276126150179173161173185146175149158154148184174191199191192148', '페이지': '571-576', '학술 문서': 'Image quilting for texture synthesis and transferAA Efros, WT Freeman\\xa0- Seminal Graphics Papers: Pushing the Boundaries\\xa0…, 20233574회 인용 관련 학술자료 전체 12개의 버전 '}, title='Image quilting for texture synthesis and transfer', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Unsupervised visual representation learning by context prediction': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the R-CNN framework and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.', '저자': 'Carl Doersch, Abhinav Gupta, Alexei A Efros', '전체 인용횟수': '2897회 인용201620172018201920202021202220237385144219440625708570', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '1422-1430', '학술 문서': 'Unsupervised visual representation learning by context predictionC Doersch, A Gupta, AA Efros\\xa0- Proceedings of the IEEE international conference on\\xa0…, 20152897회 인용 관련 학술자료 전체 19개의 버전 '}, title='Unsupervised visual representation learning by context prediction', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Unbiased look at dataset bias': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011/6/20', '게시자': 'IEEE', '설명': 'Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech-101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set\\xa0…', '저자': 'Antonio Torralba, Alexei A Efros', '전체 인용횟수': '2683회 인용201120122013201420152016201720182019202020212022202384375118134150159184261311403423368', '컨퍼런스': 'CVPR 2011', '페이지': '1521-1528', '학술 문서': 'Unbiased look at dataset biasA Torralba, AA Efros\\xa0- CVPR 2011, 20112673회 인용 관련 학술자료 전체 12개의 버전 Unbiased look at dataset bias*T Antonio, AA Efros\\xa0- CVPR, 201124회 인용 관련 학술자료 Unbiased look at dataset bias. 42, 7 (2011), 1521--1528*A Torralba, AA Efros\\xa0- Google Scholar Google Scholar Digital Library Digital\\xa0…, 20113회 인용 관련 학술자료 Computer Vision and Pattern Recognition (CVPR)*T Antonio, A Efros Alexei\\xa0- 2011 IEEE Conference. IEEE, 20112회 인용 관련 학술자료 et almbox. 2011. Unbiased look at dataset bias*A Torralba, AA Efros\\xa0- IEEE conference on computer vision and pattern\\xa0…2회 인용 관련 학술자료 '}, title='Unbiased look at dataset bias', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Discovering objects and their location in images': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2005/10/17', '게시자': 'IEEE', '권': '1', '설명': 'We seek to discover the object categories depicted in a set of unlabelled images. We achieve this using a model developed in the statistical text literature: probabilistic latent semantic analysis (pLSA). In text analysis, this is used to discover topics in a corpus using the bag-of-words document representation. Here we treat object categories as topics, so that an image containing instances of several categories is modeled as a mixture of topics. The model is applied to images by using a visual analogue of a word, formed by vector quantizing SIFT-like region descriptors. The topic discovery approach successfully translates to the visual domain: for a small set of objects, we show that both the object categories and their approximate spatial layout are found without supervision. Performance of this unsupervised method is compared to the supervised approach of Fergus et al. (2003) on a set of unseen images containing\\xa0…', '저자': 'Josef Sivic, Bryan C Russell, Alexei A Efros, Andrew Zisserman, William T Freeman', '전체 인용횟수': '1307회 인용2005200620072008200920102011201220132014201520162017201820192020202120222023740829813313911510911195776348384628221512', '컨퍼런스': \"Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1\", '페이지': '370-377', '학술 문서': 'Discovering objects and their location in imagesJ Sivic, BC Russell, AA Efros, A Zisserman…\\xa0- Tenth IEEE International Conference on Computer\\xa0…, 20051307회 인용 관련 학술자료 전체 39개의 버전 '}, title='Discovering objects and their location in images', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Toward Multimodal Image-to-Image Translation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a distribution of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.', '저자': 'Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, Eli Shechtman', '전체 인용횟수': '1603회 인용20182019202020212022202390258317370298259', '컨퍼런스': 'Advances in Neural Information Processing Systems', '페이지': '465-476', '학술 문서': 'Toward multimodal image-to-image translationJY Zhu, R Zhang, D Pathak, T Darrell, AA Efros…\\xa0- Advances in neural information processing systems, 20171602회 인용 관련 학술자료 전체 7개의 버전 Efros Alexei A., Wang Oliver, and Shechtman Eli. 2017*Z Jun-Yan, Z Richard, P Deepak\\xa0- Toward multimodal image-to-image translation. In\\xa0…10회 인용 관련 학술자료 Efros Alexei A, Wang Oliver, and Shechtman Eli*Z Jun-Yan, Z Richard, P Deepak\\xa0- Toward multimodal image-to-image translation. In\\xa0…, 20174회 인용 관련 학술자료 Trevor Darrell Alexei A Efros Oliver Wang and Eli Shechtman. Toward multimodal image-to-image translationJY Zhu, R Zhang, D Pathak\\xa0- Advances in Neural Information Processing Systems, 20173회 인용 관련 학술자료 Toward multimodal image-to-image translation. arXiv 2017*JY Zhu, R Zhang, D Pathak, T Darrell, AA Efros…\\xa0- arXiv preprint arXiv:1711.115863회 인용 관련 학술자료 '}, title='Toward Multimodal Image-to-Image Translation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Generative visual manipulation on the natural image manifold': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '게시자': 'Springer International Publishing', '설명': ' Realistic image manipulation is challenging because it requires modifying the image appearance in a user-controlled way, while preserving the realism of the result. Unless the user has considerable artistic skill, it is easy to “fall off” the manifold of natural images while editing. In this paper, we propose to learn the natural image manifold directly from data using a generative adversarial neural network. We then define a class of image editing operations, and constrain their output to lie on that learned manifold at all times. The model automatically adjusts the output keeping all edits as realistic as possible. All our manipulations are expressed in terms of constrained optimization and are applied in near-real time. We evaluate our algorithm on the task of realistic photo manipulation of shape and color. The presented method can further be used for changing one image to look like the other, as well as generating\\xa0…', '저자': 'Jun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman, Alexei A Efros', '전체 인용횟수': '1471회 인용2016201720182019202020212022202312101218216246246221193', '컨퍼런스': 'Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14', '페이지': '597-613', '학술 문서': 'Generative visual manipulation on the natural image manifoldJY Zhu, P Krähenbühl, E Shechtman, AA Efros\\xa0- Computer Vision–ECCV 2016: 14th European\\xa0…, 20161468회 인용 관련 학술자료 전체 8개의 버전 Generative Visual Manipulation on the Natural Image Manifold. arXiv 2018*JY Zhu, P Krähenbühl, E Shechtman, AA Efros\\xa0- arXiv preprint arXiv:1609.035524회 인용 관련 학술자료 ECCV*JY Zhu, P Krähenbühl, E Shechtman, AA Efros - 20163회 인용 관련 학술자료 Generative visual manipulation on the natural image manifold (2016)*JY Zhu, P Krähenbühl, E Shechtman, A Efros\\xa0- Google Scholar, 20163회 인용 관련 학술자료 '}, title='Generative visual manipulation on the natural image manifold', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Scene completion using millions of photographs': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2007/7/29', '게시자': 'ACM', '권': '26', '설명': 'What can you do with a million images? In this paper we present a new image completion algorithm powered by a huge database of photographs gathered from the Web. The algorithm patches up holes in images by finding similar image regions in the database that are not only seamless but also semantically valid. Our chief insight is that while the space of images is effectively infinite, the space of semantically differentiable scenes is actually not that large. For many image completion tasks we are able to find similar scenes which contain image fragments that will convincingly complete the image. Our algorithm is entirely data-driven, requiring no annotations or labelling by the user. Unlike existing image completion methods, our algorithm can generate a diverse set of results for each input image and we allow users to select among them. We demonstrate the superiority of our algorithm over existing image\\xa0…', '저널': 'ACM Transactions on Graphics (ToG)', '저자': 'James Hays, Alexei A Efros', '전체 인용횟수': '1216회 인용20062007200820092010201120122013201420152016201720182019202020212022202341051757678897880677265908870608652', '페이지': '4-es', '학술 문서': 'Scene completion using millions of photographsJ Hays, AA Efros\\xa0- ACM Transactions on Graphics (ToG), 20071216회 인용 관련 학술자료 전체 13개의 버전 ', '호': '3'}, title='Scene completion using millions of photographs', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Ensemble of Exemplar-SVMs for Object Detection and Beyond': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011/11/6', '게시자': 'IEEE', '설명': 'This paper proposes a conceptually simple but surprisingly powerful method which combines the effectiveness of a discriminative object detector with the explicit correspondence offered by a nearest-neighbor approach. The method is based on training a separate linear SVM classifier for every exemplar in the training set. Each of these Exemplar-SVMs is thus defined by a single positive instance and millions of negatives. While each detector is quite specific to its exemplar, we empirically observe that an ensemble of such Exemplar-SVMs offers surprisingly good generalization. Our performance on the PASCAL VOC detection task is on par with the much more complex latent part-based model of Felzenszwalb et al., at only a modest computational cost increase. But the central benefit of our approach is that it creates an explicit association between each detection and a single training exemplar. Because most\\xa0…', '저자': 'Tomasz Malisiewicz, Abhinav Gupta, Alexei A Efros', '전체 인용횟수': '1170회 인용201020112012201320142015201620172018201920202021202220233105497124163142120967056737651', '컨퍼런스': 'International Conference on Computer Vision (ICCV)', '페이지': '89-96', '학술 문서': 'Ensemble of exemplar-svms for object detection and beyondT Malisiewicz, A Gupta, AA Efros\\xa0- 2011 International conference on computer vision, 20111168회 인용 관련 학술자료 전체 15개의 버전 a.(2011, novembre). Ensemble of exemplar-SVMs for object detection and beyond*T Malisiewicz, A Gupta, A Efros\\xa0- Ieee international conference on computer vision2회 인용 관련 학술자료 '}, title='Ensemble of Exemplar-SVMs for Object Detection and Beyond', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'IM2GPS: estimating geographic information from a single image': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2008/6/23', '게시자': 'IEEE', '설명': 'Estimating geographic information from an image is an excellent, difficult high-level computer vision problem whose time has come. The emergence of vast amounts of geographically-calibrated image data is a great reason for computer vision to start looking globally - on the scale of the entire planet! In this paper, we propose a simple algorithm for estimating a distribution over geographic locations from a single image using a purely data-driven scene matching approach. For this task, we leverage a dataset of over 6 million GPS-tagged images from the Internet. We represent the estimated image location as a probability distribution over the Earthpsilas surface. We quantitatively evaluate our approach in several geolocation tasks and demonstrate encouraging performance (up to 30 times better than chance). We show that geolocation estimates can provide the basis for numerous other image understanding tasks\\xa0…', '저자': 'James Hays, Alexei A Efros', '전체 인용횟수': '1116회 인용2007200820092010201120122013201420152016201720182019202020212022202348426472801011031479172815348453838', '컨퍼런스': 'IEEE Conference on Computer Vision and Pattern Recognition (CVPR)', '페이지': '1-8', '학술 문서': 'Im2gps: estimating geographic information from a single imageJ Hays, AA Efros\\xa0- 2008 ieee conference on computer vision and pattern\\xa0…, 20081109회 인용 관련 학술자료 전체 13개의 버전 Im2gps: estimating geographic images from single images*J Hays, A Efros\\xa0- Computer Vision and Pattern Recognition (CVPR), 20088회 인용 관련 학술자료 '}, title='IM2GPS: estimating geographic information from a single image', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Automatic photo pop-up': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2005/7/1', '도서': 'ACM SIGGRAPH 2005 Papers', '설명': 'This paper presents a fully automatic method for creating a 3D model from a single photograph. The model is made up of several texture-mapped planar billboards and has the complexity of a typical children\\'s pop-up book illustration. Our main insight is that instead of attempting to recover precise geometry, we statistically model geometric classes defined by their orientations in the scene. Our algorithm labels regions of the input image into coarse categories: \"ground\", \"sky\", and \"vertical\". These labels are then used to \"cut and fold\" the image into a pop-up model using a set of simple assumptions. Because of the inherent ambiguity of the problem and the statistical nature of the approach, the algorithm is not expected to work on every image. However. it performs surprisingly well for a wide range of scenes taken from a typical person\\'s photo album.', '저자': 'Derek Hoiem, Alexei A Efros, Martial Hebert', '전체 인용횟수': '951회 인용20052006200720082009201020112012201320142015201620172018201920202021202220237283446614964576159596258725238404832', '페이지': '577-584', '학술 문서': 'Automatic photo pop-upD Hoiem, AA Efros, M Hebert\\xa0- ACM SIGGRAPH 2005 Papers, 2005951회 인용 관련 학술자료 전체 14개의 버전 '}, title='Automatic photo pop-up', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Geometric context from a single image': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2005/10/17', '게시자': 'IEEE', '권': '1', '설명': 'Many computer vision algorithms limit their performance by ignoring the underlying 3D geometric structure in the image. We show that we can estimate the coarse geometric properties of a scene by learning appearance-based models of geometric classes, even in cluttered natural scenes. Geometric classes describe the 3D orientation of an image region with respect to the camera. We provide a multiple-hypothesis framework for robustly estimating scene structure from a single image and obtaining confidences for each geometric label. These confidences can then be used to improve the performance of many other applications. We provide a thorough quantitative evaluation of our algorithm on a set of outdoor images and demonstrate its usefulness in two applications: object detection and automatic single-view reconstruction.', '저자': 'Derek Hoiem, Alexei A Efros, Martial Hebert', '전체 인용횟수': '953회 인용2004200520062007200820092010201120122013201420152016201720182019202020212022202363273251626974766483576654394846293016', '컨퍼런스': \"Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1\", '페이지': '654-661', '학술 문서': 'Geometric context from a single imageD Hoiem, AA Efros, M Hebert\\xa0- Tenth IEEE International Conference on Computer\\xa0…, 2005953회 인용 관련 학술자료 전체 16개의 버전 '}, title='Geometric context from a single image', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning deep features for discriminative localization': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': 'In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them.', '저자': 'Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba', '전체 인용횟수': '9713회 인용20162017201820192020202120222023362295419741453200522852081', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '2921-2929', '학술 문서': 'Learning deep features for discriminative localizationB Zhou, A Khosla, A Lapedriza, A Oliva, A Torralba\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20169713회 인용 관련 학술자료 전체 23개의 버전 '}, title='Learning deep features for discriminative localization', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Modeling the shape of the scene: A holistic representation of the spatial envelope': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2001', '권': '42', '설명': ' In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimensional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a\\xa0…', '저널': 'International Journal of Computer Vision', '저자': 'Aude Oliva, Antonio Torralba', '전체 인용횟수': '8249회 인용2003200420052006200720082009201020112012201320142015201620172018201920202021202220236149525688123235332417495600716753795734648571424420350227', '페이지': '145-175', '학술 문서': 'Modeling the shape of the scene: A holistic representation of the spatial envelopeA Oliva, A Torralba\\xa0- International journal of computer vision, 20018249회 인용 관련 학술자료 전체 27개의 버전 ', '호': '3'}, title='Modeling the shape of the scene: A holistic representation of the spatial envelope', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Labelme: a database and web-based tool for image annotation. Int': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2007/10/31', '권': '77', '저널': 'International Journal of Computer Vision', '저자': 'B Russell, Antonio Torralba, K Murphy, W Freeman', '전체 인용횟수': '4260회 인용200620072008200920102011201220132014201520162017201820192020202120222023224996177210228219276242245275260233271302333398342', '페이지': '157–173', '학술 문서': 'LabelMe: a database and web-based tool for image annotation*BC Russell, A Torralba, KP Murphy, WT Freeman\\xa0- International journal of computer vision, 20084255회 인용 관련 학술자료 전체 30개의 버전 Labelme: a database and web-based tool for image annotation. IntB Russell, A Torralba, K Murphy, W Freeman\\xa0- Journal of Computer Vision, 20076회 인용 관련 학술자료 KevinP. Murphy and WilliamT*BC Russell, A Torralba - 20082회 인용 관련 학술자료 '}, title='Labelme: a database and web-based tool for image annotation. Int', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Places: A 10 million image database for scene recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/7/4', '게시자': 'IEEE', '권': '40', '설명': 'The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification performance at tasks such as visual object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories, comprising a large and diverse list of the types of environments encountered in the world. Using the state-of-the-art Convolutional Neural Networks (CNNs), we provide scene classification CNNs (Places-CNNs) as baselines, that significantly outperform the previous approaches. Visualization of the CNNs trained on Places shows that object detectors emerge as an intermediate representation of scene classification. With its high-coverage and high-diversity of exemplars, the Places Database along with the Places-CNNs offer a novel resource to guide future progress on scene\\xa0…', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, Antonio Torralba', '전체 인용횟수': '3724회 인용201720182019202020212022202337230437551761886804', '페이지': '1452-1464', '학술 문서': 'Places: A 10 million image database for scene recognitionB Zhou, A Lapedriza, A Khosla, A Oliva, A Torralba\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20173724회 인용 관련 학술자료 전체 11개의 버전 ', '호': '6'}, title='Places: A 10 million image database for scene recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning deep features for scene recognition using places database': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '권': '27', '설명': \"Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks.\", '저널': 'Advances in neural information processing systems', '저자': 'Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, Aude Oliva', '전체 인용횟수': '3540회 인용201420152016201720182019202020212022202320194419586513507398361266197', '학술 문서': 'Learning deep features for scene recognition using places databaseB Zhou, A Lapedriza, J Xiao, A Torralba, A Oliva\\xa0- Advances in neural information processing systems, 20143540회 인용 관련 학술자료 전체 27개의 버전 '}, title='Learning deep features for scene recognition using places database', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Sun database: Large-scale scene recognition from abbey to zoo': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/6/13', '게시자': 'IEEE', '설명': 'Scene categorization is a fundamental problem in computer vision. However, scene understanding research has been constrained by the limited scope of currently-used databases which do not capture the full variety of scene categories. Whereas standard databases for object categorization contain hundreds of different classes of objects, the largest available dataset of scene categories contains only 15 classes. In this paper we propose the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images. We use 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance. We measure human scene classification performance on the SUN database and compare this with computational methods. Additionally, we study a finer-grained scene representation to detect scenes embedded inside of larger\\xa0…', '저자': 'Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, Antonio Torralba', '전체 인용횟수': '3507회 인용20092010201120122013201420152016201720182019202020212022202391258100175225267330281268327284307362437', '컨퍼런스': '2010 IEEE computer society conference on computer vision and pattern recognition', '페이지': '3485-3492', '학술 문서': 'Sun database: Large-scale scene recognition from abbey to zooJ Xiao, J Hays, KA Ehinger, A Oliva, A Torralba\\xa0- 2010 IEEE computer society conference on computer\\xa0…, 20103507회 인용 관련 학술자료 전체 7개의 버전 '}, title='Sun database: Large-scale scene recognition from abbey to zoo', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Skip-thought vectors': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '권': '28', '설명': 'We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.', '저널': 'Advances in neural information processing systems', '저자': 'Ryan Kiros, Yukun Zhu, Russ R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, Sanja Fidler', '전체 인용횟수': '2966회 인용20152016201720182019202020212022202336177306478561488391294207', '학술 문서': 'Skip-thought vectorsR Kiros, Y Zhu, RR Salakhutdinov, R Zemel, R Urtasun…\\xa0- Advances in neural information processing systems, 20152966회 인용 관련 학술자료 전체 19개의 버전 '}, title='Skip-thought vectors', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Aligning books and movies: Towards story-like visual explanations by watching movies and reading books': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in the current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.', '저자': 'Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler', '전체 인용횟수': '2681회 인용20152016201720182019202020212022202375567101251434523681544', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '19-27', '학술 문서': 'Aligning books and movies: Towards story-like visual explanations by watching movies and reading booksY Zhu, R Kiros, R Zemel, R Salakhutdinov, R Urtasun…\\xa0- Proceedings of the IEEE international conference on\\xa0…, 20152681회 인용 관련 학술자료 전체 21개의 버전 '}, title='Aligning books and movies: Towards story-like visual explanations by watching movies and reading books', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Scene parsing through ade20k dataset': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': \"Scene parsing, or recognizing and segmenting objects and stuff in an image, is one of the key problems in computer vision. Despite the community's efforts in data collection, there are still few image datasets covering a wide range of scenes and object categories with dense and detailed annotations for scene parsing. In this paper, we introduce and analyze the ADE20K dataset, spanning diverse annotations of scenes, objects, parts of objects, and in some cases even parts of parts. A scene parsing benchmark is built upon the ADE20K with 150 object and stuff classes included. Several segmentation baseline models are evaluated on the benchmark. A novel network design called Cascade Segmentation Module is proposed to parse a scene into stuff, objects, and object parts in a cascade and improve over the baselines. We further show that the trained scene parsing networks can lead to applications such as image content removal and scene synthesis (Dataset and pretrained models are available at http://groups. csail. mit. edu/vision/datasets/ADE20K/).\", '저자': 'Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, Antonio Torralba', '전체 인용횟수': '2545회 인용201720182019202020212022202322124265356457592716', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '633-641', '학술 문서': 'Scene parsing through ade20k datasetB Zhou, H Zhao, X Puig, S Fidler, A Barriuso…\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20172545회 인용 관련 학술자료 전체 12개의 버전 '}, title='Scene parsing through ade20k dataset', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning to predict where humans look': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009/9/29', '게시자': 'IEEE', '설명': 'For many applications in graphics, design, and human computer interaction, it is essential to understand where humans look in a scene. Where eye tracking devices are not a viable option, models of saliency can be used to predict fixation locations. Most saliency approaches are based on bottom-up computation that does not consider top-down image semantics and often does not match actual eye movements. To address this problem, we collected eye tracking data of 15 viewers on 1003 images and use this database as training and testing examples to learn a model of saliency based on low, middle and high-level image features. This large database of eye tracking data is publicly available with this paper.', '저자': 'Tilke Judd, Krista Ehinger, Frédo Durand, Antonio Torralba', '전체 인용횟수': '2463회 인용201020112012201320142015201620172018201920202021202220232674115170231231250269237215193158149107', '컨퍼런스': '2009 IEEE 12th international conference on computer vision', '페이지': '2106-2113', '학술 문서': 'Learning to predict where humans lookT Judd, K Ehinger, F Durand, A Torralba\\xa0- 2009 IEEE 12th international conference on computer\\xa0…, 20092462회 인용 관련 학술자료 전체 21개의 버전 Learning to predict where humans look, 2009*T Judd, K Ehinger, F Durand, A Torralba\\xa0- Computer Vision, IEEE 12th international conference\\xa0…, 20093회 인용 관련 학술자료 '}, title='Learning to predict where humans look', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search.': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2006/10', '게시자': 'American Psychological Association', '권': '113', '설명': 'Many experiments have shown that the human visual system makes extensive use of contextual information for facilitating object search in natural scenes. However, the question of how to formally model contextual influences is still open. On the basis of a Bayesian framework, the authors present an original approach of attentional guidance by global scene context. The model comprises 2 parallel pathways; one pathway computes local features (saliency) and the other computes global (scene-centered) features. The contextual guidance model of attention combines bottom-up saliency, scene context, and top-down mechanisms at an early stage of visual processing and predicts the image regions likely to be fixated by human observers performing natural search tasks in real-world scenes.(PsycINFO Database Record (c) 2016 APA, all rights reserved)', '저널': 'Psychological review', '저자': 'Antonio Torralba, Aude Oliva, Monica S Castelhano, John M Henderson', '전체 인용횟수': '2125회 인용200720082009201020112012201320142015201620172018201920202021202220233690123119141144180152139166183117148106987966', '페이지': '766', '학술 문서': 'Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search.A Torralba, A Oliva, MS Castelhano, JM Henderson\\xa0- Psychological review, 20062125회 인용 관련 학술자료 전체 25개의 버전 ', '호': '4'}, title='Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search.', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Sift flow: Dense correspondence across scenes and its applications': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/8/19', '게시자': 'IEEE', '권': '33', '설명': 'While image alignment has been studied in different areas of computer vision for decades, aligning images depicting different scenes remains a challenging problem. Analogous to optical flow, where an image is aligned to its temporally adjacent frame, we propose SIFT flow, a method to align an image to its nearest neighbors in a large image corpus containing a variety of scenes. The SIFT flow algorithm consists of matching densely sampled, pixelwise SIFT features between two images while preserving spatial discontinuities. The SIFT features allow robust matching across different scene/object appearances, whereas the discontinuity-preserving spatial model allows matching of objects located at different parts of the scene. Experiments show that the proposed approach robustly aligns complex scene pairs containing significant spatial differences. Based on SIFT flow, we propose an alignment-based large\\xa0…', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Ce Liu, Jenny Yuen, Antonio Torralba', '전체 인용횟수': '1968회 인용2009201020112012201320142015201620172018201920202021202220238225709713817224122021522515913713883', '페이지': '978-994', '학술 문서': 'Sift flow: Dense correspondence across scenes and its applicationsC Liu, J Yuen, A Torralba\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20101968회 인용 관련 학술자료 전체 19개의 버전 ', '호': '5'}, title='Sift flow: Dense correspondence across scenes and its applications', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Building the gist of a scene: The role of global image features in recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2006/1/1', '게시자': 'Elsevier', '권': '155', '설명': 'Humans can recognize the gist of a novel image in a single glance, independent of its complexity. How is this remarkable feat accomplished? On the basis of behavioral and computational evidence, this paper describes a formal approach to the representation and the mechanism of scene gist understanding, based on scene-centered, rather than object-centered primitives. We show that the structure of a scene image can be estimated by the mean of global image features, providing a statistical summary of the spatial layout properties (Spatial Envelope representation) of the scene. Global features are based on configurations of spatial scales and are estimated without invoking segmentation or grouping operations. The scene-centered approach is not an alternative to local image analysis but would serve as a feed-forward and parallel pathway of visual processing, able to quickly constrain local feature analysis and\\xa0…', '저자': 'Aude Oliva, Antonio Torralba', '전체 인용횟수': '1881회 인용2006200720082009201020112012201320142015201620172018201920202021202220235324558909711511516518113117512011313210310573', '출처': 'Progress in brain research', '페이지': '23-36', '학술 문서': 'Building the gist of a scene: The role of global image features in recognitionA Oliva, A Torralba\\xa0- Progress in brain research, 20061881회 인용 관련 학술자료 전체 13개의 버전 '}, title='Building the gist of a scene: The role of global image features in recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Recognizing indoor scenes': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009/6/20', '게시자': 'IEEE', '설명': 'Indoor scene recognition is a challenging open problem in high level vision. Most scene recognition models that work well for outdoor scenes perform poorly in the indoor domain. The main difficulty is that while some indoor scenes (e.g. corridors) can be well characterized by global spatial properties, others (e.g, bookstores) are better characterized by the objects they contain. More generally, to address the indoor scenes recognition problem we need a model that can exploit local and global discriminative information. In this paper we propose a prototype based model that can successfully combine both sources of information. To test our approach we created a dataset of 67 indoor scenes categories (the largest available) covering a wide range of domains. The results show that our approach can significantly outperform a state of the art classifier for the task.', '저자': 'Ariadna Quattoni, Antonio Torralba', '전체 인용횟수': '1825회 인용2009201020112012201320142015201620172018201920202021202220231035427786104155177156160147160168172140', '컨퍼런스': '2009 IEEE conference on computer vision and pattern recognition', '페이지': '413-420', '학술 문서': 'Recognizing indoor scenesA Quattoni, A Torralba\\xa0- 2009 IEEE conference on computer vision and pattern\\xa0…, 20091825회 인용 관련 학술자료 전체 8개의 버전 '}, title='Recognizing indoor scenes', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Sharing features: efficient boosting procedures for multiclass object detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2004', '설명': 'We consider the problem of detecting a large number of different object classes in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, which can be slow and require much training data. We present a multi-class boosting procedure (joint boosting) that reduces both the computational and sample complexity, by finding common features that can be shared across the classes. The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required is observed to scale approximately logarithmically with the number of classes. In addition, we find that the features selected by independently trained classifiers are often specific to the class, whereas the features selected by the jointly trained classifiers are more generic features, such as lines and edges.', '저널': 'Proc. of IEEE Conf. on CVPR', '저자': 'Antonio Torralba, K Murphy, W Freeman', '전체 인용횟수': '1766회 인용20032004200520062007200820092010201120122013201420152016201720182019202020212022202369599910512812917214315514812811010068504029213312', '학술 문서': 'Sharing visual features for multiclass and multiview object detection*A Torralba, KP Murphy, WT Freeman\\xa0- IEEE Transactions on Pattern Analysis and Machine\\xa0…, 2007973회 인용 관련 학술자료 전체 33개의 버전 Sharing features: efficient boosting procedures for multiclass object detectionA Torralba, KP Murphy, WT Freeman\\xa0- Proceedings of the 2004 IEEE Computer Society\\xa0…, 2004852회 인용 관련 학술자료 전체 26개의 버전 '}, title='Sharing features: efficient boosting procedures for multiclass object detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Generating videos with scene dynamics': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '권': '29', '설명': \"We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (eg action classification) and video generation tasks (eg future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation.\", '저널': 'Advances in neural information processing systems', '저자': 'Carl Vondrick, Hamed Pirsiavash, Antonio Torralba', '전체 인용횟수': '1586회 인용2016201720182019202020212022202312103231263293239255162', '학술 문서': 'Generating videos with scene dynamicsC Vondrick, H Pirsiavash, A Torralba\\xa0- Advances in neural information processing systems, 20161580회 인용 관련 학술자료 전체 17개의 버전 Generating videos with scene dynamics (2016)*C Vondrick, H Pirsiavash, A Torralba\\xa0- arXiv preprint arXiv:1609.02612, 20164회 인용 관련 학술자료 Generating videos with scene dynamics. CoRR abs/1609.02612 (2016)*C Vondrick, H Pirsiavash, A Torralba\\xa0- arXiv preprint arXiv:1609.02612, 20164회 인용 관련 학술자료 '}, title='Generating videos with scene dynamics', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Network dissection: Quantifying interpretability of deep visual representations': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a data set of concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are labeled across a broad range of visual concepts including objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability is an axis-independent property of the representation space, then we apply the method to compare the latent representations of various networks when trained to solve different classification problems. We further analyze the effect of training iterations, compare networks trained with different initializations, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power', '저널': 'Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition', '저자': 'David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, Antonio Torralba', '전체 인용횟수': '1472회 인용201720182019202020212022202332159188252276302252', '학술 문서': 'Network dissection: Quantifying interpretability of deep visual representationsD Bau, B Zhou, A Khosla, A Oliva, A Torralba\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20171472회 인용 관련 학술자료 전체 15개의 버전 '}, title='Network dissection: Quantifying interpretability of deep visual representations', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The flipped voltage follower: A useful cell for low-voltage low-power circuit design': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2002/5/26', '게시자': 'IEEE', '권': '3', '설명': 'In this paper a new basic cell for low-power and/or low-voltage operation is identified. It is shown that different versions of this cell, called \"flipped voltage follower\", have been used in the past for different applications. New circuits using this cell are also proposed here.', '저자': 'J Ramirez-Angulo, RG Carvajal, A Torralba, JAGJ Galan, AP Vega-Leal, JATJ Tombs', '전체 인용횟수': '190회 인용20022003200420052006200720082009201020112012201320142015201620172018201920202021202220235112429161814748633345447572', '컨퍼런스': '2002 IEEE International Symposium on Circuits and Systems. Proceedings (Cat. No. 02CH37353)', '페이지': 'III-III', '학술 문서': 'The flipped voltage follower: A useful cell for low-voltage low-power circuit designJ Ramirez-Angulo, RG Carvajal, A Torralba, J Galan…\\xa0- 2002 IEEE International Symposium on Circuits and\\xa0…, 2002190회 인용 관련 학술자료 전체 3개의 버전 '}, title='The flipped voltage follower: A useful cell for low-voltage low-power circuit design', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Speed control of induction motors using a novel fuzzy sliding-mode structure': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2002/6', '게시자': 'IEEE', '권': '10', '설명': 'This paper presents a new approach to indirect vector control of induction motors. Two nonlinear controllers, one of sliding mode type and the other PI-fuzzy logic-based, define a new control structure. Both controllers are combined by means of an expert system based on Takagi-Sugeno fuzzy reasoning. The sliding-mode controller acts mainly in a transient state while the PI-like fuzzy controller acts in the steady state. The new structure embodies the advantages that both nonlinear controllers offer: sliding-mode controllers increasing system stability limits, and PI-like fuzzy logic based controllers reducing the chattering in permanent state. The scheme has been implemented and experimentally validated.', '저널': 'IEEE Transactions on Fuzzy Systems', '저자': 'F Barrero, A Gonzalez, A Torralba, Eduardo Galvan, Leopoldo García Franquelo', '전체 인용횟수': '220회 인용2002200320042005200620072008200920102011201220132014201520162017201820192020202120222023147118109810241117171014161394592', '페이지': '375-383', '학술 문서': 'Speed control of induction motors using a novel fuzzy sliding-mode structureF Barrero, A Gonzalez, A Torralba, E Galvan…\\xa0- IEEE Transactions on Fuzzy Systems, 2002220회 인용 관련 학술자료 전체 5개의 버전 ', '호': '3'}, title='Speed control of induction motors using a novel fuzzy sliding-mode structure', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Low supply voltage high-performance CMOS current mirror with low input and output voltage requirements': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2004/3/22', '게시자': 'IEEE', '권': '51', '설명': \"This paper presents a scheme for the efficient implementation of a low supply voltage continuous-time high-performance CMOS current mirror with low input and output voltage requirements. This circuit combines a shunt input feedback and a regulated cascode output stage to achieve low input resistance and very high output resistance. It can be used as a high-precision current mirror in analog and mixed signal circuits with a power supply close to a transistor's threshold voltage. The proposed current mirror has been simulated and a bandwidth of 40 MHz has been obtained. An experimental chip prototype has been sent for fabrication and has been experimentally verified, obtaining 0.15-V input-output voltage requirements, 100-/spl Omega/ input resistance, and more than 200-M/spl Omega/ (G/spl Omega/ ideally) output resistance with a 1.2-V supply in a standard CMOS technology.\", '저널': 'IEEE Transactions on Circuits and Systems II: Express Briefs', '저자': 'Jaime Ramirez-Angulo, Ramon Gonzalez Carvajal, Antonio Torralba', '전체 인용횟수': '159회 인용2001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202312118675671312148131366531136', '페이지': '124-129', '학술 문서': 'Low supply voltage high-performance CMOS current mirror with low input and output voltage requirementsJ Ramirez-Angulo, RG Carvajal, A Torralba\\xa0- IEEE Transactions on Circuits and Systems II: Express\\xa0…, 2004152회 인용 관련 학술자료 전체 6개의 버전 Low supply voltage high-performance CMOS current mirror with low input and output voltage requirements*J Ramirez-Angulo, RG Carvajal, A Torralba\\xa0- Proceedings of the 43rd IEEE Midwest Symposium on\\xa0…, 20009회 인용 관련 학술자료 ', '호': '3'}, title='Low supply voltage high-performance CMOS current mirror with low input and output voltage requirements', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A continuous-time/spl sigma//spl delta/adc with increased immunity to interferers': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2004/11/30', '게시자': 'IEEE', '권': '39', '설명': 'Receivers are being digitized in a quest for flexibility. Analog filters and programmable gain stages are being exchanged for digital processing at the price of a very challenging ADC. This paper presents an alternative solution where the filter and programmable gain functionality is integrated into a /spl Sigma//spl Delta/ ADC. The novel filtering ADC is realized by adding a high-pass feedback path to a conventional /spl Sigma//spl Delta/ ADC while a compensating low-pass filter in the forward path maintains stability. As such, the ADC becomes highly immune to interferers even if they exceed the maximum allowable input level for the wanted channel. As a consequence, the ADC input range can be programmed dynamically to the level of the wanted signal only. This results in an input-referred dynamic range of 89 dB in 1-MHz bandwidth and an intentionally moderate output signal-to-noise-and-distortion ratio of 46-59\\xa0…', '저널': 'IEEE Journal of Solid-State Circuits', '저자': 'Kathleen Philips, Peter ACM Nuijten, Raf LJ Roovers, Arthur HM van Roermund, Fernando Muñoz Chavero, Macarena Tejero Pallarés, Antonio Torralba', '전체 인용횟수': '136회 인용2004200520062007200820092010201120122013201420152016201720182019202020212022202314497613881810111110711231', '페이지': '2170-2178', '학술 문서': 'A continuous-time/spl sigma//spl delta/adc with increased immunity to interferersK Philips, PACM Nuijten, RLJ Roovers…\\xa0- IEEE Journal of Solid-State Circuits, 2004136회 인용 관련 학술자료 전체 17개의 버전 ', '호': '12'}, title='A continuous-time/spl sigma//spl delta/adc with increased immunity to interferers', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " '0.7-V three-stage class-AB CMOS operational transconductance amplifier': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/8/30', '게시자': 'IEEE', '권': '63', '설명': 'A simple high-performance architecture for bulk-driven operational transconductance amplifiers (OTAs) is presented. The solution, suitable for operation under sub 1-V single supply, is made up of three gain stages and, as an additional feature, provides inherent class-AB behavior with accurate and robust standby current control. The OTA is fabricated in a 180-nm standard CMOS technology, occupies an area of  and is powered from 0.7 V with a standby current consumption of around 36 . DC gain and unity gain frequency are 57 dB and 3 MHz, respectively, under a capacitive load of 20 pF. Overall good large-signal and small-signal performances are achieved, making the solution extremely competitive in comparison to the state of the art.', '저널': 'IEEE Transactions on Circuits and Systems I: Regular Papers', '저자': 'Elena Cabrera-Bernal, Salvatore Pennisi, Alfio Dario Grasso, Antonio Torralba, Ramón Gonzalez Carvajal', '전체 인용횟수': '130회 인용20172018201920202021202220234201424371713', '페이지': '1807-1815', '학술 문서': '0.7-V three-stage class-AB CMOS operational transconductance amplifierE Cabrera-Bernal, S Pennisi, AD Grasso, A Torralba…\\xa0- IEEE Transactions on Circuits and Systems I: Regular\\xa0…, 2016130회 인용 관련 학술자료 ', '호': '11'}, title='0.7-V three-stage class-AB CMOS operational transconductance amplifier', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A new family of very low-voltage analog circuits based on quasi-floating-gate transistors': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2003/5/13', '게시자': 'IEEE', '권': '50', '설명': 'A new family of very low-voltage analog circuits is introduced. These circuits do not show the GB degradation that characterizes other low-voltage approaches based on floating-gate transistors. The proposed approach is validated with experimental results of a CMOS mixer in 0.5-/spl mu/m CMOS technology with 0.7-V input signal swing that operates on a single 0.8-V supply with transistor threshold voltages of 0.67 V.', '저널': 'IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing', '저자': 'Jaime Ramirez-Angulo, Carlos A Urquidi, Ramon González-Carvajal, Antonio Torralba, Antonio López-Martín', '전체 인용횟수': '126회 인용200320042005200620072008200920102011201220132014201520162017201820192020202120222023210821687137236668266106', '페이지': '214-220', '학술 문서': 'A new family of very low-voltage analog circuits based on quasi-floating-gate transistorsJ Ramirez-Angulo, CA Urquidi, R González-Carvajal…\\xa0- IEEE Transactions on Circuits and Systems II: Analog\\xa0…, 2003126회 인용 관련 학술자료 전체 2개의 버전 ', '호': '5'}, title='A new family of very low-voltage analog circuits based on quasi-floating-gate transistors', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'FASY: A fuzzy-logic based tool for analog synthesis': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1996/7', '게시자': 'IEEE', '권': '15', '설명': 'A CAD tool for analog circuit synthesis is presented. This tool, called FASY, uses fuzzy-logic based reasoning to select one topology among a fixed set of alternatives. For the selected topology, a two-phase optimizer sizes all elements to satisfy the performance constraints minimizing a cost function. In FASY, the decision rules used in the topology selection process are introduced by an expert designer or automatically generated by means of a learning process that uses the optimizer mentioned above. The capability of learning topology selection rules by experience, is unique in FASY. Practical examples demonstrate the tool ability of this tool to learn topology selection rules and to synthesize analog cells with different circuit topologies.', '저널': 'IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems', '저자': 'Antonio Torralba, Jorge Chavez, Leopoldo García Franquelo', '전체 인용횟수': '112회 인용19961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220231855942543884353353483251', '페이지': '705-715', '학술 문서': 'FASY: A fuzzy-logic based tool for analog synthesisA Torralba, J Chavez, LG Franquelo\\xa0- IEEE Transactions on Computer-Aided Design of\\xa0…, 1996112회 인용 관련 학술자료 전체 7개의 버전 ', '호': '7'}, title='FASY: A fuzzy-logic based tool for analog synthesis', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A low-power low-voltage OTA-C sinusoidal oscillator with a large tuning range': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2005/2/14', '게시자': 'IEEE', '권': '52', '설명': 'A new operational transconductance amplifier and capacitor based sinusoidal voltage controlled oscillator is presented. The transconductor uses two cross-coupled class-AB pseudo-differential pairs biased by a flipped voltage follower, and it exhibits a wide transconductance range with low power consumption and high linearity. The oscillator has been fabricated in a standard 0.8-/spl mu/m CMOS process. Experimental results show a frequency tuning range from 1 to 25 MHz. The amplitude is controlled by the transconductor nonlinear characteristic. The circuit is operated at 2-V supply voltage with only 1.58 mW of maximum quiescent power consumption.', '저널': 'IEEE Transactions on Circuits and Systems I: Regular Papers', '저자': 'J Galan, Ramón González Carvajal, Antonio Torralba, Fernando Muñoz, Jaime Ramirez-Angulo', '전체 인용횟수': '102회 인용20062007200820092010201120122013201420152016201720182019202020212022202374511657105763434465', '페이지': '283-291', '학술 문서': 'A low-power low-voltage OTA-C sinusoidal oscillator with a large tuning rangeJ Galan, RG Carvajal, A Torralba, F Muñoz…\\xa0- IEEE Transactions on Circuits and Systems I: Regular\\xa0…, 2005102회 인용 관련 학술자료 전체 2개의 버전 ', '호': '2'}, title='A low-power low-voltage OTA-C sinusoidal oscillator with a large tuning range', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Low-voltage CMOS op-amp with rail-to-rail input and output signal swing for continuous-time signal processing using multiple-input floating-gate transistors': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2001/1', '게시자': 'IEEE', '권': '48', '설명': 'A scheme for low-voltage CMOS op-amp operation with rail-to-rail input and output signal swing and constant g/sub m/ is presented. Single-ended and fully differential versions are discussed. The scheme is based on the use of multiple-input floating-gate transistors and allows direct implementation of linear weighted addition of continuous-time signals. Simulations are presented that verify the scheme operating with a 1.2-V single supply, 1.2-V input and output solving, 5-MHz op-amp gain-bandwidth product, and a 192-/spl mu/W power dissipation with a 50-pF load and 300/spl times/300 /spl mu/m/sup 2/ silicon area. These results are obtained for 0.85-V transistor threshold voltages. Experimental results are shown that verify the correct functionality of the proposed approach.', '저널': 'IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing', '저자': 'J Ramirez-Angulo, RG Carvajal, J Tombs, A Torralba', '전체 인용횟수': '98회 인용20012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023176861296273173322212114', '페이지': '111-116', '학술 문서': 'Low-voltage CMOS op-amp with rail-to-rail input and output signal swing for continuous-time signal processing using multiple-input floating-gate transistorsJ Ramirez-Angulo, RG Carvajal, J Tombs, A Torralba\\xa0- IEEE Transactions on Circuits and Systems II: Analog\\xa0…, 200198회 인용 관련 학술자료 전체 2개의 버전 ', '호': '1'}, title='Low-voltage CMOS op-amp with rail-to-rail input and output signal swing for continuous-time signal processing using multiple-input floating-gate transistors', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Fuzzy hardware: architectures and applications': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1998', '게시자': 'Springer Science & Business Media', '설명': 'Fuzzy hardware developments have been a major force driving the applications of fuzzy set theory and fuzzy logic in both science and engineering. This volume provides the reader with a comprehensive up-to-date look at recent works describing new innovative developments of fuzzy hardware. An important research trend is the design of improved fuzzy hardware. There is an increasing interest in both analog and digital implementations of fuzzy controllers in particular and fuzzy systems in general. Specialized analog and digital VLSI implementations of fuzzy systems, in the form of dedicated architectures, aim at the highest implementation efficiency. This particular efficiency is asserted in terms of processing speed and silicon utilization. Processing speed in particular has caught the attention of developers of fuzzy hardware and researchers in the field. The volume includes detailed material on a variety of fuzzy hardware related topics such as: Historical review of fuzzy hardware research Fuzzy hardware based on encoded trapezoids Pulse stream techniques for fuzzy hardware Hardware realization of fuzzy neural networks Design of analog neuro-fuzzy systems in CMOS digital technologies Fuzzy controller synthesis method Automatic design of digital and analog neuro-fuzzy controllers Electronic implementation of complex controllers Silicon compilation of fuzzy hardware systems Digital fuzzy hardware processing Parallel processor architecture for real-time fuzzy applications Fuzzy cellular systems Fuzzy Hardware: Architectures and Applications is a technical reference book for researchers, engineers and scientists interested in fuzzy\\xa0…', '저자': 'Abraham Kandel, Gideon Langholz', '전체 인용횟수': '92회 인용199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202226344368444547533263311', '학술 문서': 'Fuzzy hardware: architectures and applicationsA Kandel, G Langholz - 199892회 인용 관련 학술자료 전체 6개의 버전 '}, title='Fuzzy hardware: architectures and applications', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Comparison of conventional and new flipped voltage structures with increased input/output signal swing and current sourcing/sinking capabilities': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2005/8/7', '게시자': 'IEEE', '설명': 'A systematic comparison of flipped voltage followers circuits is presented. Two new versions of the flipped voltage follower are introduced. They are characterized by very low output impedance, high bandwidth speed, wide signal swing. All structures can be easily modified for class AB operation. Simulation results show that the newly introduced structures have optimal characteristics', '저자': 'Jaime Ramírez-Angulo, S Gupta, Ivan Padilla, RG Carvajal, A Torralba, M Jimenez, F Munoz', '전체 인용횟수': '77회 인용20082009201020112012201320142015201620172018201920202021202220235184676225655456', '컨퍼런스': '48th Midwest Symposium on Circuits and Systems, 2005.', '페이지': '1151-1154', '학술 문서': 'Comparison of conventional and new flipped voltage structures with increased input/output signal swing and current sourcing/sinking capabilitiesJ Ramírez-Angulo, S Gupta, I Padilla, RG Carvajal…\\xa0- 48th Midwest Symposium on Circuits and Systems\\xa0…, 200577회 인용 관련 학술자료 전체 3개의 버전 '}, title='Comparison of conventional and new flipped voltage structures with increased input/output signal swing and current sourcing/sinking capabilities', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A 4.7 mW 89.5 dB DR CT complex/spl Delta//spl Sigma/ADC with built-in LPF': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2005/2/10', '게시자': 'IEEE', '설명': 'A CT complex /spl Delta//spl Sigma/ ADC with built-in LPF is presented. A modified feedback topology is used to improve robustness to interferers near f/sub g//2 or f/sub s/. Adding programmable gain control, the 0.18 /spl mu/m CMOS ADC achieves 89.5dB DR in a 1MHz BW, consuming 4.7mW from a 1.8V supply.', '저자': 'F Munoz, K Philips, A Torralba', '전체 인용횟수': '76회 인용2006200720082009201020112012201320142015201620172018201920202021202220235142109441444561111', '컨퍼런스': 'ISSCC. 2005 IEEE International Digest of Technical Papers. Solid-State Circuits Conference, 2005.', '페이지': '500-613', '학술 문서': 'A 4.7 mW 89.5 dB DR CT complex/spl Delta//spl Sigma/ADC with built-in LPFF Munoz, K Philips, A Torralba\\xa0- ISSCC. 2005 IEEE International Digest of Technical\\xa0…, 200576회 인용 관련 학술자료 전체 4개의 버전 '}, title='A 4.7 mW 89.5 dB DR CT complex/spl Delta//spl Sigma/ADC with built-in LPF', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Floating-gate-based tunable CMOS low-voltage linear transconductor and its application to HF g/sub m/-C filter design': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2001/1', '게시자': 'IEEE', '권': '48', '설명': 'This paper presents a new CMOS low-voltage linear transconductor for very high frequency. It uses multiple-input floating-gate transistors in each inverter of the differential structure transconductor presented by Nauta [1992]. The proposed transconductor operates under a constant low-voltage supply as low as 1.2 V, and its transconductance and output resistance are independently tunable, as shown by experimental measurements. This architecture is suitable for use in high-frequency continuous-time filters with programmable center frequency and quality factor. Simulation results of a 10.7-MHz and Q=40 g/sub m/-C filter operating with a voltage supply of 1.4 V and with rail-to rail input swing are presented.', '저널': 'IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing', '저자': 'Fernando Muñoz, Antonio Torralba, Ramón G Carvajal, J Tombs, Jaime Ramirez-Angulo', '전체 인용횟수': '72회 인용2001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202322421591121365315334261', '페이지': '106-110', '학술 문서': 'Floating-gate-based tunable CMOS low-voltage linear transconductor and its application to HF g/sub m/-C filter designF Muñoz, A Torralba, RG Carvajal, J Tombs…\\xa0- IEEE Transactions on Circuits and Systems II: Analog\\xa0…, 200172회 인용 관련 학술자료 전체 3개의 버전 ', '호': '1'}, title='Floating-gate-based tunable CMOS low-voltage linear transconductor and its application to HF g/sub m/-C filter design', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Low-voltage CMOS operational amplifiers with wide input-output swing based on a novel scheme': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2000/5', '게시자': 'IEEE', '권': '47', '설명': 'A scheme to achieve low-voltage wide-bandwidth operation of CMOS op amps with rail-to-rail input and output swing and constant gm is presented. It is based on a novel concept that uses a floating voltage controlled voltage source in the feedback path of the op amp in order to keep its input terminals close to one of the supply rails. Postlayout simulations on a 1.2 V rail-to-rail op amp with 13 MHz GB are presented which verify the proposed scheme.', '저널': 'IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications', '저자': 'J Ramirez-Angulo, A Torralba, RG Carvajal, J Tombs', '전체 인용횟수': '67회 인용20002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202326445342523247342221', '페이지': '772-774', '학술 문서': 'Low-voltage CMOS operational amplifiers with wide input-output swing based on a novel schemeJ Ramirez-Angulo, A Torralba, RG Carvajal, J Tombs\\xa0- IEEE Transactions on Circuits and Systems I\\xa0…, 200067회 인용 관련 학술자료 전체 4개의 버전 ', '호': '5'}, title='Low-voltage CMOS operational amplifiers with wide input-output swing based on a novel scheme', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'ASIC implementation of a digital tachometer with high precision in a wide speed range': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1996/12', '게시자': 'IEEE', '권': '43', '설명': 'A common method in adjustable speed drives uses an incremental shaft encoder and an electronic circuit for velocity estimation. The usual method of counting pulses coming from the encoder in a fixed period of time produces a high-precision velocity estimate in the high-speed range. High precision in the low-speed range can be achieved measuring the elapsed time between two successive pulses coming from the encoder. In this paper, a mixed method that combines the best of the two previously mentioned approaches has been implemented using a simple electronic circuit based on one field-programmable gate array (FPGA) and one read-only memory (ROM).', '저널': 'IEEE Transactions on Industrial Electronics', '저자': 'Eduardo Galvan, Antonio Torralba, Leopoldo García Franquelo', '전체 인용횟수': '66회 인용19981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023153133141434363334321212', '페이지': '655-660', '학술 문서': 'ASIC implementation of a digital tachometer with high precision in a wide speed rangeE Galvan, A Torralba, LG Franquelo\\xa0- IEEE Transactions on Industrial Electronics, 199666회 인용 관련 학술자료 전체 7개의 버전 ', '호': '6'}, title='ASIC implementation of a digital tachometer with high precision in a wide speed range', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Simple class-AB voltage follower with slew rate and bandwidth enhancement and no extra static power or supply requirements': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2006/7/6', '게시자': 'The Institution of Engineering & Technology', '권': '42', '설명': 'A simple modification of the MOS voltage follower is introduced, that provides it with efficient class-AB operation. The modified circuit has dynamic output currents and bandwidth that are essentially larger than the conventional MOS voltage follower. This is achieved with the same static power dissipation, very small additional circuit complexity and lower distortion. Experimental verification of all these characteristics is provided.', '저널': 'Electronics letters', '저자': 'J Ramirez-Angulo, AJ Lopez-Martin, RG Carvajal, A Torralba, M Jimenez', '전체 인용횟수': '64회 인용2007200820092010201120122013201420152016201720182019202020212022202334542324624534552', '페이지': '1', '학술 문서': 'Simple class-AB voltage follower with slew rate and bandwidth enhancement and no extra static power or supply requirementsJ Ramirez-Angulo, AJ Lopez-Martin, RG Carvajal…\\xa0- Electronics letters, 200664회 인용 관련 학술자료 전체 6개의 버전 ', '호': '14'}, title='Simple class-AB voltage follower with slew rate and bandwidth enhancement and no extra static power or supply requirements', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A new class AB differential input stage for implementation of low-voltage high slew rate op amps and linear transconductors': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2001/5/6', '게시자': 'IEEE', '권': '1', '설명': 'A new class AB differential stage that operates with a single supply voltage of less than two transistor threshold voltages is introduced. This circuit has utilization in high slew rate one stage op amps, two stage op amps with class AB input and output stages and linear transconductors. The circuit was verified with simulations and experimentally. It it is shown to have lower voltage supply requirements than other commonly used structures reported in literature.', '저자': 'Jaime Ramirez-Angulo, Ramon Gonzalez-Carvajal, Antonio Torralba, Carlos Nieva', '전체 인용횟수': '64회 인용20012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023141846111132243751621', '컨퍼런스': 'ISCAS 2001. The 2001 IEEE International Symposium on Circuits and Systems (Cat. No. 01CH37196)', '페이지': '671-674', '학술 문서': 'A new class AB differential input stage for implementation of low-voltage high slew rate op amps and linear transconductorsJ Ramirez-Angulo, R Gonzalez-Carvajal, A Torralba…\\xa0- ISCAS 2001. The 2001 IEEE International Symposium\\xa0…, 200164회 인용 관련 학술자료 전체 8개의 버전 '}, title='A new class AB differential input stage for implementation of low-voltage high slew rate op amps and linear transconductors', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'New output stage for low supply voltage, high-performance CMOS current mirrors': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2003/5/25', '게시자': 'IEEE', '권': '1', '설명': 'This paper presents a new simple output stage for CMOS current mirrors. The stage is a new low-voltage regulated cascode circuit, which achieves a very high output impedance and accurate current copy when combined with a suitable input stage. A 122 MHz bandwidth with 1 V supply voltage has been obtained using a standard 0.35 /spl mu/m CMOS technology.', '저자': 'A Torralba, Ramón González Carvajal, Fernando Muñoz, Jaime Ramirez-Angulo', '전체 인용횟수': '58회 인용200320042005200620072008200920102011201220132014201520162017201820192020202120223232211513456311114', '컨퍼런스': \"Proceedings of the 2003 International Symposium on Circuits and Systems, 2003. ISCAS'03.\", '페이지': 'I-I', '학술 문서': 'New output stage for low supply voltage, high-performance CMOS current mirrorsA Torralba, RG Carvajal, F Muñoz, J Ramirez-Angulo\\xa0- Proceedings of the 2003 International Symposium on\\xa0…, 200358회 인용 관련 학술자료 전체 5개의 버전 '}, title='New output stage for low supply voltage, high-performance CMOS current mirrors', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Tunable linear MOS resistors using quasi-floating-gate techniques': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009/1/19', '게시자': 'IEEE', '권': '56', '설명': 'A family of tunable MOS resistors based on quasi-floating-gate (QFG) transistors biased in the triode region is analyzed in this paper. From the study results, a new device that outperforms previous implementations, is presented. By means of a capacitive divider, the ac component of the drain-to-source voltage scaled with a factor alpha les 1 is added to the gate-to-source voltage leading to a cancellation of the nonlinear terms. The effect of alpha on resistor linearity is analytically studied. Simulation results are also provided for different technologies. Finally, a complete transconductor has been built which preserves the linearity of the MOS resistor. Three versions of the transconductor have been fabricated for different values of alpha (alpha = 0, 0.5, and 1) in a 0.5 mum CMOS technology with plusmn1.65-V supply voltage. Experimental results show (for alpha = 1 ) a THD of - 57 dB (HD2=-70 dB) at 1 MHz for 2-V\\xa0…', '저널': 'IEEE Transactions on Circuits and Systems II: Express Briefs', '저자': 'A Torralba, C Lujan-Martinez, Roman G Carvajal, J Galan, Melita Pennisi, Jaime Ramirez-Angulo, A Lopez-Martin', '전체 인용횟수': '53회 인용200920102011201220132014201520162017201820192020202120222023233954424344321', '페이지': '41-45', '학술 문서': 'Tunable linear MOS resistors using quasi-floating-gate techniquesA Torralba, C Lujan-Martinez, RG Carvajal, J Galan…\\xa0- IEEE Transactions on Circuits and Systems II: Express\\xa0…, 200953회 인용 관련 학술자료 전체 3개의 버전 ', '호': '1'}, title='Tunable linear MOS resistors using quasi-floating-gate techniques', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Online object tracking: A benchmark': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013', '설명': 'Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.', '저자': 'Yi Wu, Jongwoo Lim, Ming-Hsuan Yang', '전체 인용횟수': '6824회 인용2013201420152016201720182019202020212022202324196449669680897947855823694508', '컨퍼런스': 'IEEE Conference on Computer Vision and Pattern Recognition', '페이지': '2411-2418', '학술 문서': 'Online object tracking: A benchmarkY Wu, J Lim, MH Yang\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20136817회 인용 관련 학술자료 전체 37개의 버전 Online object tracking: A benchmark [c]. computer vision and pattern recogniton (cvpr)*Y Wu, J Lim, MH Yang\\xa0- 2013 IEEE Conference on, IEEE, 20137회 인용 관련 학술자료 Online object tracking: A benchmark supplemental material*W Yi, J Lim, M Yang\\xa0- IEEE, 20135회 인용 관련 학술자료 '}, title='Online object tracking: A benchmark', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Detecting Faces in Images: A Survey': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2002/1', '권': '24', '설명': 'Images containing faces are essential to intelligent vision-based human-computer interaction, and research efforts in face processing include face recognition, face tracking, pose estimation and expression recognition. However, many reported methods assume that the faces in an image or an image sequence have been identified and localized. To build fully automated systems that analyze the information contained in face images, robust and efficient face detection algorithms are required. Given a single image, the goal of face detection is to identify all image regions which contain a face, regardless of its 3D position, orientation and lighting conditions. Such a problem is challenging because faces are non-rigid and have a high degree of variability in size, shape, color and texture. Numerous techniques have been developed to detect faces in a single image, and the purpose of this paper is to categorize and\\xa0…', '저널': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', '저자': 'Ming-Hsuan Yang, David J Kriegman, Narendra Ahuja', '전체 인용횟수': '6040회 인용2001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202318942313254133584064254284013733913833372672331731751541059710360', '페이지': '34 - 58', '학술 문서': 'Detecting faces in images: A surveyMH Yang, DJ Kriegman, N Ahuja\\xa0- IEEE Transactions on pattern analysis and machine\\xa0…, 20026003회 인용 관련 학술자료 전체 14개의 버전 Member, Ieee, David J*MH Yang\\xa0- Kriegman, Senior Member, IEEE, and Narendra Ahuja\\xa0…, 200227회 인용 관련 학술자료 Kriegman, and Narendra Ahuja,\" Detecting faces in images: A survey,\"*MH Yang, J David\\xa0- IEEE Trans. Pattern Anal. Mach. Inteli, 200218회 인용 관련 학술자료 J. & AHUJA*MH Yang, D Kriegman\\xa0- NARENDRA:\" Detecting faces in images: a survey\" en\\xa0…, 20029회 인용 관련 학술자료 Detecting Faces i n Images: A Survey*MH Yang, DJ Kriegman, N Ahuja\\xa0- IEEE Trans. on Pattern Analysis andMachine\\xa0…3회 인용 관련 학술자료 KD-J., AN, Detecting faces in images: a survey, Pattern Analysis and Machine Intelligence*MH Yang\\xa0- IEEE Transactions on, 20022회 인용 관련 학술자료 ', '호': '1'}, title='Detecting Faces in Images: A Survey', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Incremental learning for robust visual tracking': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2008/5/1', '게시자': 'Springer US', '권': '77', '설명': '  Visual tracking, in essence, deals with non-stationary image streams that change over time. While most existing algorithms are able to track objects well in controlled environments, they usually fail in the presence of significant variation of the object’s appearance or surrounding illumination. One reason for such failures is that many algorithms employ fixed appearance models of the target. Such models are trained using only appearance data available before tracking begins, which in practice limits the range of appearances that are modeled, and ignores the large volume of information (such as shape changes or specific lighting conditions) that becomes available during tracking. In this paper, we present a tracking method that incrementally learns a low-dimensional subspace representation, efficiently adapting online to changes in the appearance of the target. The model update, based on incremental\\xa0…', '저널': 'International Journal of Computer Vision', '저자': 'David A Ross, Jongwoo Lim, Ruei-Sung Lin, Ming-Hsuan Yang', '전체 인용횟수': '3982회 인용2008200920102011201220132014201520162017201820192020202120222023255711514921928840951051939836929820815512380', '페이지': '125-141', '학술 문서': 'Incremental learning for robust visual trackingDA Ross, J Lim, RS Lin, MH Yang\\xa0- International journal of computer vision, 20083982회 인용 관련 학술자료 전체 35개의 버전 ', '호': '1-3'}, title='Incremental learning for robust visual tracking', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Saliency Detection via Graph-Based Manifold Ranking': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013', '설명': 'Most existing bottom-up methods measure the foreground saliency of a pixel or region based on its contrast within a local context or the entire image, whereas a few methods focus on segmenting out background regions and thereby salient objects. Instead of considering the contrast between the salient objects and their surrounding regions, we consider both foreground and background cues in a different way. We rank the similarity of the image elements (pixels or regions) with foreground cues or background cues via graph-based manifold ranking. The saliency of the image elements is defined based on their relevances to the given seeds or queries. We represent the image as a close-loop graph with superpixels as nodes. These nodes are ranked based on the similarity to background and foreground queries, based on affinity matrices. Saliency detection is carried out in a two-stage scheme to extract background regions and foreground salient objects efficiently. Experimental results on two large benchmark databases demonstrate the proposed method performs well when against the state-of-the-art methods in terms of accuracy and speed. We also create a more difficult benchmark database containing 5,172 images to test the proposed saliency model and make this database publicly available with this paper for further studies in the saliency field.', '저자': 'Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, Ming-Hsuan Yang', '전체 인용횟수': '2712회 인용20132014201520162017201820192020202120222023983198258317321337327313275238', '컨퍼런스': 'IEEE Conference on Computer Vision and Pattern Recognition', '페이지': '3166-3173', '학술 문서': 'Saliency detection via graph-based manifold rankingC Yang, L Zhang, H Lu, X Ruan, MH Yang\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20132712회 인용 관련 학술자료 전체 23개의 버전 '}, title='Saliency Detection via Graph-Based Manifold Ranking', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Robust Object Tracking with Online Multiple Instance Learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011', '권': '33', '설명': 'In this paper, we address the problem of tracking an object in a video given its location in the first frame and no other information. Recently, a class of tracking techniques called “tracking by detection” has been shown to give promising results at real-time speeds. These methods train a discriminative classifier in an online manner to separate the object from the background. This classifier bootstraps itself by using the current tracker state to extract positive and negative examples from the current frame. Slight inaccuracies in the tracker can therefore lead to incorrectly labeled training examples, which degrade the classifier and can cause drift. In this paper, we show that using Multiple Instance Learning (MIL) instead of traditional supervised learning avoids these problems and can therefore lead to a more robust tracker with fewer parameter tweaks. We propose a novel online MIL algorithm for object tracking that\\xa0…', '저널': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', '저자': 'Boris Babenko, Ming-Hsuan Yang, Serge Belongie', '전체 인용횟수': '2650회 인용201120122013201420152016201720182019202020212022202313541852903873523163162521721249149', '페이지': '1619-1632', '학술 문서': 'Robust object tracking with online multiple instance learningB Babenko, MH Yang, S Belongie\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20102650회 인용 관련 학술자료 전체 13개의 버전 ', '호': '8'}, title='Robust Object Tracking with Online Multiple Instance Learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep laplacian pyramid networks for fast and accurate super-resolution': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'Convolutional neural networks have recently demonstrated high-quality reconstruction for single-image super-resolution. In this paper, we propose the Laplacian Pyramid Super-Resolution Network (LapSRN) to progressively reconstruct the sub-band residuals of high-resolution images. At each pyramid level, our model takes coarse-resolution feature maps as input, predicts the high-frequency residuals, and uses transposed convolutions for upsampling to the finer level. Our method does not require the bicubic interpolation as the pre-processing step and thus dramatically reduces the computational complexity. We train the proposed LapSRN with deep supervision using a robust Charbonnier loss function and achieve high-quality reconstruction. Furthermore, our network generates multi-scale predictions in one feed-forward pass through the progressive reconstruction, thereby facilitates resource-aware applications. Extensive quantitative and qualitative evaluations on benchmark datasets show that the proposed algorithm performs favorably against the state-of-the-art methods in terms of speed and accuracy.', '저자': 'Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, Ming-Hsuan Yang', '전체 인용횟수': '2638회 인용201720182019202020212022202318211355454540566476', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '624-632', '학술 문서': 'Deep laplacian pyramid networks for fast and accurate super-resolutionWS Lai, JB Huang, N Ahuja, MH Yang\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20172638회 인용 관련 학술자료 전체 21개의 버전 '}, title='Deep laplacian pyramid networks for fast and accurate super-resolution', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Fast Compressive Tracking': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '게시자': 'IEEE', '권': '36', '설명': 'It is a challenging task to develop effective and efficient appearance models for robust object tracking due to factors such as pose variation, illumination change, occlusion, and motion blur. Existing online tracking algorithms often update models with samples from observations in recent frames. Despite much success has been demonstrated, numerous issues remain to be addressed. First, while these adaptive appearance models are data-dependent, there does not exist sufficient amount of data for online algorithms to learn at the outset. Second, online tracking algorithms often encounter the drift problems. As a result of self-taught learning, misaligned samples are likely to be added and degrade the appearance models. In this paper, we propose a simple yet effective and efficient tracking algorithm with an appearance model based on features extracted from a multiscale image feature space with dataindependent\\xa0…', '저널': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', '저자': 'Kaihua Zhang, Lei Zhang, M.-H. Yang', '전체 인용횟수': '2471회 인용20132014201520162017201820192020202120222023104243411443352333219149805231', '페이지': '2002-2015', '학술 문서': 'Fast compressive trackingK Zhang, L Zhang, MH Yang\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20142445회 인용 관련 학술자료 전체 31개의 버전 Fast compressive tracking*Z Kaihua, Z Lei, Y Ming-Hsuan\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 201451회 인용 관련 학술자료 ', '호': '10'}, title='Fast Compressive Tracking', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Real-time compressive tracking': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012/10/8', '저자': 'Kaihua Zhang, Lei Zhang, Ming-Hsuan Yang', '전체 인용횟수': '2445회 인용20132014201520162017201820192020202120222023104242407436347330215148805131', '컨퍼런스': 'European conference on Computer Vision', '페이지': '864-877', '학술 문서': 'Fast compressive tracking*K Zhang, L Zhang, MH Yang\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20142445회 인용 관련 학술자료 전체 31개의 버전 '}, title='Real-time compressive tracking', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Hierarchical convolutional features for visual tracking': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'Visual object tracking is challenging as target objects often undergo significant appearance changes caused by deformation, abrupt motion, background clutter and occlusion. In this paper, we exploit features extracted from deep convolutional neural networks trained on object recognition datasets to improve tracking accuracy and robustness. The outputs of the last convolutional layers encode the semantic information of targets and such representations are robust to significant appearance variations. However, their spatial resolution is too coarse to precisely localize targets. In contrast, earlier convolutional layers provide more precise localization but are less invariant to appearance changes. We interpret the hierarchies of convolutional layers as a nonlinear counterpart of an image pyramid representation and exploit these multiple levels of abstraction for visual tracking. Specifically, we adaptively learn correlation filters on each convolutional layer to encode the target appearance. We hierarchically infer the maximum response of each layer to locate targets. Extensive experimental results on a largescale benchmark dataset show that the proposed algorithm performs favorably against state-of-the-art methods.', '저자': 'Chao Ma, Jia-Bin Huang, Xiaokang Yang, Ming-Hsuan Yang', '전체 인용횟수': '2099회 인용2016201720182019202020212022202363205389439342300202133', '컨퍼런스': 'IEEE International Conference on Computer Vision', '페이지': '3074-3082', '학술 문서': 'Hierarchical convolutional features for visual trackingC Ma, JB Huang, X Yang, MH Yang\\xa0- Proceedings of the IEEE international conference on\\xa0…, 20152099회 인용 관련 학술자료 전체 18개의 버전 '}, title='Hierarchical convolutional features for visual tracking', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Res2net: A new multi-scale backbone architecture': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019/8/30', '게시자': 'IEEE', '설명': 'Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over\\xa0…', '저널': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', '저자': 'Shanghua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang, Philip HS Torr', '전체 인용횟수': '2018회 인용2019202020212022202328164431670711', '학술 문서': 'Res2net: A new multi-scale backbone architectureSH Gao, MM Cheng, K Zhao, XY Zhang, MH Yang…\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20192018회 인용 관련 학술자료 전체 15개의 버전 '}, title='Res2net: A new multi-scale backbone architecture', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Single image dehazing via multi-scale convolutional neural networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': ' The performance of existing image dehazing methods is limited by hand-designed features, such as the dark channel, color disparity and maximum contrast, with complex fusion schemes. In this paper, we propose a multi-scale deep neural network for single-image dehazing by learning the mapping between hazy images and their corresponding transmission maps. The proposed algorithm consists of a coarse-scale net which predicts a holistic transmission map based on the entire image, and a fine-scale net which refines results locally. To train the multi-scale deep network, we synthesize a dataset comprised of hazy images and corresponding transmission maps based on the NYU Depth dataset. Extensive experiments demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods on both synthetic and real-world images in terms of quality and speed.', '저자': 'Wenqi Ren, Si Liu, Hua Zhang, Jinshan Pan, Xiaochun Cao, Ming-Hsuan Yang', '전체 인용횟수': '1618회 인용201720182019202020212022202341162223269309326281', '컨퍼런스': 'European conference on Computer Vision', '페이지': '154-169', '학술 문서': 'Single image dehazing via multi-scale convolutional neural networksW Ren, S Liu, H Zhang, J Pan, X Cao, MH Yang\\xa0- Computer Vision–ECCV 2016: 14th European\\xa0…, 20161618회 인용 관련 학술자료 전체 7개의 버전 '}, title='Single image dehazing via multi-scale convolutional neural networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Visual tracking via adaptive structural local sparse appearance model': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012/6/16', '게시자': 'IEEE', '설명': 'Sparse representation has been applied to visual tracking by finding the best candidate with minimal reconstruction error using target templates. However most sparse representation based trackers only consider the holistic representation and do not make full use of the sparse coefficients to discriminate between the target and the background, and hence may fail with more possibility when there is similar object or occlusion in the scene. In this paper we develop a simple yet robust tracking method based on the structural local sparse appearance model. This representation exploits both partial information and spatial information of the target based on a novel alignment-pooling method. The similarity obtained by pooling across the local patches helps not only locate the target more accurately but also handle occlusion. In addition, we employ a template update strategy which combines incremental subspace learning\\xa0…', '저자': 'Xu Jia, Huchuan Lu, Ming-Hsuan Yang', '전체 인용횟수': '1599회 인용201220132014201520162017201820192020202120222023959133245281219209169106725222', '컨퍼런스': '2012 IEEE Conference on computer vision and pattern recognition', '페이지': '1822-1829', '학술 문서': 'Visual tracking via adaptive structural local sparse appearance modelX Jia, H Lu, MH Yang\\xa0- 2012 IEEE Conference on computer vision and pattern\\xa0…, 20121599회 인용 관련 학술자료 전체 19개의 버전 '}, title='Visual tracking via adaptive structural local sparse appearance model', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning to adapt structured output space for semantic segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/2/28', '설명': 'Convolutional neural network-based approaches for semantic segmentation rely on supervision with pixel-level ground truth, but may not generalize well to unseen image domains. As the labeling process is tedious and labor intensive, developing algorithms that can adapt source ground truth labels to the target domain is of great interest. In this paper, we propose an adversarial learning method for domain adaptation in the context of semantic segmentation. Considering semantic segmentations as structured outputs that contain spatial similarities between the source and target domains, we adopt adversarial learning in the output space. To further enhance the adapted model, we construct a multi-level adversarial network to effectively perform output space domain adaptation at different feature levels. To further improve our method, we utilize multi-level output adaptation based on feature maps at different levels. Extensive experiments and ablation study are conducted under various domain adaptation settings, including synthetic-to-real and cross-city scenarios. We show that the proposed method performs favorably against the state-of-the-art methods in terms of accuracy and visual quality.', '저자': 'Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, Manmohan Chandraker', '전체 인용횟수': '1448회 인용20182019202020212022202320125225328390355', '컨퍼런스': 'IEEE/CVF Conference on Computer Vision and Pattern Recognition', '학술 문서': 'Learning to adapt structured output space for semantic segmentationYH Tsai, WC Hung, S Schulter, K Sohn, MH Yang…\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20181447회 인용 관련 학술자료 전체 19개의 버전 1019 M. Chandraker,“Learning to adapt structured output space for semantic 1020 segmentation,”YH Tsai, WC Hung, S Schulter, K Sohn, MH Yang\\xa0- Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit, 20186회 인용 관련 학술자료 '}, title='Learning to adapt structured output space for semantic segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Robust object tracking via sparsity-based collaborative model': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012/6/16', '게시자': 'IEEE', '설명': 'In this paper we propose a robust object tracking algorithm using a collaborative model. As the main challenge for object tracking is to account for drastic appearance change, we propose a robust appearance model that exploits both holistic templates and local representations. We develop a sparsity-based discriminative classifier (SD-C) and a sparsity-based generative model (SGM). In the S-DC module, we introduce an effective method to compute the confidence value that assigns more weights to the foreground than the background. In the SGM module, we propose a novel histogram-based method that takes the spatial information of each patch into consideration with an occlusion handing scheme. Furthermore, the update scheme considers both the latest observations and the original template, thereby enabling the tracker to deal with appearance change effectively and alleviate the drift problem. Numerous\\xa0…', '저자': 'Wei Zhong, Huchuan Lu, Ming-Hsuan Yang', '전체 인용횟수': '1306회 인용20122013201420152016201720182019202020212022202375211521125518616911680373713', '컨퍼런스': '2012 IEEE Conference on Computer vision and pattern recognition', '페이지': '1838-1845', '학술 문서': 'Robust object tracking via sparsity-based collaborative modelW Zhong, H Lu, MH Yang\\xa0- 2012 IEEE Conference on Computer vision and pattern\\xa0…, 20121306회 인용 관련 학술자료 전체 15개의 버전 '}, title='Robust object tracking via sparsity-based collaborative model', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Diverse image-to-image translation via disentangled representations': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/8/2', '권': '1', '설명': 'Image-to-image translation aims to learn the mapping between two visual domains. There are two main challenges for many applications: 1) the lack of aligned training pairs and 2) multiple possible outputs from a single input image. In this work, we present an approach based on disentangled representation for producing diverse outputs without paired training images. To achieve diversity, we propose to embed images onto two spaces: a domain-invariant content space capturing shared information across domains and a domain-specific attribute space. Using the disentangled features as inputs greatly reduces mode collapse. To handle unpaired training data, we introduce a novel cross-cycle consistency loss. Qualitative results show that our model can generate diverse and realistic images on a wide range of tasks. We validate the effectiveness of our approach through extensive evaluation.', '저자': 'Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Singh, Ming-Hsuan Yang', '전체 인용횟수': '1228회 인용20182019202020212022202313158257302285209', '컨퍼런스': 'European Conference on Computer Vision', '페이지': '36-52', '학술 문서': 'Diverse image-to-image translation via disentangled representationsHY Lee, HY Tseng, JB Huang, M Singh, MH Yang\\xa0- Proceedings of the European conference on computer\\xa0…, 20181228회 인용 관련 학술자료 전체 17개의 버전 '}, title='Diverse image-to-image translation via disentangled representations', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Face recognition using kernel methods': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2002', '설명': 'Principal Component Analysis and Fisher Linear Discriminant methods have demonstrated their success in face detection, recog (cid: 173) nition, and tracking. The representation in these subspace methods is based on second order statistics of the image set, and does not address higher order statistical dependencies such as the relation (cid: 173) ships among three or more pixels. Recently Higher Order Statistics and Independent Component Analysis (ICA) have been used as in (cid: 173) formative low dimensional representations for visual recognition. In this paper, we investigate the use of Kernel Principal Compo (cid: 173) nent Analysis and Kernel Fisher Linear Discriminant for learning low dimensional representations for face recognition, which we call Kernel Eigenface and Kernel Fisherface methods. While Eigenface and Fisherface methods aim to find projection directions based on the second order correlation of samples, Kernel Eigenface and Ker (cid: 173) nel Fisherface methods provide generalizations which take higher order correlations into account. We compare the performance of kernel methods with Eigenface, Fisherface and ICA-based meth (cid: 173) ods for face recognition with variation in pose, scale, lighting and expression. Experimental results show that kernel methods pro (cid: 173) vide better representations and achieve lower error rates for face recognition.', '저자': 'Ming-Hsuan Yang', '전체 인용횟수': '1181회 인용2001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202349203974849691857678816563615638324431211011', '컨퍼런스': 'Advances in Neural Informaiton Processing Systems', '페이지': '215-220', '학술 문서': 'Face recognition using kernel methodsMH Yang\\xa0- Advances in Neural Information Processing Systems, 20011134회 인용 관련 학술자료 전체 20개의 버전 Face recognition using kernel methodsMH Yang\\xa0- Advances in neural information processing systems, 200266회 인용 관련 학술자료 전체 9개의 버전 '}, title='Face recognition using kernel methods', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Long-term correlation tracking': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'In this paper, we address the problem of long-term visual tracking where the target objects undergo significant appearance variation due to deformation, abrupt motion, heavy occlusion and out-of-the-view. In this setting, we decompose the task of tracking into translation and scale estimation of objects. We show that the correlation between temporal context considerably improves the accuracy and reliability for translation estimation, and it is effective to learn the discriminative correlation filters from the most confident frames to estimate the scale change. In addition, we train an online random fern classifier to re-detect objects in case of tracking failure. Extensive experimental results on large-scale benchmark datasets show that the proposed algorithm performs favorably against the state-of-the-art methods in terms of efficiency, accuracy, and robustness.', '저자': 'Chao Ma, Xiaokang Yang, Chongyang Zhang, Ming-Hsuan Yang', '전체 인용횟수': '1148회 인용2015201620172018201920202021202220239571272342531811309151', '컨퍼런스': 'IEEE Conference on Computer Vision and Pattern Recognition', '페이지': '5388-5396', '학술 문서': 'Long-term correlation trackingC Ma, X Yang, C Zhang, MH Yang\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20151148회 인용 관련 학술자료 전체 18개의 버전 '}, title='Long-term correlation tracking', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Clinically applicable deep learning for diagnosis and referral in retinal disease': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/9', '게시자': 'Nature Publishing Group', '권': '24', '설명': 'The volume and complexity of diagnostic imaging is increasing at a pace faster than the availability of human expertise to interpret it. Artificial intelligence has shown great promise in classifying two-dimensional photographs of some common diseases and typically relies on databases of millions of annotated images. Until now, the challenge of reaching the performance of expert clinicians in a real-world clinical pathway with three-dimensional diagnostic scans has remained unsolved. Here, we apply a novel deep learning architecture to a clinically heterogeneous set of three-dimensional optical coherence tomography scans from patients referred to a major eye hospital. We demonstrate performance in making a referral recommendation that reaches or exceeds that of experts on a range of sight-threatening retinal diseases after training on only 14,884 scans. Moreover, we demonstrate that the tissue\\xa0…', '저널': 'Nature medicine', '저자': 'Jeffrey De Fauw, Joseph R Ledsam, Bernardino Romera-Paredes, Stanislav Nikolov, Nenad Tomasev, Sam Blackwell, Harry Askham, Xavier Glorot, Brendan O’Donoghue, Daniel Visentin, George Van Den Driessche, Balaji Lakshminarayanan, Clemens Meyer, Faith Mackinder, Simon Bouton, Kareem Ayoub, Reena Chopra, Dominic King, Alan Karthikesalingam, Cían O Hughes, Rosalind Raine, Julian Hughes, Dawn A Sim, Catherine Egan, Adnan Tufail, Hugh Montgomery, Demis Hassabis, Geraint Rees, Trevor Back, Peng T Khaw, Mustafa Suleyman, Julien Cornebise, Pearse A Keane, Olaf Ronneberger', '전체 인용횟수': '2074회 인용20182019202020212022202342301457465404363', '페이지': '1342-1350', '학술 문서': 'Clinically applicable deep learning for diagnosis and referral in retinal diseaseJ De Fauw, JR Ledsam, B Romera-Paredes, S Nikolov…\\xa0- Nature medicine, 20182074회 인용 관련 학술자료 전체 10개의 버전 ', '호': '9'}, title='Clinically applicable deep learning for diagnosis and referral in retinal disease', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Highly accurate protein structure prediction for the human proteome': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021/8/26', '게시자': 'Nature Publishing Group UK', '권': '596', '설명': 'Protein structures can provide invaluable information, both for reasoning about biological processes and for enabling interventions such as structure-based drug development or targeted mutagenesis. After decades of effort, 17% of the total residues in human protein sequences are covered by an experimentally determined structure. Here we markedly expand the structural coverage of the proteome by applying the state-of-the-art machine learning method, AlphaFold, at a scale that covers almost the entire human proteome (98.5% of human proteins). The resulting dataset covers 58% of residues with a confident prediction, of which a subset (36% of all residues) have very high confidence. We introduce several metrics developed by building on the AlphaFold model and use them to interpret the dataset, identifying strong multi-domain predictions as well as regions that are likely to be disordered. Finally, we provide\\xa0…', '저널': 'Nature', '저자': 'Kathryn Tunyasuvunakool, Jonas Adler, Zachary Wu, Tim Green, Michal Zielinski, Augustin Žídek, Alex Bridgland, Andrew Cowie, Clemens Meyer, Agata Laydon, Sameer Velankar, Gerard J Kleywegt, Alex Bateman, Richard Evans, Alexander Pritzel, Michael Figurnov, Olaf Ronneberger, Russ Bates, Simon AA Kohl, Anna Potapenko, Andrew J Ballard, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Ellen Clancy, David Reiman, Stig Petersen, Andrew W Senior, Koray Kavukcuoglu, Ewan Birney, Pushmeet Kohli, John Jumper, Demis Hassabis', '전체 인용횟수': '1687회 인용202120222023159753761', '페이지': '590-596', '학술 문서': 'Highly accurate protein structure prediction for the human proteomeK Tunyasuvunakool, J Adler, Z Wu, T Green, M Zielinski…\\xa0- Nature, 20211687회 인용 관련 학술자료 전체 10개의 버전 ', '호': '7873'}, title='Highly accurate protein structure prediction for the human proteome', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Medical image computing and computer-assisted intervention–MICCAI 2015': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/10/5', '게시자': 'Springer International Publishing', '권': '9351', '저널': 'Lecture Notes in Computer Science', '저자': 'Olaf Ronneberger, Philipp Fischer, Thomas Brox, Nassir Navab, Joachim Hornegger, William M Wells, Alejandro F Frangi', '전체 인용횟수': '1658회 인용20172018201920202021202220231454110238410467360', '페이지': '234-241', '학술 문서': 'Medical image computing and computer-assisted intervention–MICCAI 2015O Ronneberger, P Fischer, T Brox, N Navab…\\xa0- Lecture Notes in Computer Science, 20151658회 인용 관련 학술자료 '}, title='Medical image computing and computer-assisted intervention–MICCAI 2015', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Protein complex prediction with AlphaFold-Multimer': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021/10/4', '게시자': 'Cold Spring Harbor Laboratory', '설명': 'While the vast majority of well-structured single protein chains can now be predicted to high accuracy due to the recent AlphaFold [1] model, the prediction of multi-chain protein complexes remains a challenge in many cases. In this work, we demonstrate that an AlphaFold model trained specifically for multimeric inputs of known stoichiometry, which we call AlphaFold-Multimer, significantly increases accuracy of predicted multimeric interfaces over input-adapted single-chain AlphaFold while maintaining high intra-chain accuracy. On a benchmark dataset of 17 heterodimer proteins without templates (introduced in [2]) we achieve at least medium accuracy (DockQ [3]≥0.49) on 13 targets and high accuracy (DockQ≥0.8) on 7 targets, compared to 9 targets of at least medium accuracy and 4 of high accuracy for the previous state of the art system (an AlphaFold-based system from [2]). We also predict structures for a large dataset of 4,446 recent protein complexes, from which we score all non-redundant interfaces with low template identity. For heteromeric interfaces we successfully predict the interface (DockQ≥0.23) in 70% of cases, and produce high accuracy predictions (DockQ≥0.8) in 26% of cases, an improvement of +27 and +14 percentage points over the flexible linker modification of AlphaFold [4] respectively. For homomeric interfaces we successfully predict the interface in 72% of cases, and produce high accuracy predictions in 36% of cases, an improvement of +8 and +7 percentage points respectively.', '저널': 'biorxiv', '저자': 'Richard Evans, Michael O’Neill, Alexander Pritzel, Natasha Antropova, Andrew Senior, Tim Green, Augustin Žídek, Russ Bates, Sam Blackwell, Jason Yim, Olaf Ronneberger, Sebastian Bodenstein, Michal Zielinski, Alex Bridgland, Anna Potapenko, Andrew Cowie, Kathryn Tunyasuvunakool, Rishub Jain, Ellen Clancy, Pushmeet Kohli, John Jumper, Demis Hassabis', '전체 인용횟수': '1173회 인용20212022202327434702', '페이지': '2021.10. 04.463034', '학술 문서': \"Protein complex prediction with AlphaFold-MultimerR Evans, M O'Neill, A Pritzel, N Antropova, A Senior…\\xa0- biorxiv, 20211173회 인용 관련 학술자료 전체 5개의 버전 \"}, title='Protein complex prediction with AlphaFold-Multimer', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A large annotated medical image dataset for the development and evaluation of segmentation algorithms': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019/2/25', '설명': 'Semantic segmentation of medical images aims to associate a pixel with a label in a medical image without human initialization. The success of semantic segmentation algorithms is contingent on the availability of high-quality imaging data with corresponding labels provided by experts. We sought to create a large collection of annotated medical image datasets of various clinically relevant anatomies available under open source license to facilitate the development of semantic segmentation algorithms. Such a resource would allow: 1) objective assessment of general-purpose segmentation methods through comprehensive benchmarking and 2) open and free access to medical image data for any researcher interested in the problem domain. Through a multi-institutional effort, we generated a large, curated dataset representative of several highly variable segmentation tasks that was used in a crowd-sourced challenge - the Medical Segmentation Decathlon held during the 2018 Medical Image Computing and Computer Aided Interventions Conference in Granada, Spain. Here, we describe these ten labeled image datasets so that these data may be effectively reused by the research community.', '저널': 'arXiv preprint arXiv:1902.09063', '저자': 'Amber L Simpson, Michela Antonelli, Spyridon Bakas, Michel Bilello, Keyvan Farahani, Bram Van Ginneken, Annette Kopp-Schneider, Bennett A Landman, Geert Litjens, Bjoern Menze, Olaf Ronneberger, Ronald M Summers, Patrick Bilic, Patrick F Christ, Richard KG Do, Marc Gollub, Jennifer Golia-Pernicka, Stephan H Heckers, William R Jarnagin, Maureen K McHugo, Sandy Napel, Eugene Vorontsov, Lena Maier-Hein, M Jorge Cardoso', '전체 인용횟수': '795회 인용201920202021202220232490197238239', '학술 문서': 'A large annotated medical image dataset for the development and evaluation of segmentation algorithmsAL Simpson, M Antonelli, S Bakas, M Bilello…\\xa0- arXiv preprint arXiv:1902.09063, 2019795회 인용 관련 학술자료 전체 2개의 버전 '}, title='A large annotated medical image dataset for the development and evaluation of segmentation algorithms', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Gland segmentation in colon histology images: The glas challenge contest': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/1/1', '게시자': 'Elsevier', '권': '35', '설명': 'Colorectal adenocarcinoma originating in intestinal glandular structures is the most common form of colon cancer. In clinical practice, the morphology of intestinal glands, including architectural appearance and glandular formation, is used by pathologists to inform prognosis and plan the treatment of individual patients. However, achieving good inter-observer as well as intra-observer reproducibility of cancer grading is still a major challenge in modern pathology. An automated approach which quantifies the morphology of glands is a solution to the problem.This paper provides an overview to the Gland Segmentation in Colon Histology Images Challenge Contest (GlaS) held at MICCAI’2015. Details of the challenge, including organization, dataset and evaluation criteria, are presented, along with the method descriptions and evaluation results from the top performing methods.', '저자': 'Korsuk Sirinukunwattana, Josien PW Pluim, Hao Chen, Xiaojuan Qi, Pheng-Ann Heng, Yun Bo Guo, Li Yang Wang, Bogdan J Matuszewski, Elia Bruni, Urko Sanchez, Anton Böhm, Olaf Ronneberger, Bassem Ben Cheikh, Daniel Racoceanu, Philipp Kainz, Michael Pfeiffer, Martin Urschler, David RJ Snead, Nasir M Rajpoot', '전체 인용횟수': '655회 인용2016201720182019202020212022202393140959496125161', '출처': 'Medical image analysis', '페이지': '489-502', '학술 문서': 'Gland segmentation in colon histology images: The glas challenge contestK Sirinukunwattana, JPW Pluim, H Chen, X Qi…\\xa0- Medical image analysis, 2017655회 인용 관련 학술자료 전체 30개의 버전 '}, title='Gland segmentation in colon histology images: The glas challenge contest', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'An objective comparison of cell-tracking algorithms': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/12/1', '게시자': 'Nature Publishing Group UK', '권': '14', '설명': \"We present a combined report on the results of three editions of the Cell Tracking Challenge, an ongoing initiative aimed at promoting the development and objective evaluation of cell segmentation and tracking algorithms. With 21 participating algorithms and a data repository consisting of 13 data sets from various microscopy modalities, the challenge displays today's state-of-the-art methodology in the field. We analyzed the challenge results using performance measures for segmentation and tracking that rank all participating methods. We also analyzed the performance of all of the algorithms in terms of biological measures and practical usability. Although some methods scored high in all technical aspects, none obtained fully correct solutions. We found that methods that either take prior information into account using learning strategies or analyze cells in a global spatiotemporal video context performed better\\xa0…\", '저널': 'Nature methods', '저자': 'Vladimír Ulman, Martin Maška, Klas EG Magnusson, Olaf Ronneberger, Carsten Haubold, Nathalie Harder, Pavel Matula, Petr Matula, David Svoboda, Miroslav Radojevic, Ihor Smal, Karl Rohr, Joakim Jaldén, Helen M Blau, Oleh Dzyubachyk, Boudewijn Lelieveldt, Pengdong Xiao, Yuexiang Li, Siu-Yeung Cho, Alexandre C Dufour, Jean-Christophe Olivo-Marin, Constantino C Reyes-Aldasoro, Jose A Solis-Lemus, Robert Bensch, Thomas Brox, Johannes Stegmaier, Ralf Mikut, Steffen Wolf, Fred A Hamprecht, Tiago Esteves, Pedro Quelhas, Ömer Demirel, Lars Malmström, Florian Jug, Pavel Tomancak, Erik Meijering, Arrate Muñoz-Barrutia, Michal Kozubek, Carlos Ortiz-de-Solorzano', '전체 인용횟수': '519회 인용201720182019202020212022202355176889911281', '페이지': '1141-1152', '학술 문서': 'An objective comparison of cell-tracking algorithmsV Ulman, M Maška, KEG Magnusson, O Ronneberger…\\xa0- Nature methods, 2017519회 인용 관련 학술자료 전체 25개의 버전 ', '호': '12'}, title='An objective comparison of cell-tracking algorithms', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A probabilistic u-net for segmentation of ambiguous images': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '권': '31', '설명': 'Many real-world vision problems suffer from inherent ambiguities. In clinical applications for example, it might not be clear from a CT scan alone which particular region is cancer tissue. Therefore a group of graders typically produces a set of diverse but plausible segmentations. We consider the task of learning a distribution over segmentations given an input. To this end we propose a generative segmentation model based on a combination of a U-Net with a conditional variational autoencoder that is capable of efficiently producing an unlimited number of plausible hypotheses. We show on a lung abnormalities segmentation task and on a Cityscapes segmentation task that our model reproduces the possible segmentation variants as well as the frequencies with which they occur, doing so significantly better than published approaches. These models could have a high impact in real-world applications, such as being used as clinical decision-making algorithms accounting for multiple plausible semantic segmentation hypotheses to provide possible diagnoses and recommend further actions to resolve the present ambiguities.', '저널': 'Advances in neural information processing systems', '저자': 'Simon Kohl, Bernardino Romera-Paredes, Clemens Meyer, Jeffrey De Fauw, Joseph R Ledsam, Klaus Maier-Hein, SM Eslami, Danilo Jimenez Rezende, Olaf Ronneberger', '전체 인용횟수': '495회 인용20182019202020212022202324193108127120', '학술 문서': 'A probabilistic u-net for segmentation of ambiguous imagesS Kohl, B Romera-Paredes, C Meyer, J De Fauw…\\xa0- Advances in neural information processing systems, 2018495회 인용 관련 학술자료 전체 12개의 버전 '}, title='A probabilistic u-net for segmentation of ambiguous images', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A new fate mapping system reveals context-dependent random or clonal expansion of microglia': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/6', '게시자': 'Nature Publishing Group US', '권': '20', '설명': 'Microglia constitute a highly specialized network of tissue-resident immune cells that is important for the control of tissue homeostasis and the resolution of diseases of the CNS. Little is known about how their spatial distribution is established and maintained in vivo. Here we establish a new multicolor fluorescence fate mapping system to monitor microglial dynamics during steady state and disease. Our findings suggest that microglia establish a dense network with regional differences, and the high regional turnover rates found challenge the universal concept of microglial longevity. Microglial self-renewal under steady state conditions constitutes a stochastic process. During pathology this randomness shifts to selected clonal microglial expansion. In the resolution phase, excess disease-associated microglia are removed by a dual mechanism of cell egress and apoptosis to re-establish the stable microglial network\\xa0…', '저널': 'Nature neuroscience', '저자': 'Tuan Leng Tay, Dominic Mai, Jana Dautzenberg, Francisco Fernández-Klett, Gen Lin, null Sagar, Moumita Datta, Anne Drougard, Thomas Stempfl, Alberto Ardura-Fabregat, Ori Staszewski, Anca Margineanu, Anje Sporbert, Lars M Steinmetz, J Andrew Pospisilik, Steffen Jung, Josef Priller, Dominic Gruen, Olaf Ronneberger, Marco Prinz', '전체 인용횟수': '494회 인용201720182019202020212022202320789581937550', '페이지': '793-803', '학술 문서': 'A new fate mapping system reveals context-dependent random or clonal expansion of microgliaTL Tay, D Mai, J Dautzenberg, F Fernández-Klett, G Lin…\\xa0- Nature neuroscience, 2017494회 인용 관련 학술자료 전체 13개의 버전 ', '호': '6'}, title='A new fate mapping system reveals context-dependent random or clonal expansion of microglia', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'U-Net: Convolutional networks for biomedical image segmentation. arXiv 2015': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '저널': 'arXiv preprint arXiv:1505.04597', '저자': 'Olaf Ronneberger, Philipp Fischer, Thomas Brox', '전체 인용횟수': '783회 인용2018201920202021202220235174985162455', '학술 문서': 'U-Net: Convolutional networks for biomedical image segmentation. arXiv 2015O Ronneberger, P Fischer, T Brox\\xa0- arXiv preprint arXiv:1505.04597, 2015783회 인용 관련 학술자료 '}, title='U-Net: Convolutional networks for biomedical image segmentation. arXiv 2015', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The medical segmentation decathlon': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2022/7/15', '게시자': 'Nature Publishing Group UK', '권': '13', '설명': 'International challenges have become the de facto standard for comparative assessment of image analysis algorithms. Although segmentation is the most widely investigated medical image processing task, the various challenges have been organized to focus only on specific clinical tasks. We organized the Medical Segmentation Decathlon (MSD)—a biomedical image analysis challenge, in which algorithms compete in a multitude of both tasks and modalities to investigate the hypothesis that a method capable of performing well on multiple tasks will generalize well to a previously unseen task and potentially outperform a custom-designed solution. MSD results confirmed this hypothesis, moreover, MSD winner continued generalizing well to a wide range of other clinical problems for the next two years. Three main conclusions can be drawn from this study: (1) state-of-the-art image segmentation algorithms\\xa0…', '저널': 'Nature communications', '저자': 'Michela Antonelli, Annika Reinke, Spyridon Bakas, Keyvan Farahani, Annette Kopp-Schneider, Bennett A Landman, Geert Litjens, Bjoern Menze, Olaf Ronneberger, Ronald M Summers, Bram Van Ginneken, Michel Bilello, Patrick Bilic, Patrick F Christ, Richard KG Do, Marc J Gollub, Stephan H Heckers, Henkjan Huisman, William R Jarnagin, Maureen K McHugo, Sandy Napel, Jennifer S Golia Pernicka, Kawal Rhode, Catalina Tobon-Gomez, Eugene Vorontsov, James A Meakin, Sebastien Ourselin, Manuel Wiesenfarth, Pablo Arbeláez, Byeonguk Bae, Sihong Chen, Laura Daza, Jianjiang Feng, Baochun He, Fabian Isensee, Yuanfeng Ji, Fucang Jia, Ildoo Kim, Klaus Maier-Hein, Dorit Merhof, Akshay Pai, Beomhee Park, Mathias Perslev, Ramin Rezaiifar, Oliver Rippel, Ignacio Sarasua, Wei Shen, Jaemin Son, Christian Wachinger, Liansheng Wang, Yan Wang, Yingda Xia, Daguang Xu, Zhanwei Xu, Yefeng Zheng, Amber L Simpson, Lena Maier-Hein, M Jorge Cardoso', '전체 인용횟수': '459회 인용20212022202322128306', '페이지': '4128', '학술 문서': 'The medical segmentation decathlonM Antonelli, A Reinke, S Bakas, K Farahani…\\xa0- Nature communications, 2022459회 인용 관련 학술자료 전체 17개의 버전 ', '호': '1'}, title='The medical segmentation decathlon', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Chemotaxonomic identification of single bacteria by micro-Raman spectroscopy: application to clean-room-relevant biological contaminations': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2005/3', '게시자': 'American Society for Microbiology', '권': '71', '설명': ' Microorganisms, such as bacteria, which might be present as contamination inside an industrial food or pharmaceutical clean room process need to be identified on short time scales in order to minimize possible health hazards as well as production downtimes causing financial deficits. Here we describe the first results of single-particle micro-Raman measurements in combination with a classification method, the so-called support vector machine technique, allowing for a fast, reliable, and nondestructive online identification method for single bacteria.', '저널': 'Applied and environmental microbiology', '저자': 'Petra Rösch, Michaela Harz, Michael Schmitt, Klaus-Dieter Peschke, Olaf Ronneberger, Hans Burkhardt, Hans-Walter Motzkus, Markus Lankers, Stefan Hofer, Hans Thiele, Jurgen Popp', '전체 인용횟수': '363회 인용200520062007200820092010201120122013201420152016201720182019202020212022202351721193239352326252113181631712106', '페이지': '1626-1637', '학술 문서': 'Chemotaxonomic identification of single bacteria by micro-Raman spectroscopy: application to clean-room-relevant biological contaminationsP Rösch, M Harz, M Schmitt, KD Peschke…\\xa0- Applied and environmental microbiology, 2005363회 인용 관련 학술자료 전체 25개의 버전 ', '호': '3'}, title='Chemotaxonomic identification of single bacteria by micro-Raman spectroscopy: application to clean-room-relevant biological contaminations', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Standardized evaluation of algorithms for computer-aided diagnosis of dementia based on structural MRI: the CADDementia challenge': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/5/1', '게시자': 'Academic Press', '권': '111', '설명': \"Algorithms for computer-aided diagnosis of dementia based on structural MRI have demonstrated high performance in the literature, but are difficult to compare as different data sets and methodology were used for evaluation. In addition, it is unclear how the algorithms would perform on previously unseen data, and thus, how they would perform in clinical practice when there is no real opportunity to adapt the algorithm to the data at hand. To address these comparability, generalizability and clinical applicability issues, we organized a grand challenge that aimed to objectively compare algorithms based on a clinically representative multi-center data set. Using clinical practice as the starting point, the goal was to reproduce the clinical diagnosis. Therefore, we evaluated algorithms for multi-class classification of three diagnostic groups: patients with probable Alzheimer's disease, patients with mild cognitive impairment\\xa0…\", '저널': 'NeuroImage', '저자': \"Esther E Bron, Marion Smits, Wiesje M Van Der Flier, Hugo Vrenken, Frederik Barkhof, Philip Scheltens, Janne M Papma, Rebecca ME Steketee, Carolina Méndez Orellana, Rozanna Meijboom, Madalena Pinto, Joana R Meireles, Carolina Garrett, António J Bastos-Leite, Ahmed Abdulkadir, Olaf Ronneberger, Nicola Amoroso, Roberto Bellotti, David Cárdenas-Peña, Andrés M Álvarez-Meza, Chester V Dolph, Khan M Iftekharuddin, Simon F Eskildsen, Pierrick Coupé, Vladimir S Fonov, Katja Franke, Christian Gaser, Christian Ledig, Ricardo Guerrero, Tong Tong, Katherine R Gray, Elaheh Moradi, Jussi Tohka, Alexandre Routier, Stanley Durrleman, Alessia Sarica, Giuseppe Di Fatta, Francesco Sensi, Andrea Chincarini, Garry M Smith, Zhivko V Stoyanov, Lauge Sørensen, Mads Nielsen, Sabina Tangaro, Paolo Inglese, Christian Wachinger, Martin Reuter, John C van Swieten, Wiro J Niessen, Stefan Klein, Alzheimer's Disease Neuroimaging Initiative\", '전체 인용횟수': '341회 인용20142015201620172018201920202021202220231133552444639413233', '페이지': '562-579', '학술 문서': 'Standardized evaluation of algorithms for computer-aided diagnosis of dementia based on structural MRI: the CADDementia challengeEE Bron, M Smits, WM Van Der Flier, H Vrenken…\\xa0- NeuroImage, 2015341회 인용 관련 학술자료 전체 40개의 버전 '}, title='Standardized evaluation of algorithms for computer-aided diagnosis of dementia based on structural MRI: the CADDementia challenge', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A benchmark for comparison of dental radiography analysis algorithms': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/7/1', '게시자': 'Elsevier', '권': '31', '설명': 'Dental radiography plays an important role in clinical diagnosis, treatment and surgery. In recent years, efforts have been made on developing computerized dental X-ray image analysis systems for clinical usages. A novel framework for objective evaluation of automatic dental radiography analysis algorithms has been established under the auspices of the IEEE International Symposium on Biomedical Imaging 2015 Bitewing Radiography Caries Detection Challenge and Cephalometric X-ray Image Analysis Challenge. In this article, we present the datasets, methods and results of the challenge and lay down the principles for future uses of this benchmark. The main contributions of the challenge include the creation of the dental anatomy data repository of bitewing radiographs, the creation of the anatomical abnormality classification data repository of cephalometric radiographs, and the definition of objective\\xa0…', '저널': 'Medical image analysis', '저자': 'Ching-Wei Wang, Cheng-Ta Huang, Jia-Hong Lee, Chung-Hsing Li, Sheng-Wei Chang, Ming-Jhih Siao, Tat-Ming Lai, Bulat Ibragimov, Tomaž Vrtovec, Olaf Ronneberger, Philipp Fischer, Tim F Cootes, Claudia Lindner', '전체 인용횟수': '309회 인용2016201720182019202020212022202337253048577462', '페이지': '63-76', '학술 문서': 'A benchmark for comparison of dental radiography analysis algorithmsCW Wang, CT Huang, JH Lee, CH Li, SW Chang…\\xa0- Medical image analysis, 2016309회 인용 관련 학술자료 전체 10개의 버전 '}, title='A benchmark for comparison of dental radiography analysis algorithms', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep learning to achieve clinically applicable segmentation of head and neck anatomy for radiotherapy': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/9/12', '설명': \"Over half a million individuals are diagnosed with head and neck cancer each year worldwide. Radiotherapy is an important curative treatment for this disease, but it requires manual time consuming delineation of radio-sensitive organs at risk (OARs). This planning process can delay treatment, while also introducing inter-operator variability with resulting downstream radiation dose differences. While auto-segmentation algorithms offer a potentially time-saving solution, the challenges in defining, quantifying and achieving expert performance remain. Adopting a deep learning approach, we demonstrate a 3D U-Net architecture that achieves expert-level performance in delineating 21 distinct head and neck OARs commonly segmented in clinical practice. The model was trained on a dataset of 663 deidentified computed tomography (CT) scans acquired in routine clinical practice and with both segmentations taken from clinical practice and segmentations created by experienced radiographers as part of this research, all in accordance with consensus OAR definitions. We demonstrate the model's clinical applicability by assessing its performance on a test set of 21 CT scans from clinical practice, each with the 21 OARs segmented by two independent experts. We also introduce surface Dice similarity coefficient (surface DSC), a new metric for the comparison of organ delineation, to quantify deviation between OAR surface contours rather than volumes, better reflecting the clinical task of correcting errors in the automated organ segmentations. The model's generalisability is then demonstrated on two distinct open source datasets, reflecting different\\xa0…\", '저널': 'arXiv preprint arXiv:1809.04430', '저자': \"Stanislav Nikolov, Sam Blackwell, Alexei Zverovitch, Ruheena Mendes, Michelle Livne, Jeffrey De Fauw, Yojan Patel, Clemens Meyer, Harry Askham, Bernardino Romera-Paredes, Christopher Kelly, Alan Karthikesalingam, Carlton Chu, Dawn Carnell, Cheng Boon, Derek D'Souza, Syed Ali Moinuddin, Bethany Garie, Yasmin McQuinlan, Sarah Ireland, Kiarna Hampton, Krystle Fuller, Hugh Montgomery, Geraint Rees, Mustafa Suleyman, Trevor Back, Cían Hughes, Joseph R Ledsam, Olaf Ronneberger\", '전체 인용횟수': '295회 인용20182019202020212022202332562777552', '학술 문서': 'Deep learning to achieve clinically applicable segmentation of head and neck anatomy for radiotherapyS Nikolov, S Blackwell, A Zverovitch, R Mendes…\\xa0- arXiv preprint arXiv:1809.04430, 2018295회 인용 관련 학술자료 전체 6개의 버전 '}, title='Deep learning to achieve clinically applicable segmentation of head and neck anatomy for radiotherapy', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Comprehensive catecholaminergic projectome analysis reveals single-neuron integration of zebrafish ascending and descending dopaminergic systems': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011/1/25', '게시자': 'Nature Publishing Group UK', '권': '2', '설명': \"Essential components of animal behaviour are modulated by dopaminergic (DA) and noradrenergic circuitry. In this study, we reveal at cellular resolution the complete set of projections ('projectome') of every single type of DA and noradrenergio neurons in the central nervous system of zebrafish larvae. The most extensive DA projections are established by posterior tubercular otp-dependent neurons, with individual somata integrating the ascending DA system, the descending diencephalospinal, as well as the endohypothalamic circuitry. These findings suggest a major role in the modulation of physiology and behaviour for otp-dependent DA neurons, which correlate with the mammalian A11 group. We further identified an endogenous subpallial DA system that not only provides most of the local DA projections, but also connects to the ventral diencephalon. The catecholaminergic projectome map provides a\\xa0…\", '저널': 'Nature communications', '저자': 'Tuan Leng Tay, Olaf Ronneberger, Soojin Ryu, Roland Nitschke, Wolfgang Driever', '전체 인용횟수': '293회 인용20112012201320142015201620172018201920202021202220235192716222227182731292821', '페이지': '171', '학술 문서': 'Comprehensive catecholaminergic projectome analysis reveals single-neuron integration of zebrafish ascending and descending dopaminergic systemsTL Tay, O Ronneberger, S Ryu, R Nitschke, W Driever\\xa0- Nature communications, 2011293회 인용 관련 학술자료 전체 12개의 버전 ', '호': '1'}, title='Comprehensive catecholaminergic projectome analysis reveals single-neuron integration of zebrafish ascending and descending dopaminergic systems', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Combined Measurement of the Higgs Boson Mass in  Collisions at  and 8 TeV with the ATLAS and CMS Experiments': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/3/26', '설명': 'A measurement of the Higgs boson mass is presented based on the combined data samples of the ATLAS and CMS experiments at the CERN LHC in the  and  decay channels. The results are obtained from a simultaneous fit to the reconstructed invariant mass peaks in the two channels and for the two experiments. The measured masses from the individual channels and the two experiments are found to be consistent among themselves. The combined measured mass of the Higgs boson is .', '저널': 'arXiv preprint arXiv:1503.07589', '저자': 'CMS Collaborations', '전체 인용횟수': '2572회 인용20152016201720182019202020212022202323047247246730321316712985', '학술 문서': 'Combined Measurement of the Higgs Boson Mass in $ pp $ Collisions at $\\\\sqrt {s}= 7$ and 8 TeV with the ATLAS and CMS ExperimentsCMS Collaborations\\xa0- arXiv preprint arXiv:1503.07589, 20151164회 인용 관련 학술자료 전체 50개의 버전 Combined Measurement of the Higgs Boson Mass in p p Collisions at s= 7 and 8 TeV with the ATLAS and CMS ExperimentsG Aad, B Abbott, J Abdallah, R Aben, M Abolins…\\xa0- Physical review letters, 2015939회 인용 관련 학술자료 전체 157개의 버전 ATLAS and CMS collaborations*G Aad\\xa0- Phys. Rev. Lett, 2015770회 인용 관련 학술자료 '}, title='Combined Measurement of the Higgs Boson Mass in  Collisions at  and 8 TeV with the ATLAS and CMS Experiments', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Jet energy measurement and its systematic uncertainty in proton–proton collisions at \\xa0TeV with the ATLAS detector': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/1', '게시자': 'Springer Berlin Heidelberg', '권': '75', '설명': ' The jet energy scale (JES) and its systematic uncertainty are determined for jets measured with the ATLAS detector using proton–proton collision data with a centre-of-mass energy of \\xa0TeV corresponding to an integrated luminosity of  . Jets are reconstructed from energy deposits forming topological clusters of calorimeter cells using the anti- algorithm with distance parameters  or , and are calibrated using MC simulations. A residual JES correction is applied to account for differences between data and MC simulations. This correction and its systematic uncertainty are estimated using a combination of in situ techniques exploiting the transverse momentum balance between a jet and a reference object such as a photon or a  boson, for  and pseudorapidities . The effect of multiple proton–proton interactions is corrected for, and an uncertainty is evaluated using in situ techniques\\xa0…', '저널': 'The European Physical Journal C', '저자': 'Atlas Collaboration atlas. publications@ cern. ch, Georges Aad, T Abajyan, B Abbott, J Abdallah, S Abdel Khalek, O Abdinov, R Aben, B Abi, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, TN Addy, J Adelman, S Adomeit, T Adye, S Aefsky, T Agatonovic-Jovin, JA Aguilar-Saavedra, M Agustoni, SP Ahlen, A Ahmad, F Ahmadov, G Aielli, TPA Åkesson, G Akimoto, AV Akimov, MA Alam, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, F Alessandria, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, M Aliev, G Alimonti, L Alio, J Alison, BMM Allbrooke, LJ Allison, PP Allport, SE Allwood-Spiers, J Almond, A Aloisio, R Alon, A Alonso, F Alonso, A Altheimer, B Alvarez Gonzalez, MG Alviggi, K Amako, Y Amaral Coutinho, C Amelung, VV Ammosov, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, S Angelidakis, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, ATH Arce, S Arfaoui, JF Arguin, S Argyropoulos, E Arik, M Arik, AJ Armbruster, O Arnaez, V Arnal, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, S Ask, B Åsman, L Asquith, K Assamagan, R Astalos, A Astbury, M Atkinson, NB Atlay, B Auerbach, E Auge, K Augsten, M Aurousseau, G Avolio, G Azuelos, Y Azuma, MA Baak, C Bacci, AM Bach, H Bachacou, K Bachas, M Backes, M Backhaus, J Backus Mayes, E Badescu, P Bagiacchi, P Bagnaia, Y Bai, DC Bailey, T Bain, JT Baines, OK Baker, S Baker, P Balek, F Balli, E Banas, Sw Banerjee, D Banfi, A Bangert, V Bansal', '전체 인용횟수': '2063회 인용20142015201620172018201920202021202220237650445945224713052603232', '페이지': '1-101', '학술 문서': 'Jet energy measurement and its systematic uncertainty in proton–proton collisions at s= 7 s= 7 TeV with the ATLAS detectorAtlas Collaboration atlas. publications@ cern. ch…\\xa0- The European Physical Journal C, 20152063회 인용 관련 학술자료 전체 118개의 버전 Jet energy measurement and its systematic uncertainty in proton–proton collisions at√ s= 7 TeV with the ATLAS detectorMJ Alconada Verzini, F Alonso, XS Anduaga, MT Dova…\\xa0- The European Physical Journal C, 2015관련 학술자료 전체 2개의 버전 Jet energy measurement and its systematic uncertainty in proton-proton collisions at root s= 7 TeV with the ATLAS detector*MA Díaz - 2015관련 학술자료 전체 2개의 버전 '}, title='Jet energy measurement and its systematic uncertainty in proton–proton collisions at \\xa0TeV with the ATLAS detector', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Performance of b-jet identification in the ATLAS experiment': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/4/4', '게시자': 'IOP Publishing', '권': '11', '설명': 'The identification of jets containing b hadrons is important for the physics programme of the ATLAS experiment at the Large Hadron Collider. Several algorithms to identify jets containing b hadrons are described, ranging from those based on the reconstruction of an inclusive secondary vertex or the presence of tracks with large impact parameters to combined tagging algorithms making use of multi-variate discriminants. An independent b-tagging algorithm based on the reconstruction of muons inside jets as well as the b-tagging algorithm used in the online trigger are also presented. The b-jet tagging efficiency, the c-jet tagging efficiency and the mistag rate for light flavour jets in data have been measured with a number of complementary methods. The calibration results are presented as scale factors defined as the ratio of the efficiency (or mistag rate) in data to that in simulation. In the case of b jets, where more\\xa0…', '저널': 'Journal of instrumentation', '저자': 'Atlas Collaboration', '전체 인용횟수': '1285회 인용20162017201820192020202120222023132261407209111635735', '페이지': 'P04008', '학술 문서': 'Performance of b-jet identification in the ATLAS experimentAtlas Collaboration\\xa0- Journal of instrumentation, 20161285회 인용 관련 학술자료 전체 80개의 버전 ', '호': '04'}, title='Performance of b-jet identification in the ATLAS experiment', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Measurements of Higgs boson production and couplings in diboson final states with the ATLAS detector at the LHC': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/10/7', '게시자': 'North-Holland', '권': '726', '설명': 'Measurements are presented of production properties and couplings of the recently discovered Higgs boson using the decays into boson pairs, H→ γ γ, H→ Z Z⁎→ 4 ℓ and H→ W W⁎→ ℓ ν ℓ ν. The results are based on the complete pp collision data sample recorded by the ATLAS experiment at the CERN Large Hadron Collider at centre-of-mass energies of s= 7 TeV and s= 8 TeV, corresponding to an integrated luminosity of about 25 fb− 1. Evidence for Higgs boson production through vector-boson fusion is reported. Results of combined fits probing Higgs boson couplings to fermions and bosons, as well as anomalous contributions to loop-induced production and decay modes, are presented. All measurements are consistent with expectations for the Standard Model Higgs boson.', '저널': 'Physics Letters B', '저자': 'Georges Aad, Tatevik Abajyan, Brad Abbott, Jalal Abdallah, S Abdel Khalek, Rosemarie Aben, B Abi, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, TN Addy, J Adelman, S Adomeit, T Adye, S Aefsky, T Agatonovic-Jovin, JA Aguilar-Saavedra, M Agustoni, SP Ahlen, A Ahmad, M Ahsan, G Aielli, TPA Åkesson, G Akimoto, AV Akimov, MA Alam, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, F Alessandria, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, M Aliev, G Alimonti, L Alio, J Alison, BMM Allbrooke, LJ Allison, PP Allport, SE Allwood-Spiers, J Almond, A Aloisio, R Alon, A Alonso, F Alonso, A Altheimer, B Alvarez Gonzalez, MG Alviggi, K Amako, Y Amaral Coutinho, C Amelung, VV Ammosov, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, S Angelidakis, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, ATH Arce, S Arfaoui, JF Arguin, S Argyropoulos, E Arik, M Arik, AJ Armbruster, O Arnaez, V Arnal, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, S Ask, B Åsman, L Asquith, K Assamagan, R Astalos, A Astbury, M Atkinson, NB Atlay, B Auerbach, E Auge, K Augsten, M Aurousseau, G Avolio, D Axen, G Azuelos, Y Azuma, MA Baak, C Bacci, AM Bach, H Bachacou, K Bachas, M Backes, M Backhaus, J Backus Mayes, E Badescu, P Bagiacchi, P Bagnaia, Y Bai, DC Bailey, T Bain, JT Baines, OK Baker, S Baker, P Balek, F Balli, E Banas, Sw Banerjee, D Banfi, A Bangert, V Bansal, HS Bansil, L Barak', '전체 인용횟수': '1233회 인용201320142015201620172018201920202021202220239438334312998574228231511', '페이지': '88-119', '학술 문서': 'Measurements of Higgs boson production and couplings in diboson final states with the ATLAS detector at the LHCG Aad, T Abajyan, B Abbott, J Abdallah, SA Khalek…\\xa0- Physics Letters B, 20131233회 인용 관련 학술자료 전체 104개의 버전 ', '호': '1-3'}, title='Measurements of Higgs boson production and couplings in diboson final states with the ATLAS detector at the LHC', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The roostats project': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/9/6', '설명': 'RooStats is a project to create advanced statistical tools required for the analysis of LHC data, with emphasis on discoveries, confidence intervals, and combined measurements. The idea is to provide the major statistical techniques as a set of C++ classes with coherent interfaces, so that can be used on arbitrary model and datasets in a common way. The classes are built on top of the RooFit package, which provides functionality for easily creating probability models, for analysis combinations and for digital publications of the results. We will present in detail the design and the implementation of the different statistical methods of RooStats. We will describe the various classes for interval estimation and for hypothesis test depending on different statistical techniques such as those based on the likelihood function, or on frequentists or bayesian statistics. These methods can be applied in complex problems, including cases with multiple parameters of interest and various nuisance parameters.', '저널': 'arXiv preprint arXiv:1009.1003', '저자': 'Lorenzo Moneta, Kevin Belasco, Kyle Cranmer, Sven Kreiss, Alfio Lazzaro, Danilo Piparo, Gregory Schott, Wouter Verkerke, Matthias Wolf', '전체 인용횟수': '1150회 인용201020112012201320142015201620172018201920202021202220235336065728611112317411855936883', '학술 문서': 'The roostats projectL Moneta, K Belasco, K Cranmer, S Kreiss, A Lazzaro…\\xa0- arXiv preprint arXiv:1009.1003, 20101150회 인용 관련 학술자료 전체 10개의 버전 '}, title='The roostats project', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Measurements of the Higgs boson production and decay rates and coupling strengths using pp collision data at  s = 7 and 8\\xa0TeV in the ATLAS experiment': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/1', '게시자': 'Springer Berlin Heidelberg', '권': '76', '설명': 'Combined analyses of the Higgs boson production and decay rates as well as its coupling strengths to vector bosons and fermions are presented. The combinations include the results of the analyses of the and decay modes, and the constraints on the associated production with a pair of top quarks and on the off-shell coupling strengths of the Higgs boson. The results are based on the LHC proton-proton collision datasets, with integrated luminosities of up to 4.7 at TeV and 20.3 at TeV, recorded by the ATLAS detector in 2011 and 2012. Combining all production modes and decay channels, the measured signal yield, normalised to the Standard Model expectation, is. The observed Higgs boson production and decay rates are interpreted in a leading-order coupling framework, exploring a wide range of benchmark coupling models both with and without\\xa0…', '저널': 'The European Physical Journal C', '저자': 'Georges Aad, Brad Abbott, Jalal Abdallah, R Aben, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, J Adelman, S Adomeit, T Adye, AA Affolder, T Agatonovic-Jovin, JA Aguilar-Saavedra, SP Ahlen, F Ahmadov, G Aielli, H Akerstedt, TPA Åkesson, G Akimoto, AV Akimov, GL Alberghi, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, T Alexopoulos, M Alhroob, G Alimonti, L Alio, J Alison, SP Alkire, BMM Allbrooke, PP Allport, A Aloisio, A Alonso, F Alonso, C Alpigiani, A Altheimer, B Alvarez Gonzalez, D Álvarez Piqueras, MG Alviggi, BT Amadio, K Amako, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, JK Anders, KJ Anderson, A Andreazza, V Andrei, S Angelidakis, I Angelozzi, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, Nuno Anjos, A Annovi, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, G Arabidze, Y Arai, JP Araque, ATH Arce, FA Arduh, JF Arguin, S Argyropoulos, M Arik, AJ Armbruster, O Arnaez, V Arnal, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, A Ashkenazi, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, B Auerbach, K Augsten, M Aurousseau, G Avolio, B Axen, MK Ayoub, G Azuelos, MA Baak, AE Baas, C Bacci, H Bachacou, K Bachas, M Backes, M Backhaus, P Bagiacchi, P Bagnaia, Y Bai, T Bain, JT Baines, OK Baker, Petr Balek, T Balestri, F Balli, E Banas, Sw Banerjee, AAE Bannoura, HS Bansil, L Barak, EL Barberio, D Barberis, M Barbero, T Barillari, M Barisonzi, T Barklow, N Barlow, SL Barnes, BM Barnett, RM Barnett, Z Barnovska, A Baroncelli, G Barone', '전체 인용횟수': '932회 인용20152016201720182019202020212022202310421213122110046543123', '페이지': '1-51', '학술 문서': 'Measurements of the Higgs boson production and decay rates and coupling strengths using pp collision data at $$\\\\sqrt {s}= 7$$ s= 7 and 8 TeV in the ATLAS experimentG Aad, B Abbott, J Abdallah, R Aben, M Abolins…\\xa0- The European Physical Journal C, 2016928회 인용 관련 학술자료 전체 77개의 버전 Measurements of the Higgs boson production and decay rates and coupling strengths using pp collision data at $\\\\sqrt {s}= 7$ and 8 TeV in the ATLAS experimentG Aad, B Abbott, J Abdallah, O Abdinov, R Aben…\\xa0- Eur. Phys. JC, 20165회 인용 관련 학술자료 전체 31개의 버전 Measurements of the Higgs boson production and decay rates and coupling strengths using pp collision data at√ s= 7 and 8*ATLAS collaboration, G Aad\\xa0- TeV in the ATLAS experiment2회 인용 관련 학술자료 Measurements of the Higgs boson production and decay rates and coupling strengths using pp collision data at root s= 7 and 8 TeV in the ATLAS experimentATLAS Collaboration - 20161회 인용 관련 학술자료 Measurements of the Higgs boson production and decay rates and coupling strengths using pp collision data at [... formula...] and 8 TeV in the ATLAS experimentG Aad, B Abbott, J Abdallah, R Aben, M Abolins…\\xa0- The European Physical Journal. C, Particles and Fields, 20161회 인용 관련 학술자료 ', '호': '1'}, title='Measurements of the Higgs boson production and decay rates and coupling strengths using pp collision data at  s = 7 and 8\\xa0TeV in the ATLAS experiment', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Observation of Associated Near-Side and Away-Side Long-Range Correlations in  Proton-Lead Collisions with the ATLAS Detector': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/5/1', '게시자': 'American Physical Society', '권': '110', '설명': 'Two-particle correlations in relative azimuthal angle (Δ ϕ) and pseudorapidity (Δ η) are measured in s NN= 5.02 TeV p+ Pb collisions using the ATLAS detector at the LHC. The measurements are performed using approximately 1 μ b− 1 of data as a function of transverse momentum (p T) and the transverse energy (Σ E T Pb) summed over 3.1< η< 4.9 in the direction of the Pb beam. The correlation function, constructed from charged particles, exhibits a long-range (2<| Δ η|< 5)“near-side”(Δ ϕ∼ 0) correlation that grows rapidly with increasing Σ E T Pb. A long-range “away-side”(Δ ϕ∼ π) correlation, obtained by subtracting the expected contributions from recoiling dijets and other sources estimated using events with small Σ E T Pb, is found to match the near-side correlation in magnitude, shape (in Δ η and Δ ϕ) and Σ E T Pb dependence. The resultant Δ ϕ correlation is approximately symmetric about π/2, and is\\xa0…', '저널': 'Physical review letters', '저자': 'Georges Aad, Tatevik Abajyan, Brad Abbott, Jalal Abdallah, S Abdel Khalek, AA Abdelalim, R Aben, B Abi, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, BS Acharya, L Adamczyk, DL Adams, TN Addy, J Adelman, S Adomeit, P Adragna, T Adye, S Aefsky, JA Aguilar-Saavedra, M Agustoni, SP Ahlen, F Ahles, A Ahmad, M Ahsan, G Aielli, TPA Åkesson, G Akimoto, AV Akimov, MA Alam, J Albert, S Albrand, M Aleksa, IN Aleksandrov, F Alessandria, C Alexa, G Alexander, G Alexandre, T Alexopoulos, M Alhroob, M Aliev, G Alimonti, J Alison, BMM Allbrooke, LJ Allison, PP Allport, SE Allwood-Spiers, J Almond, A Aloisio, R Alon, Alejandro Alonso, F Alonso, A Altheimer, B Alvarez Gonzalez, MG Alviggi, K Amako, C Amelung, VV Ammosov, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, S Angelidakis, P Anger, A Angerami, F Anghinolfi, A Anisenkov, N Anjos, A Annovi, A Antonaki, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, R Apolle, G Arabidze, I Aracena, Y Arai, ATH Arce, S Arfaoui, JF Arguin, S Argyropoulos, E Arik, M Arik, AJ Armbruster, O Arnaez, V Arnal, A Artamonov, G Artoni, D Arutinov, S Asai, S Ask, B Åsman, L Asquith, K Assamagan, R Astalos, A Astbury, M Atkinson, B Auerbach, E Auge, K Augsten, M Aurousseau, G Avolio, D Axen, G Azuelos, Y Azuma, MA Baak, G Baccaglioni, C Bacci, AM Bach, H Bachacou, K Bachas, M Backes, M Backhaus, J Backus Mayes, E Badescu, P Bagnaia, Y Bai, DC Bailey, T Bain, JT Baines, OK Baker, S Baker, P Balek, F Balli, E Banas, P Banerjee, Sw Banerjee, D Banfi, A Bangert, V Bansal, HS Bansil, L Barak, SP Baranov, T Barber, EL Barberio', '전체 인용횟수': '928회 인용2012201320142015201620172018201920202021202220233561351021151171027957524761', '페이지': '182302', '학술 문서': 'Observation of associated near-side and away-side long-range correlations in s N N= 5.02 TeV proton-lead collisions with the ATLAS detectorG Aad, T Abajyan, B Abbott, J Abdallah, SA Khalek…\\xa0- Physical review letters, 2013906회 인용 관련 학술자료 전체 72개의 버전 Observation of Associated Near-side and Away-side Long-range Correlations in√ sNN= 5.02 TeV Proton-lead Collisions with the ATLAS DetectorG Aad, T Abajyan, B Abbott, J Abdallah, SA Khalek… - 201218회 인용 관련 학술자료 전체 19개의 버전 Observation of Associated Near-Side and Away-Side Long-Range Correlations in*G Aad - 20134회 인용 관련 학술자료 전체 5개의 버전 Observation of Associated Near-Side and Away-Side Long-Range Correlations in√ sNN= 5.02 TeV Proton-Lead Collisions with the ATLAS DetectorC ATLAS, M Agustoni, L Ancu, A Battaglia, HP Beck…\\xa0- Physical review letters, 20134회 인용 관련 학술자료 Observation of associated near-side and away-side long-range correlations in root S-NN= 5.02 TeV Proton-Lead collisions with the ATLAS DetectorATLAS Collaboration - 2013관련 학술자료 전체 3개의 버전 ', '호': '18'}, title='Observation of Associated Near-Side and Away-Side Long-Range Correlations in  Proton-Lead Collisions with the ATLAS Detector', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Measurement of the inclusive W±and Z/γ* cross sections in the e and μ decay channels in pp collisions at√ s= 7 TeV with the ATLAS detector': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012', '게시자': 'American Physical Society', '권': '85', '설명': 'The inclusive Drell-Yan [1] production cross sections of W and Z bosons have been an important testing ground for QCD. Theoretical calculations of this process extend to next-to-leading order (NLO)[2–4] and next-to-next-toleading order (NNLO)[5–9] perturbation theory. Crucial ingredients of the resulting QCD cross section calculations are the parametrizations of the momentum distribution functions of partons in the proton (PDFs). These have been determined recently in a variety of phenomenological analyses to NLO QCD by the CTEQ [10, 11] group and to NNLO by the MSTW [12], ABKM [13, 14], HERAPDF [15, 16], JR [17], and NNPDF [18, 19] groups. The present measurement determines the cross sections times leptonic branching ratios, WÆ Á BRðW!‘Þ and Z= Ã Á BRðZ= Ã!‘‘Þ, of inclusive W and Z production for electron and muon final states, where ‘¼ e,. Compared to the initial measurement by the ATLAS Collaboration [20], the data set is enlarged by 100 and the luminosity uncertainty significantly reduced [21] from 11% to 3.4%. The CMS Collaboration has updated their initial measurement of total W and Z cross sections [22] to include data corresponding to an integrated luminosity similar to that used here [23]. Similar measurements have been performed at the p\" p collider Tevatron by the CDF and D0 collaborations [24, 25]. The presented cross section values are integrated over the fiducial region of the analysis and also extrapolated to the full kinematic range. The data are also reported differentially, as functions of the lepton pseudorapidity, 3 l, for the Wæ cross sections, and of the boson rapidity, yZ, for the Z= Ã cross section. For the\\xa0…', '저널': 'Physical review. D-particles, fields, gravitation, and cosmology', '저자': 'Lucian Ancu, Andreas Battaglia, Hans Peter Beck, Claudia Borer, Antonio Ereditato, Maria Fonseca, Valentina Gallo, Sigve Haug, Sonja Kabana, Tobias Kruker, Lukas Marti, Klaus Pretzl, Cyril Topfel, Nicola Venturi, Michael Weber, ATLAS Collaboration', '전체 인용횟수': '838회 인용2011201220132014201520162017201820192020202120222023181261561259012965471710211910', '페이지': '72004', '학술 문서': 'Measurement of the inclusive W±and Z/γ* cross sections in the e and μ decay channels in pp collisions at√ s= 7 TeV with the ATLAS detectorL Ancu, A Battaglia, HP Beck, C Borer, A Ereditato…\\xa0- Physical review. D-particles, fields, gravitation, and\\xa0…, 2012546회 인용 관련 학술자료 전체 23개의 버전 Measurement of the isolated diphoton cross-section in pp collisions at sqrt (s)= 7 TeV with the ATLAS detector*Atlas Collaboration\\xa0- arXiv preprint arXiv:1107.0581, 2011209회 인용 관련 학술자료 전체 6개의 버전 A search for t̄t resonances with the ATLAS detector in 2.05 fb− 1 of proton-proton collisions at s=7~TeV*ATLAS Collaboration atlas. publications@ cern. ch…\\xa0- The European Physical Journal C, 2012101회 인용 관련 학술자료 전체 73개의 버전 A Search for tt Resonances with ATLAS in 2.05 fb− 1 of Proton-Proton Collisions at√ s= 7 TeV*ATLAS Collaboration\\xa0- arXiv preprint arXiv:1205.53714회 인용 관련 학술자료 A search for $ t\\\\bar {t} $ resonances with the ATLAS detector in 2.05 fb-1 of proton-proton collisions at $\\\\sqrt {s}= 7~\\\\mathrm {TeV} $*G Aad, B Abbott, J Abdallah, S Abdel Khalek…\\xa0- European Physical Journal. C, Particles and Fields, 20121회 인용 관련 학술자료 전체 11개의 버전 Kshort and Lambda production in pp interactions at sqrt (s)= 0.9 and 7 TeV measured with the ATLAS detector at the LHC*ATLAS Collaboration\\xa0- arXiv preprint arXiv:1111.1297, 20111회 인용 관련 학술자료 Measurement of the inclusive W±and Z/γ* cross sections in the e and μ decay channels in pp collisions at√ s= 7 TeV with the ATLAS detectorG Aad, B Acharya\\xa0- Phys. Rev., 2012A search for t (t) over-bar resonances with the ATLAS detector in 2.05 fb (-1) of proton-proton collisions at root s= 7 TeV*ATLAS Collaboration - 2012관련 학술자료 전체 2개의 버전 A search for ttbar resonances with the ATLAS detector in 2.05 fb^-1 of proton-proton collisions at sqrt (s)= 7 TeV*W Bhimji, R Harrington, V Martin, ATLAS Collaboration\\xa0- European Physical Journal C, 2012전체 2개의 버전 ', '호': '7'}, title='Measurement of the inclusive W±and Z/γ* cross sections in the e and μ decay channels in pp collisions at√ s= 7 TeV with the ATLAS detector', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Study of the spin and parity of the Higgs boson in diboson decays with the ATLAS detector': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/10', '게시자': 'Springer Berlin Heidelberg', '권': '75', '설명': ' Studies of the spin, parity and tensor couplings of the Higgs boson in the ,  and  decay processes at the LHC are presented. The investigations are based on  of pp collision data collected by the ATLAS experiment at \\xa0TeV and \\xa0TeV. The Standard Model (SM) Higgs boson hypothesis, corresponding to the quantum numbers , is tested against several alternative spin scenarios, including non-SM spin-0 and spin-2 models with universal and non-universal couplings to fermions and vector bosons. All tested alternative models are excluded in favour of the SM Higgs boson hypothesis at more than 99.9\\xa0% confidence level. Using the  and  decays, the tensor structure of the interaction between the spin-0 boson and the SM vector bosons is also investigated. The observed distributions of variables sensitive to the non-SM\\xa0…', '저널': 'The European Physical Journal C', '저자': 'Georges Aad, B Abbott, J Abdallah, O Abdinov, R Aben, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, J Adelman, S Adomeit, T Adye, AA Affolder, T Agatonovic-Jovin, J Agricola, JA Aguilar-Saavedra, SP Ahlen, F Ahmadov, G Aielli, H Akerstedt, TPA Åkesson, AV Akimov, GL Alberghi, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, T Alexopoulos, M Alhroob, G Alimonti, L Alio, J Alison, SP Alkire, BMM Allbrooke, PP Allport, A Aloisio, A Alonso, F Alonso, C Alpigiani, A Altheimer, B Alvarez Gonzalez, D Alvarez Piqueras, MG Alviggi, BT Amadio, K Amako, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, JK Anders, KJ Anderson, A Andreazza, V Andrei, S Angelidakis, I Angelozzi, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, G Arabidze, Y Arai, JP Araque, ATH Arce, FA Arduh, JF Arguin, S Argyropoulos, M Arik, AJ Armbruster, O Arnaez, V Arnal, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, A Ashkenazi, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, B Auerbach, K Augsten, M Aurousseau, G Avolio, B Axen, MK Ayoub, G Azuelos, MA Baak, AE Baas, C Bacci, H Bachacou, K Bachas, M Backes, M Backhaus, P Bagiacchi, P Bagnaia, Y Bai, T Bain, JT Baines, OK Baker, EM Baldin, P Balek, T Balestri, F Balli, E Banas, Sw Banerjee, AAE Bannoura, HS Bansil, L Barak, EL Barberio, D Barberis, M Barbero, T Barillari, M Barisonzi, T Barklow, N Barlow, SL Barnes, BM Barnett, RM Barnett, Z Barnovska', '전체 인용횟수': '767회 인용2015201620172018201920202021202220233012211014611697545437', '페이지': '1-36', '학술 문서': 'Study of the spin and parity of the Higgs boson in diboson decays with the ATLAS detectorG Aad, B Abbott, J Abdallah, O Abdinov, R Aben…\\xa0- The European Physical Journal C, 2015767회 인용 관련 학술자료 전체 147개의 버전 Study of the spin and parity of the Higgs boson in diboson decays with the ATLAS detector (vol 75, 476, 2015) i*G Aad, B Abbott, J Abdallah, R Aben, M Abolins…\\xa0- …\\xa0PHYSICAL JOURNAL. C, PARTICLES AND FIELDS, 2016전체 3개의 버전 Study of the spin and parity of the Higgs boson in diboson decays with the ATLAS detectorA Collaboration\\xa0- Eur. Phys. J. C, 2015관련 학술자료 '}, title='Study of the spin and parity of the Higgs boson in diboson decays with the ATLAS detector', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Evidence for the Higgs-boson Yukawa coupling to tau leptons with the ATLAS detector': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/4', '게시자': 'Springer Berlin Heidelberg', '권': '2015', '설명': 'Results of a search for H→ ττ decays are presented, based on the full set of proton-proton collision data recorded by the ATLAS experiment at the LHC during 2011 and 2012. The data correspond to integrated luminosities of 4.5 fb− 1 and 20.3 fb− 1 at centre-of-mass energies of TeV and TeV respectively. All combinations of leptonic ( with ℓ= e, μ) and hadronic (τ→ hadrons ν) tau decays are considered. An excess of events over the expected background from other Standard Model processes is found with an observed (expected) significance of 4.5 (3.4) standard deviations. This excess provides evidence for the direct coupling of the recently discovered Higgs boson to fermions. The measured signal strength, normalised to the Standard Model expectation, of μ= 1. 43+ 0.43− 0.37 is consistent with the predicted Yukawa coupling strength in the Standard Model.', '저널': 'Journal of High Energy Physics', '저자': 'Georges Aad, Brad Abbott, Jalal Abdallah, Samah Abdel Khalek, R Aben, B Abi, M Abolins, OS AbouZeid, H Abramowicz, H Abreu, R Abreu, Y Abulaiti, BS Acharya, L Adamczyk, DL Adams, J Adelman, S Adomeit, T Adye, T Agatonovic-Jovin, JA Aguilar-Saavedra, M Agustoni, SP Ahlen, F Ahmadov, G Aielli, H Akerstedt, TPA Åkesson, G Akimoto, AV Akimov, GL Alberghi, J Albert, S Albrand, MJ Alconada Verzini, M Aleksa, IN Aleksandrov, C Alexa, G Alexander, G Alexandre, T Alexopoulos, Muhammad Alhroob, G Alimonti, L Alio, J Alison, BMM Allbrooke, LJ Allison, PP Allport, A Aloisio, A Alonso, F Alonso, C Alpigiani, A Altheimer, B Alvarez Gonzalez, D Álvarez Piqueras, MG Alviggi, K Amako, Y Amaral Coutinho, C Amelung, D Amidei, SP Amor Dos Santos, A Amorim, S Amoroso, N Amram, G Amundsen, C Anastopoulos, LS Ancu, N Andari, T Andeen, CF Anders, G Anders, KJ Anderson, A Andreazza, V Andrei, XS Anduaga, S Angelidakis, I Angelozzi, P Anger, A Angerami, F Anghinolfi, AV Anisenkov, N Anjos, A Annovi, M Antonelli, A Antonov, J Antos, F Anulli, M Aoki, L Aperio Bella, G Arabidze, Y Arai, JP Araque, ATH Arce, FA Arduh, JF Arguin, S Argyropoulos, M Arik, AJ Armbruster, O Arnaez, V Arnal, H Arnold, M Arratia, O Arslan, A Artamonov, G Artoni, S Asai, N Asbah, A Ashkenazi, B Åsman, L Asquith, K Assamagan, R Astalos, M Atkinson, NB Atlay, B Auerbach, K Augsten, M Aurousseau, G Avolio, B Axen, G Azuelos, Y Azuma, MA Baak, AE Baas, C Bacci, H Bachacou, K Bachas, M Backes, M Backhaus, E Badescu, P Bagiacchi, P Bagnaia, Y Bai, T Bain, JT Baines, OK Baker, P Balek, F Balli, E Banas, Sw Banerjee, AAE Bannoura, HS Bansil, L Barak, SP Baranov, EL Barberio, D Barberis, M Barbero, T Barillari, M Barisonzi, T Barklow, N Barlow, SL Barnes, BM Barnett, RM Barnett', '전체 인용횟수': '740회 인용201420152016201720182019202020212022202361351851011059141202426', '페이지': '1-74', '학술 문서': 'Evidence for the Higgs-boson Yukawa coupling to tau leptons with the ATLAS detectorG Aad, B Abbott, J Abdallah, S Abdel Khalek, R Aben…\\xa0- Journal of High Energy Physics, 2015740회 인용 관련 학술자료 전체 89개의 버전 Evidence for the Higgs-boson Yukawa coupling to tau leptons with the ATLAS detectorATLAS collaboration - 2015관련 학술자료 전체 3개의 버전 Evidence for the Higgs-boson Yukawa coupling to tau leptons with the ATLAS detectorM Beckingham, PF Harrison, M Janus, C Jeske…\\xa0- Journal of High Energy Physics, 2015관련 학술자료 전체 3개의 버전 ', '호': '4'}, title='Evidence for the Higgs-boson Yukawa coupling to tau leptons with the ATLAS detector', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Pyramid scene parsing network': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes.', '저자': 'Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia', '전체 인용횟수': '12346회 인용201720182019202020212022202312167412911734244229883010', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '2881-2890', '학술 문서': 'Pyramid scene parsing networkH Zhao, J Shi, X Qi, X Wang, J Jia\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 201712346회 인용 관련 학술자료 전체 24개의 버전 '}, title='Pyramid scene parsing network', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/10/1', '설명': 'Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing text-to-image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256x256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.', '저널': 'IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV)', '저자': 'Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris Metaxas', '전체 인용횟수': '3122회 인용2017201820192020202120222023110331477552565545515', '페이지': '5907-5915', '학술 문서': 'Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networksH Zhang, T Xu, H Li, S Zhang, X Wang, X Huang…\\xa0- Proceedings of the IEEE international conference on\\xa0…, 20173122회 인용 관련 학술자료 전체 30개의 버전 '}, title='Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deepreid: Deep filter pairing neural network for person re-identification': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '설명': 'Person re-identification is to match pedestrian images from disjoint camera views detected by pedestrian detectors. Challenges are presented in the form of complex variations of lightings, poses, viewpoints, blurring effects, image resolutions, camera settings, occlusions and background clutter across camera views. In addition, misalignment introduced by the pedestrian detector will affect most existing person re-identification methods that use manually cropped pedestrian images and assume perfect detection. In this paper, we propose a novel filter pairing neural network (FPNN) to jointly handle misalignment, photometric and geometric transforms, occlusions and background clutter. All the key components are jointly optimized to maximize the strength of each component when cooperating with others. In contrast to existing works that use handcrafted features, our method automatically learns features optimal for the re-identification task from data. The learned filter pairs encode photometric transforms. Its deep architecture makes it possible to model a mixture of complex photometric and geometric transforms. We build the largest benchmark re-id dataset with 13,164 images of 1,360 pedestrians. Unlike existing datasets, which only provide manually cropped pedestrian images, our dataset provides automatically detected bounding boxes for evaluation close to practical applications. Our neural network significantly outperforms state-of-the-art methods on this dataset.', '저자': 'Wei Li, Rui Zhao, Tong Xiao, Xiaogang Wang', '전체 인용횟수': '2848회 인용20152016201720182019202020212022202350121254348485437435385286', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '152-159', '학술 문서': 'Deepreid: Deep filter pairing neural network for person re-identificationW Li, R Zhao, T Xiao, X Wang\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20142848회 인용 관련 학술자료 전체 17개의 버전 '}, title='Deepreid: Deep filter pairing neural network for person re-identification', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep learning for generic object detection: A survey': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020/2', '게시자': 'Springer US', '권': '128', '설명': ' Object detection, one of the most fundamental and challenging problems in computer vision, seeks to locate object instances from a large number of predefined categories in natural images. Deep learning techniques have emerged as a powerful strategy for learning feature representations directly from data and have led to remarkable breakthroughs in the field of generic object detection. Given this period of rapid evolution, the goal of this paper is to provide a comprehensive survey of the recent achievements in this field brought about by deep learning techniques. More than 300 research contributions are included in this survey, covering many aspects of generic object detection: detection frameworks, object feature representation, object proposal generation, context modeling, training strategies, and evaluation metrics. We finish the survey by identifying promising directions for future research. ', '저널': 'International journal of computer vision', '저자': 'Li Liu, Wanli Ouyang, Xiaogang Wang, Paul Fieguth, Jie Chen, Xinwang Liu, Matti Pietikäinen', '전체 인용횟수': '2594회 인용20192020202120222023122398726752571', '페이지': '261-318', '학술 문서': 'Deep learning for generic object detection: A surveyL Liu, W Ouyang, X Wang, P Fieguth, J Chen, X Liu…\\xa0- International journal of computer vision, 20202594회 인용 관련 학술자료 전체 14개의 버전 '}, title='Deep learning for generic object detection: A survey', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Pointrcnn: 3d object proposal generation and detection from point cloud': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': \"In this paper, we propose PointRCNN for 3D object detection from raw point cloud. The whole framework is composed of two stages: stage-1 for the bottom-up 3D proposal generation and stage-2 for refining proposals in the canonical coordinates to obtain the final detection results. Instead of generating proposals from RGB image or projecting point cloud to bird's view or voxels as previous methods do, our stage-1 sub-network directly generates a small number of high-quality 3D proposals from point cloud in a bottom-up manner via segmenting the point cloud of the whole scene into foreground points and background. The stage-2 sub-network transforms the pooled points of each proposal to canonical coordinates to learn better local spatial features, which is combined with global semantic features of each point learned in stage-1 for accurate box refinement and confidence prediction. Extensive experiments on the 3D detection benchmark of KITTI dataset show that our proposed architecture outperforms state-of-the-art methods with remarkable margins by using only point cloud as input. The code is available at https://github. com/sshaoshuai/PointRCNN.\", '저자': 'Shaoshuai Shi, Xiaogang Wang, Hongsheng Li', '전체 인용횟수': '2038회 인용2019202020212022202369285434619620', '컨퍼런스': 'Proceedings of the IEEE/CVF conference on computer vision and pattern recognition', '페이지': '770-779', '학술 문서': 'Pointrcnn: 3d object proposal generation and detection from point cloudS Shi, X Wang, H Li\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 20192038회 인용 관련 학술자료 전체 14개의 버전 '}, title='Pointrcnn: 3d object proposal generation and detection from point cloud', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Pv-rcnn: Point-voxel feature set abstraction for 3d object detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020', '설명': 'We present a novel and high-performance 3D object detection framework, named PointVoxel-RCNN (PV-RCNN), for accurate 3D object detection from point clouds. Our proposed method deeply integrates both 3D voxel Convolutional Neural Network (CNN) and PointNet-based set abstraction to learn more discriminative point cloud features. It takes advantages of efficient learning and high-quality proposals of the 3D voxel CNN and the flexible receptive fields of the PointNet-based networks. Specifically, the proposed framework summarizes the 3D scene with a 3D voxel CNN into a small set of keypoints via a novel voxel set abstraction module to save follow-up computations and also to encode representative scene features. Given the high-quality 3D proposals generated by the voxel CNN, the RoI-grid pooling is proposed to abstract proposal-specific features from the keypoints to the RoI-grid points via keypoint set abstraction. Compared with conventional pooling operations, the RoI-grid feature points encode much richer context information for accurately estimating object confidences and locations. Extensive experiments on both the KITTI dataset and the Waymo Open dataset show that our proposed PV-RCNN surpasses state-of-the-art 3D detection methods with remarkable margins.', '저자': 'Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, Hongsheng Li', '전체 인용횟수': '1401회 인용202020212022202356282475577', '컨퍼런스': 'Proceedings of the IEEE/CVF conference on computer vision and pattern recognition', '페이지': '10529-10538', '학술 문서': 'Pv-rcnn: Point-voxel feature set abstraction for 3d object detectionS Shi, C Guo, L Jiang, Z Wang, J Shi, X Wang, H Li\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 20201401회 인용 관련 학술자료 전체 9개의 버전 '}, title='Pv-rcnn: Point-voxel feature set abstraction for 3d object detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Context encoding for semantic segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'Recent work has made significant progress in improving spatial resolution for pixelwise labeling with Fully Convolutional Network (FCN) framework by employing Dilated/Atrous convolution, utilizing multi-scale features and refining boundaries. In this paper, we explore the impact of global contextual information in semantic segmentation by introducing the Context Encoding Module, which captures the semantic context of scenes and selectively highlights class-dependent featuremaps. The proposed Context Encoding Module significantly improves semantic segmentation results with only marginal extra computation cost over FCN. Our approach has achieved new state-of-the-art results 51.7% mIoU on PASCAL-Context, 85.9% mIoU on PASCAL VOC 2012. Our single model achieves a final score of 0.5567 on ADE20K test set, which surpass the winning entry of COCO-Place Challenge in 2017. In addition, we also explore how the Context Encoding Module can improve the feature representation of relatively shallow networks for the image classification on CIFAR-10 dataset. Our 14 layer network has achieved an error rate of 3.45%, which is comparable with state-of-the-art approaches with over 10 times more layers. The source code for the complete system are publicly available.', '저자': 'Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi, Amit Agrawal', '전체 인용횟수': '1343회 인용20182019202020212022202331163270304296270', '컨퍼런스': 'Proceedings of the IEEE conference on Computer Vision and Pattern Recognition', '페이지': '7151-7160', '학술 문서': 'Context encoding for semantic segmentationH Zhang, K Dana, J Shi, Z Zhang, X Wang, A Tyagi…\\xa0- Proceedings of the IEEE conference on Computer\\xa0…, 20181343회 인용 관련 학술자료 전체 12개의 버전 '}, title='Context encoding for semantic segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Cross-scene crowd counting via deep convolutional neural networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'Cross-scene crowd counting is a challenging task where no laborious data annotation is required for counting people in new target surveillance crowd scenes unseen in the training set. The performance of most existing crowd counting methods drops significantly when they are applied to an unseen scene. To address this problem, we propose a deep convolutional neural network (CNN) for crowd counting, and it is trained alternatively with two related learning objectives, crowd density and crowd count. This proposed switchable learning approach is able to obtain better local optimum for both objectives. To handle an unseen target crowd scene, we present a data-driven method to fine-tune the trained CNN model for the target scene. A new dataset including 108 crowd scenes with nearly 200,000 head annotations is introduced to better evaluate the accuracy of cross-scene crowd counting methods. Extensive experiments on the proposed and another two existing datasets demonstrate the effectiveness and reliability of our approach.', '저자': 'Cong Zhang, Hongsheng Li, Xiaogang Wang, Xiaokang Yang', '전체 인용횟수': '1341회 인용20152016201720182019202020212022202374490153234229232198141', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '833-841', '학술 문서': 'Cross-scene crowd counting via deep convolutional neural networksC Zhang, H Li, X Wang, X Yang\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20151341회 인용 관련 학술자료 전체 14개의 버전 '}, title='Cross-scene crowd counting via deep convolutional neural networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Unsupervised salience learning for person re-identification': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013', '설명': 'Human eyes can recognize person identities based on some small salient regions. However, such valuable salient information is often hidden when computing similarities of images with existing approaches. Moreover, many existing approaches learn discriminative features and handle drastic viewpoint change in a supervised way and require labeling new training data for a different pair of camera views. In this paper, we propose a novel perspective for person re-identification based on unsupervised salience learning. Distinctive features are extracted without requiring identity labels in the training procedure. First, we apply adjacency constrained patch matching to build dense correspondence between image pairs, which shows effectiveness in handling misalignment caused by large viewpoint and pose variations. Second, we learn human salience in an unsupervised manner. To improve the performance of person re-identification, human salience is incorporated in patch matching to find reliable and discriminative matched patches. The effectiveness of our approach is validated on the widely used VIPeR dataset and ETHZ dataset.', '저자': 'Rui Zhao, Wanli Ouyang, Xiaogang Wang', '전체 인용횟수': '1319회 인용201320142015201620172018201920202021202220231067134152199205193139915845', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '3586-3593', '학술 문서': 'Unsupervised salience learning for person re-identificationR Zhao, W Ouyang, X Wang\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20131319회 인용 관련 학술자료 전체 17개의 버전 '}, title='Unsupervised salience learning for person re-identification', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Stackgan++: Realistic image synthesis with stacked generative adversarial networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/7/16', '게시자': 'IEEE', '권': '41', '설명': 'Although Generative Adversarial Networks (GANs) have shown remarkable success in various tasks, they still face challenges in generating high quality images. In this paper, we propose Stacked Generative Adversarial Networks (StackGANs) aimed at generating high-resolution photo-realistic images. First, we propose a two-stage generative adversarial network architecture, StackGAN-v1, for text-to-image synthesis. The Stage-I GAN sketches the primitive shape and colors of a scene based on a given text description, yielding low-resolution images. The Stage-II GAN takes Stage-I results and the text description as inputs, and generates high-resolution images with photo-realistic details. Second, an advanced multi-stage generative adversarial network architecture, StackGAN-v2, is proposed for both conditional and unconditional generative tasks. Our StackGAN-v2 consists of multiple generators and multiple\\xa0…', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris N Metaxas', '전체 인용횟수': '1306회 인용20182019202020212022202337120241287336276', '페이지': '1947-1962', '학술 문서': 'Stackgan++: Realistic image synthesis with stacked generative adversarial networksH Zhang, T Xu, H Li, S Zhang, X Wang, X Huang…\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20181306회 인용 관련 학술자료 전체 8개의 버전 ', '호': '8'}, title='Stackgan++: Realistic image synthesis with stacked generative adversarial networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Visual tracking with fully convolutional networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'We propose a new approach for general object tracking with fully convolutional neural network. Instead of treating convolutional neural network (CNN) as a black-box feature extractor, we conduct in-depth study on the properties of CNN features offline pre-trained on massive image data and classification task on ImageNet. The discoveries motivate the design of our tracking system. It is found that convolutional layers in different levels characterize the target from different perspectives. A top layer encodes more semantic features and serves as a category detector, while a lower layer carries more discriminative information and can better separate the target from distracters with similar appearance. Both layers are jointly used with a switch mechanism during tracking. It is also found that for a tracking target, only a subset of neurons are relevant. A feature map selection method is developed to remove noisy and irrelevant feature maps, which can reduce computation redundancy and improve tracking accuracy. Extensive evaluation on the widely used tracking benchmark shows that the proposed tacker outperforms the state-of-the-art significantly.', '저자': 'Lijun Wang, Wanli Ouyang, Xiaogang Wang, Huchuan Lu', '전체 인용횟수': '1211회 인용2015201620172018201920202021202220233621662642381871457946', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '3119-3127', '학술 문서': 'Visual tracking with fully convolutional networksL Wang, W Ouyang, X Wang, H Lu\\xa0- Proceedings of the IEEE international conference on\\xa0…, 20151211회 인용 관련 학술자료 전체 15개의 버전 '}, title='Visual tracking with fully convolutional networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning from massive noisy labeled data for image classification': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'Large-scale supervised datasets are crucial to train convolutional neural networks (CNNs) for various computer vision problems. However, obtaining a massive amount of well-labeled data is usually very expensive and time consuming. In this paper, we introduce a general framework to train CNNs with only a limited number of clean labels and millions of easily obtained noisy labels. We model the relationships between images, class labels and label noises with a probabilistic graphical model and further integrate it into an end-to-end deep learning system. To demonstrate the effectiveness of our approach, we collect a large-scale real-world clothing classification dataset with both noisy and clean labels. Experiments on this dataset indicate that our approach can better correct the noisy labels and improves the performance of trained CNNs.', '저자': 'Tong Xiao, Tian Xia, Yi Yang, Chang Huang, Xiaogang Wang', '전체 인용횟수': '1135회 인용2015201620172018201920202021202220235195983115173215237215', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '2691-2699', '학술 문서': 'Learning from massive noisy labeled data for image classificationT Xiao, T Xia, Y Yang, C Huang, X Wang\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20151135회 인용 관련 학술자료 전체 16개의 버전 '}, title='Learning from massive noisy labeled data for image classification', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Probabilistic robotics': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2005', '게시자': 'MIT Press', '설명': 'Planning and navigation algorithms exploit statistics gleaned from uncertain, imperfect real-world environments to guide robots toward their goals and around obstacles.', '저자': 'Sebastian Thrun, Wolfram Burgard, Dieter Fox', '전체 인용횟수': '13613회 인용200620072008200920102011201220132014201520162017201820192020202120222023179397496661796821829857895895868854915978874836741572', '학술 문서': 'Probabilistic roboticsS Thrun\\xa0- Communications of the ACM, 200213460회 인용 관련 학술자료 전체 15개의 버전 Probalistic robotics*S Thrun, W Burgard, D Fox\\xa0- Kybernetes, 200674회 인용 관련 학술자료 Probabilistic robotics*W Burgard, D Fox, S Thrun\\xa0- The MIT Press, 200564회 인용 관련 학술자료 Probabilistic robotics cambridgeS Thrun, W Burgard, D Fox - 200527회 인용 관련 학술자료 Probability robotics*S Thrun, W Burgard, D Fox\\xa0- MIT press, 200517회 인용 관련 학술자료 Probabilistic Robotics [hardcover]S Thrun, W Burgard, D Fox - 20052회 인용 관련 학술자료 '}, title='Probabilistic robotics', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Principles of robot motion: theory, algorithms, and implementation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2005/6', '게시자': 'The MIT Press', '설명': 'A text that makes the mathematical underpinnings of robot motion accessible and relates low-level details of implementation to high-level algorithmic concepts. Robot motion planning has become a major focus of robotics. Research findings can be applied not only to robotics but to planning routes on circuit boards, directing digital actors in computer graphics, robot-assisted surgery and medicine, and in novel areas such as drug design and protein folding. This text reflects the great advances that have taken place in the last ten years, including sensor-based planning, probabalistic planning, localization and mapping, and motion planning for dynamic and nonholonomic systems. Its presentation makes the mathematical underpinnings of robot motion accessible to students of computer science and engineering, rleating low-level implementation details to high-level algorithmic concepts.', '저자': 'Howie Choset, Kevin M. Lynch, Seth Hutchinson, George Kantor, Wolfram Burgard, Lydia E. Kavraki, Sebastian Thrun', '전체 인용횟수': '4131회 인용20052006200720082009201020112012201320142015201620172018201920202021202220231780158193218248278282287297280252232257245218226203124', '페이지': '625', '학술 문서': 'Principles of robot motion: theory, algorithms, and implementationsH Choset, KM Lynch, S Hutchinson, GA Kantor… - 20054119회 인용 관련 학술자료 전체 6개의 버전 Principles of robot motion: theory, algorithms, and implementations, ser. Intelligent Robotics and Autonomous Agents*H Choset, KM Lynch, S Hutchinson, GA Kantor…\\xa0- Massachusetts Institute of Technology, Cambridge, 200512회 인용 관련 학술자료 Principles of Robot Motion: Theory, Algorithms, and Implementation ERRATA!!!!H Choset, K Lynch, S Hutchinson, G Kantor, W Burgard… - 20074회 인용 관련 학술자료 전체 13개의 버전 '}, title='Principles of robot motion: theory, algorithms, and implementation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The dynamic window approach to collision avoidance': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1997', '권': '4', '설명': 'This approach, designed for mobile robots equipped with synchro-drives, is derived directly from the motion dynamics of the robot. In experiments, the dynamic window approach safely controlled the mobile robot RHINO at speeds of up to 95 cm/sec, in populated and dynamic environments.', '저널': 'IEEE Robotics and Automation Magazine', '저자': 'D. Fox, W Burgard, S. Thrun', '전체 인용횟수': '3952회 인용19971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202310262120163952426465111101131142132153162167166173196241268310348403356', '페이지': '1', '학술 문서': 'The dynamic window approach to collision avoidanceD Fox, W Burgard, S Thrun\\xa0- IEEE Robotics & Automation Magazine, 19973952회 인용 관련 학술자료 전체 10개의 버전 '}, title='The dynamic window approach to collision avoidance', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A benchmark for the evaluation of RGB-D SLAM systems': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012/10/7', '게시자': 'IEEE', '설명': 'In this paper, we present a novel benchmark for the evaluation of RGB-D SLAM systems. We recorded a large set of image sequences from a Microsoft Kinect with highly accurate and time-synchronized ground truth camera poses from a motion capture system. The sequences contain both the color and depth images in full sensor resolution (640 × 480) at video frame rate (30 Hz). The ground-truth trajectory was obtained from a motion-capture system with eight high-speed tracking cameras (100 Hz). The dataset consists of 39 sequences that were recorded in an office environment and an industrial hall. The dataset covers a large variety of scenes and camera motions. We provide sequences for debugging with slow motions as well as longer trajectories with and without loop closures. Most sequences were recorded from a handheld Kinect with unconstrained 6-DOF motions but we also provide sequences from a\\xa0…', '저자': 'Jürgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, Daniel Cremers', '전체 인용횟수': '3549회 인용2013201420152016201720182019202020212022202383116151217258346445435507462483', '컨퍼런스': '2012 IEEE/RSJ international conference on intelligent robots and systems', '페이지': '573-580', '학술 문서': 'A benchmark for the evaluation of RGB-D SLAM systemsJ Sturm, N Engelhard, F Endres, W Burgard, D Cremers\\xa0- 2012 IEEE/RSJ international conference on intelligent\\xa0…, 20123549회 인용 관련 학술자료 전체 19개의 버전 '}, title='A benchmark for the evaluation of RGB-D SLAM systems', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'OctoMap: An efficient probabilistic 3D mapping framework based on octrees': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/4', '게시자': 'Springer US', '권': '34', '설명': ' Three-dimensional models provide a volumetric representation of space which is important for a variety of robotic applications including flying robots and robots that are equipped with manipulators. In this paper, we present an open-source framework to generate volumetric 3D\\xa0environment models. Our mapping approach is based on octrees and uses probabilistic occupancy estimation. It explicitly represents not only occupied space, but also free and unknown areas. Furthermore, we propose an octree map compression method that keeps the 3D models compact. Our framework is available as an open-source C++ library and has already been successfully applied in several robotics projects. We present a series of experimental results carried out with real robots and on publicly available real-world datasets. The results demonstrate that our approach is able to update the representation efficiently and\\xa0…', '저널': 'Autonomous robots', '저자': 'Armin Hornung, Kai M Wurm, Maren Bennewitz, Cyrill Stachniss, Wolfram Burgard', '전체 인용횟수': '2978회 인용2013201420152016201720182019202020212022202356114140223268334315373391404342', '페이지': '189-206', '학술 문서': 'OctoMap: An efficient probabilistic 3D mapping framework based on octreesA Hornung, KM Wurm, M Bennewitz, C Stachniss…\\xa0- Autonomous robots, 20132978회 인용 관련 학술자료 전체 20개의 버전 '}, title='OctoMap: An efficient probabilistic 3D mapping framework based on octrees', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Improved techniques for grid mapping with rao-blackwellized particle filters': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2007/2/5', '게시자': 'IEEE', '권': '23', '설명': \"Recently, Rao-Blackwellized particle filters (RBPF) have been introduced as an effective means to solve the simultaneous localization and mapping problem. This approach uses a particle filter in which each particle carries an individual map of the environment. Accordingly, a key question is how to reduce the number of particles. In this paper, we present adaptive techniques for reducing this number in a RBPF for learning grid maps. We propose an approach to compute an accurate proposal distribution, taking into account not only the movement of the robot, but also the most recent observation. This drastically decreases the uncertainty about the robot's pose in the prediction step of the filter. Furthermore, we present an approach to selectively carry out resampling operations, which seriously reduces the problem of particle depletion. Experimental results carried out with real mobile robots in large-scale indoor, as\\xa0…\", '저널': 'IEEE transactions on Robotics', '저자': 'Giorgio Grisetti, Cyrill Stachniss, Wolfram Burgard', '전체 인용횟수': '2941회 인용200720082009201020112012201320142015201620172018201920202021202220232242737284106156160178210223257291287275298193', '페이지': '34-46', '학술 문서': 'Improved techniques for grid mapping with rao-blackwellized particle filtersG Grisetti, C Stachniss, W Burgard\\xa0- IEEE transactions on Robotics, 20072941회 인용 관련 학술자료 전체 25개의 버전 ', '호': '1'}, title='Improved techniques for grid mapping with rao-blackwellized particle filters', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'g2o: A general framework for graph optimization': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011/5/9', '게시자': 'IEEE', '설명': 'Many popular problems in robotics and computer vision including various types of simultaneous localization and mapping (SLAM) or bundle adjustment (BA) can be phrased as least squares optimization of an error function that can be represented by a graph. This paper describes the general structure of such problems and presents g 2 o, an open-source C++ framework for optimizing graph-based nonlinear error functions. Our system has been designed to be easily extensible to a wide range of problems and a new problem typically can be specified in a few lines of code. The current implementation provides solutions to several variants of SLAM and BA. We provide evaluations on a wide range of real-world and simulated datasets. The results demonstrate that while being general g 2 o offers a performance comparable to implementations of state of-the-art approaches for the specific problems.', '저자': 'Rainer Kuemmerle, Giorgio Grisetti, Hauke Strasdat, Kurt Konolige, Wolfram Burgard', '전체 인용횟수': '2752회 인용20112012201320142015201620172018201920202021202220231973141154210241253309338261283257195', '컨퍼런스': 'Robotics and Automation (ICRA), 2011 IEEE International Conference on', '페이지': '3607-3613', '학술 문서': 'g 2 o: A general framework for graph optimizationR Kümmerle, G Grisetti, H Strasdat, K Konolige…\\xa0- 2011 IEEE International Conference on Robotics and\\xa0…, 20112748회 인용 관련 학술자료 전체 13개의 버전 G¡ sup¿ 2¡/sup¿ o: A general framework for graph optimization*R Kummerle, G Grisetti, H Strasdat, K Konolige…\\xa0- International Conference on Robotics and Automation2회 인용 관련 학술자료 g2o: A general framework for graph optimization*R Kummer, G Grisetti, H Strasdat, K Konolige…\\xa0- IEEE International Conference on Robotics and\\xa0…2회 인용 관련 학술자료 '}, title='g2o: A general framework for graph optimization', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Robust Monte Carlo localization for mobile robots': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2001/5/1', '게시자': 'Elsevier', '권': '128', '설명': \"Mobile robot localization is the problem of determining a robot's pose from sensor data. This article presents a family of probabilistic localization algorithms known as Monte Carlo Localization (MCL). MCL algorithms represent a robot's belief by a set of weighted hypotheses (samples), which approximate the posterior under a common Bayesian formulation of the localization problem. Building on the basic MCL algorithm, this article develops a more robust algorithm called Mixture-MCL, which integrates two complimentary ways of generating samples in the estimation. To apply this algorithm to mobile robots equipped with range finders, a kernel density tree is learned that permits fast sampling. Systematic empirical results illustrate the robustness and computational efficiency of the approach.\", '저널': 'Artificial intelligence', '저자': 'Sebastian Thrun, Dieter Fox, Wolfram Burgard, Frank Dellaert', '전체 인용횟수': '2687회 인용20012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023185380107139146148147152179148142150142125134114101109110767560', '페이지': '99-141', '학술 문서': 'Robust Monte Carlo localization for mobile robotsS Thrun, D Fox, W Burgard, F Dellaert\\xa0- Artificial intelligence, 20012685회 인용 관련 학술자료 전체 38개의 버전 WB, and Dellaert, F. 2001. Robust Monte Carlo Localization for Mobile Robots*S Thrun, D Fox\\xa0- Artificial Intelligence4회 인용 관련 학술자료 ', '호': '1-2'}, title='Robust Monte Carlo localization for mobile robots', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Monte carlo localization for mobile robots': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1999/5/10', '게시자': 'IEEE', '권': '2', '설명': \"To navigate reliably in indoor environments, a mobile robot must know where it is. Thus, reliable position estimation is a key problem in mobile robotics. We believe that probabilistic approaches are among the most promising candidates to providing a comprehensive and real-time solution to the robot localization problem. However, current methods still face considerable hurdles. In particular the problems encountered are closely related to the type of representation used to represent probability densities over the robot's state space. Earlier work on Bayesian filtering with particle-based density representations opened up a new approach for mobile robot localization based on these principles. We introduce the Monte Carlo localization method, where we represent the probability density involved by maintaining a set of samples that are randomly drawn from it. By using a sampling-based representation we obtain a\\xa0…\", '저자': 'Frank Dellaert, Dieter Fox, Wolfram Burgard, Sebastian Thrun', '전체 인용횟수': '2335회 인용19992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220231528234262628410910312313512611511610812210511194113111971178494', '컨퍼런스': 'Proceedings 1999 IEEE international conference on robotics and automation (Cat. No. 99CH36288C)', '페이지': '1322-1328', '학술 문서': 'Monte carlo localization for mobile robotsF Dellaert, D Fox, W Burgard, S Thrun\\xa0- Proceedings 1999 IEEE international conference on\\xa0…, 19992335회 인용 관련 학술자료 전체 37개의 버전 Monte carlo localization for mobile robots*W Burgard, S Thrun, F Dellaert, D Fox\\xa0- Proc. of IEEE/RSJ International Conference on\\xa0…, 19993회 인용 관련 학술자료 '}, title='Monte carlo localization for mobile robots', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep learning with convolutional neural networks for EEG decoding and visualization': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/11', '권': '38', '설명': ' Deep learning with convolutional neural networks (deep ConvNets) has revolutionized computer vision through end‐to‐end learning, that is, learning from the raw data. There is increasing interest in using deep ConvNets for end‐to‐end EEG analysis, but a better understanding of how to design and train ConvNets for end‐to‐end EEG decoding and how to visualize the informative EEG features the ConvNets learn is still needed. Here, we studied deep ConvNets with a range of different architectures, designed for decoding imagined or executed tasks from raw EEG. Our results show that recent advances from the machine learning field, including batch normalization and exponential linear units, together with a cropped training strategy, boosted the deep ConvNets decoding performance, reaching at least as good performance as the widely used filter bank common spatial patterns (FBCSP) algorithm (mean\\xa0…', '저널': 'Human brain mapping', '저자': 'Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, Tonio Ball', '전체 인용횟수': '2147회 인용201720182019202020212022202317157233300420533469', '페이지': '5391-5420', '학술 문서': 'Deep learning with convolutional neural networks for EEG decoding and visualizationRT Schirrmeister, JT Springenberg, LDJ Fiederer…\\xa0- Human brain mapping, 20172147회 인용 관련 학술자료 전체 13개의 버전 ', '호': '11'}, title='Deep learning with convolutional neural networks for EEG decoding and visualization', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Experiences with an interactive museum tour-guide robot': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1999/10/1', '게시자': 'Elsevier', '권': '114', '설명': \"This article describes the software architecture of an autonomous, interactive tour-guide robot. It presents a modular and distributed software architecture, which integrates localization, mapping, collision avoidance, planning, and various modules concerned with user interaction and Web-based telepresence. At its heart, the software approach relies on probabilistic computation, on-line learning, and any-time algorithms. It enables robots to operate safely, reliably, and at high speeds in highly dynamic environments, and does not require any modifications of the environment to aid the robot's operation. Special emphasis is placed on the design of interactive capabilities that appeal to people's intuition. The interface provides new means for human-robot interaction with crowds of people in public places, and it also provides people all around the world with the ability to establish a “virtual telepresence” using the Web\\xa0…\", '저널': 'Artificial intelligence', '저자': 'Wolfram Burgard, Armin B Cremers, Dieter Fox, Dirk Hähnel, Gerhard Lakemeyer, Dirk Schulz, Walter Steiner, Sebastian Thrun', '전체 인용횟수': '1960회 인용19981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023255756698610798106889483779069819472786779626681624545', '페이지': '3-55', '학술 문서': \"Experiences with an interactive museum tour-guide robotW Burgard, AB Cremers, D Fox, D Hähnel…\\xa0- Artificial intelligence, 19991955회 인용 관련 학술자료 전체 48개의 버전 gThe Interactive Museum Tour-Guide Robot,• hProc*W Burgard, AB Cremers, D Fox, D Hahnel…\\xa0- of National Conference on Artificial Intelligence(AAAI'\\xa0…, 199810회 인용 관련 학술자료 Lakemeyer*W Burgard, BA Cremers, D Fox, D Hahnel\\xa0- R., Schulz, D., Steiner, W., Thrun, S., ilixpericnces with\\xa0…, 19993회 인용 관련 학술자료 \", '호': '1-2'}, title='Experiences with an interactive museum tour-guide robot', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Monte carlo localization: Efficient position estimation for mobile robots': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1999/7/18', '권': '1999', '설명': 'This paper presents a new algorithm for mobile robot localization, called Monte Carlo Localization (MCL). MCL is a version of Markov localization, a family of probabilistic approaches that have recently been applied with great practical success. However, previous approaches were either computationally cumbersome (such as grid-based approaches that represent the state space by high-resolution 3D grids), or had to resort to extremely coarse-grained resolutions. Our approach is computationally efficient while retaining the ability to represent (almost) arbitrary distributions. MCL applies sampling-based methods for approximating probability distributions, in a way that places computation “where needed.” The number of samples is adapted on-line, thereby invoking large sample sets only when necessary. Empirical results illustrate that MCL yields improved accuracy while requiring an order of magnitude less computation when compared to previous approaches. It is also much easier to implement.', '저널': 'Aaai/iaai', '저자': 'Dieter Fox, Wolfram Burgard, Frank Dellaert, Sebastian Thrun', '전체 인용횟수': '1705회 인용1999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023142739656771908911479956867717965566163808376636144', '페이지': '2-2', '학술 문서': 'Monte carlo localization: Efficient position estimation for mobile robotsD Fox, W Burgard, F Dellaert, S Thrun\\xa0- Aaai/iaai, 19991705회 인용 관련 학술자료 전체 44개의 버전 Monte Carlo localization: Efficient position estimation for mobile robots, Proceeding of the National Conference on Artificial Intelligence, 343–349*D Fox, W Burgard, F Dellaert, S Thrun - 19992회 인용 관련 학술자료 ', '호': '343-349'}, title='Monte carlo localization: Efficient position estimation for mobile robots', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A tutorial on graph-based SLAM': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010', '게시자': 'IEEE', '권': '2', '설명': 'Being able to build a map of the environment and to simultaneously localize within this map is an essential skill for mobile robots navigating in unknown environments in absence of external referencing systems such as GPS. This so-called simultaneous localization and mapping (SLAM) problem has been one of the most popular research topics in mobile robotics for the last two decades and efficient approaches for solving this task have been proposed. One intuitive way of formulating SLAM is to use a graph whose nodes correspond to the poses of the robot at different points in time and whose edges represent constraints between the poses. The latter are obtained from observations of the environment or from movement actions carried out by the robot. Once such a graph is constructed, the map can be computed by finding the spatial configuration of the nodes that is mostly consistent with the measurements\\xa0…', '저자': 'Giorgio Grisetti, Rainer Kümmerle, Cyrill Stachniss, Wolfram Burgard', '전체 인용횟수': '1507회 인용2011201220132014201520162017201820192020202120222023720465797101145164173177170193144', '출처': 'IEEE Intelligent Transportation Systems Magazine', '페이지': '31-43', '학술 문서': 'A tutorial on graph-based SLAMG Grisetti, R Kümmerle, C Stachniss, W Burgard\\xa0- IEEE Intelligent Transportation Systems Magazine, 20101507회 인용 관련 학술자료 전체 19개의 버전 ', '호': '4'}, title='A tutorial on graph-based SLAM', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Coordinated multi-robot exploration': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2005/5/31', '게시자': 'IEEE', '권': '21', '설명': 'In this paper, we consider the problem of exploring an unknown environment with a team of robots. As in single-robot exploration the goal is to minimize the overall exploration time. The key problem to be solved in the context of multiple robots is to choose appropriate target points for the individual robots so that they simultaneously explore different regions of the environment. We present an approach for the coordination of multiple robots, which simultaneously takes into account the cost of reaching a target point and its utility. Whenever a target point is assigned to a specific robot, the utility of the unexplored area visible from this target position is reduced. In this way, different target locations are assigned to the individual robots. We furthermore describe how our algorithm can be extended to situations in which the communication range of the robots is limited. Our technique has been implemented and tested\\xa0…', '저널': 'IEEE Transactions on robotics', '저자': 'Wolfram Burgard, Mark Moors, Cyrill Stachniss, Frank E Schneider', '전체 인용횟수': '1436회 인용200520062007200820092010201120122013201420152016201720182019202020212022202364255768290819810380858372799487757757', '페이지': '376-386', '학술 문서': 'Coordinated multi-robot explorationW Burgard, M Moors, C Stachniss, FE Schneider\\xa0- IEEE Transactions on robotics, 20051436회 인용 관련 학술자료 전체 19개의 버전 ', '호': '3'}, title='Coordinated multi-robot exploration', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Markov localization for mobile robots in dynamic environments': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1999', '권': '11', '설명': \"Localization, that is the estimation of a robot's location from sensor data, is a fundamental problem in mobile robotics. This papers presents a version of Markov localization which provides accurate position estimates and which is tailored towards dynamic environments. The key idea of Markov localization is to maintain a probability density over the space of all locations of a robot in its environment. Our approach represents this space metrically, using a fine-grained grid to approximate densities. It is able to globally localize the robot from scratch and to recover from localization failures. It is robust to approximate models of the environment (such as occupancy grid maps) and noisy sensors (such as ultrasound sensors). Our approach also includes a filtering technique which allows a mobile robot to reliably estimate its position even in densely populated environments in which crowds of people block the robot's sensors for extended periods of time. The method described here has been implemented and tested in several real-world applications of mobile robots, including the deployments of two mobile robots as interactive museum tour-guides.\", '저널': 'Journal of Artificial Intelligence Research (JAIR)', '저자': 'Dieter Fox, Sebastian Burgard, Wolfram, Thrun', '전체 인용횟수': '1415회 인용20002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202316386078748110411691827461884951624432413937232423', '학술 문서': 'Markov localization for mobile robots in dynamic environmentsD Fox, W Burgard, S Thrun\\xa0- Journal of artificial intelligence research, 19991415회 인용 관련 학술자료 전체 11개의 버전 '}, title='Markov localization for mobile robots in dynamic environments', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A probabilistic approach to concurrent mapping and localization for mobile robots': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1998/7', '게시자': 'Kluwer Academic Publishers', '권': '5', '설명': ' This paper addresses the problem of building large-scale geometric maps of indoor environments with mobile robots. It poses the map building problem as a constrained, probabilistic maximum-likelihood estimation problem. It then devises a practical algorithm for generating the most likely map from data, along with the most likely path taken by the robot. Experimental results in cyclic environments of size up to 80×25 m illustrate the appropriateness of the approach.', '저널': 'Autonomous Robots', '저자': 'Sebastian Thrun, Wolfram Burgard, Dieter Fox', '전체 인용횟수': '1340회 인용1998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202318343558858110989849184695159415640313131243433172213', '페이지': '253-271', '학술 문서': 'A probabilistic approach to concurrent mapping and localization for mobile robotsS Thrun, W Burgard, D Fox\\xa0- Autonomous Robots, 19981340회 인용 관련 학술자료 전체 16개의 버전 '}, title='A probabilistic approach to concurrent mapping and localization for mobile robots', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'MINERVA: A second-generation museum tour-guide robot': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1999/5/10', '게시자': 'IEEE', '권': '3', '설명': 'This paper describes an interactive tour-guide robot, which was successfully exhibited in a Smithsonian museum. During its two weeks of operation, the robot interacted with thousands of people, traversing more than 44 km at speeds of up to 163 cm/sec. Our approach specifically addresses issues such as safe navigation in unmodified and dynamic environments, and short-term human-robot interaction. It uses learning pervasively at all levels of the software architecture.', '저자': 'Sebastian Thrun, Maren Bennewitz, Wolfram Burgard, Armin B Cremers, Frank Dellaert, Dieter Fox, Dirk Hahnel, Charles Rosenberg, Nicholas Roy, Jamieson Schulte, Dirk Schulz', '전체 인용횟수': '1127회 인용199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202318363345454848445347464154585752555152594045313620', '컨퍼런스': 'Proceedings 1999 IEEE International Conference on Robotics and Automation (Cat. No. 99CH36288C)', '학술 문서': 'MINERVA: A second-generation museum tour-guide robotS Thrun, M Bennewitz, W Burgard, AB Cremers…\\xa0- Proceedings 1999 IEEE International Conference on\\xa0…, 19991127회 인용 관련 학술자료 전체 32개의 버전 '}, title='MINERVA: A second-generation museum tour-guide robot', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Improving grid-based slam with rao-blackwellized particle filters by adaptive proposals and selective resampling': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2005/4/18', '게시자': 'IEEE', '설명': \"Recently Rao-Blackwellized particle filters have been introduced as effective means to solve the simultaneous localization and mapping (SLAM) problem. This approach uses a particle filter in which each particle carries an individual map of the environment. Accordingly, a key question is how to reduce the number of particles. In this paper we present adaptive techniques to reduce the number of particles in a Rao-Blackwellized particle filter for learning grid maps. We propose an approach to compute an accurate proposal distribution taking into account not only the movement of the robot but also the most recent observation. This drastically decrease the uncertainty about the robot's pose in the prediction step of the filter. Furthermore, we present an approach to selectively carry out re-sampling operations which seriously reduces the problem of particle depletion. Experimental results carried out with mobile robots in\\xa0…\", '저자': 'Giorgio Grisetti, Cyrill Stachniss, Wolfram Burgard', '전체 인용횟수': '1111회 인용200520062007200820092010201120122013201420152016201720182019202020212022202320425047525142365557636186868091836736', '컨퍼런스': 'Robotics and Automation, 2005. ICRA 2005. Proceedings of the 2005 IEEE International Conference on', '페이지': '2432-2437', '학술 문서': 'Improving grid-based slam with rao-blackwellized particle filters by adaptive proposals and selective resamplingG Grisetti, C Stachniss, W Burgard\\xa0- Proceedings of the 2005 IEEE international conference\\xa0…, 20051111회 인용 관련 학술자료 전체 16개의 버전 '}, title='Improving grid-based slam with rao-blackwellized particle filters by adaptive proposals and selective resampling', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Collaborative multi-robot exploration': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2000/4/24', '게시자': 'IEEE', '권': '1', '설명': 'In this paper we consider the problem of exploring an unknown environment by a team of robots. As in single-robot exploration the goal is to minimize the overall exploration time. The key problem to be solved therefore is to choose appropriate target points for the individual robots so that they simultaneously explore different regions of their environment. We present a probabilistic approach for the coordination of multiple robots which, in contrast to previous approaches, simultaneously takes into account the costs of reaching a target point and the utility of target points. The utility of target points is given by the size of the unexplored area that a robot can cover with its sensors upon reaching a target position. Whenever a target point is assigned to a specific robot, the utility of the unexplored area visible from this target position is reduced for the other robots. This way, a team of multiple robots assigns different target\\xa0…', '저자': 'Wolfram Burgard, Mark Moors, Dieter Fox, Reid Simmons, Sebastian Thrun', '전체 인용횟수': '1102회 인용20002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202371547596053464347525055747061455635484230452426', '컨퍼런스': 'Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No. 00CH37065)', '페이지': '476-481', '학술 문서': 'Collaborative multi-robot explorationW Burgard, M Moors, D Fox, R Simmons, S Thrun\\xa0- Proceedings 2000 ICRA. Millennium Conference. IEEE\\xa0…, 20001102회 인용 관련 학술자료 전체 27개의 버전 '}, title='Collaborative multi-robot exploration', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Mapping and localization with RFID technology': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2004/4/26', '게시자': 'IEEE', '권': '1', '설명': 'We analyze whether radio frequency identification (RFID) technology can be used to improve the localization of mobile robots and persons in their environment. In particular we study the problem of localizing RFID tags with a mobile platform that is equipped with a pair of RFID antennas. We present a probabilistic measurement model for RFID readers that allow us to accurately localize RFID tags in the environment. We also demonstrate how such maps can be used to localize a robot and persons in their environment. Finally, we present experiments illustrating that the computational requirements for global robot localization can be reduced strongly by fusing RFID information with laser data.', '저자': 'Dirk Hahnel, Wolfram Burgard, Dieter Fox, Ken Fishkin, Matthai Philipose', '전체 인용횟수': '1097회 인용2004200520062007200820092010201120122013201420152016201720182019202020212022202310306870991199994726760524235422331202818', '컨퍼런스': \"IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA'04. 2004\", '페이지': '1015-1020', '학술 문서': 'Mapping and localization with RFID technologyD Hahnel, W Burgard, D Fox, K Fishkin, M Philipose\\xa0- IEEE International Conference on Robotics and\\xa0…, 20041095회 인용 관련 학술자료 전체 12개의 버전 Mapping and Localization with RFID Tags*D Hähnel, W Burgard, D Fox, K Fishkin, M Philipose\\xa0- ICRA, 20043회 인용 관련 학술자료 '}, title='Mapping and localization with RFID technology', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Dehazenet: An end-to-end system for single image haze removal': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/8/10', '게시자': 'IEEE', '권': '25', '설명': 'Single image haze removal is a challenging ill-posed problem. Existing methods use various constraints/priors to get plausible dehazing solutions. The key to achieve haze removal is to estimate a medium transmission map for an input hazy image. In this paper, we propose a trainable end-to-end system called DehazeNet, for medium transmission estimation. DehazeNet takes a hazy image as input, and outputs its medium transmission map that is subsequently used to recover a haze-free image via atmospheric scattering model. DehazeNet adopts convolutional neural network-based deep architecture, whose layers are specially designed to embody the established assumptions/priors in image dehazing. Specifically, the layers of Maxout units are used for feature extraction, which can generate almost all haze-relevant features. We also propose a novel nonlinear activation function in DehazeNet, called bilateral\\xa0…', '저널': 'IEEE transactions on image processing', '저자': 'Bolun Cai, Xiangmin Xu, Kui Jia, Chunmei Qing, Dacheng Tao', '전체 인용횟수': '2486회 인용201620172018201920202021202220231077231299373461517507', '페이지': '5187-5198', '학술 문서': 'Dehazenet: An end-to-end system for single image haze removalB Cai, X Xu, K Jia, C Qing, D Tao\\xa0- IEEE transactions on image processing, 20162486회 인용 관련 학술자료 전체 12개의 버전 ', '호': '11'}, title='Dehazenet: An end-to-end system for single image haze removal', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep ordinal regression network for monocular depth estimation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'Monocular depth estimation, which plays a crucial role in understanding 3D scene geometry, is an ill-posed prob-lem. Recent methods have gained significant improvement by exploring image-level information and hierarchical features from deep convolutional neural networks (DCNNs). These methods model depth estimation as a regression problem and train the regression networks by minimizing mean squared error, which suffers from slow convergence and unsatisfactory local solutions. Besides, existing depth estimation networks employ repeated spatial pooling operations, resulting in undesirable low-resolution feature maps. To obtain high-resolution depth maps, skip-connections or multi-layer deconvolution networks are required, which complicates network training and consumes much more computations. To eliminate or at least largely reduce these problems, we introduce a spacing-increasing discretization (SID) strategy to discretize depth and recast depth network learning as an ordinal regression problem. By training the network using an ordinary regression loss, our method achieves much higher accuracy and faster convergence in synch. Furthermore, we adopt a multi-scale network structure which avoids unnecessary spatial pooling and captures multi-scale information in parallel. The proposed deep ordinal regression network (DORN) achieves state-of-the-art results on three challenging benchmarks, ie, KITTI [16], Make3D [49], and NYU Depth v2 [41], and outperforms existing methods by a large margin.', '저자': 'Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, Dacheng Tao', '전체 인용횟수': '1623회 인용20182019202020212022202316162292385418337', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '2002-2011', '학술 문서': 'Deep ordinal regression network for monocular depth estimationH Fu, M Gong, C Wang, K Batmanghelich, D Tao\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20181623회 인용 관련 학술자료 전체 17개의 버전 '}, title='Deep ordinal regression network for monocular depth estimation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Knowledge distillation: A survey': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021/6', '게시자': 'Springer US', '권': '129', '설명': ' In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the\\xa0…', '저널': 'International Journal of Computer Vision', '저자': 'Jianping Gou, Baosheng Yu, Stephen J Maybank, Dacheng Tao', '전체 인용횟수': '1612회 인용202020212022202312185559847', '페이지': '1789-1819', '학술 문서': 'Knowledge distillation: A surveyJ Gou, B Yu, SJ Maybank, D Tao\\xa0- International Journal of Computer Vision, 20211612회 인용 관련 학술자료 전체 10개의 버전 '}, title='Knowledge distillation: A survey', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the BRATS challenge': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/11/5', '설명': 'Gliomas are the most common primary brain malignancies, with different degrees of aggressiveness, variable prognosis and various heterogeneous histologic sub-regions, i.e., peritumoral edematous/invaded tissue, necrotic core, active and non-enhancing core. This intrinsic heterogeneity is also portrayed in their radio-phenotype, as their sub-regions are depicted by varying intensity profiles disseminated across multi-parametric magnetic resonance imaging (mpMRI) scans, reflecting varying biological properties. Their heterogeneous shape, extent, and location are some of the factors that make these tumors difficult to resect, and in some cases inoperable. The amount of resected tumor is a factor also considered in longitudinal scans, when evaluating the apparent tumor for potential diagnosis of progression. Furthermore, there is mounting evidence that accurate segmentation of the various tumor sub-regions can offer the basis for quantitative image analysis towards prediction of patient overall survival. This study assesses the state-of-the-art machine learning (ML) methods used for brain tumor image analysis in mpMRI scans, during the last seven instances of the International Brain Tumor Segmentation (BraTS) challenge, i.e., 2012-2018. Specifically, we focus on i) evaluating segmentations of the various glioma sub-regions in pre-operative mpMRI scans, ii) assessing potential tumor progression by virtue of longitudinal growth of tumor sub-regions, beyond use of the RECIST/RANO criteria, and iii) predicting the overall survival from pre-operative mpMRI scans of patients that underwent gross total resection. Finally, we investigate the\\xa0…', '저널': 'arXiv preprint arXiv:1811.02629', '저자': 'Spyridon Bakas, Mauricio Reyes, Andras Jakab, Stefan Bauer, Markus Rempfler, Alessandro Crimi, Russell Takeshi Shinohara, Christoph Berger, Sung Min Ha, Martin Rozycki, Marcel Prastawa, Esther Alberts, Jana Lipkova, John Freymann, Justin Kirby, Michel Bilello, Hassan Fathallah-Shaykh, Roland Wiest, Jan Kirschke, Benedikt Wiestler, Rivka Colen, Aikaterini Kotrotsou, Pamela Lamontagne, Daniel Marcus, Mikhail Milchenko, Arash Nazeri, Marc-Andre Weber, Abhishek Mahajan, Ujjwal Baid, Elizabeth Gerstner, Dongjin Kwon, Gagan Acharya, Manu Agarwal, Mahbubul Alam, Alberto Albiol, Antonio Albiol, Francisco J Albiol, Varghese Alex, Nigel Allinson, Pedro HA Amorim, Abhijit Amrutkar, Ganesh Anand, Simon Andermatt, Tal Arbel, Pablo Arbelaez, Aaron Avery, Muneeza Azmat, W Bai, Subhashis Banerjee, Bill Barth, Thomas Batchelder, Kayhan Batmanghelich, Enzo Battistella, Andrew Beers, Mikhail Belyaev, Martin Bendszus, Eze Benson, Jose Bernal, Halandur Nagaraja Bharath, George Biros, Sotirios Bisdas, James Brown, Mariano Cabezas, Shilei Cao, Jorge M Cardoso, Eric N Carver, Adrià Casamitjana, Laura Silvana Castillo, Marcel Catà, Philippe Cattin, Albert Cerigues, Vinicius S Chagas, Siddhartha Chandra, Yi-Ju Chang, Shiyu Chang, Ken Chang, Joseph Chazalon, Shengcong Chen, Wei Chen, Jefferson W Chen, Zhaolin Chen, Kun Cheng, Ahana Roy Choudhury, Roger Chylla, Albert Clérigues, Steven Colleman, Ramiro German Rodriguez Colmeiro, Marc Combalia, Anthony Costa, Xiaomeng Cui, Zhenzhen Dai, Lutao Dai, Laura Alexandra Daza, Eric Deutsch, Changxing Ding, Chao Dong, Shidu Dong, Wojciech Dudzik, Zach Eaton-Rosen, Gary Egan, Guilherme Escudero, Théo Estienne, Richard Everson, Jonathan Fabrizio, Yong Fan, Longwei Fang, Xue Feng, Enzo Ferrante, Lucas Fidon, Martin Fischer, Andrew P French, Naomi Fridman, Huan Fu, David Fuentes, Yaozong Gao, Evan Gates, David Gering, Amir Gholami, Willi Gierke, Ben Glocker, Mingming Gong, Sandra González-Villá, T Grosges, Yuanfang Guan, Sheng Guo, Sudeep Gupta, Woo-Sup Han, Il Song Han, Konstantin Harmuth, Huiguang He, Aura Hernández-Sabaté, Evelyn Herrmann, Naveen Himthani, Winston Hsu, Cheyu Hsu, Xiaojun Hu, Xiaobin Hu, Yan Hu, Yifan Hu, Rui Hua, Teng-Yi Huang, Weilin Huang, Sabine Van Huffel, Quan Huo, Vivek HV, Khan M Iftekharuddin, Fabian Isensee, Mobarakol Islam, Aaron S Jackson, Sachin R Jambawalikar', '전체 인용횟수': '1610회 인용2018201920202021202220236105285407391401', '학술 문서': 'Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the BRATS challengeS Bakas, M Reyes, A Jakab, S Bauer, M Rempfler…\\xa0- arXiv preprint arXiv:1811.02629, 20181610회 인용 관련 학술자료 전체 10개의 버전 '}, title='Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the BRATS challenge', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'General tensor discriminant analysis and gabor features for gait recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2007/8/27', '게시자': 'IEEE', '권': '29', '설명': 'The traditional image representations are not suited to conventional classification methods, such as the linear discriminant analysis (LDA), because of the under sample problem (USP): the dimensionality of the feature space is much higher than the number of training samples. Motivated by the successes of the two dimensional LDA (2DLDA) for face recognition, we develop a general tensor discriminant analysis (GTDA) as a preprocessing step for LDA. The benefits of GTDA compared with existing preprocessing methods, e.g., principal component analysis (PCA) and 2DLDA, include 1) the USP is reduced in subsequent classification by, for example, LDA; 2) the discriminative information in the training tensors is preserved; and 3) GTDA provides stable recognition rates because the alternating projection optimization algorithm to obtain a solution of GTDA converges, while that of 2DLDA does not.We use human\\xa0…', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Dacheng Tao, Xuelong Li, Xindong Wu, Stephen J Maybank', '전체 인용횟수': '1344회 인용200720082009201020112012201320142015201620172018201920202021202220238788991798911910713012597916357483329', '페이지': '1700-1715', '학술 문서': 'General tensor discriminant analysis and gabor features for gait recognitionD Tao, X Li, X Wu, SJ Maybank\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20071344회 인용 관련 학술자료 전체 18개의 버전 ', '호': '10'}, title='General tensor discriminant analysis and gabor features for gait recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A survey on multi-view learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/4/20', '설명': 'In recent years, a great many methods of learning from multi-view data by considering the diversity of different views have been proposed. These views may be obtained from multiple sources or different feature subsets. In trying to organize and highlight similarities and differences between the variety of multi-view learning approaches, we review a number of representative multi-view learning algorithms in different areas and classify them into three groups: 1) co-training, 2) multiple kernel learning, and 3) subspace learning. Notably, co-training style algorithms train alternately to maximize the mutual agreement on two distinct views of the data; multiple kernel learning algorithms exploit kernels that naturally correspond to different views and combine kernels either linearly or non-linearly to improve learning performance; and subspace learning algorithms aim to obtain a latent subspace shared by multiple views by assuming that the input views are generated from this latent subspace. Though there is significant variance in the approaches to integrating multiple views to improve learning performance, they mainly exploit either the consensus principle or the complementary principle to ensure the success of multi-view learning. Since accessing multiple views is the fundament of multi-view learning, with the exception of study on learning a model from multiple views, it is also valuable to study how to construct multiple views and how to evaluate these views. Overall, by exploring the consistency and complementary properties of different views, multi-view learning is rendered more effective, more promising, and has better generalization ability than\\xa0…', '저널': 'arXiv preprint arXiv:1304.5634', '저자': 'Chang Xu, Dacheng Tao, Chao Xu', '전체 인용횟수': '1289회 인용2013201420152016201720182019202020212022202343062119148151178164166133119', '학술 문서': 'A survey on multi-view learningC Xu, D Tao, C Xu\\xa0- arXiv preprint arXiv:1304.5634, 20131262회 인용 관련 학술자료 전체 7개의 버전 Chao Xu*C Xu, D Tao\\xa0- A survey on multi-view learning. CoRR, abs/1304.5634, 201344회 인용 관련 학술자료 '}, title='A survey on multi-view learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Benchmarking single-image dehazing and beyond': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/8/30', '게시자': 'IEEE', '권': '28', '설명': 'We present a comprehensive study and evaluation of existing single-image dehazing algorithms, using a new large-scale benchmark consisting of both synthetic and real-world hazy images, called REalistic Single-Image DEhazing (RESIDE). RESIDE highlights diverse data sources and image contents, and is divided into five subsets, each serving different training or evaluation purposes. We further provide a rich variety of criteria for dehazing algorithm evaluation, ranging from full-reference metrics to no-reference metrics and to subjective evaluation, and the novel task-driven evaluation. Experiments on RESIDE shed light on the comparisons and limitations of the state-of-the-art dehazing algorithms, and suggest promising future directions.', '저널': 'IEEE Transactions on Image Processing', '저자': 'Boyi Li, Wenqi Ren, Dengpan Fu, Dacheng Tao, Dan Feng, Wenjun Zeng, Zhangyang Wang', '전체 인용횟수': '1254회 인용201820192020202120222023980178262350370', '페이지': '492-505', '학술 문서': 'Benchmarking single-image dehazing and beyondB Li, W Ren, D Fu, D Tao, D Feng, W Zeng, Z Wang\\xa0- IEEE Transactions on Image Processing, 20181254회 인용 관련 학술자료 전체 11개의 버전 ', '호': '1'}, title='Benchmarking single-image dehazing and beyond', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Asymmetric bagging and random subspace for support vector machines-based relevance feedback in image retrieval': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2006/6/5', '게시자': 'IEEE', '권': '28', '설명': \"Relevance feedback schemes based on support vector machines (SVM) have been widely used in content-based image retrieval (CBIR). However, the performance of SVM-based relevance feedback is often poor when the number of labeled positive feedback samples is small. This is mainly due to three reasons: 1) an SVM classifier is unstable on a small-sized training set, 2) SVM's optimal hyperplane may be biased when the positive feedback samples are much less than the negative feedback samples, and 3) overfitting happens because the number of feature dimensions is much higher than the size of the training set. In this paper, we develop a mechanism to overcome these problems. To address the first two problems, we propose an asymmetric bagging-based SVM (AB-SVM). For the third problem, we combine the random subspace method and SVM for relevance feedback, which is named random\\xa0…\", '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Dacheng Tao, Xiaoou Tang, Xuelong Li, Xindong Wu', '전체 인용횟수': '1075회 인용2006200720082009201020112012201320142015201620172018201920202021202220235275466566357108831089583674347423625', '페이지': '1088-1099', '학술 문서': 'Asymmetric bagging and random subspace for support vector machines-based relevance feedback in image retrievalD Tao, X Tang, X Li, X Wu\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20061075회 인용 관련 학술자료 전체 14개의 버전 ', '호': '7'}, title='Asymmetric bagging and random subspace for support vector machines-based relevance feedback in image retrieval', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'An underwater image enhancement benchmark dataset and beyond': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019/11/28', '게시자': 'IEEE', '권': '29', '설명': 'Underwater image enhancement has been attracting much attention due to its significance in marine engineering and aquatic robotics. Numerous underwater image enhancement algorithms have been proposed in the last few years. However, these algorithms are mainly evaluated using either synthetic datasets or few selected real-world images. It is thus unclear how these algorithms would perform on images acquired in the wild and how we could gauge the progress in the field. To bridge this gap, we present the first comprehensive perceptual study and analysis of underwater image enhancement using large-scale real-world images. In this paper, we construct an Underwater Image Enhancement Benchmark (UIEB) including 950 real-world underwater images, 890 of which have the corresponding reference images. We treat the rest 60 underwater images which cannot obtain satisfactory reference images as\\xa0…', '저널': 'IEEE Transactions on Image Processing', '저자': 'Chongyi Li, Chunle Guo, Wenqi Ren, Runmin Cong, Junhui Hou, Sam Kwong, Dacheng Tao', '전체 인용횟수': '928회 인용201920202021202220231479164312355', '페이지': '4376-4389', '학술 문서': 'An underwater image enhancement benchmark dataset and beyondC Li, C Guo, W Ren, R Cong, J Hou, S Kwong, D Tao\\xa0- IEEE Transactions on Image Processing, 2019928회 인용 관련 학술자료 전체 9개의 버전 '}, title='An underwater image enhancement benchmark dataset and beyond', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A survey of graph edit distance': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/2', '게시자': 'Springer-Verlag', '권': '13', '설명': ' Inexact graph matching has been one of the significant research foci in the area of pattern analysis. As an important way to measure the similarity between pairwise graphs error-tolerantly, graph edit distance (GED) is the base of inexact graph matching. The research advance of GED is surveyed in order to provide a review of the existing literatures and offer some insights into the studies of GED. Since graphs may be attributed or non-attributed and the definition of costs for edit operations is various, the existing GED algorithms are categorized according to these two factors and described in detail. After these algorithms are analyzed and their limitations are identified, several promising directions for further research are proposed.', '저널': 'Pattern Analysis and applications', '저자': 'Xinbo Gao, Bing Xiao, Dacheng Tao, Xuelong Li', '전체 인용횟수': '881회 인용201020112012201320142015201620172018201920202021202220231027316161787275937465828164', '페이지': '113-129', '학술 문서': 'A survey of graph edit distanceX Gao, B Xiao, D Tao, X Li\\xa0- Pattern Analysis and applications, 2010881회 인용 관련 학술자료 전체 15개의 버전 '}, title='A survey of graph edit distance', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A survey on vision transformer': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2022/2/18', '게시자': 'IEEE', '권': '45', '설명': 'Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer\\xa0…', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang, Dacheng Tao', '전체 인용횟수': '846회 인용20212022202312215604', '페이지': '87-110', '학술 문서': 'A survey on vision transformerK Han, Y Wang, H Chen, X Chen, J Guo, Z Liu, Y Tang…\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 2022846회 인용 관련 학술자료 전체 7개의 버전 ', '호': '1'}, title='A survey on vision transformer', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Classification with noisy labels by importance reweighting': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/7/15', '게시자': 'IEEE', '권': '38', '설명': 'In this paper, we study a classification problem in which sample labels are randomly corrupted. In this scenario, there is an unobservable sample with noise-free labels. However, before being observed, the true labels are independently flipped with a probability   , and the random label noise can be class-conditional. Here, we address two fundamental problems raised by this scenario. The first is how to best use the abundant surrogate loss functions designed for the traditional classification problem when there is label noise. We prove that any surrogate loss function can be used for classification with noisy labels by using importance reweighting, with consistency assurance that the label noise does not ultimately hinder the search for the optimal classifier of the noise-free sample. The other is the open problem of how to obtain the noise rate   . We show that the rate is upper bounded by the conditional\\xa0…', '저널': 'IEEE Transactions on pattern analysis and machine intelligence', '저자': 'Tongliang Liu, Dacheng Tao', '전체 인용횟수': '836회 인용2015201620172018201920202021202220231277596785106156138134', '페이지': '447-461', '학술 문서': 'Classification with noisy labels by importance reweightingT Liu, D Tao\\xa0- IEEE Transactions on pattern analysis and machine\\xa0…, 2015836회 인용 관련 학술자료 전체 15개의 버전 ', '호': '3'}, title='Classification with noisy labels by importance reweighting', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Godec: Randomized low-rank & sparse matrix decomposition in noisy case': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011/10/7', '설명': 'Low-rank and sparse structures have been profoundly studied in matrix completion and compressed sensing. In this paper, we develop \"Go Decomposition\" (GoDec) to efficiently and robustly estimate the low-rank part L and the sparse part 5 of a matrix X = L + S + G with noise G. GoDec alternatively assigns the low-rank approximation of X - S to L and the sparse approximation of X - L to S. The algorithm can be significantly accelerated by bilateral random projections (BRP). We also propose GoDec for matrix completion as an important variant. We prove that the objective value ∥X - L - S∥F2 converges to a local minimum, while L and S linearly converge to local optimums. Theoretically, we analyze the influence of L, S and G to the asymptotic/convergence speeds in order to discover the robustness of GoDec. Empirical studies suggest the efficiency, robustness and effectiveness of GoDec comparing with representative matrix decomposition and completion tools, e.g., Robust PCA and OptSpace. Copyright 2011 by the author(s)/owner(s).', '저널': 'Proceedings of the 28th International Conference on Machine Learning, ICML 2011', '저자': 'Tianyi Zhou, Dacheng Tao', '전체 인용횟수': '799회 인용20112012201320142015201620172018201920202021202220232232650697489968275777454', '학술 문서': 'Godec: Randomized low-rank & sparse matrix decomposition in noisy caseT Zhou, D Tao\\xa0- Proceedings of the 28th International Conference on\\xa0…, 2011799회 인용 관련 학술자료 전체 12개의 버전 '}, title='Godec: Randomized low-rank & sparse matrix decomposition in noisy case', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Multi-store tracker (muster): A cognitive psychology inspired approach to object tracking': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'Variations in the appearance of a tracked object, such as changes in geometry/photometry, camera viewpoint, illumination, or partial occlusion, pose a major challenge to object tracking. Here, we adopt cognitive psychology principles to design a flexible representation that can adapt to changes in object appearance during tracking. Inspired by the well-known Atkinson-Shiffrin Memory Model, we propose MUlti-Store Tracker (MUSTer), a dual-component approach consisting of short-and long-term memory stores to process target appearance memories. A powerful and efficient Integrated Correlation Filter (ICF) is employed in the short-term store for short-term tracking. The integrated long-term component, which is based on keypoint matching-tracking and RANSAC estimation, can interact with the long-term memory and provide additional information for output control. MUSTer was extensively evaluated on the CVPR2013 Online Object Tracking Benchmark (OOTB) and ALOV++ datasets. The experimental results demonstrated the superior performance of MUSTer in comparison with other state-of-art trackers.', '저자': 'Zhibin Hong, Zhe Chen, Chaohui Wang, Xue Mei, Danil Prokhorov, Dacheng Tao', '전체 인용횟수': '751회 인용201520162017201820192020202120222023117395162131104804828', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '749-758', '학술 문서': 'Multi-store tracker (muster): A cognitive psychology inspired approach to object trackingZ Hong, Z Chen, C Wang, X Mei, D Prokhorov, D Tao\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2015751회 인용 관련 학술자료 전체 16개의 버전 '}, title='Multi-store tracker (muster): A cognitive psychology inspired approach to object tracking', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep modular co-attention networks for visual question answering': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': \"Visual Question Answering (VQA) requires a fine-grained and simultaneous understanding of both the visual content of images and the textual content of questions. Therefore, designing an effectiveco-attention'model to associate key words in questions with key objects in images is central to VQA performance. So far, most successful attempts at co-attention learning have been achieved by using shallow models, and deep co-attention models show little improvement over their shallow counterparts. In this paper, we propose a deep Modular Co-Attention Network (MCAN) that consists of Modular Co-Attention (MCA) layers cascaded in depth. Each MCA layer models the self-attention of questions and images, as well as the question-guided-attention of images jointly using a modular composition of two basic attention units. We quantitatively and qualitatively evaluate MCAN on the benchmark VQA-v2 dataset and conduct extensive ablation studies to explore the reasons behind MCAN's effectiveness. Experimental results demonstrate that MCAN significantly outperforms the previous state-of-the-art. Our best single model delivers 70.63% overall accuracy on the test-dev set.\", '저자': 'Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, Qi Tian', '전체 인용횟수': '751회 인용2019202020212022202312113182196247', '컨퍼런스': 'Proceedings of the IEEE/CVF conference on computer vision and pattern recognition', '페이지': '6281-6290', '학술 문서': 'Deep modular co-attention networks for visual question answeringZ Yu, J Yu, Y Cui, D Tao, Q Tian\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 2019751회 인용 관련 학술자료 전체 12개의 버전 '}, title='Deep modular co-attention networks for visual question answering', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Multi-modal factorized bilinear pooling with co-attention learning for visual question answering': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'Visual question answering (VQA) is challenging because it requires a simultaneous understanding of both the visual content of images and the textual content of questions. The approaches used to represent the images and questions in a fine-grained manner and questions and to fuse these multi-modal features play key roles in performance. Bilinear pooling based models have been shown to outperform traditional linear models for VQA, but their high-dimensional representations and high computational complexity may seriously limit their applicability in practice. For multi-modal feature fusion, here we develop a Multi-modal Factorized Bilinear (MFB) pooling approach to efficiently and effectively combine multi-modal features, which results in superior performance for VQA compared with other bilinear pooling approaches. For fine-grained image and question representation, we develop a co-attention mechanism using an end-to-end deep network architecture to jointly learn both the image and question attentions. Combining the proposed MFB approach with co-attention learning in a new network architecture provides a unified model for VQA. Our experimental results demonstrate that the single MFB with co-attention model achieves new state-of-the-art performance on the real-world VQA dataset. Code available at https://github. com/yuzcccc/mfb', '저자': 'Zhou Yu, Jun Yu, Jianping Fan, Dacheng Tao', '전체 인용횟수': '683회 인용2017201820192020202120222023542101114139153125', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '1821-1830', '학술 문서': 'Multi-modal factorized bilinear pooling with co-attention learning for visual question answeringZ Yu, J Yu, J Fan, D Tao\\xa0- Proceedings of the IEEE international conference on\\xa0…, 2017683회 인용 관련 학술자료 전체 10개의 버전 '}, title='Multi-modal factorized bilinear pooling with co-attention learning for visual question answering', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Superpixel classification based optic disc and optic cup segmentation for glaucoma screening': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/2/18', '게시자': 'IEEE', '권': '32', '설명': 'Glaucoma is a chronic eye disease that leads to vision loss. As it cannot be cured, detecting the disease in time is important. Current tests using intraocular pressure (IOP) are not sensitive enough for population based glaucoma screening. Optic nerve head assessment in retinal fundus images is both more promising and superior. This paper proposes optic disc and optic cup segmentation using superpixel classification for glaucoma screening. In optic disc segmentation, histograms, and center surround statistics are used to classify each superpixel as disc or non-disc. A self-assessment reliability score is computed to evaluate the quality of the automated optic disc segmentation. For optic cup segmentation, in addition to the histograms and center surround statistics, the location information is also included into the feature space to boost the performance. The proposed segmentation methods have been evaluated in\\xa0…', '저널': 'IEEE transactions on medical imaging', '저자': 'Jun Cheng, Jiang Liu, Yanwu Xu, Fengshou Yin, Damon Wing Kee Wong, Ngan-Meng Tan, Dacheng Tao, Ching-Yu Cheng, Tin Aung, Tien Yin Wong', '전체 인용횟수': '678회 인용20132014201520162017201820192020202120222023424644660839781896062', '페이지': '1019-1032', '학술 문서': 'Superpixel classification based optic disc and optic cup segmentation for glaucoma screeningJ Cheng, J Liu, Y Xu, F Yin, DWK Wong, NM Tan…\\xa0- IEEE transactions on medical imaging, 2013678회 인용 관련 학술자료 전체 8개의 버전 ', '호': '6'}, title='Superpixel classification based optic disc and optic cup segmentation for glaucoma screening', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A review on generative adversarial networks: Algorithms, theory, and applications': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021/11/23', '게시자': 'IEEE', '권': '35', '설명': 'Generative adversarial networks (GANs) have recently become a hot research topic; however, they have been studied since 2014, and a large number of algorithms have been proposed. Nevertheless, few comprehensive studies explain the connections among different GAN variants and how they have evolved. In this paper, we attempt to provide a review of the various GAN methods from the perspectives of algorithms, theory, and applications. First, the motivations, mathematical representations, and structures of most GAN algorithms are introduced in detail, and we compare their commonalities and differences. Second, theoretical issues related to GANs are investigated. Finally, typical applications of GANs in image processing and computer vision, natural language processing, music, speech and audio, the medical field, and data science are discussed.', '저자': 'Jie Gui, Zhenan Sun, Yonggang Wen, Dacheng Tao, Jieping Ye', '전체 인용횟수': '673회 인용202020212022202342134219276', '출처': 'IEEE transactions on knowledge and data engineering', '페이지': '3313-3332', '학술 문서': 'A review on generative adversarial networks: Algorithms, theory, and applicationsJ Gui, Z Sun, Y Wen, D Tao, J Ye\\xa0- IEEE transactions on knowledge and data engineering, 2021673회 인용 관련 학술자료 전체 12개의 버전 ', '호': '4'}, title='A review on generative adversarial networks: Algorithms, theory, and applications', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Probabilistic graphical models: principles and techniques': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009/7/31', '게시자': 'MIT press', '설명': 'A general framework for constructing and using probabilistic models of complex systems that would enable a computer to use available information for making decisions. Most tasks require a person or an automated system to reason—to reach conclusions based on available information. The framework of probabilistic graphical models, presented in this book, provides a general approach for this task. The approach is model-based, allowing interpretable models to be constructed and then manipulated by reasoning algorithms. These models can also be learned automatically from data, allowing the approach to be used in cases where manually constructing a model is difficult or even impossible. Because uncertainty is an inescapable aspect of most real-world applications, the book focuses on probabilistic models, which make the uncertainty explicit and provide models that are more faithful to reality. Probabilistic Graphical Models discusses a variety of models, spanning Bayesian networks, undirected Markov networks, discrete and continuous models, and extensions to deal with dynamical systems and relational data. For each class of models, the text describes the three fundamental cornerstones: representation, inference, and learning, presenting both basic concepts and advanced techniques. Finally, the book considers the use of the proposed framework for causal reasoning and decision making under uncertainty. The main text in each chapter provides the detailed technical development of the key ideas. Most chapters also include boxes with additional material: skill boxes, which describe techniques; case study boxes, which discuss\\xa0…', '저자': 'Daphne Koller, Nir Friedman', '전체 인용횟수': '10436회 인용20102011201220132014201520162017201820192020202120222023159344503669778799863829905885959922906774', '학술 문서': 'Probabilistic graphical models: principles and techniquesD Koller, N Friedman - 200910436회 인용 관련 학술자료 전체 15개의 버전 '}, title='Probabilistic graphical models: principles and techniques', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The genotype-tissue expression (GTEx) project': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/6', '게시자': 'Nature Publishing Group', '권': '45', '설명': 'Genome-wide association studies have identified thousands of loci for common diseases, but, for the majority of these, the mechanisms underlying disease susceptibility remain unknown. Most associated variants are not correlated with protein-coding changes, suggesting that polymorphisms in regulatory regions probably contribute to many disease phenotypes. Here we describe the Genotype-Tissue Expression (GTEx) project, which will establish a resource database and associated tissue bank for the scientific community to study the relationship between genetic variation and gene expression in human tissues.', '저널': 'Nature genetics', '저자': 'John Lonsdale, Jeffrey Thomas, Mike Salvatore, Rebecca Phillips, Edmund Lo, Saboor Shad, Richard Hasz, Gary Walters, Fernando Garcia, Nancy Young, Barbara Foster, Mike Moser, Ellen Karasik, Bryan Gillard, Kimberley Ramsey, Susan Sullivan, Jason Bridge, Harold Magazine, John Syron, Johnelle Fleming, Laura Siminoff, Heather Traino, Maghboeba Mosavel, Laura Barker, Scott Jewell, Dan Rohrer, Dan Maxim, Dana Filkins, Philip Harbach, Eddie Cortadillo, Bree Berghuis, Lisa Turner, Eric Hudson, Kristin Feenstra, Leslie Sobin, James Robb, Phillip Branton, Greg Korzeniewski, Charles Shive, David Tabor, Liqun Qi, Kevin Groch, Sreenath Nampally, Steve Buia, Angela Zimmerman, Anna Smith, Robin Burges, Karna Robinson, Kim Valentino, Deborah Bradbury, Mark Cosentino, Norma Diaz-Mayoral, Mary Kennedy, Theresa Engel, Penelope Williams, Kenyon Erickson, Kristin Ardlie, Wendy Winckler, Gad Getz, David DeLuca, Daniel MacArthur, Manolis Kellis, Alexander Thomson, Taylor Young, Ellen Gelfand, Molly Donovan, Yan Meng, George Grant, Deborah Mash, Yvonne Marcus, Margaret Basile, Jun Liu, Jun Zhu, Zhidong Tu, Nancy J Cox, Dan L Nicolae, Eric R Gamazon, Hae Kyung Im, Anuar Konkashbaev, Jonathan Pritchard, Matthew Stevens, Timothèe Flutre, Xiaoquan Wen, Emmanouil T Dermitzakis, Tuuli Lappalainen, Roderic Guigo, Jean Monlong, Michael Sammeth, Daphne Koller, Alexis Battle, Sara Mostafavi, Mark McCarthy, Manual Rivas, Julian Maller, Ivan Rusyn, Andrew Nobel, Fred Wright, Andrey Shabalin, Mike Feolo, Nataliya Sharopova, Anne Sturcke, Justin Paschal, James M Anderson, Elizabeth L Wilder, Leslie K Derr, Eric D Green, Jeffery P Struewing, Gary Temple, Simona Volpi, Joy T Boyer, Elizabeth J Thomson, Mark S Guyer, Cathy Ng, Assya Abdallah, Deborah Colantuoni, Thomas R Insel, Susan E Koester, A Roger Little, Patrick K Bender, Thomas Lehner, Yin Yao, Carolyn C Compton, Jimmie B Vaught, Sherilyn Sawyer, Nicole C Lockhart, Joanne Demchok, Helen F Moore', '전체 인용횟수': '7037회 인용201420152016201720182019202020212022202311621632247871888599911981121945', '페이지': '580-585', '학술 문서': 'The genotype-tissue expression (GTEx) projectJ Lonsdale, J Thomas, M Salvatore, R Phillips, E Lo…\\xa0- Nature genetics, 20137037회 인용 관련 학술자료 전체 21개의 버전 ', '호': '6'}, title='The genotype-tissue expression (GTEx) project', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The Genotype-Tissue Expression (GTEx) pilot analysis: multitissue gene regulation in humans': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/5/8', '게시자': 'American Association for the Advancement of Science', '권': '348', '설명': 'Understanding the functional consequences of genetic variation, and how it affects complex human disease and quantitative traits, remains a critical challenge for biomedicine. We present an analysis of RNA sequencing data from 1641 samples across 43 tissues from 175 individuals, generated as part of the pilot phase of the Genotype-Tissue Expression (GTEx) project. We describe the landscape of gene expression across tissues, catalog thousands of tissue-specific and shared regulatory expression quantitative trait loci (eQTL) variants, describe complex network relationships, and identify signals from genome-wide association studies explained by eQTLs. These findings provide a systematic understanding of the cellular and biological consequences of human genetic variation and of the heterogeneity of such effects among a diverse set of human tissues.', '저널': 'Science', '저자': 'GTEx Consortium, Kristin G Ardlie, David S Deluca, Ayellet V Segrè, Timothy J Sullivan, Taylor R Young, Ellen T Gelfand, Casandra A Trowbridge, Julian B Maller, Taru Tukiainen, Monkol Lek, Lucas D Ward, Pouya Kheradpour, Benjamin Iriarte, Yan Meng, Cameron D Palmer, Tõnu Esko, Wendy Winckler, Joel N Hirschhorn, Manolis Kellis, Daniel G MacArthur, Gad Getz, Andrey A Shabalin, Gen Li, Yi-Hui Zhou, Andrew B Nobel, Ivan Rusyn, Fred A Wright, Tuuli Lappalainen, Pedro G Ferreira, Halit Ongen, Manuel A Rivas, Alexis Battle, Sara Mostafavi, Jean Monlong, Michael Sammeth, Marta Mele, Ferran Reverter, Jakob M Goldmann, Daphne Koller, Roderic Guigó, Mark I McCarthy, Emmanouil T Dermitzakis, Eric R Gamazon, Hae Kyung Im, Anuar Konkashbaev, Dan L Nicolae, Nancy J Cox, Timothée Flutre, Xiaoquan Wen, Matthew Stephens, Jonathan K Pritchard, Zhidong Tu, Bin Zhang, Tao Huang, Quan Long, Luan Lin, Jialiang Yang, Jun Zhu, Jun Liu, Amanda Brown, Bernadette Mestichelli, Denee Tidwell, Edmund Lo, Mike Salvatore, Saboor Shad, Jeffrey A Thomas, John T Lonsdale, Michael T Moser, Bryan M Gillard, Ellen Karasik, Kimberly Ramsey, Christopher Choi, Barbara A Foster, John Syron, Johnell Fleming, Harold Magazine, Rick Hasz, Gary D Walters, Jason P Bridge, Mark Miklos, Susan Sullivan, Laura K Barker, Heather M Traino, Maghboeba Mosavel, Laura A Siminoff, Dana R Valley, Daniel C Rohrer, Scott D Jewell, Philip A Branton, Leslie H Sobin, Mary Barcus, Liqun Qi, Jeffrey McLean, Pushpa Hariharan, Ki Sung Um, Shenpei Wu, David Tabor, Charles Shive, Anna M Smith, Stephen A Buia, Anita H Undale, Karna L Robinson, Nancy Roche, Kimberly M Valentino, Angela Britton, Robin Burges, Debra Bradbury, Kenneth W Hambright, John Seleski, Greg E Korzeniewski, Kenyon Erickson, Yvonne Marcus, Jorge Tejada, Mehran Taherian, Chunrong Lu, Margaret Basile, Deborah C Mash, Simona Volpi, Jeffery P Struewing, Gary F Temple, Joy Boyer, Deborah Colantuoni, Roger Little, Susan Koester, Latarsha J Carithers, Helen M Moore, Ping Guan, Carolyn Compton, Sherilyn J Sawyer, Joanne P Demchok, Jimmie B Vaught, Chana A Rabiner, Nicole C Lockhart, Kristin G Ardlie, Gad Getz, Fred A Wright, Manolis Kellis, Simona Volpi, Emmanouil T Dermitzakis', '전체 인용횟수': '4352회 인용20152016201720182019202020212022202395386565681563591637450362', '페이지': '648-660', '학술 문서': 'The Genotype-Tissue Expression (GTEx) pilot analysis: multitissue gene regulation in humansGTEx Consortium, KG Ardlie, DS Deluca, AV Segrè…\\xa0- Science, 20154341회 인용 관련 학술자료 전체 34개의 버전 Ardlie, KG, Deluca, DS, Segre, AV, Sullivan, TJ, Young*GTEx Consortium - 201542회 인용 관련 학술자료 ', '호': '6235'}, title='The Genotype-Tissue Expression (GTEx) pilot analysis: multitissue gene regulation in humans', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'FastSLAM: A factored solution to the simultaneous localization and mapping problem': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2002/7/28', '권': '593598', '설명': 'The ability to simultaneously localize a robot and accurately map its surroundings is considered by many to be a key prerequisite of truly autonomous robots. However, few approaches to this problem scale up to handle the very large number of landmarks present in real environments. Kalman filter-based algorithms, for example, require time quadratic in the number of landmarks to incorporate each sensor observation. This paper presents FastSLAM, an algorithm that recursively estimates the full posterior distribution over robot pose and landmark locations, yet scales logarithmically with the number of landmarks in the map. This algorithm is based on an exact factorization of the posterior into a product of conditional landmark distributions and a distribution over robot paths. The algorithm has been run successfully on as many as 50,000 landmarks, environments far beyond the reach of previous approaches. Experimental results demonstrate the advantages and limitations of the FastSLAM algorithm on both simulated and realworld data.', '저널': 'Aaai/iaai', '저자': 'Michael Montemerlo, Sebastian Thrun, Daphne Koller, Ben Wegbreit', '전체 인용횟수': '3745회 인용2002200320042005200620072008200920102011201220132014201520162017201820192020202120222023164980110122170201200234205205201193211246223208204207165156109', '학술 문서': 'FastSLAM: A factored solution to the simultaneous localization and mapping problemM Montemerlo, S Thrun, D Koller, B Wegbreit\\xa0- Aaai/iaai, 20023745회 인용 관련 학술자료 전체 42개의 버전 '}, title='FastSLAM: A factored solution to the simultaneous localization and mapping problem', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Support vector machine active learning with applications to text classification': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2001', '권': '2', '설명': 'Support vector machines have met with significant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. In many settings, we also have the option of using pool-based active learning. Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing active learning with support vector machines, ie, an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm using the notion of a version space. We present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.', '저널': 'Journal of machine learning research', '저자': 'Simon Tong, Daphne Koller', '전체 인용횟수': '3696회 인용200320042005200620072008200920102011201220132014201520162017201820192020202120222023154357756699116131133191195244199247305305306268272219164', '페이지': '45-66', '학술 문서': 'Support vector machine active learning with applications to text classificationS Tong, D Koller\\xa0- Journal of machine learning research, 20013696회 인용 관련 학술자료 전체 25개의 버전 ', '호': 'Nov'}, title='Support vector machine active learning with applications to text classification', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A gene-coexpression network for global discovery of conserved genetic modules': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2003/10/10', '게시자': 'American Association for the Advancement of Science', '권': '302', '설명': 'To elucidate gene function on a global scale, we identified pairs of genes that are coexpressed over 3182 DNA microarrays from humans, flies, worms, and yeast. We found 22,163 such coexpression relationships, each of which has been conserved across evolution. This conservation implies that the coexpression of these gene pairs confers a selective advantage and therefore that these genes are functionally related. Manyof these relationships provide strong evidence for the involvement of new genes in core biological functions such as the cell cycle, secretion, and protein expression. We experimentallyconfirmed the predictions implied bysome of these links and identified cell proliferation functions for several genes. By assembling these links into a gene-coexpression network, we found several components that were animal-specific as well as interrelationships between newly evolved and ancient modules.', '저널': 'science', '저자': 'Joshua M Stuart, Eran Segal, Daphne Koller, Stuart K Kim', '전체 인용횟수': '2520회 인용200320042005200620072008200920102011201220132014201520162017201820192020202120222023108812313413412713610710911512311712914616414813712311912590', '페이지': '249-255', '학술 문서': 'A gene-coexpression network for global discovery of conserved genetic modulesJM Stuart, E Segal, D Koller, SK Kim\\xa0- science, 20032520회 인용 관련 학술자료 전체 35개의 버전 ', '호': '5643'}, title='A gene-coexpression network for global discovery of conserved genetic modules', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Toward optimal feature selection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1996/7/3', '권': '96', '설명': 'Our motivation: reducing the dimensionality of our feature space (that is, reducing the number of features used) Why should we strive to use less features? Very often time requirements for induction algorithms grow very fast as the number of features does (even exponentially!) Getting a model as simple as possible for a given training set can prevent overfitting (Occam’s razor principle)', '저널': 'ICML', '저자': 'Daphne Koller, Mehran Sahami', '전체 인용횟수': '2423회 인용199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023183734374361886699108114118124112109110104103116127971091131151097548', '페이지': '292', '학술 문서': 'Toward optimal feature selectionD Koller, M Sahami\\xa0- ICML, 19962423회 인용 관련 학술자료 전체 17개의 버전 ', '호': '28'}, title='Toward optimal feature selection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Module networks: identifying regulatory modules and their condition-specific regulators from gene expression data': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2003/6/1', '게시자': 'Nature Publishing Group US', '권': '34', '설명': \"Much of a cell's activity is organized as a network of interacting modules: sets of genes coregulated to respond to different conditions. We present a probabilistic method for identifying regulatory modules from gene expression data. Our procedure identifies modules of coregulated genes, their regulators and the conditions under which regulation occurs, generating testable hypotheses in the form 'regulator X regulates module Y under conditions W'. We applied the method to a Saccharomyces cerevisiae expression data set, showing its ability to identify functionally coherent modules and their correct regulators. We present microarray experiments supporting three novel predictions, suggesting regulatory roles for previously uncharacterized proteins.\", '저널': 'Nature genetics', '저자': \"Eran Segal, Michael Shapira, Aviv Regev, Dana Pe'er, David Botstein, Daphne Koller, Nir Friedman\", '전체 인용횟수': '2157회 인용200320042005200620072008200920102011201220132014201520162017201820192020202120222023211311481461631621721621231091211041069081596652414240', '페이지': '166-176', '학술 문서': \"Module networks: identifying regulatory modules and their condition-specific regulators from gene expression dataE Segal, M Shapira, A Regev, D Pe'er, D Botstein…\\xa0- Nature genetics, 20032157회 인용 관련 학술자료 전체 55개의 버전 \", '호': '2'}, title='Module networks: identifying regulatory modules and their condition-specific regulators from gene expression data', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Scape: shape completion and animation of people': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2005/7/1', '도서': 'ACM SIGGRAPH 2005 Papers', '설명': 'We introduce the SCAPE method (Shape Completion and Animation for PEople)---a data-driven method for building a human shape model that spans variation in both subject shape and pose. The method is based on a representation that incorporates both articulated and non-rigid deformations. We learn a pose deformation model that derives the non-rigid surface deformation as a function of the pose of the articulated skeleton. We also learn a separate model of variation based on body shape. Our two models can be combined to produce 3D surface models with realistic muscle deformation for different people in different poses, when neither appear in the training set. We show how the model can be used for shape completion --- generating a complete surface mesh given a limited set of markers specifying the target shape. We present applications of shape completion to partial view completion and motion capture\\xa0…', '저자': 'Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Sebastian Thrun, Jim Rodgers, James Davis', '전체 인용횟수': '2064회 인용2006200720082009201020112012201320142015201620172018201920202021202220232354365864838599101105109119147159206190201171', '페이지': '408-416', '학술 문서': 'Scape: shape completion and animation of peopleD Anguelov, P Srinivasan, D Koller, S Thrun, J Rodgers…\\xa0- ACM SIGGRAPH 2005 Papers, 20052064회 인용 관련 학술자료 전체 32개의 버전 '}, title='Scape: shape completion and animation of people', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Gene-expression profiles and transcriptional regulatory pathways that underlie the identity and diversity of mouse tissue macrophages': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012/11', '게시자': 'Nature Publishing Group US', '권': '13', '설명': 'We assessed gene expression in tissue macrophages from various mouse organs. The diversity in gene expression among different populations of macrophages was considerable. Only a few hundred mRNA transcripts were selectively expressed by macrophages rather than dendritic cells, and many of these were not present in all macrophages. Nonetheless, well-characterized surface markers, including MerTK and FcγR1 (CD64), along with a cluster of previously unidentified transcripts, were distinctly and universally associated with mature tissue macrophages. TCEF3, C/EBP-α, Bach1 and CREG-1 were among the transcriptional regulators predicted to regulate these core macrophage-associated genes. The mRNA encoding other transcription factors, such as Gata6, was associated with single macrophage populations. We further identified how these transcripts and the proteins they encode facilitated\\xa0…', '저널': 'Nature immunology', '저자': \"Emmanuel L Gautier, Tal Shay, Jennifer Miller, Melanie Greter, Claudia Jakubzick, Stoyan Ivanov, Julie Helft, Andrew Chow, Kutlu G Elpek, Simon Gordonov, Amin R Mazloom, Avi Ma'Ayan, Wei-Jen Chua, Ted H Hansen, Shannon J Turley, Miriam Merad, Gwendalyn J Randolph\", '전체 인용횟수': '2011회 인용2013201420152016201720182019202020212022202393142189184239214208209200195124', '페이지': '1118-1128', '학술 문서': 'Gene-expression profiles and transcriptional regulatory pathways that underlie the identity and diversity of mouse tissue macrophagesEL Gautier, T Shay, J Miller, M Greter, C Jakubzick…\\xa0- Nature immunology, 20122011회 인용 관련 학술자료 전체 19개의 버전 ', '호': '11'}, title='Gene-expression profiles and transcriptional regulatory pathways that underlie the identity and diversity of mouse tissue macrophages', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Max-margin Markov networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2003', '권': '16', '설명': 'In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches.', '저널': 'Advances in neural information processing systems', '저자': 'Ben Taskar, Carlos Guestrin, Daphne Koller', '전체 인용횟수': '1790회 인용2004200520062007200820092010201120122013201420152016201720182019202020212022202330737394103127155120127117142129101100835847383217', '학술 문서': 'Max-margin Markov networksB Taskar, C Guestrin, D Koller\\xa0- Advances in neural information processing systems, 20031790회 인용 관련 학술자료 전체 29개의 버전 '}, title='Max-margin Markov networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The Immunological Genome Project: networks of gene expression in immune cells': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2008/10', '게시자': 'Nature Publishing Group US', '권': '9', '설명': \"The Immunological Genome Project combines immunology and computational biology laboratories in an effort to establish a complete 'road map' of gene-expression and regulatory networks in all immune cells\", '저널': 'Nature immunology', '저자': 'Tracy SP Heng, Michio W Painter, the Immunological Genome Project Consortium, Kutlu Elpek, Veronika Lukacs-Kornek, Nora Mauermann, Shannon J Turley, Daphne Koller, Francis S Kim, Amy J Wagers, Natasha Asinovski, Scott Davis, Marlys Fassett, Markus Feuerer, Daniel HD Gray, Sokol Haxhinasto, Jonathan A Hill, Gordon Hyatt, Catherine Laplace, Kristen Leatherbee, Diane Mathis, Christophe Benoist, Radu Jianu, David H Laidlaw, J Adam Best, Jamie Knell, Ananda W Goldrath, Jessica Jarjoura, Joseph C Sun, Yanan Zhu, Lewis L Lanier, Ayla Ergun, Zheng Li, James J Collins, Susan A Shinton, Richard R Hardy, Randall Friedline, Katelyn Sylvia, Joonsoo Kang', '전체 인용횟수': '1731회 인용2009201020112012201320142015201620172018201920202021202220236182861907388107149169190183210183168', '페이지': '1091-1094', '학술 문서': 'The Immunological Genome Project: networks of gene expression in immune cellsTSP Heng, MW Painter…\\xa0- Nature immunology, 20081731회 인용 관련 학술자료 전체 16개의 버전 ', '호': '10'}, title='The Immunological Genome Project: networks of gene expression in immune cells', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'FastSLAM 2.0: An improved particle filtering algorithm for simultaneous localization and mapping that provably converges': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2003/8/9', '권': '3', '설명': 'In [15], Montemerlo et al. proposed an algorithm called FastSLAM as an efficient and robust solution to the simultaneous localization and mapping problem. This paper describes a modified version of FastSLAM which overcomes important deficiencies of the original algorithm. We prove convergence of this new algorithm for linear SLAM problems and provide real-world experimental results that illustrate an order of magnitude improvement in accuracy over the original FastSLAM algorithm.', '저널': 'IJCAI', '저자': 'Michael Montemerlo, Sebastian Thrun, Daphne Koller, Ben Wegbreit', '전체 인용횟수': '1590회 인용2003200420052006200720082009201020112012201320142015201620172018201920202021202220237262853919411512111910110791779182646259706251', '페이지': '1151-1156', '학술 문서': 'FastSLAM 2.0: An improved particle filtering algorithm for simultaneous localization and mapping that provably convergesM Montemerlo, S Thrun, D Koller, B Wegbreit\\xa0- IJCAI, 20031590회 인용 관련 학술자료 전체 29개의 버전 ', '호': '2003'}, title='FastSLAM 2.0: An improved particle filtering algorithm for simultaneous localization and mapping that provably converges', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Self-paced learning for latent variable models': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010', '권': '23', '설명': 'Latent variable models are a powerful tool for addressing several tasks in machine learning. However, the algorithms for learning the parameters of latent variable models are prone to getting stuck in a bad local optimum. To alleviate this problem, we build on the intuition that, rather than considering all samples simultaneously, the algorithm should be presented with the training data in a meaningful order that facilitates learning. The order of the samples is determined by how easy they are. The main challenge is that often we are not provided with a readily computable measure of the easiness of samples. We address this issue by proposing a novel, iterative self-paced learning algorithm where each iteration simultaneously selects easy samples and learns a new parameter vector. The number of samples selected is governed by a weight that is annealed until the entire training data has been considered. We empirically demonstrate that the self-paced learning algorithm outperforms the state of the art method for learning a latent structural SVM on four applications: object localization, noun phrase coreference, motif finding and handwritten digit recognition.', '저널': 'Advances in neural information processing systems', '저자': 'M Kumar, Benjamin Packer, Daphne Koller', '전체 인용횟수': '1484회 인용201120122013201420152016201720182019202020212022202311232426317895128158227221234211', '학술 문서': 'Self-paced learning for latent variable modelsM Kumar, B Packer, D Koller\\xa0- Advances in neural information processing systems, 20101484회 인용 관련 학술자료 전체 13개의 버전 '}, title='Self-paced learning for latent variable models', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Hierarchically classifying documents using very few words': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1997/7/8', '권': '97', '설명': 'The proliferation of topic hierarchies for text documents has resulted in a need for tools that automatically classify new documents within such hierarchies. Existing classification schemes which ignore the hierarchical structure and treat the topics as separate classes are often inade-¹uate in text classification where the there is a large number of classes and a huge number of relevant features needed to distinguish between them. We propose an approach that utilizes the hierarchical topic structure to decompose the classification task into a set of simpler problems, one at each node in the classification tree. As we show, each of these smaller problems can be solved accurately by focusing only on a very small set of features, those relevant to the task at hand. This set of relevant features varies widely throughout the hierarchy, so that, while the overall relevant feature set may be large, each classifier only examines a small subset. The use of reduced feature sets allows us to utilize more complex (probabilistic) models, without encountering many of the standard computational and robustness difficulties.', '저널': 'ICML', '저자': 'Daphne Koller, Mehran Sahami', '전체 인용횟수': '1436회 인용199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023534334654579177821058168866267605158514541333517292323', '페이지': '170-178', '학술 문서': 'Hierarchically classifying documents using very few wordsD Koller, M Sahami\\xa0- ICML, 19971436회 인용 관련 학술자료 전체 17개의 버전 '}, title='Hierarchically classifying documents using very few words', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Population genomics of human gene expression': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2007/10', '게시자': 'Nature Publishing Group US', '권': '39', '설명': 'Genetic variation influences gene expression, and this variation in gene expression can be efficiently mapped to specific genomic regions and variants. Here we have used gene expression profiling of Epstein-Barr virus–transformed lymphoblastoid cell lines of all 270 individuals genotyped in the HapMap Consortium to elucidate the detailed features of genetic variation underlying gene expression variation. We find that gene expression is heritable and that differentiation between populations is in agreement with earlier small-scale studies. A detailed association analysis of over 2.2 million common SNPs per population (5% frequency in HapMap) with gene expression identified at least 1,348 genes with association signals in cis and at least 180 in trans. Replication in at least one independent population was achieved for 37% of cis signals and 15% of trans signals, respectively. Our results strongly support an\\xa0…', '저널': 'Nature genetics', '저자': 'Barbara E Stranger, Alexandra C Nica, Matthew S Forrest, Antigone Dimas, Christine P Bird, Claude Beazley, Catherine E Ingle, Mark Dunning, Paul Flicek, Daphne Koller, Stephen Montgomery, Simon Tavaré, Panos Deloukas, Emmanouil T Dermitzakis', '전체 인용횟수': '1358회 인용200720082009201020112012201320142015201620172018201920202021202220234751171121101591241231208166606135373727', '페이지': '1217-1224', '학술 문서': 'Population genomics of human gene expressionBE Stranger, AC Nica, MS Forrest, A Dimas, CP Bird…\\xa0- Nature genetics, 20071358회 인용 관련 학술자료 전체 23개의 버전 ', '호': '10'}, title='Population genomics of human gene expression', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning probabilistic relational models': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1999/8', '권': '99', '설명': 'A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with “flat” data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning—the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases.', '저널': 'IJCAI', '저자': 'Nir Friedman, Lise Getoor, Daphne Koller, Avi Pfeffer', '전체 인용횟수': '1179회 인용19992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220235122235543866688281567069815351664135372642342514', '페이지': '1300-1309', '학술 문서': 'Learning probabilistic relational modelsN Friedman, L Getoor, D Koller, A Pfeffer\\xa0- IJCAI, 19991179회 인용 관련 학술자료 전체 37개의 버전 '}, title='Learning probabilistic relational models', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The chemical genomic portrait of yeast: uncovering a phenotype for all genes': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2008/4/18', '게시자': 'American Association for the Advancement of Science', '권': '320', '설명': 'Genetics aims to understand the relation between genotype and phenotype. However, because complete deletion of most yeast genes (∼80%) has no obvious phenotypic consequence in rich medium, it is difficult to study their functions. To uncover phenotypes for this nonessential fraction of the genome, we performed 1144 chemical genomic assays on the yeast whole-genome heterozygous and homozygous deletion collections and quantified the growth fitness of each deletion strain in the presence of chemical or environmental stress conditions. We found that 97% of gene deletions exhibited a measurable growth phenotype, suggesting that nearly all genes are essential for optimal growth in at least one condition.', '저널': 'Science', '저자': 'Maureen E Hillenmeyer, Eula Fung, Jan Wildenhain, Sarah E Pierce, Shawn Hoon, William Lee, Michael Proctor, Robert P St. Onge, Mike Tyers, Daphne Koller, Russ B Altman, Ronald W Davis, Corey Nislow, Guri Giaever', '전체 인용횟수': '1112회 인용200820092010201120122013201420152016201720182019202020212022202334829611211210484678267535334405040', '페이지': '362-365', '학술 문서': 'The chemical genomic portrait of yeast: uncovering a phenotype for all genesME Hillenmeyer, E Fung, J Wildenhain, SE Pierce…\\xa0- Science, 20081112회 인용 관련 학술자료 전체 23개의 버전 ', '호': '5874'}, title='The chemical genomic portrait of yeast: uncovering a phenotype for all genes', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Being Bayesian about network structure. A Bayesian approach to structure discovery in Bayesian networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2003/1', '게시자': 'Kluwer Academic Publishers', '권': '50', '설명': ' In many multivariate domains, we are interested in analyzing the dependency structure of the underlying distribution, e.g., whether two variables are in direct interaction. We can represent dependency structures using Bayesian network models. To analyze a given data set, Bayesian model selection attempts to find the most likely (MAP) model, and uses its structure to answer these questions. However, when the amount of available data is modest, there might be many models that have non-negligible posterior. Thus, we want compute the Bayesian posterior of a feature, i.e., the total posterior probability of all models that contain it. In this paper, we propose a new approach for this task. We first show how to efficiently compute a sum over the exponential number of networks that are consistent with a fixed order over network variables. This allows us to compute, for a given order, both the marginal probability of\\xa0…', '저널': 'Machine learning', '저자': 'Nir Friedman, Daphne Koller', '전체 인용횟수': '1093회 인용20022003200420052006200720082009201020112012201320142015201620172018201920202021202220235152525355563606074696367575455494953543955', '페이지': '95-125', '학술 문서': 'Being Bayesian about network structure. A Bayesian approach to structure discovery in Bayesian networksN Friedman, D Koller\\xa0- Machine learning, 20031093회 인용 관련 학술자료 전체 14개의 버전 '}, title='Being Bayesian about network structure. A Bayesian approach to structure discovery in Bayesian networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Simultaneous localization and mapping with sparse extended information filters': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2004/8', '게시자': 'SAGE Publications', '권': '23', '설명': 'In this paper we describe a scalable algorithm for the simultaneous mapping and                     localization (SLAM) problem. SLAM is the problem of acquiring a map of a static                     environment with a mobile robot. The vast majority of SLAM algorithms are based                     on the extended Kalman filter (EKF). In this paper we advocate an algorithm that                     relies on the dual of the EKF, the extended information filter (EIF). We show                     that when represented in the information form, map posteriors are dominated by a                     small number of links that tie together nearby features in the map. This insight                     is developed into a sparse variant of the EIF, called the sparse extended                     information filter (SEIF). SEIFs represent maps by graphical networks of                     features that are locally interconnected, where links represent relative                     information between pairs of nearby\\xa0…', '저널': 'The international journal of robotics research', '저자': 'Sebastian Thrun, Yufeng Liu, Daphne Koller, Andrew Y Ng, Zoubin Ghahramani, Hugh Durrant-Whyte', '전체 인용횟수': '1062회 인용20032004200520062007200820092010201120122013201420152016201720182019202020212022202382245818392787569585867574647393231311414', '페이지': '693-716', '학술 문서': 'Simultaneous localization and mapping with sparse extended information filtersS Thrun, Y Liu, D Koller, AY Ng, Z Ghahramani…\\xa0- The international journal of robotics research, 20041056회 인용 관련 학술자료 전체 10개의 버전 Simultaneous mapping and localization with sparse extended information fi Iters*S Thrun, D Koller, Z Ghahramani, H Durrant-Whyte…\\xa0- Pmc. WAFR, 200213회 인용 관련 학술자료 Simultaneous localization and mapping with sparse extended information filtersT Sebastian, L Yufeng, K Daphne, G Zoubin, DW Hugh\\xa0- The International Journal of Robotics Research, 20044회 인용 관련 학술자료 ', '호': '7-8'}, title='Simultaneous localization and mapping with sparse extended information filters', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Batch normalization: Accelerating deep network training by reducing internal covariate shift': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/6/1', '게시자': 'pmlr', '설명': 'Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.', '저자': 'Sergey Ioffe, Christian Szegedy', '전체 인용횟수': '51175회 인용2015201620172018201920202021202220231789332656518774338379916487816821', '컨퍼런스': 'International conference on machine learning', '페이지': '448-456', '학술 문서': 'Batch normalization: Accelerating deep network training by reducing internal covariate shiftS Ioffe, C Szegedy\\xa0- International conference on machine learning, 201551173회 인용 관련 학술자료 전체 34개의 버전 Normalization: accelerating deep network training by reducing internal covariate shift*S Ioffe, C Szegedy\\xa0- arXiv preprint arXiv:1502.031675회 인용 관련 학술자료 '}, title='Batch normalization: Accelerating deep network training by reducing internal covariate shift', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv 2015': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '저널': 'arXiv preprint arXiv:1502.03167', '저자': 'Sergey Ioffe, Christian Szegedy', '전체 인용횟수': '1102회 인용2015201620172018201920202021202220233132669170289249162117', '학술 문서': 'Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv 2015S Ioffe, C Szegedy\\xa0- arXiv preprint arXiv:1502.03167, 20151102회 인용 관련 학술자료 '}, title='Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv 2015', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'No fuss distance metric learning using proxies': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'We address the problem of distance metric learning (DML), defined as learning a distance consistent with a notion of semantic similarity. Traditionally, for this problem supervision is expressed in the form of sets of points that follow an ordinal relationship--an anchor point x is similar to a set of positive points Y, and dissimilar to a set of negative points Z, and a loss defined over these distances is minimized. While the specifics of the optimization differ, in this work we collectively call this type of supervision Triplets and all methods that follow this pattern Triplet-Based methods. These methods are challenging to optimize. A main issue is the need for finding informative triplets, which is usually achieved by a variety of tricks such as increasing the batch size, hard or semi-hard triplet mining, etc. Even with these tricks, the convergence rate of such methods is slow. In this paper we propose to optimize the triplet loss on a different space of triplets, consisting of an anchor data point and similar and dissimilar proxy points which are learned as well. These proxies approximate the original data points, so that a triplet loss over the proxies is a tight upper bound of the original loss. This proxy-based loss is empirically better behaved. As a result, the proxy-loss improves on state-of-art results for three standard zero-shot learning datasets, by up to 15% points, while converging three times as fast as other triplet-based losses.', '저자': 'Yair Movshovitz-Attias, Alexander Toshev, Thomas K Leung, Sergey Ioffe, Saurabh Singh', '전체 인용횟수': '653회 인용201720182019202020212022202332866106145168131', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '360-368', '학술 문서': 'No fuss distance metric learning using proxiesY Movshovitz-Attias, A Toshev, TK Leung, S Ioffe…\\xa0- Proceedings of the IEEE international conference on\\xa0…, 2017653회 인용 관련 학술자료 전체 10개의 버전 '}, title='No fuss distance metric learning using proxies', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Probabilistic linear discriminant analysis': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2006', '게시자': 'Springer Berlin Heidelberg', '설명': ' Linear dimensionality reduction methods, such as LDA, are often used in object recognition for feature extraction, but do not address the problem of how to use these features for recognition. In this paper, we propose Probabilistic LDA, a generative probability model with which we can both extract the features and combine them for recognition. The latent variables of PLDA represent both the class of the object and the view of the object within a class. By making examples of the same class share the class variable, we show how to train PLDA and use it for recognition on previously unseen classes. The usual LDA features are derived as a result of training PLDA, but in addition have a probability model attached to them, which automatically gives more weight to the more discriminative features. With PLDA, we can build a model of a previously unseen class from a single example, and can combine multiple\\xa0…', '저자': 'Sergey Ioffe', '전체 인용횟수': '616회 인용200920102011201220132014201520162017201820192020202120222023769591219233649621089510067', '컨퍼런스': 'Computer Vision–ECCV 2006: 9th European Conference on Computer Vision, Graz, Austria, May 7-13, 2006, Proceedings, Part IV 9', '페이지': '531-542', '학술 문서': 'Probabilistic linear discriminant analysisS Ioffe\\xa0- Computer Vision–ECCV 2006: 9th European\\xa0…, 2006616회 인용 관련 학술자료 전체 13개의 버전 '}, title='Probabilistic linear discriminant analysis', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Batch renormalization: Towards reducing minibatch dependence in batch-normalized models': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '권': '30', '설명': 'Batch Normalization is quite effective at accelerating and improving the training of deep models. However, its effectiveness diminishes when the training minibatches are small, or do not consist of independent samples. We hypothesize that this is due to the dependence of model layer inputs on all the examples in the minibatch, and different activations being produced between training and inference. We propose Batch Renormalization, a simple and effective extension to ensure that the training and inference models generate the same outputs that depend on individual examples rather than the entire minibatch. Models trained with Batch Renormalization perform substantially better than batchnorm when training with small or non-iid minibatches. At the same time, Batch Renormalization retains the benefits of batchnorm such as insensitivity to initialization and training efficiency.', '저널': 'Advances in neural information processing systems', '저자': 'Sergey Ioffe', '전체 인용횟수': '566회 인용20172018201920202021202220231061821089212583', '학술 문서': 'Batch renormalization: Towards reducing minibatch dependence in batch-normalized modelsS Ioffe\\xa0- Advances in neural information processing systems, 2017566회 인용 관련 학술자료 전체 7개의 버전 '}, title='Batch renormalization: Towards reducing minibatch dependence in batch-normalized models', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Scalable, high-quality object detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/12/3', '설명': 'Current high-quality object detection approaches use the scheme of salience-based object proposal methods followed by post-classification using deep convolutional features. This spurred recent research in improving object proposal methods. However, domain agnostic proposal generation has the principal drawback that the proposals come unranked or with very weak ranking, making it hard to trade-off quality for running time. This raises the more fundamental question of whether high-quality proposal generation requires careful engineering or can be derived just from data alone. We demonstrate that learning-based proposal methods can effectively match the performance of hand-engineered methods while allowing for very efficient runtime-quality trade-offs. Using the multi-scale convolutional MultiBox (MSC-MultiBox) approach, we substantially advance the state-of-the-art on the ILSVRC 2014 detection challenge data set, with  mAP for a single model and  mAP for an ensemble of two models. MSC-Multibox significantly improves the proposal quality over its predecessor MultiBox~method: AP increases from  to  for the ILSVRC detection challenge. Finally, we demonstrate improved bounding-box recall compared to Multiscale Combinatorial Grouping with less proposals on the Microsoft-COCO data set.', '저널': 'arXiv preprint arXiv:1412.1441', '저자': 'Christian Szegedy, Scott Reed, Dumitru Erhan, Dragomir Anguelov, Sergey Ioffe', '전체 인용횟수': '530회 인용20142015201620172018201920202021202220232345164919348494729', '학술 문서': 'Scalable, high-quality object detectionC Szegedy, S Reed, D Erhan, D Anguelov, S Ioffe\\xa0- arXiv preprint arXiv:1412.1441, 2014530회 인용 관련 학술자료 전체 10개의 버전 '}, title='Scalable, high-quality object detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep convolutional ranking for multilabel image annotation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/12/17', '설명': 'Multilabel image annotation is one of the most important challenges in computer vision with many real-world applications. While existing work usually use conventional visual features for multilabel annotation, features based on Deep Neural Networks have shown potential to significantly boost performance. In this work, we propose to leverage the advantage of such features and analyze key components that lead to better performances. Specifically, we show that a significant performance gain could be obtained by combining convolutional architectures with approximate top- ranking objectives, as thye naturally fit the multilabel tagging problem. Our experiments on the NUS-WIDE dataset outperforms the conventional visual features by about 10%, obtaining the best reported performance in the literature.', '저널': 'arXiv preprint arXiv:1312.4894', '저자': 'Yunchao Gong, Yangqing Jia, Thomas Leung, Alexander Toshev, Sergey Ioffe', '전체 인용횟수': '529회 인용20142015201620172018201920202021202220237264663848962664031', '학술 문서': 'Deep convolutional ranking for multilabel image annotationY Gong, Y Jia, T Leung, A Toshev, S Ioffe\\xa0- arXiv preprint arXiv:1312.4894, 2013529회 인용 관련 학술자료 전체 6개의 버전 '}, title='Deep convolutional ranking for multilabel image annotation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'International conference on machine learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '저널': 'International conference on machine learning', '저자': 'Sergey Ioffe, Christian Szegedy', '전체 인용횟수': '423회 인용201720182019202020212022202332640498012796', '페이지': '448-456', '학술 문서': 'International conference on machine learningS Ioffe, C Szegedy\\xa0- International conference on machine learning, 2015405회 인용 관련 학술자료 Proc. 32nd Int. Conf. Machine Learning*S Ioffe, C Szegedy, F Bach, D Blei - 201522회 인용 관련 학술자료 '}, title='International conference on machine learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Proceedings of the IEEE conference on computer vision and pattern recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '저널': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '저자': 'Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi', '전체 인용횟수': '268회 인용20172018201920202021202220231111337508571', '페이지': '779-788', '학술 문서': 'Proceedings of the IEEE conference on computer vision and pattern recognitionJ Redmon, S Divvala, R Girshick, A Farhadi\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2016268회 인용 관련 학술자료 '}, title='Proceedings of the IEEE conference on computer vision and pattern recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Automatic face detection and identity masking in images, and applications thereof': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012/1/17', '발명자': 'Sergey Ioffe, Lance Williams, Dennis Strelow, Andrea Frome, Luc Vincent', '설명': 'A method and system of identity masking to obscure identities corresponding to face regions in an image is disclosed. A face detector is applied to detect a set of possible face regions in the image. Then an identity masker is used to process the detected face regions by identity masking techniques in order to obscure identities corresponding to the regions. For example, a detected face region can be blurred as if it is in motion by a motion blur algorithm, such that the blurred region can not be recognized as the original identity. Or the detected face region can be replaced by a substitute facial image by a face replacement algorithm to obscure the corresponding identity.', '전체 인용횟수': '336회 인용201120122013201420152016201720182019202020212022202321739923204447637235', '출원번호': '12078464', '특허 번호': '8098904', '특허청': 'US', '학술 문서': 'Automatic face detection and identity masking in images, and applications thereofS Ioffe, L Williams, D Strelow, A Frome, L Vincent\\xa0- US Patent 8,098,904, 2012336회 인용 관련 학술자료 전체 4개의 버전 '}, title='Automatic face detection and identity masking in images, and applications thereof', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Probabilistic methods for finding people': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2001/6', '게시자': 'Kluwer Academic Publishers', '권': '43', '설명': ' Finding people in pictures presents a particularly difficult object recognition problem. We show how to find people by finding candidate body segments, and then constructing assemblies of segments that are consistent with the constraints on the appearance of a person that result from kinematic properties. Since a reasonable model of a person requires at least nine segments, it is not possible to inspect every group, due to the huge combinatorial complexity. We propose two approaches to this problem. In one, the search can be pruned by using projected versions of a classifier that accepts groups corresponding to people. We describe an efficient projection algorithm for one popular classifier, and demonstrate that our approach can be used to determine whether images of real scenes contain people. The second approach employs a probabilistic framework, so that we can draw samples of\\xa0…', '저널': 'International Journal of Computer Vision', '저자': 'Sergey Ioffe, David A.  Forsyth', '전체 인용횟수': '325회 인용2001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202311713282922313324192126913910454421', '페이지': '45-68', '학술 문서': 'Probabilistic methods for finding peopleS Ioffe, DA Forsyth\\xa0- International Journal of Computer Vision, 2001325회 인용 관련 학술자료 전체 15개의 버전 '}, title='Probabilistic methods for finding people', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Improved consistent sampling, weighted minhash and l1 sketching': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010/12/13', '게시자': 'IEEE', '설명': 'We propose a new Consistent Weighted Sampling method, where the probability of drawing identical samples for a pair of inputs is equal to their Jaccard similarity. Our method takes deterministic constant time per non-zero weight, improving on the best previous approach which takes expected constant time. The samples can be used as Weighted Minhash for efficient retrieval and compression (sketching) under Jaccard or L1 metric. A method is presented for using simple data statistics to reduce the running time of hash computation by two orders of magnitude. We compare our method with the random projection method and show that it has better characteristics for retrieval under L1. We present a novel method of mapping hashes to short bit-strings, apply it to Weighted Minhash, and achieve more accurate distance estimates from sketches than existing methods, as long as the inputs are sufficiently distinct. We\\xa0…', '저자': 'Sergey Ioffe', '전체 인용횟수': '248회 인용20102011201220132014201520162017201820192020202120222023111410102226253025282631', '컨퍼런스': '2010 IEEE international conference on data mining', '페이지': '246-255', '학술 문서': 'Improved consistent sampling, weighted minhash and l1 sketchingS Ioffe\\xa0- 2010 IEEE international conference on data mining, 2010248회 인용 관련 학술자료 전체 15개의 버전 '}, title='Improved consistent sampling, weighted minhash and l1 sketching', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Rethinking the inception architecture for computer vision. 2015': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '게시자': 'CoRR', '저널': 'arXiv preprint arXiv:1512.00567', '저자': 'Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna', '전체 인용횟수': '205회 인용2016201720182019202020212022202346262729423829', '학술 문서': 'Rethinking the inception architecture for computer vision. 2015C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna\\xa0- arXiv preprint arXiv:1512.00567, 2015205회 인용 관련 학술자료 '}, title='Rethinking the inception architecture for computer vision. 2015', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Rethinking the inception architecture for computer vision. arXiv 2015': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '권': '1512', '저널': 'arXiv preprint arXiv:1512.00567', '저자': 'Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna', '전체 인용횟수': '170회 인용20172018201920202021202220234121916384337', '학술 문서': 'Rethinking the inception architecture for computer vision. arXiv 2015C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna\\xa0- arXiv preprint arXiv:1512.00567, 2016170회 인용 관련 학술자료 '}, title='Rethinking the inception architecture for computer vision. arXiv 2015', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Temporal differences-based policy iteration and applications in neuro-dynamic programming': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1996', '권': '14', '설명': 'We introduce a new policy iteration method for dynamic programming problems with discounted and undiscounted cost. The method is based on the notion of temporal differences, and is primarily geared to the case of large and complex problems where the use of approximations is essential. We develop the theory of the method without approximation, we describe how to embed it within a neuro-dynamic programming/reinforcement learning context where feature-based approximation architectures are used, we relate it to TD (λ) methods, and we illustrate its use in the training of a tetris playing program.', '저널': 'Lab. for Info. and Decision Systems Report LIDS-P-2349, MIT, Cambridge, MA', '저자': 'Dimitri P Bertsekas, Sergey Ioffe', '전체 인용횟수': '155회 인용199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023211111259799109108567141363555', '학술 문서': 'Temporal differences-based policy iteration and applications in neuro-dynamic programmingDP Bertsekas, S Ioffe\\xa0- Lab. for Info. and Decision Systems Report LIDS-P\\xa0…, 1996155회 인용 관련 학술자료 전체 6개의 버전 '}, title='Temporal differences-based policy iteration and applications in neuro-dynamic programming', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Human tracking with mixtures of trees': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2001/7/7', '게시자': 'IEEE', '권': '1', '설명': 'Tree-structured probabilistic models admit simple, fast inference. However they are not well suited to phenonena such as occlusion, where multiple components of an object may disappear simultaneously. We address this problem with mixtures of trees, and demonstrate an efficient and compact representation of this mixture, which admits simple learning and inference algorithms. We use this method to build an automated tracker for Muybridge sequences of a variety of human activities. Tracking is difficult, because the temporal dependencies rule out simple inference methods. We show how to use our model for efficient inference, using a method that employs alternate spatial and temporal inference. The result is a cracker that (a) uses a very loose motion model, and so can track many different activities at a variable frame rate and (b) is entirely, automatic.', '저자': 'Sergey Ioffe, David Forsyth', '전체 인용횟수': '153회 인용20012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023310159121213138859554637111', '컨퍼런스': 'Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001', '페이지': '690-695', '학술 문서': 'Human tracking with mixtures of treesS Ioffe, D Forsyth\\xa0- Proceedings Eighth IEEE International Conference on\\xa0…, 2001153회 인용 관련 학술자료 전체 13개의 버전 '}, title='Human tracking with mixtures of trees', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Endpoint based video fingerprinting': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/12/17', '발명자': 'Jay Yagnik, Henry A Rowley, Sergey Ioffe', '설명': 'A method and system generates and compares fingerprints for videos in a video library. The video fingerprints provide a compact representation of the temporal locations of discontinuities in the video that can be used to quickly and efficiently identify video content. Discontinuities can be, for example, shot boundaries in the video frame sequence or silent points in the audio stream. Because the fingerprints are based on structural discontinuity characteristics rather than exact bit sequences, visual content of videos can be effectively compared even when there are small differences between the videos in compression factors, source resolutions, start and stop times, frame rates, and so on. Comparison of video fingerprints can be used, for example, to search for and remove copyright protected videos from a video library. Furthermore, duplicate videos can be detected and discarded in order to preserve storage space.', '전체 인용횟수': '141회 인용201520162017201820192020202120222023231622323216152', '출원번호': '11765292', '특허 번호': '8611422', '특허청': 'US', '학술 문서': 'Endpoint based video fingerprintingJ Yagnik, HA Rowley, S Ioffe\\xa0- US Patent 8,611,422, 2013141회 인용 관련 학술자료 전체 2개의 버전 '}, title='Endpoint based video fingerprinting', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Dssd: Deconvolutional single shot detector': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/1/23', '설명': 'The main contribution of this paper is an approach for introducing additional context into state-of-the-art general object detection. To achieve this we first combine a state-of-the-art classifier (Residual-101[14]) with a fast detection framework (SSD[18]). We then augment SSD+Residual-101 with deconvolution layers to introduce additional large-scale context in object detection and improve accuracy, especially for small objects, calling our resulting system DSSD for deconvolutional single shot detector. While these two contributions are easily described at a high-level, a naive implementation does not succeed. Instead we show that carefully adding additional stages of learned transformations, specifically a module for feed-forward connections in deconvolution and a new output module, enables this new approach and forms a potential way forward for further detection research. Results are shown on both PASCAL VOC and COCO detection. Our DSSD with  input achieves 81.5% mAP on VOC2007 test, 80.0% mAP on VOC2012 test, and 33.2% mAP on COCO, outperforming a state-of-the-art method R-FCN[3] on each dataset.', '저널': 'arXiv preprint arXiv:1701.06659', '저자': 'Cheng-Yang Fu, Wei Liu, Ananth Ranga, Ambrish Tyagi, Alexander C Berg', '전체 인용횟수': '2431회 인용201720182019202020212022202329160405482507462365', '학술 문서': 'Dssd: Deconvolutional single shot detectorCY Fu, W Liu, A Ranga, A Tyagi, AC Berg\\xa0- arXiv preprint arXiv:1701.06659, 20172431회 인용 관련 학술자료 전체 5개의 버전 '}, title='Dssd: Deconvolutional single shot detector', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Attribute and simile classifiers for face verification': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009/9/29', '게시자': 'IEEE', '설명': 'We present two novel methods for face verification. Our first method - “attribute” classifiers - uses binary classifiers trained to recognize the presence or absence of describable aspects of visual appearance (e.g., gender, race, and age). Our second method - “simile” classifiers - removes the manual labeling required for attribute classification and instead learns the similarity of faces, or regions of faces, to specific reference people. Neither method requires costly, often brittle, alignment between image pairs; yet, both methods produce compact visual descriptions, and work on real-world images. Furthermore, both the attribute and simile classifiers improve on the current state-of-the-art for the LFW data set, reducing the error rates compared to the current best by 23.92% and 26.34%, respectively, and 31.68% when combined. For further testing across pose, illumination, and expression, we introduce a new data set\\xa0…', '저자': 'Neeraj Kumar, Alexander C Berg, Peter N Belhumeur, Shree K Nayar', '전체 인용횟수': '1909회 인용20102011201220132014201520162017201820192020202120222023428598143196195215201174122991009588', '컨퍼런스': '2009 IEEE 12th international conference on computer vision', '페이지': '365-372', '학술 문서': 'Attribute and simile classifiers for face verificationN Kumar, AC Berg, PN Belhumeur, SK Nayar\\xa0- 2009 IEEE 12th international conference on computer\\xa0…, 20091909회 인용 관련 학술자료 전체 21개의 버전 '}, title='Attribute and simile classifiers for face verification', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'SVM-KNN: Discriminative nearest neighbor classification for visual category recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2006/6/17', '게시자': 'IEEE', '권': '2', '설명': 'We consider visual category recognition in the framework of measuring similarities, or equivalently perceptual distances, to prototype examples of categories. This approach is quite flexible, and permits recognition based on color, texture, and particularly shape, in a homogeneous framework. While nearest neighbor classifiers are natural in this setting, they suffer from the problem of high variance (in bias-variance decomposition) in the case of limited sampling. Alternatively, one could use support vector machines but they involve time-consuming optimization and computation of pairwise distances. We propose a hybrid of these two methods which deals naturally with the multiclass setting, has reasonable computational complexity both in training and at run time, and yields excellent results in practice. The basic idea is to find close neighbors to a query sample and train a local support vector machine that preserves\\xa0…', '저자': 'Hao Zhang, Alexander C Berg, Michael Maire, Jitendra Malik', '전체 인용횟수': '1673회 인용20062007200820092010201120122013201420152016201720182019202020212022202315547892120124127134129143122108957576486344', '컨퍼런스': \"2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)\", '페이지': '2126-2136', '학술 문서': 'SVM-KNN: Discriminative nearest neighbor classification for visual category recognitionH Zhang, AC Berg, M Maire, J Malik\\xa0- 2006 IEEE Computer Society Conference on Computer\\xa0…, 20061673회 인용 관련 학술자료 전체 18개의 버전 '}, title='SVM-KNN: Discriminative nearest neighbor classification for visual category recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Babytalk: Understanding and generating simple image descriptions': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/5/31', '게시자': 'IEEE', '권': '35', '설명': 'We present a system to automatically generate natural language descriptions from images. This system consists of two parts. The first part, content planning, smooths the output of computer vision-based detection and recognition algorithms with statistics mined from large pools of visually descriptive text to determine the best content words to use to describe an image. The second step, surface realization, chooses words to construct natural language sentences based on the predicted content and general statistics from natural language. We present multiple approaches for the surface realization step and evaluate each using automatic measures of similarity to human generated reference descriptions. We also collect forced choice human evaluations between descriptions from the proposed generation system and descriptions from competing approaches. The proposed system is very effective at producing relevant\\xa0…', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Girish Kulkarni, Visruth Premraj, Vicente Ordonez, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C Berg, Tamara L Berg', '전체 인용횟수': '1533회 인용201220132014201520162017201820192020202120222023324883117136167152186137153157120', '페이지': '2891-2903', '학술 문서': 'Babytalk: Understanding and generating simple image descriptionsG Kulkarni, V Premraj, V Ordonez, S Dhar, S Li, Y Choi…\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20131533회 인용 관련 학술자료 전체 24개의 버전 ', '호': '12'}, title='Babytalk: Understanding and generating simple image descriptions', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Parsenet: Looking wider to see better': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/6/15', '설명': 'We present a technique for adding global context to deep convolutional networks for semantic segmentation. The approach is simple, using the average feature for a layer to augment the features at each location. In addition, we study several idiosyncrasies of training, significantly increasing the performance of baseline networks (e.g. from FCN). When we add our proposed global feature, and a technique for learning normalization parameters, accuracy increases consistently even over our improved versions of the baselines. Our proposed approach, ParseNet, achieves state-of-the-art performance on SiftFlow and PASCAL-Context with small additional computational cost over baselines, and near current state-of-the-art performance on PASCAL VOC 2012 semantic segmentation with a simple approach. Code is available at https://github.com/weiliu89/caffe/tree/fcn .', '저널': 'arXiv preprint arXiv:1506.04579', '저자': 'Wei Liu, Andrew Rabinovich, Alexander C Berg', '전체 인용횟수': '1403회 인용201520162017201820192020202120222023440112153208241266210151', '학술 문서': 'Parsenet: Looking wider to see betterW Liu, A Rabinovich, AC Berg\\xa0- arXiv preprint arXiv:1506.04579, 20151403회 인용 관련 학술자료 전체 9개의 버전 '}, title='Parsenet: Looking wider to see better', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Classification using intersection kernel support vector machines is efficient': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2008/6/23', '게시자': 'IEEE', '설명': 'Straightforward classification using kernelized SVMs requires evaluating the kernel for a test vector and each of the support vectors. For a class of kernels we show that one can do this much more efficiently. In particular we show that one can build histogram intersection kernel SVMs (IKSVMs) with runtime complexity of the classifier logarithmic in the number of support vectors as opposed to linear for the standard approach. We further show that by precomputing auxiliary tables we can construct an approximate classifier with constant runtime and space requirements, independent of the number of support vectors, with negligible loss in classification accuracy on various tasks. This approximation also applies to 1 - chi 2  and other kernels of similar form. We also introduce novel features based on a multi-level histograms of oriented edge energy and present experiments on various detection datasets. On the INRIA\\xa0…', '저자': 'Subhransu Maji, Alexander C Berg, Jitendra Malik', '전체 인용횟수': '1359회 인용2008200920102011201220132014201520162017201820192020202120222023740869614715818114011888786237412914', '컨퍼런스': '2008 IEEE conference on computer vision and pattern recognition', '페이지': '1-8', '학술 문서': 'Classification using intersection kernel support vector machines is efficientS Maji, AC Berg, J Malik\\xa0- 2008 IEEE conference on computer vision and pattern\\xa0…, 20081359회 인용 관련 학술자료 전체 17개의 버전 '}, title='Classification using intersection kernel support vector machines is efficient', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Shape matching and object recognition using low distortion correspondences': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2005/6/20', '게시자': 'IEEE', '권': '1', '설명': 'We approach recognition in the framework of deformable shape matching, relying on a new algorithm for finding correspondences between feature points. This algorithm sets up correspondence as an integer quadratic programming problem, where the cost function has terms based on similarity of corresponding geometric blur point descriptors as well as the geometric distortion between pairs of corresponding feature points. The algorithm handles outliers, and thus enables matching of exemplars to query images in the presence of occlusion and clutter. Given the correspondences, we estimate an aligning transform, typically a regularized thin plate spline, resulting in a dense correspondence between the two shapes. Object recognition is then handled in a nearest neighbor framework where the distance between exemplar and query is the matching cost between corresponding points. We show results on two\\xa0…', '저자': 'Alexander C Berg, Tamara L Berg, Jitendra Malik', '전체 인용횟수': '1198회 인용2004200520062007200820092010201120122013201420152016201720182019202020212022202332462718379106767910071846047434232453632', '컨퍼런스': \"2005 IEEE computer society conference on computer vision and pattern recognition (CVPR'05)\", '페이지': '26-33', '학술 문서': 'Shape matching and object recognition using low distortion correspondencesAC Berg, TL Berg, J Malik\\xa0- 2005 IEEE computer society conference on computer\\xa0…, 20051198회 인용 관련 학술자료 전체 34개의 버전 '}, title='Shape matching and object recognition using low distortion correspondences', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Matchnet: Unifying feature and metric learning for patch-based matching': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'Motivated by recent successes on learning feature representations and on learning feature comparison functions, we propose a unified approach to combining both for training a patch matching system. Our system, dubbed MatchNet, consists of a deep convolutional network that extracts features from patches and a network of three fully connected layers that computes a similarity between the extracted features. To ensure experimental repeatability, we train MatchNet on standard datasets and employ an input sampler to augment the training set with synthetic exemplar pairs that reduce overfitting. Once trained, we achieve better computational efficiency during matching by disassembling MatchNet and separately applying the feature computation and similarity networks in two sequential stages. We perform a comprehensive set of experiments on standard datasets to carefully study the contributions of each aspect of MatchNet, with direct comparisons to established methods. Our results confirm that our unified approach improves accuracy over previous state-of-the-art results on patch matching datasets, while reducing the storage requirement for descriptors. We make pre-trained MatchNet publicly available.', '저자': 'Xufeng Han, Thomas Leung, Yangqing Jia, Rahul Sukthankar, Alexander C Berg', '전체 인용횟수': '1005회 인용201520162017201820192020202120222023137111514116812816310285', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '3279-3286', '학술 문서': 'Matchnet: Unifying feature and metric learning for patch-based matchingX Han, T Leung, Y Jia, R Sukthankar, AC Berg\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20151005회 인용 관련 학술자료 전체 22개의 버전 '}, title='Matchnet: Unifying feature and metric learning for patch-based matching', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Modeling context in referring expressions': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '게시자': 'Springer International Publishing', '설명': ' Humans refer to objects in their environments all the time, especially in dialogue with other people. We explore generating and comprehending natural language referring expressions for objects in images. In particular, we focus on incorporating better measures of visual context into referring expression models and find that visual comparison to other objects within an image helps improve performance significantly. We also develop methods to tie the language generation process together, so that we generate expressions for all objects of a particular category jointly. Evaluation on three recent datasets - RefCOCO, RefCOCO+, and RefCOCOg (Datasets and toolbox can be downloaded from                      https://github.com/lichengunc/refer                                        ), shows the advantages of our methods for both referring expression generation and comprehension.', '저자': 'Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, Tamara L Berg', '전체 인용횟수': '788회 인용201520162017201820192020202120222023232952678782183275', '컨퍼런스': 'Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14', '페이지': '69-85', '학술 문서': 'Modeling context in referring expressionsL Yu, P Poirson, S Yang, AC Berg, TL Berg\\xa0- Computer Vision–ECCV 2016: 14th European\\xa0…, 2016788회 인용 관련 학술자료 전체 6개의 버전 '}, title='Modeling context in referring expressions', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'What does classifying more than 10,000 image categories tell us?': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010', '게시자': 'Springer Berlin Heidelberg', '설명': ' Image classification is a critical task for both humans and computers. One of the challenges lies in the large scale of the semantic space. In particular, humans can recognize tens of thousands of object classes and scenes. No computer vision algorithm today has been tested at this scale. This paper presents a study of large scale categorization including a series of challenging experiments on classification with more than 10,000 image classes. We find that a) computational issues become crucial in algorithm design; b) conventional wisdom from a couple of hundred image categories on relative performance of different classifiers does not necessarily hold when the number of categories increases; c) there is a surprisingly strong relationship between the structure of WordNet (developed for studying language) and the difficulty of visual categorization; d) classification can be improved by exploiting the\\xa0…', '저자': 'Jia Deng, Alexander C Berg, Kai Li, Li Fei-Fei', '전체 인용횟수': '667회 인용2009201020112012201320142015201620172018201920202021202220233435588680635945513343423221', '컨퍼런스': 'Computer Vision–ECCV 2010: 11th European Conference on Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part V 11', '페이지': '71-84', '학술 문서': 'What does classifying more than 10,000 image categories tell us?J Deng, AC Berg, K Li, L Fei-Fei\\xa0- Computer Vision–ECCV 2010: 11th European\\xa0…, 2010667회 인용 관련 학술자료 전체 14개의 버전 '}, title='What does classifying more than 10,000 image categories tell us?', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Describable visual attributes for face verification and image search': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011/3/10', '게시자': 'IEEE', '권': '33', '설명': 'We introduce the use of describable visual attributes for face verification and image search. Describable visual attributes are labels that can be given to an image to describe its appearance. This paper focuses on images of faces and the attributes used to describe them, although the concepts also apply to other domains. Examples of face attributes include gender, age, jaw shape, nose size, etc. The advantages of an attribute-based representation for vision tasks are manifold: They can be composed to create descriptions at various levels of specificity; they are generalizable, as they can be learned once and then applied to recognize new objects or categories without any further training; and they are efficient, possibly requiring exponentially fewer attributes (and training data) than explicitly naming each category. We show how one can create and label large data sets of real-world images to train classifiers which\\xa0…', '저널': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', '저자': 'Neeraj Kumar, Alexander Berg, Peter N Belhumeur, Shree Nayar', '전체 인용횟수': '611회 인용20102011201220132014201520162017201820192020202120222023310274883606879605530262320', '페이지': '1962-1977', '학술 문서': 'Describable visual attributes for face verification and image searchN Kumar, A Berg, PN Belhumeur, S Nayar\\xa0- IEEE Transactions on Pattern Analysis and Machine\\xa0…, 2011611회 인용 관련 학술자료 전체 21개의 버전 Describable Visual Attributes for Face Images*N Kumar, N Kumar, AC Berg, PN Belhumeur, SK Nayar…\\xa0- Technical Report CUCS-035-11, Department of\\xa0…1회 인용 관련 학술자료 ', '호': '10'}, title='Describable visual attributes for face verification and image search', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Who are you with and where are you going?': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011/6/20', '게시자': 'IEEE', '설명': 'We propose an agent-based behavioral model of pedestrians to improve tracking performance in realistic scenarios. In this model, we view pedestrians as decision-making agents who consider a plethora of personal, social, and environmental factors to decide where to go next. We formulate prediction of pedestrian behavior as an energy minimization on this model. Two of our main contributions are simple, yet effective estimates of pedestrian destination and social relationships (groups). Our final contribution is to incorporate these hidden properties into an energy formulation that results in accurate behavioral prediction. We evaluate both our estimates of destination and grouping, as well as our accuracy at prediction and tracking against state of the art behavioral model and show improvements, especially in the challenging observational situation of infrequent appearance observations-something that might occur\\xa0…', '저자': 'Kota Yamaguchi, Alexander C Berg, Luis E Ortiz, Tamara L Berg', '전체 인용횟수': '604회 인용20112012201320142015201620172018201920202021202220233163042374637586091665738', '컨퍼런스': 'CVPR 2011', '페이지': '1345-1352', '학술 문서': 'Who are you with and where are you going?K Yamaguchi, AC Berg, LE Ortiz, TL Berg\\xa0- CVPR 2011, 2011604회 인용 관련 학술자료 전체 23개의 버전 '}, title='Who are you with and where are you going?', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Automatic attribute discovery and characterization from noisy web data': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2010', '게시자': 'Springer Berlin Heidelberg', '설명': ' It is common to use domain specific terminology – attributes – to describe the visual appearance of objects. In order to scale the use of these describable visual attributes to a large number of categories, especially those not well studied by psychologists or linguists, it will be necessary to find alternative techniques for identifying attribute vocabularies and for learning to recognize attributes without hand labeled training data. We demonstrate that it is possible to accomplish both these tasks automatically by mining text and image data sampled from the Internet. The proposed approach also characterizes attributes according to their visual representation: global or local, and type: color, texture, or shape. This work focuses on discovering attributes and their visual appearance, and is as agnostic as possible about the textual description.', '저자': 'Tamara L Berg, Alexander C Berg, Jonathan Shih', '전체 인용횟수': '558회 인용201120122013201420152016201720182019202020212022202320346172797355372221261820', '컨퍼런스': 'Computer Vision–ECCV 2010: 11th European Conference on Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part I 11', '페이지': '663-676', '학술 문서': 'Automatic attribute discovery and characterization from noisy web dataTL Berg, AC Berg, J Shih\\xa0- Computer Vision–ECCV 2010: 11th European\\xa0…, 2010558회 인용 관련 학술자료 전체 22개의 버전 '}, title='Automatic attribute discovery and characterization from noisy web data', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Midge: Generating image descriptions from computer vision detections': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012/4', '설명': 'This paper introduces a novel generation system that composes humanlike descriptions of images from computer vision detections. By leveraging syntactically informed word co-occurrence statistics, the generator filters and constrains the noisy detections output from a vision system to generate syntactic trees that detail what the computer vision system sees. Results show that the generation system outperforms state-of-the-art systems, automatically generating some of the most natural image descriptions to date.', '저자': 'Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Yamaguchi, Karl Stratos, Xufeng Han, Alyssa Mensch, Alexander Berg, Tamara Berg, Hal Daumé III', '전체 인용횟수': '542회 인용20122013201420152016201720182019202020212022202351633544351646744485453', '컨퍼런스': 'Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics', '페이지': '747-756', '학술 문서': 'Midge: Generating image descriptions from computer vision detectionsM Mitchell, J Dodge, A Goyal, K Yamaguchi, K Stratos…\\xa0- Proceedings of the 13th Conference of the European\\xa0…, 2012542회 인용 관련 학술자료 전체 28개의 버전 '}, title='Midge: Generating image descriptions from computer vision detections', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Names and faces in the news': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2004/6/27', '게시자': 'IEEE', '권': '2', '설명': 'We show quite good face clustering is possible for a dataset of inaccurately and ambiguously labelled face images. Our dataset is 44,773 face images, obtained by applying a face finder to approximately half a million captioned news images. This dataset is more realistic than usual face recognition datasets, because it contains faces captured \"in the wild\" in a variety of configurations with respect to the camera, taking a variety of expressions, and under illumination of widely varying color. Each face image is associated with a set of names, automatically extracted from the associated caption. Many, but not all such sets contain the correct name. We cluster face images in appropriate discriminant coordinates. We use a clustering procedure to break ambiguities in labelling and identify incorrectly labelled faces. A merging procedure then identifies variants of names that refer to the same individual. The resulting\\xa0…', '저자': 'Tamara L Berg, Alexander C Berg, Jaety Edwards, Michael Maire, Ryan White, Yee-Whye Teh, Erik Learned-Miller, David A Forsyth', '전체 인용횟수': '533회 인용20032004200520062007200820092010201120122013201420152016201720182019202020212022202324122735423832393435374941241719910117', '컨퍼런스': 'Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.', '페이지': 'II-II', '학술 문서': 'Names and faces in the newsTL Berg, AC Berg, J Edwards, M Maire, R White…\\xa0- Proceedings of the 2004 IEEE Computer Society\\xa0…, 2004533회 인용 관련 학술자료 전체 47개의 버전 '}, title='Names and faces in the news', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Where to buy it: Matching street clothing photos in online shops': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'In this paper, we define a new task, Exact Street to Shop, where our goal is to match a real-world example of a garment item to the same item in an online shop. This is an extremely challenging task due to visual differences between street photos (pictures of people wearing clothing in everyday uncontrolled settings) and online shop photos (pictures of clothing items on people, mannequins, or in isolation, captured by professionals in more controlled settings). We collect a new dataset for this application containing 404,683 shop photos collected from 25 different online retailers and 20,357 street photos, providing a total of 39,479 clothing item matches between street and shop photos. We develop three different methods for Exact Street to Shop retrieval, including two deep learning baseline methods, and a method to learn a similarity measure between the street and shop domains. Experiments demonstrate that our learned similarity significantly outperforms our baselines that use existing deep learning based representations.', '저자': 'M Hadi Kiapour, Xufeng Han, Svetlana Lazebnik, Alexander C Berg, Tamara L Berg', '전체 인용횟수': '518회 인용201620172018201920202021202220232375799774715435', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '3343-3351', '학술 문서': 'Where to buy it: Matching street clothing photos in online shopsM Hadi Kiapour, X Han, S Lazebnik, AC Berg, TL Berg\\xa0- Proceedings of the IEEE international conference on\\xa0…, 2015518회 인용 관련 학술자료 전체 22개의 버전 '}, title='Where to buy it: Matching street clothing photos in online shops', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A taxonomy and evaluation of dense two-frame stereo correspondence algorithms': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2002/4', '게시자': 'Kluwer Academic Publishers', '권': '47', '설명': ' Stereo matching is one of the most active research areas in computer vision. While a large number of algorithms for stereo correspondence have been developed, relatively little work has been done on characterizing their performance. In this paper, we present a taxonomy of dense, two-frame stereo methods. Our taxonomy is designed to assess the different components and design decisions made in individual stereo algorithms. Using this taxonomy, we compare existing stereo methods and present experiments evaluating the performance of many different variants. In order to establish a common software platform and a collection of data sets for easy evaluation, we have designed a stand-alone, flexible C++ implementation that enables the evaluation of individual components and that can easily be extended to include new algorithms. We have also produced several new multi-frame stereo data sets with\\xa0…', '저널': 'International journal of computer vision', '저자': 'Daniel Scharstein, Richard Szeliski', '전체 인용횟수': '10139회 인용200220032004200520062007200820092010201120122013201420152016201720182019202020212022202346104202267332380456533630651781670672658608583503454415438361265', '페이지': '7-42', '학술 문서': 'A taxonomy and evaluation of dense two-frame stereo correspondence algorithmsD Scharstein, R Szeliski\\xa0- International journal of computer vision, 200210139회 인용 관련 학술자료 전체 35개의 버전 '}, title='A taxonomy and evaluation of dense two-frame stereo correspondence algorithms', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Computer vision: algorithms and applications': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2022/1/3', '게시자': 'Springer Nature', '설명': 'Humans perceive the three-dimensional structure of the world with apparent ease. However, despite all of the recent advances in computer vision research, the dream of having a computer interpret an image at the same level as a two-year old remains elusive. Why is computer vision such a challenging problem and what is the current state of the art? Computer Vision: Algorithms and Applications explores the variety of techniques commonly used to analyze and interpret images. It also describes challenging real-world applications where vision is being successfully used, both for specialized applications such as medical imaging, and for fun, consumer-level tasks such as image editing and stitching, which students can apply to their own personal photos and videos. More than just a source of “recipes,” this exceptionally authoritative and comprehensive textbook/reference also takes a scientific approach to basic vision problems, formulating physical models of the imaging process before inverting them to produce descriptions of a scene. These problems are also analyzed using statistical models and solved using rigorous engineering techniques Topics and features: Structured to support active curricula and project-oriented courses, with tips in the Introduction for using the book in a variety of customized courses Presents exercises at the end of each chapter with a heavy emphasis on testing algorithms and containing numerous suggestions for small mid-term projects Provides additional material and more detailed mathematical topics in the Appendices, which cover linear algebra, numerical techniques, and Bayesian estimation theory Suggests\\xa0…', '저자': 'Richard Szeliski', '전체 인용횟수': '8775회 인용2010201120122013201420152016201720182019202020212022202337197353464572661724772788747862928836688', '학술 문서': 'Computer vision: algorithms and applicationsR Szeliski - 20228731회 인용 관련 학술자료 전체 29개의 버전 Computer vision. Texts in computer science*R Szeliski\\xa0- Texts in Computer Science, 201152회 인용 관련 학술자료 Computer vision: applications and algorithms*R Szeliski - 20116회 인용 관련 학술자료 '}, title='Computer vision: algorithms and applications', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Photo tourism: exploring photo collections in 3D': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2006/7/1', '도서': 'ACM siggraph 2006 papers', '설명': 'We present a system for interactively browsing and exploring large unstructured collections of photographs of a scene using a novel 3D interface. Our system consists of an image-based modeling front end that automatically computes the viewpoint of each photograph as well as a sparse 3D model of the scene and image to model correspondences. Our photo explorer uses image-based rendering techniques to smoothly transition between photographs, while also enabling full 3D navigation and exploration of the set of images and world geometry, along with auxiliary information such as overhead maps. Our system also makes it easy to construct photo tours of scenic or historic locations, and to annotate image details, which are automatically transferred to other relevant images. We demonstrate our system on several large personal photo collections as well as images gathered from Internet photo sharing sites.', '저자': 'Noah Snavely, Steven M Seitz, Richard Szeliski', '전체 인용횟수': '4507회 인용2006200720082009201020112012201320142015201620172018201920202021202220231583137185232301359372367403358314286241221197179156', '페이지': '835-846', '학술 문서': 'Photo tourism: exploring photo collections in 3DN Snavely, SM Seitz, R Szeliski\\xa0- ACM siggraph 2006 papers, 20064507회 인용 관련 학술자료 전체 44개의 버전 '}, title='Photo tourism: exploring photo collections in 3D', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The lumigraph': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2023/8/1', '도서': 'Seminal Graphics Papers: Pushing the Boundaries, Volume 2', '설명': 'This paper discusses a new method for capturing the complete appearance of bothsynthetic and real world objects and scenes,representing this information, and then using this representation to render images of the object from new camera positions. Unlike the shape capture process traditionally used in computer vision and the rendering process traditionally used in computer graphics, our approach does not rely on geometric representations. Instead we sample and reconstruct a 4D function. which we call a Lumigraph. The Lumigraph is a subset o f the complete plenoptic fu nction that describes the flow of light at all positions in all directions. With the Lumigraph. new images of the object can be generated very quick]y,independentof the geometric or illumination complexity of the scene or object. The paper discusses a complete working system including the capture of sainples. the construction of the Lumigraph\\xa0…', '저자': 'Steven J Gortler, Radek Grzeszczuk, Richard Szeliski, Michael F Cohen', '전체 인용횟수': '3883회 인용1997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220236310014614417119921421921421715515112210595101105103114118130116137123110157149', '페이지': '453-464', '학술 문서': 'The lumigraphSJ Gortler, R Grzeszczuk, R Szeliski, MF Cohen\\xa0- Seminal Graphics Papers: Pushing the Boundaries\\xa0…, 20233883회 인용 관련 학술자료 전체 22개의 버전 '}, title='The lumigraph', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A comparison and evaluation of multi-view stereo reconstruction algorithms': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2006/6/17', '게시자': 'IEEE', '권': '1', '설명': 'This paper presents a quantitative comparison of several multi-view stereo reconstruction algorithms. Until now, the lack of suitable calibrated multi-view image datasets with known ground truth (3D shape models) has prevented such direct comparisons. In this paper, we first survey multi-view stereo algorithms and compare them qualitatively using a taxonomy that differentiates their key properties. We then describe our process for acquiring and calibrating multiview image datasets with high-accuracy ground truth and introduce our evaluation methodology. Finally, we present the results of our quantitative comparison of state-of-the-art multi-view stereo reconstruction algorithms on six benchmark datasets. The datasets, evaluation details, and instructions for submitting new models are available online at http://vision.middlebury.edu/mview.', '저자': 'Steven M Seitz, Brian Curless, James Diebel, Daniel Scharstein, Richard Szeliski', '전체 인용횟수': '3207회 인용200620072008200920102011201220132014201520162017201820192020202120222023227288149174212220220243218216235196181172193163165', '컨퍼런스': \"2006 IEEE computer society conference on computer vision and pattern recognition (CVPR'06)\", '페이지': '519-528', '학술 문서': 'A comparison and evaluation of multi-view stereo reconstruction algorithmsSM Seitz, B Curless, J Diebel, D Scharstein, R Szeliski\\xa0- 2006 IEEE computer society conference on computer\\xa0…, 20063207회 인용 관련 학술자료 전체 20개의 버전 '}, title='A comparison and evaluation of multi-view stereo reconstruction algorithms', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A database and evaluation methodology for optical flow': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2007/10/14', '게시자': 'IEEE', '저자': 'Simon Baker, Daniel Scharstein, JP Lewis, Stefan Roth, Michael J Black, Richard Szeliski', '전체 인용횟수': '3191회 인용20102011201220132014201520162017201820192020202120222023167176187222240262269248212222211205216172', '컨퍼런스': 'Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on', '페이지': '1-8', '학술 문서': 'A database and evaluation methodology for optical flow*S Baker, D Scharstein, JP Lewis, S Roth, MJ Black…\\xa0- International journal of computer vision, 20113191회 인용 관련 학술자료 전체 56개의 버전 '}, title='A database and evaluation methodology for optical flow', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Modeling the world from internet photo collections': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2008/11', '게시자': 'Springer US', '권': '80', '설명': '  There are billions of photographs on the Internet, comprising the largest and most diverse photo collection ever assembled. How can computer vision researchers exploit this imagery? This paper explores this question from the standpoint of 3D scene modeling and visualization. We present structure-from-motion and image-based rendering algorithms that operate on hundreds of images downloaded as a result of keyword-based image search queries like “Notre Dame” or “Trevi Fountain.” This approach, which we call Photo Tourism, has enabled reconstructions of numerous well-known world sites. This paper presents these algorithms and results as a first step towards 3D modeling of the world’s well-photographed sites, cities, and landscapes from Internet imagery, and discusses key open problems and challenges for the research community. ', '저널': 'International journal of computer vision', '저자': 'Noah Snavely, Steven M Seitz, Richard Szeliski', '전체 인용횟수': '2804회 인용20082009201020112012201320142015201620172018201920202021202220231163109138170191211267235263249207194183144127', '페이지': '189-210', '학술 문서': 'Modeling the world from internet photo collectionsN Snavely, SM Seitz, R Szeliski\\xa0- International journal of computer vision, 20082804회 인용 관련 학술자료 전체 31개의 버전 '}, title='Modeling the world from internet photo collections', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Image alignment and stitching: A tutorial': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2007/1/1', '게시자': 'Now Publishers, Inc.', '권': '2', '설명': 'This tutorial reviews image alignment and image stitching algorithms. Image alignment algorithms can discover the correspondence relationships among images with varying degrees of overlap. They are ideally suited for applications such as video stabilization, summarization, and the creation of panoramic mosaics. Image stitching algorithms take the alignment estimates produced by such registration algorithms and blend the images in a seamless manner, taking care to deal with potential problems such as blurring or ghosting caused by parallax and scene movement as well as varying image exposures. This tutorial reviews the basic motion models underlying alignment and stitching algorithms, describes effective direct (pixel-based) and feature-based alignment algorithms, and describes blending algorithms used to produce seamless mosaics. It ends with a discussion of open research problems in the area.', '저자': 'Richard Szeliski', '전체 인용횟수': '2650회 인용2006200720082009201020112012201320142015201620172018201920202021202220232849879616116416719719818021518817819115314711495', '출처': 'Foundations and Trends® in Computer Graphics and Vision', '페이지': '1-104', '학술 문서': 'Image alignment and stitching: A tutorialR Szeliski\\xa0- Foundations and Trends® in Computer Graphics and\\xa0…, 20072610회 인용 관련 학술자료 전체 41개의 버전 Image alignment and stitching*R Szeliski, R Szeliski\\xa0- Computer Vision: Algorithms and Applications, 202264회 인용 관련 학술자료 전체 11개의 버전 ', '호': '1'}, title='Image alignment and stitching: A tutorial', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Building rome in a day': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2011/10/1', '게시자': 'ACM', '권': '54', '설명': 'We present a system that can reconstruct 3D geometry from large, unorganized collections of photographs such as those found by searching for a given city (e.g., Rome) on Internet photo-sharing sites. Our system is built on a set of new, distributed computer vision algorithms for image matching and 3D reconstruction, designed to maximize parallelism at each stage of the pipeline and to scale gracefully with both the size of the problem and the amount of available computation. Our experimental results demonstrate that it is now possible to reconstruct city-scale image collections with more than a hundred thousand images in less than a day.', '저널': 'Communications of the ACM', '저자': 'Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven M Seitz, Richard Szeliski', '전체 인용횟수': '2640회 인용2010201120122013201420152016201720182019202020212022202390126153195212252258232220180166173151170', '페이지': '105-112', '학술 문서': 'Building rome in a dayS Agarwal, Y Furukawa, N Snavely, I Simon, B Curless…\\xa0- Communications of the ACM, 20112640회 인용 관련 학술자료 전체 24개의 버전 ', '호': '10'}, title='Building rome in a day', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'High-accuracy stereo depth maps using structured light': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2003/6/18', '게시자': 'IEEE', '권': '1', '설명': 'Progress in stereo algorithm performance is quickly outpacing the ability of existing stereo data sets to discriminate among the best-performing algorithms, motivating the need for more challenging scenes with accurate ground truth information. This paper describes a method for acquiring high-complexity stereo image pairs with pixel-accurate correspondence information using structured light. Unlike traditional range-sensing approaches, our method does not require the calibration of the light sources and yields registered disparity maps between all pairs of cameras and illumination projectors. We present new stereo data sets acquired with our method and demonstrate their suitability for stereo algorithm evaluation. Our results are available at http://www.middlebury.edu/stereo/.', '저자': 'Daniel Scharstein, Richard Szeliski', '전체 인용횟수': '2290회 인용200420052006200720082009201020112012201320142015201620172018201920202021202220231761517476969114016414215715716015014813113512110875', '컨퍼런스': '2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings.', '페이지': 'I-I', '학술 문서': 'High-accuracy stereo depth maps using structured lightD Scharstein, R Szeliski\\xa0- 2003 IEEE Computer Society Conference on Computer\\xa0…, 20032290회 인용 관련 학술자료 전체 22개의 버전 '}, title='High-accuracy stereo depth maps using structured light', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'High-quality video view interpolation using a layered representation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2004/8/1', '게시자': 'ACM', '권': '23', '설명': 'The ability to interactively control viewpoint while watching a video is an exciting application of image-based rendering. The goal of our work is to render dynamic scenes with interactive viewpoint control using a relatively small number of video cameras. In this paper, we show how high-quality video-based rendering of dynamic scenes can be accomplished using multiple synchronized video streams combined with novel image-based modeling and rendering algorithms. Once these video streams have been processed, we can synthesize any intermediate view between cameras at any time, with the potential for space-time manipulation.In our approach, we first use a novel color segmentation-based stereo algorithm to generate high-quality photoconsistent correspondences across all camera views. Mattes for areas near depth discontinuities are then automatically extracted to reduce artifacts during view synthesis\\xa0…', '저널': 'ACM transactions on graphics (TOG)', '저자': 'C Lawrence Zitnick, Sing Bing Kang, Matthew Uyttendaele, Simon Winder, Richard Szeliski', '전체 인용횟수': '1945회 인용2004200520062007200820092010201120122013201420152016201720182019202020212022202312529712110115916215214615411210875101627358705543', '페이지': '600-608', '학술 문서': 'High-quality video view interpolation using a layered representationCL Zitnick, SB Kang, M Uyttendaele, S Winder…\\xa0- ACM transactions on graphics (TOG), 20041945회 인용 관련 학술자료 전체 14개의 버전 ', '호': '3'}, title='High-quality video view interpolation using a layered representation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Layered depth images': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1998/7/24', '도서': 'Proceedings of the 25th annual conference on Computer graphics and interactive techniques', '설명': 'In this paper we present a set of efficient image based rendering methods capable of rendering multiple frames per second on a PC. The first method warps Sprites with Depth representing smooth surfaces without the gaps found in other techniques. A second method for more general scenes performs warping from an intermediate representation called a Layered Depth Image (LDI). An LDI is a view of the scene from a single input camera view, but with multiple pixels along each line of sight. The size of the representation grows only linearly with the observed depth complexity in the scene. Moreover, because the LDI data are represented in a single image coordinate system, McMillan’s warp ordering algorithm can be successfully adapted. As a result, pixels are drawn in the output image in backto-front order. No z-buffer is required, so alpha-compositing can be done efficiently without depth sorting. This makes\\xa0…', '저자': 'Jonathan Shade, Steven Gortler, Li-wei He, Richard Szeliski', '전체 인용횟수': '1814회 인용1998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202313527170951261111031109791808664647456433742374747565452', '페이지': '231-242', '학술 문서': 'Layered depth imagesJ Shade, S Gortler, L He, R Szeliski\\xa0- Proceedings of the 25th annual conference on\\xa0…, 19981814회 인용 관련 학술자료 전체 13개의 버전 '}, title='Layered depth images', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Edge-preserving decompositions for multi-scale tone and detail manipulation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2008/8/1', '게시자': 'ACM', '권': '27', '설명': 'Many recent computational photography techniques decompose an image into a piecewise smooth base layer, containing large scale variations in intensity, and a residual detail layer capturing the smaller scale details in the image. In many of these applications, it is important to control the spatial scale of the extracted details, and it is often desirable to manipulate details at multiple scales, while avoiding visual artifacts. In this paper we introduce a new way to construct edge-preserving multi-scale image decompositions. We show that current basedetail decomposition techniques, based on the bilateral filter, are limited in their ability to extract detail at arbitrary scales. Instead, we advocate the use of an alternative edge-preserving smoothing operator, based on the weighted least squares optimization framework, which is particularly well suited for progressive coarsening of images and for multi-scale detail extraction\\xa0…', '저널': 'ACM transactions on graphics (TOG)', '저자': 'Zeev Farbman, Raanan Fattal, Dani Lischinski, Richard Szeliski', '전체 인용횟수': '1761회 인용20092010201120122013201420152016201720182019202020212022202321433762103113174180176165155135143113121', '페이지': '1-10', '학술 문서': 'Edge-preserving decompositions for multi-scale tone and detail manipulationZ Farbman, R Fattal, D Lischinski, R Szeliski\\xa0- ACM transactions on graphics (TOG), 20081761회 인용 관련 학술자료 전체 22개의 버전 ', '호': '3'}, title='Edge-preserving decompositions for multi-scale tone and detail manipulation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Fast surface interpolation using hierarchical basis functions': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1990/6', '게시자': 'IEEE', '권': '12', '설명': 'An alternative to multigrid relaxation that is much easier to implement and more generally applicable is presented. Conjugate gradient descent is used in conjunction with a hierarchical (multiresolution) set of basis functions. The resultant algorithm uses a pyramid to smooth the residual vector before the direction is computed. Simulation results showing the speed of convergence and its dependence on the choice of interpolator, the number of smoothing levels, and other factors are presented. The relationship of this approach to other multiresolution relaxation and representation schemes is also discussed.< >', '저널': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', '저자': 'Richard Szeliski', '전체 인용횟수': '1503회 인용1989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202336415247568965899011189114995719232021212434333348443224124122231', '페이지': '513-528', '학술 문서': 'Fast surface interpolation using hierarchical basis functionsR Szeliski\\xa0- IEEE Transactions on Pattern Analysis and Machine\\xa0…, 19901503회 인용 관련 학술자료 전체 10개의 버전 ', '호': '6'}, title='Fast surface interpolation using hierarchical basis functions', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Digital photography with flash and no-flash image pairs': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2004/8/8', '게시자': 'ACM', '권': '23', '설명': 'Digital photography has made it possible to quickly and easily take a pair of images of low-light environments: one with flash to capture detail and one without flash to capture ambient illumination. We present a variety of applications that analyze and combine the strengths of such flash/no-flash image pairs. Our applications include denoising and detail transfer (to merge the ambient qualities of the no-flash image with the high-frequency flash detail), white-balancing (to change the color tone of the ambient image), continuous flash (to interactively adjust flash intensity), and red-eye removal (to repair artifacts in the flash image). We demonstrate how these applications can synthesize new images that are of higher quality than either of the originals.', '저자': 'Georg Petschnigg, Richard Szeliski, Maneesh Agrawala, Michael Cohen, Hugues Hoppe, Kentaro Toyama', '전체 인용횟수': '1491회 인용2005200620072008200920102011201220132014201520162017201820192020202120222023920265053797588971141441181208910486766747', '컨퍼런스': 'ACM Transactions on Graphics (TOG)', '페이지': '664-672', '학술 문서': 'Digital photography with flash and no-flash image pairsG Petschnigg, R Szeliski, M Agrawala, M Cohen…\\xa0- ACM transactions on graphics (TOG), 20041491회 인용 관련 학술자료 전체 33개의 버전 ', '호': '3'}, title='Digital photography with flash and no-flash image pairs', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Creating full view panoramic image mosaics and environment maps': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2023/8/1', '도서': 'Seminal Graphics Papers: Pushing the Boundaries, Volume 2', '설명': 'This paper presents a novel approach to creating full viewpanoramic mosaics from image sequences. Unlike current panoramic stitching methods, which usually require pure horizontal camera panning, our system does not require any controlled motions or constraints on how the images are taken (as long as there is no strong motion parallax). For example, images taken from a hand-held digital camera can be stitched seamlessly into panoramic mosaics. Because we represent our image mosaics using a set of transforms, there are no singularity problems such as those existing at the top and bottom of cylindrical or spherical maps. Our algorithm is fast and robust because it directly recovers 3D rotations instead of general 8 parameter planar perspective transforms. Methods to recover camera focal length are also presented. We also present an algorithm for efficiently extracting environment maps from our image\\xa0…', '저자': 'Richard Szeliski, Heung-Yeung Shum', '전체 인용횟수': '1446회 인용199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220233243627753779069776961626850586772435860435017241913', '페이지': '653-660', '학술 문서': 'Creating full view panoramic image mosaics and environment mapsR Szeliski, HY Shum\\xa0- Seminal Graphics Papers: Pushing the Boundaries\\xa0…, 20231446회 인용 관련 학술자료 전체 28개의 버전 '}, title='Creating full view panoramic image mosaics and environment maps', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Video mosaics for virtual environments': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1996/3', '게시자': 'IEEE', '권': '16', '설명': 'As computer-based video becomes ubiquitous with the expansion of transmission, storage, and manipulation capabilities, it will offer a rich source of imagery for computer graphics applications. This article looks at one way to use video as a new source of high-resolution, photorealistic imagery for these applications. If you walked through an environment, such as a building interior, and filmed a video sequence of what you saw you could subsequently register and composite the video images together into large mosaics of the scene. In this way, you can achieve an essentially unlimited resolution. Furthermore, since you can acquire the images using any optical technology, you can reconstruct any scene regardless of its range or scale. Video mosaics can be used in many different applications, including the creation of virtual reality environments, computer-game settings, and movie special effects. I present\\xa0…', '저널': 'IEEE computer Graphics and Applications', '저자': 'Richard Szeliski', '전체 인용횟수': '1435회 인용1996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023627547779736478858188866960575866565730433022151911105', '페이지': '22-30', '학술 문서': 'Video mosaics for virtual environmentsR Szeliski\\xa0- IEEE computer Graphics and Applications, 19961435회 인용 관련 학술자료 전체 21개의 버전 ', '호': '2'}, title='Video mosaics for virtual environments', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Locally adapted hierarchical basis preconditioning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2006/7/1', '도서': 'ACM SIGGRAPH 2006 Papers', '설명': 'This paper develops locally adapted hierarchical basis functions for effectively preconditioning large optimization problems that arise in computer graphics applications such as tone mapping, gradient-domain blending, colorization, and scattered data interpolation. By looking at the local structure of the coefficient matrix and performing a recursive set of variable eliminations, combined with a simplification of the resulting coarse level problems, we obtain bases better suited for problems with inhomogeneous (spatially varying) data, smoothness, and boundary constraints. Our approach removes the need to heuristically adjust the optimal number of preconditioning levels, significantly outperforms previously proposed approaches, and also maps cleanly onto data-parallel architectures such as modern GPUs.', '저자': 'Richard Szeliski', '전체 인용횟수': '1368회 인용19881989199019911992199319941995199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202318343533354269548076968010486471412714172932434357483926148437744', '페이지': '1135-1143', '학술 문서': 'Locally adapted hierarchical basis preconditioningR Szeliski\\xa0- ACM SIGGRAPH 2006 Papers, 20061368회 인용 관련 학술자료 전체 12개의 버전 '}, title='Locally adapted hierarchical basis preconditioning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A comparative study of energy minimization methods for markov random fields with smoothness-based priors': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2008/4/18', '게시자': 'IEEE', '권': '30', '설명': 'Among the most exciting advances in early vision has been the development of efficient energy minimization algorithms for pixel-labeling tasks such as depth or texture computation. It has been known for decades that such problems can be elegantly expressed as Markov random fields, yet the resulting energy minimization problems have been widely viewed as intractable. Algorithms such as graph cuts and loopy belief propagation (LBP) have proven to be very powerful: For example, such methods form the basis for almost all the top-performing stereo methods. However, the trade-offs among different energy minimization algorithms are still not well understood. In this paper, we describe a set of energy minimization benchmarks and use them to compare the solution quality and runtime of several common energy minimization algorithms. We investigate three promising methods-graph cuts, LBP, and tree\\xa0…', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Richard Szeliski, Ramin Zabih, Daniel Scharstein, Olga Veksler, Vladimir Kolmogorov, Aseem Agarwala, Marshall Tappen, Carsten Rother', '전체 인용횟수': '1339회 인용2008200920102011201220132014201520162017201820192020202120222023317810312113113413813010979694553313422', '페이지': '1068-1080', '학술 문서': 'A comparative study of energy minimization methods for markov random fields with smoothness-based priorsR Szeliski, R Zabih, D Scharstein, O Veksler…\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 20081339회 인용 관련 학술자료 전체 14개의 버전 ', '호': '6'}, title='A comparative study of energy minimization methods for markov random fields with smoothness-based priors', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Synthesizing realistic facial expressions from photographs': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2006/7/30', '도서': 'Acm siggraph 2006 courses', '설명': \"We present new techniques for creating photorealistic textured 3D facial models from photographs of a human subject, and for creating smooth transitions between different facial expressions by morphing between these different models. Starting from several uncalibrated views of a human subject, we employ a user-assisted technique to recover the camera poses corresponding to the views as well as the 3D coordinates of a sparse set of chosen locations on the subject's face. A scattered data interpolation technique is then used to deform a generic face mesh to fit the particular geometry of the subject's face. Having recovered the camera poses and the facial geometry, we extract from the input images one or more texture maps for the model. This process is repeated for several facial expressions of a particular subject. To generate transitions between these facial expressions we use 3D shape morphing between\\xa0…\", '저자': 'Frédéric Pighin, Jamie Hecker, Dani Lischinski, Richard Szeliski, David H Salesin', '전체 인용횟수': '1320회 인용199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220235304772728594100122868865665948384137302225172012910', '페이지': '19-es', '학술 문서': 'Synthesizing realistic facial expressions from photographsF Pighin, J Hecker, D Lischinski, R Szeliski, DH Salesin\\xa0- Acm siggraph 2006 courses, 20061320회 인용 관련 학술자료 전체 31개의 버전 '}, title='Synthesizing realistic facial expressions from photographs', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Densely Connected Convolutional Networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': 'Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections--one between each layer and its subsequent layer--our network has L (L+ 1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github. com/liuzhuang13/DenseNet.', '저자': 'Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger', '전체 인용횟수': '40361회 인용2017201820192020202120222023387225344846547843295698343', '출처': 'IEEE Conference on Computer Vision and Pattern Recognition', '학술 문서': 'Densely connected convolutional networksG Huang, Z Liu, L Van Der Maaten, KQ Weinberger\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 201740071회 인용 관련 학술자료 전체 37개의 버전 Densely connected convolutional networks. arXiv 2016*G Huang, Z Liu, L van der Maaten, KQ Weinberger\\xa0- arXiv preprint arXiv:1608.06993, 2018263회 인용 관련 학술자료 Densely connected convolutional networks (2016)*G Huang, Z Liu, L van der Maaten, KQ Weinberger\\xa0- arXiv preprint arXiv:1608.06993, 2016173회 인용 관련 학술자료 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*G Huang, Z Liu, L Van Der Maaten, KQ Weinberger - 2017105회 인용 관련 학술자료 van, and Weinberger*G Huang, Z Liu, L Maaten\\xa0- Kilian Q,“Densely Connected Convolutional Networks,”\\xa0…, 201686회 인용 관련 학술자료 Van Der, and Weinberger KQ. 2017*G Huang, Z Liu, L Maaten\\xa0- …\\xa0connected convolutional networks. In Proceedings of\\xa0…, 201716회 인용 관련 학술자료 Densely Connected Convolutional Networks, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*G Huang, Z Liu, L Van Der Maaten, KQ Weinberger - 20179회 인용 관련 학술자료 vol 2017, Densely connected convolutional networks*G Huang, Z Liu, L Maaten, K Weinberger\\xa0- 2017 IEEE conference on computer vision and pattern\\xa0…, 20177회 인용 관련 학술자료 Dense Connected Convolutional Networks [C]//CVPR*G Huang, Z Liu, LVD Maaten\\xa0- IEEE Computer Society, 20177회 인용 관련 학술자료 Densely connected convolutional networks. Paper presented at the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*G Huang, Z Liu, LVD Maaten, KQ Weinberger - 20175회 인용 관련 학술자료 van der & W*G Huang, Z Liu, L Maaten\\xa0- KQ Densely connected convolutional networks. arXiv, 20184회 인용 관련 학술자료 '}, title='Densely Connected Convolutional Networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Visualizing data using t-SNE': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2008/11', '권': '9', '설명': 'We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.', '저널': 'The Journal of Machine Learning Research', '저자': 'Laurens van der Maaten, Geoffrey Hinton', '전체 인용횟수': '38612회 인용20122013201420152016201720182019202020212022202311814928055410201663280840575088677377897845', '페이지': '85', '학술 문서': 'Visualizing data using t-SNE.L Van der Maaten, G Hinton\\xa0- Journal of machine learning research, 200838329회 인용 관련 학술자료 전체 45개의 버전 Visualizing data using t-SNE*G Hinton, L van der Maaten\\xa0- J. Mach. Learn. Res., 2008409회 인용 관련 학술자료 Van Der; Hinton, G. 2008.“Visualizing data using t-SNE.”*L Maaten\\xa0- Journal of machine learning research41회 인용 관련 학술자료 van der & Hinton, GE (2008)*L Maaten\\xa0- J. Mach. Learn. Res18회 인용 관련 학술자료 t-SNE*L Van Der Maaten\\xa0- Recuperado de https://lvdmaaten. github. io/tsne, 201916회 인용 관련 학술자료 Visualizing high-dimensional data using t-629 sne*L Van Der Maaten, G Hinton\\xa0- Journal of Machine Learning Research, 20088회 인용 관련 학술자료 Journal of Machine Learning Research*L Maaten, G Hinton - 20083회 인용 관련 학술자료 ', '호': '2579-2605'}, title='Visualizing data using t-SNE', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Dimensionality reduction: A comparative review': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009', '설명': 'In recent years, a variety of nonlinear dimensionality reduction techniques have been proposed that aim to address the limitations of traditional techniques such as PCA. The paper presents a review and systematic comparison of these techniques. The performances of the nonlinear techniques are investigated on artificial and natural tasks. The results of the experiments reveal that nonlinear techniques perform well on selected artificial tasks, but do not outperform the traditional PCA on real-world tasks. The paper explains these results by identifying weaknesses of current nonlinear techniques, and suggests how the performance of nonlinear dimensionality reduction techniques may be improved.', '저널': 'Technical Report TiCC TR 2009-005', '저자': 'LJP Van der Maaten, EO Postma, HJ Van den Herik', '전체 인용횟수': '3636회 인용2007200820092010201120122013201420152016201720182019202020212022202314283983117119163209228292264307342349414371259', '학술 문서': 'Dimensionality reduction: A comparative reviewL Van Der Maaten, EO Postma, HJ van den Herik\\xa0- Journal of Machine Learning Research, 20092444회 인용 관련 학술자료 전체 7개의 버전 Dimensionality reduction: a comparative*L Van Der Maaten, E Postma, J Van den Herik\\xa0- J Mach Learn Res, 20091339회 인용 관련 학술자료 전체 14개의 버전 Vak*E Maanen\\xa0- Informatie-Maandblad voor de Informatievoorziening, 200442회 인용 관련 학술자료 '}, title='Dimensionality reduction: A comparative review', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Accelerating t-SNE using Tree-Based Algorithms': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/1/1', '게시자': 'JMLR. org', '권': '15', '설명': 'The paper investigates the acceleration of t-SNE—an embedding technique that is commonly used for the visualization of high-dimensional data in scatter plots—using two treebased algorithms. In particular, the paper develops variants of the Barnes-Hut algorithm and of the dual-tree algorithm that approximate the gradient used for learning t-SNE embeddings in O (N log N). Our experiments show that the resulting algorithms substantially accelerate t-SNE, and that they make it possible to learn embeddings of data sets with millions of objects. Somewhat counterintuitively, the Barnes-Hut variant of t-SNE appears to outperform the dual-tree variant.', '저널': 'The Journal of Machine Learning Research', '저자': 'Laurens Van Der Maaten', '전체 인용횟수': '2735회 인용20152016201720182019202020212022202336106183328457443406425321', '페이지': '3221-3245', '학술 문서': 'Accelerating t-SNE using tree-based algorithmsL Van Der Maaten\\xa0- The journal of machine learning research, 20142735회 인용 관련 학술자료 전체 11개의 버전 ', '호': '1'}, title='Accelerating t-SNE using Tree-Based Algorithms', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Countering adversarial images using input transformations': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/10/31', '설명': 'This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods', '저널': 'arXiv preprint arXiv:1711.00117', '저자': 'Chuan Guo, Mayank Rana, Moustapha Cisse, Laurens Van Der Maaten', '전체 인용횟수': '1356회 인용2017201820192020202120222023689174258278309241', '학술 문서': 'Countering adversarial images using input transformationsC Guo, M Rana, M Cisse, L Van Der Maaten\\xa0- arXiv preprint arXiv:1711.00117, 20171356회 인용 관련 학술자료 전체 3개의 버전 '}, title='Countering adversarial images using input transformations', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Self-supervised learning of pretext-invariant representations': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020', '설명': \"The goal of self-supervised learning from images is to construct image representations that are semantically meaningful via pretext tasks that do not require semantic annotations. Many pretext tasks lead to representations that are covariant with image transformations. We argue that, instead, semantic representations ought to be invariant under such transformations. Specifically, we develop Pretext-Invariant Representation Learning (PIRL, pronounced aspearl') that learns invariant representations based on pretext tasks. We use PIRL with a commonly used pretext task that involves solving jigsaw puzzles. We find that PIRL substantially improves the semantic quality of the learned image representations. Our approach sets a new state-of-the-art in self-supervised learning from images on several popular benchmarks for self-supervised learning. Despite being unsupervised, PIRL outperforms supervised pre-training in learning image representations for object detection. Altogether, our results demonstrate the potential of self-supervised representations with good invariance properties.\", '저자': 'Ishan Misra, Laurens van der Maaten', '전체 인용횟수': '1317회 인용2020202120222023128325461395', '컨퍼런스': 'Proceedings of the IEEE/CVF conference on computer vision and pattern recognition', '페이지': '6707-6717', '학술 문서': 'Self-supervised learning of pretext-invariant representationsI Misra, L Maaten\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 20201317회 인용 관련 학술자료 전체 8개의 버전 '}, title='Self-supervised learning of pretext-invariant representations', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " '3d semantic segmentation with submanifold sparse convolutional networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': \"Convolutional networks are the de-facto standard for analyzing spatio-temporal data such as images, videos, and 3D shapes. Whilst some of this data is naturally dense (eg, photos), many other data sources are inherently sparse. Examples include 3D point clouds that were obtained using a LiDAR scanner or RGB-D camera. Standard``dense''implementations of convolutional networks are very inefficient when applied on such sparse data. We introduce new sparse convolutional operations that are designed to process spatially-sparse data more efficiently, and use them to develop spatially-sparse convolutional networks. We demonstrate the strong performance of the resulting models, called submanifold sparse convolutional networks (SSCNs), on two tasks involving semantic segmentation of 3D point clouds. In particular, our models outperform all prior state-of-the-art on the test set of a recent semantic segmentation competition.\", '저자': 'Benjamin Graham, Martin Engelcke, Laurens Van Der Maaten', '전체 인용횟수': '1295회 인용2018201920202021202220231369155285374392', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '9224-9232', '학술 문서': '3d semantic segmentation with submanifold sparse convolutional networksB Graham, M Engelcke, L Van Der Maaten\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20181295회 인용 관련 학술자료 전체 11개의 버전 '}, title='3d semantic segmentation with submanifold sparse convolutional networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Feature denoising for improving adversarial robustness': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': 'Adversarial attacks to image classification systems present challenges to convolutional networks and opportunities for understanding them. This study suggests that adversarial perturbations on images lead to noise in the features constructed by these networks. Motivated by this observation, we develop new network architectures that increase adversarial robustness by performing feature denoising. Specifically, our networks contain blocks that denoise the features using non-local means or other filters; the entire networks are trained end-to-end. When combined with adversarial training, our feature denoising networks substantially improve the state-of-the-art in adversarial robustness in both white-box and black-box attack settings. On ImageNet, under 10-iteration PGD white-box attacks where prior art has 27.9% accuracy, our method achieves 55.7%; even under extreme 2000-iteration PGD white-box attacks, our method secures 42.6% accuracy. Our method was ranked first in Competition on Adversarial Attacks and Defenses (CAAD) 2018---it achieved 50.6% classification accuracy on a secret, ImageNet-like test dataset against 48 unknown attackers, surpassing the runner-up approach by 10%. Code is available at https://github. com/facebookresearch/ImageNet-Adversarial-Training.', '저자': 'Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan L Yuille, Kaiming He', '전체 인용횟수': '882회 인용201820192020202120222023473196213199193', '컨퍼런스': 'Proceedings of the IEEE/CVF conference on computer vision and pattern recognition', '페이지': '501-509', '학술 문서': 'Feature denoising for improving adversarial robustnessC Xie, Y Wu, L Maaten, AL Yuille, K He\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 2019882회 인용 관련 학술자료 전체 12개의 버전 Feature-level Denoising Improves Adversarial Robustness*C Xie, Y Wu, L van der Maaten, A Yuille, K He관련 학술자료 전체 2개의 버전 '}, title='Feature denoising for improving adversarial robustness', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Condensenet: An efficient densenet using learned group convolutions': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '설명': 'Deep neural networks are increasingly used on mobile devices, where computational resources are limited. In this paper we develop CondenseNet, a novel network architecture with unprecedented efficiency. It combines dense connectivity with a novel module called learned group convolution. The dense connectivity facilitates feature re-use in the network, whereas learned group convolutions remove connections between layers for which this feature re-use is superfluous. At test time, our model can be implemented using standard group convolutions, allowing for efficient computation in practice. Our experiments show that CondenseNets are far more efficient than state-of-the-art compact convolutional networks such as MobileNets and ShuffleNets.', '저자': 'Gao Huang, Shichen Liu, Laurens Van der Maaten, Kilian Q Weinberger', '전체 인용횟수': '878회 인용20182019202020212022202346133177196179142', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '2752-2761', '학술 문서': 'Condensenet: An efficient densenet using learned group convolutionsG Huang, S Liu, L Van der Maaten, KQ Weinberger\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2018878회 인용 관련 학술자료 전체 15개의 버전 '}, title='Condensenet: An efficient densenet using learned group convolutions', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Multi-scale dense networks for resource efficient image classification': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/3/29', '설명': 'In this paper we investigate image classification with computational resource limits at test time. Two such settings are: 1. anytime classification, where the network\\'s prediction for a test example is progressively updated, facilitating the output of a prediction at any time; and 2. budgeted batch classification, where a fixed amount of computation is available to classify a set of examples that can be spent unevenly across \"easier\" and \"harder\" inputs. In contrast to most prior work, such as the popular Viola and Jones algorithm, our approach is based on convolutional neural networks. We train multiple classifiers with varying resource demands, which we adaptively apply during test time. To maximally re-use computation between the classifiers, we incorporate them as early-exits into a single deep convolutional neural network and inter-connect them with dense connectivity. To facilitate high quality classification early on, we use a two-dimensional multi-scale network architecture that maintains coarse and fine level features all-throughout the network. Experiments on three image-classification tasks demonstrate that our framework substantially improves the existing state-of-the-art in both settings.', '저널': 'arXiv preprint arXiv:1703.09844', '저자': 'Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens Van Der Maaten, Kilian Q Weinberger', '전체 인용횟수': '670회 인용201820192020202120222023246989151162171', '학술 문서': 'Multi-scale dense networks for resource efficient image classificationG Huang, D Chen, T Li, F Wu, L Van Der Maaten…\\xa0- arXiv preprint arXiv:1703.09844, 2017670회 인용 관련 학술자료 전체 4개의 버전 '}, title='Multi-scale dense networks for resource efficient image classification', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning a parametric embedding by preserving local structure': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009', '설명': 'The paper presents a new unsupervised dimensionality reduction technique, called parametric t-SNE, that learns a parametric mapping between the high-dimensional data space and the low-dimensional latent space. Parametric t-SNE learns the parametric mapping in such a way that the local structure of the data is preserved as well as possible in the latent space. We evaluate the performance of parametric t-SNE in experiments on two datasets, in which we compare it to the performance of two other unsupervised parametric dimensionality reduction techniques. The results of experiments illustrate the strong performance of parametric t-SNE, in particular, in learning settings in which the dimensionality of the latent space is relatively low.', '저널': 'Proceedings of AI-STATS', '저자': 'Laurens van der Maaten', '전체 인용횟수': '670회 인용200920102011201220132014201520162017201820192020202120222023291121148233342688793978969', '학술 문서': 'Learning a parametric embedding by preserving local structureL Van Der Maaten\\xa0- Artificial intelligence and statistics, 2009670회 인용 관련 학술자료 전체 13개의 버전 '}, title='Learning a parametric embedding by preserving local structure', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Rtsne: T-distributed stochastic neighbor embedding using Barnes-Hut implementation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/1/10', '저널': 'R package version 0.13, URL https://github. com/jkrijthe/Rtsne', '저자': 'Jesse H Krijthe, Laurens Van der Maaten', '전체 인용횟수': '472회 인용20152016201720182019202020212022202321018456590888271', '학술 문서': 'Rtsne: T-distributed stochastic neighbor embedding using Barnes-Hut implementationJH Krijthe, L Van der Maaten\\xa0- R package version 0.13, URL https://github. com/jkrijthe\\xa0…, 2015427회 인용 관련 학술자료 Rtsne: T-distributed stochastic neighbor embedding using Barnes-Hut implementation. R package version 0.13*JH Krijthe, L Van der Maaten\\xa0- Computer Software, 201555회 인용 관련 학술자료 Package ‘Rtsne’*J Krijthe, L van der Maaten, MJ Krijthe\\xa0- R package version 0.13, 201820회 인용 관련 학술자료 전체 33개의 버전 '}, title='Rtsne: T-distributed stochastic neighbor embedding using Barnes-Hut implementation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Submanifold sparse convolutional networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/6/5', '설명': 'Convolutional network are the de-facto standard for analysing spatio-temporal data such as images, videos, 3D shapes, etc. Whilst some of this data is naturally dense (for instance, photos), many other data sources are inherently sparse. Examples include pen-strokes forming on a piece of paper, or (colored) 3D point clouds that were obtained using a LiDAR scanner or RGB-D camera. Standard \"dense\" implementations of convolutional networks are very inefficient when applied on such sparse data. We introduce a sparse convolutional operation tailored to processing sparse data that differs from prior work on sparse convolutional networks in that it operates strictly on submanifolds, rather than \"dilating\" the observation with every layer in the network. Our empirical analysis of the resulting submanifold sparse convolutional networks shows that they perform on par with state-of-the-art methods whilst requiring substantially less computation.', '저널': 'arXiv preprint arXiv:1706.01307', '저자': 'Benjamin Graham, Laurens Van der Maaten', '전체 인용횟수': '420회 인용20182019202020212022202321275886116110', '학술 문서': 'Submanifold sparse convolutional networksB Graham, L Van der Maaten\\xa0- arXiv preprint arXiv:1706.01307, 2017420회 인용 관련 학술자료 전체 2개의 버전 '}, title='Submanifold sparse convolutional networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning Visual Features from Large Weakly Supervised Data': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': ' Convolutional networks trained on large supervised datasets produce visual features which form the basis for the state-of-the-art in many computer-vision problems. Further improvements of these visual features will likely require even larger manually labeled data sets, which severely limits the pace at which progress can be made. In this paper, we explore the potential of leveraging massive, weakly-labeled image collections for learning good visual features. We train convolutional networks on a dataset of 100 million Flickr photos and comments, and show that these networks produce features that perform well in a range of vision problems. We also show that the networks appropriately capture word similarity and learn correspondences between different languages.', '저자': 'Armand Joulin, Laurens van der Maaten, Allan Jabri, Nicolas Vasilache', '전체 인용횟수': '386회 인용201620172018201920202021202220231737374056626466', '컨퍼런스': 'European Conference on Computer Vision', '학술 문서': 'Learning visual features from large weakly supervised dataA Joulin, L Van Der Maaten, A Jabri, N Vasilache\\xa0- Computer Vision–ECCV 2016: 14th European\\xa0…, 2016386회 인용 관련 학술자료 전체 3개의 버전 '}, title='Learning Visual Features from Large Weakly Supervised Data', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Convolutional networks with dense connectivity': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019/5/23', '게시자': 'IEEE', '권': '44', '설명': 'Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with    layers have    connections—one between each layer and its subsequent layer—our network has    direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, encourage feature reuse and substantially improve parameter efficiency. We evaluate our proposed\\xa0…', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Gao Huang, Zhuang Liu, Geoff Pleiss, Laurens Van Der Maaten, Kilian Q Weinberger', '전체 인용횟수': '381회 인용2019202020212022202324698109125', '페이지': '8704-8716', '학술 문서': 'Convolutional networks with dense connectivityG Huang, Z Liu, G Pleiss, L Van Der Maaten…\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 2019381회 인용 관련 학술자료 전체 14개의 버전 ', '호': '12'}, title='Convolutional networks with dense connectivity', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Revisiting visual question answering baselines': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/9/17', '게시자': 'Springer International Publishing', '도서': 'European conference on computer vision', '설명': ' Visual question answering (VQA) is an interesting learning setting for evaluating the abilities and shortcomings of current systems for image understanding. Many of the recently proposed VQA systems include attention or memory mechanisms designed to perform “reasoning”. Furthermore, for the task of multiple-choice VQA, nearly all of these systems train a multi-class classifier on image and question features to predict an answer. This paper questions the value of these common practices and develops a simple alternative model based on binary classification. Instead of treating answers as competing choices, our model receives the answer as input and predicts whether or not an image-question-answer triplet is correct. We evaluate our model on the Visual7W Telling and the VQA Real Multiple Choice tasks, and find that even simple versions of our model perform competitively. Our best model achieves\\xa0…', '저자': 'Allan Jabri, Armand Joulin, Laurens Van Der Maaten', '전체 인용횟수': '305회 인용2015201620172018201920202021202220232541556147372823', '페이지': '727-739', '학술 문서': 'Revisiting visual question answering baselinesA Jabri, A Joulin, L Van Der Maaten\\xa0- European conference on computer vision, 2016305회 인용 관련 학술자료 전체 5개의 버전 '}, title='Revisiting visual question answering baselines', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Visualizing non-metric similarities in multiple maps': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012', '게시자': 'Springer Netherlands', '설명': ' Techniques for multidimensional scaling visualize objects as points in a low-dimensional metric map. As a result, the visualizations are subject to the fundamental limitations of metric spaces. These limitations prevent multidimensional scaling from faithfully representing non-metric similarity data such as word associations or event co-occurrences. In particular, multidimensional scaling cannot faithfully represent intransitive pairwise similarities in a visualization, and it cannot faithfully visualize “central” objects. In this paper, we present an extension of a recently proposed multidimensional scaling technique called t-SNE. The extension aims to address the problems of traditional multidimensional scaling techniques when these techniques are used to visualize non-metric similarities. The new technique, called multiple maps t-SNE, alleviates these problems by constructing a collection of maps that reveal\\xa0…', '저널': 'Machine Learning', '저자': 'Laurens van der Maaten, Geoffrey Hinton', '전체 인용횟수': '300회 인용201220132014201520162017201820192020202120222023347171720264835493339', '페이지': '1-23', '학술 문서': 'Visualizing non-metric similarities in multiple mapsL Van der Maaten, G Hinton\\xa0- Machine learning, 2012300회 인용 관련 학술자료 전체 26개의 버전 '}, title='Visualizing non-metric similarities in multiple maps', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Nas-fpn: Learning scalable feature pyramid architecture for object detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': 'Current state-of-the-art convolutional architectures for object detection are manually designed. Here we aim to learn a better architecture of feature pyramid network for object detection. We adopt Neural Architecture Search and discover a new feature pyramid architecture in a novel scalable search space covering all cross-scale connections. The discovered architecture, named NAS-FPN, consists of a combination of top-down and bottom-up connections to fuse features across scales. NAS-FPN, combined with various backbone models in the RetinaNet framework, achieves better accuracy and latency tradeoff compared to state-of-the-art object detection models. NAS-FPN improves mobile detection accuracy by 2 AP compared to state-of-the-art SSDLite with MobileNetV2 model in [32] and achieves 48.3 AP which surpasses Mask R-CNN [10] detection accuracy with less computation time.', '저자': 'Golnaz Ghiasi, Tsung-Yi Lin, Quoc V Le', '전체 인용횟수': '1454회 인용2019202020212022202333195424405384', '컨퍼런스': 'Proceedings of the IEEE/CVF conference on computer vision and pattern recognition', '페이지': '7036-7045', '학술 문서': 'Nas-fpn: Learning scalable feature pyramid architecture for object detectionG Ghiasi, TY Lin, QV Le\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 20191454회 인용 관련 학술자료 전체 8개의 버전 '}, title='Nas-fpn: Learning scalable feature pyramid architecture for object detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning to refine object segments': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '게시자': 'Springer International Publishing', '설명': ' Object segmentation requires both object-level information and low-level pixel data. This presents a challenge for feedforward networks: lower layers in convolutional nets capture rich spatial information, while upper layers encode object-level knowledge but are invariant to factors such as pose and appearance. In this work we propose to augment feedforward nets for object segmentation with a novel top-down refinement approach. The resulting bottom-up/top-down architecture is capable of efficiently generating high-fidelity object masks. Similarly to skip connections, our approach leverages features at all layers of the net. Unlike skip connections, our approach does not attempt to output independent predictions at each layer. Instead, we first output a coarse ‘mask encoding’ in a feedforward pass, then refine this mask encoding in a top-down pass utilizing features at successively lower layers. The\\xa0…', '저자': 'Pedro O Pinheiro, Tsung-Yi Lin, Ronan Collobert, Piotr Dollár', '전체 인용횟수': '1053회 인용20152016201720182019202020212022202361913016518317215412972', '컨퍼런스': 'Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part I 14', '페이지': '75-91', '학술 문서': 'Learning to refine object segmentsPO Pinheiro, TY Lin, R Collobert, P Dollár\\xa0- Computer Vision–ECCV 2016: 14th European\\xa0…, 20161053회 인용 관련 학술자료 전체 13개의 버전 '}, title='Learning to refine object segments', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Dropblock: A regularization method for convolutional networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018', '권': '31', '설명': 'Deep neural networks often work well when they are over-parameterized and trained with a massive amount of noise and regularization, such as weight decay and dropout. Although dropout is widely used as a regularization technique for fully connected layers, it is often less effective for convolutional layers. This lack of success of dropout for convolutional layers is perhaps due to the fact that activation units in convolutional layers are spatially correlated so information can still flow through convolutional networks despite dropout. Thus a structured form of dropout is needed to regularize convolutional networks. In this paper, we introduce DropBlock, a form of structured dropout, where units in a contiguous region of a feature map are dropped together. We found that applying DropbBlock in skip connections in addition to the convolution layers increases the accuracy. Also, gradually increasing number of dropped units during training leads to better accuracy and more robust to hyperparameter choices. Extensive experiments show that DropBlock works better than dropout in regularizing convolutional networks. On ImageNet classification, ResNet-50 architecture with DropBlock achieves  accuracy, which is more than  improvement on the baseline. On COCO detection, DropBlock improves Average Precision of RetinaNet from  to .', '저널': 'Advances in neural information processing systems', '저자': 'Golnaz Ghiasi, Tsung-Yi Lin, Quoc V Le', '전체 인용횟수': '1010회 인용2019202020212022202380157280264218', '학술 문서': 'Dropblock: A regularization method for convolutional networksG Ghiasi, TY Lin, QV Le\\xa0- Advances in neural information processing systems, 20181010회 인용 관련 학술자료 전체 6개의 버전 '}, title='Dropblock: A regularization method for convolutional networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Bottleneck transformers for visual recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021', '설명': 'We present BoTNet, a conceptually simple yet powerful backbone architecture that incorporates self-attention for multiple computer vision tasks including image classification, object detection and instance segmentation. By just replacing the spatial convolutions with global self-attention in the final three bottleneck blocks of a ResNet and no other changes, our approach improves upon the baselines significantly on instance segmentation and object detection while also reducing the parameters, with minimal overhead in latency. Through the design of BoTNet, we also point out how ResNet bottleneck blocks with self-attention can be viewed as Transformer blocks. Without any bells and whistles, BoTNet achieves 44.4% Mask AP and 49.7% Box AP on the COCO Instance Segmentation benchmark using the Mask R-CNN framework; surpassing the previous best published single model and single scale results of ResNeSt evaluated on the COCO validation set. Finally, we present a simple adaptation of the BoTNet design for image classification, resulting in models that achieve a strong performance of 84.7% top-1 accuracy on the ImageNet benchmark while being up to 1.64 x faster in compute time than the popular EfficientNet models on TPU-v3 hardware. We hope our simple and effective approach will serve as a strong baseline for future research in self-attention models for vision.', '저자': 'Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, Ashish Vaswani', '전체 인용횟수': '845회 인용202120222023134341367', '컨퍼런스': 'Proceedings of the IEEE/CVF conference on computer vision and pattern recognition', '페이지': '16519-16529', '학술 문서': 'Bottleneck transformers for visual recognitionA Srinivas, TY Lin, N Parmar, J Shlens, P Abbeel…\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 2021845회 인용 관련 학술자료 전체 6개의 버전 '}, title='Bottleneck transformers for visual recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Simple copy-paste is a strong data augmentation method for instance segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021', '설명': 'Building instance segmentation models that are data-efficient and can handle rare object categories is an important challenge in computer vision. Leveraging data augmentations is a promising direction towards addressing this challenge. Here, we perform a systematic study of the Copy-Paste augmentation (eg,[13, 12]) for instance segmentation where we randomly paste objects onto an image. Prior studies on Copy-Paste relied on modeling the surrounding visual context for pasting the objects. However, we find that the simple mechanism of pasting objects randomly is good enough and can provide solid gains on top of strong baselines. Furthermore, we show Copy-Paste is additive with semi-supervised methods that leverage extra data through pseudo labeling (eg. self-training). On COCO instance segmentation, we achieve 49.1 mask AP and 57.3 box AP, an improvement of+ 0.6 mask AP and+ 1.5 box AP over the previous state-of-the-art. We further demonstrate that Copy-Paste can lead to significant improvements on the LVIS benchmark. Our baseline model outperforms the LVIS 2020 Challenge winning entry by+ 3.6 mask AP on rare categories.', '저자': 'Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V Le, Barret Zoph', '전체 인용횟수': '729회 인용202120222023101275350', '컨퍼런스': 'Proceedings of the IEEE/CVF conference on computer vision and pattern recognition', '페이지': '2918-2928', '학술 문서': 'Simple copy-paste is a strong data augmentation method for instance segmentationG Ghiasi, Y Cui, A Srinivas, R Qian, TY Lin, ED Cubuk…\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 2021729회 인용 관련 학술자료 전체 6개의 버전 '}, title='Simple copy-paste is a strong data augmentation method for instance segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Rethinking pre-training and self-training': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020', '권': '33', '설명': 'Pre-training is a dominant paradigm in computer vision. For example, supervised ImageNet pre-training is commonly used to initialize the backbones of object detection and segmentation models. He et al., however, show a striking result that ImageNet pre-training has limited impact on COCO object detection. Here we investigate self-training as another method to utilize additional data on the same setup and contrast it against ImageNet pre-training. Our study reveals the generality and flexibility of self-training with three additional insights: 1) stronger data augmentation and more labeled data further diminish the value of pre-training, 2) unlike pre-training, self-training is always helpful when using stronger data augmentation, in both low-data and high-data regimes, and 3) in the case that pre-training is helpful, self-training improves upon pre-training. For example, on the COCO object detection dataset, pre-training benefits when we use one fifth of the labeled data, and hurts accuracy when we use all labeled data. Self-training, on the other hand, shows positive improvements from+ 1.3 to+ 3.4 AP across all dataset sizes. In other words, self-training works well exactly on the same setup that pre-training does not work (using ImageNet to help COCO). On the PASCAL segmentation dataset, which is a much smaller dataset than COCO, though pre-training does help significantly, self-training improves upon the pre-trained model. On COCO object detection, we achieve 53.8 AP, an improvement of+ 1.7 AP over the strongest SpineNet model. On PASCAL segmentation, we achieve 90.5 mIOU, an improvement of+ 1.5 mIOU over the previous state-of-the\\xa0…', '저널': 'Advances in neural information processing systems', '저자': 'Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin Dogus Cubuk, Quoc Le', '전체 인용횟수': '563회 인용202020212022202327147209179', '페이지': '3833-3845', '학술 문서': 'Rethinking pre-training and self-trainingB Zoph, G Ghiasi, TY Lin, Y Cui, H Liu, ED Cubuk, Q Le\\xa0- Advances in neural information processing systems, 2020563회 인용 관련 학술자료 전체 8개의 버전 '}, title='Rethinking pre-training and self-training', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning data augmentation strategies for object detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2020', '게시자': 'Springer International Publishing', '설명': ' Much research on object detection focuses on building better model architectures and detection algorithms. Changing the model architecture, however, comes at the cost of adding more complexity to inference, making models slower. Data augmentation, on the other hand, doesn’t add any inference complexity, but is insufficiently studied in object detection for two reasons. First it is more difficult to design plausible augmentation strategies for object detection than for classification, because one must handle the complexity of bounding boxes if geometric transformations are applied. Secondly, data augmentation attracts less research attention perhaps because it is believed to add less value and to transfer poorly compared to advances in network architectures. This paper serves two main purposes. First, we propose to use AutoAugment\\xa0[3] to design better data augmentation strategies for object\\xa0…', '저자': 'Barret Zoph, Ekin D Cubuk, Golnaz Ghiasi, Tsung-Yi Lin, Jonathon Shlens, Quoc V Le', '전체 인용횟수': '531회 인용201920202021202220231887132150143', '컨퍼런스': 'Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVII 16', '페이지': '566-583', '학술 문서': 'Learning data augmentation strategies for object detectionB Zoph, ED Cubuk, G Ghiasi, TY Lin, J Shlens, QV Le\\xa0- Computer Vision–ECCV 2020: 16th European\\xa0…, 2020531회 인용 관련 학술자료 전체 4개의 버전 '}, title='Learning data augmentation strategies for object detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Collaborative metric learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/4/3', '도서': 'Proceedings of the 26th international conference on world wide web', '설명': \"Metric learning algorithms produce distance metrics that capture the important relationships among data. In this work, we study the connection between metric learning and collaborative filtering. We propose Collaborative Metric Learning (CML) which learns a joint metric space to encode not only users' preferences but also the user-user and item-item similarity. The proposed algorithm outperforms state-of-the-art collaborative filtering algorithms on a wide range of recommendation tasks and uncovers the underlying spectrum of users' fine-grained preferences. CML also achieves significant speedup for Top-K recommendation tasks using off-the-shelf, approximate nearest-neighbor search, with negligible accuracy reduction.\", '저자': 'Cheng-Kang Hsieh, Longqi Yang, Yin Cui, Tsung-Yi Lin, Serge Belongie, Deborah Estrin', '전체 인용횟수': '518회 인용2017201820192020202120222023944879010010975', '페이지': '193-201', '학술 문서': 'Collaborative metric learningCK Hsieh, L Yang, Y Cui, TY Lin, S Belongie, D Estrin\\xa0- Proceedings of the 26th international conference on\\xa0…, 2017518회 인용 관련 학술자료 전체 15개의 버전 '}, title='Collaborative metric learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Open-vocabulary Object Detection via Vision and Language Knowledge Distillation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021/4/28', '설명': 'We aim at advancing open-vocabulary object detection, which detects objects described by arbitrary text inputs. The fundamental challenge is the availability of training data. It is costly to further scale up the number of classes contained in existing object detection datasets. To overcome this challenge, we propose ViLD, a training method via Vision and Language knowledge Distillation. Our method distills the knowledge from a pretrained open-vocabulary image classification model (teacher) into a two-stage detector (student). Specifically, we use the teacher model to encode category texts and image regions of object proposals. Then we train a student detector, whose region embeddings of detected boxes are aligned with the text and image embeddings inferred by the teacher. We benchmark on LVIS by holding out all rare categories as novel categories that are not seen during training. ViLD obtains 16.1 mask AP with a ResNet-50 backbone, even outperforming the supervised counterpart by 3.8. When trained with a stronger teacher model ALIGN, ViLD achieves 26.3 AP. The model can directly transfer to other datasets without finetuning, achieving 72.2 AP on PASCAL VOC, 36.6 AP on COCO and 11.8 AP on Objects365. On COCO, ViLD outperforms the previous state-of-the-art by 4.8 on novel AP and 11.4 on overall AP. Code and demo are open-sourced at https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild.', '저널': 'arXiv preprint arXiv:2104.13921', '저자': 'Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, Yin Cui', '전체 인용횟수': '445회 인용2021202220236108331', '학술 문서': 'Open-vocabulary object detection via vision and language knowledge distillationX Gu, TY Lin, W Kuo, Y Cui\\xa0- arXiv preprint arXiv:2104.13921, 2021400회 인용 관련 학술자료 전체 5개의 버전 Zero-shot detection via vision and language knowledge distillation*X Gu, TY Lin, W Kuo, Y Cui\\xa0- arXiv preprint arXiv:2104.13921, 202148회 인용 관련 학술자료 '}, title='Open-vocabulary Object Detection via Vision and Language Knowledge Distillation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning Deep Representations for Ground-to-Aerial Geolocalization': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'The recent availability of geo-tagged images and rich geospatial data has inspired a number of algorithms for image based geolocalization. Most approaches predict the location of a query image by matching to ground-level images with known locations (eg, street-view data). However, most of the Earth does not have ground-level reference photos available. Fortunately, more complete coverage is provided by oblique aerial or\" bird\\'s eye\" imagery. In this work, we localize a ground-level query image by matching it to a reference database of aerial imagery. We use publicly available data to build a dataset of 78K aligned cross-view image pairs. The primary challenge for this task is that traditional computer vision approaches cannot handle the wide baseline and appearance variation of these cross-view pairs. We use our dataset to learn a feature representation in which matching views are near one another and mismatched views are far apart. Our proposed approach, Where-CNN, is inspired by deep learning success in face verification and achieves significant improvements over traditional hand-crafted features and existing deep features learned from other large-scale databases. We show the effectiveness of Where-CNN in finding matches between street view and aerial view imagery and demonstrate the ability of our learned features to generalize to novel locations.', '저자': 'Tsung-Yi Lin, Yin Cui, Serge Belongie, James Hays', '전체 인용횟수': '382회 인용20142015201620172018201920202021202220232113544514739395250', '컨퍼런스': 'Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition', '페이지': '5007-5015', '학술 문서': 'Learning deep representations for ground-to-aerial geolocalizationTY Lin, Y Cui, S Belongie, J Hays\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2015382회 인용 관련 학술자료 전체 14개의 버전 '}, title='Learning Deep Representations for Ground-to-Aerial Geolocalization', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A multipath network for object detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/4/7', '권': '2016', '설명': 'The recent COCO object detection dataset presents several new challenges for object detection. In particular, it contains objects at a broad range of scales, less prototypical images, and requires more precise localization. To address these challenges, we test three modifications to the standard Fast R-CNN object detector: (1) skip connections that give the detector access to features at multiple network layers, (2) a foveal structure to exploit object context at multiple object resolutions, and (3) an integral loss function and corresponding network adjustment that improve localization. The result of these modifications is that information can flow along multiple paths in our network, including through features from multiple network layers and from multiple object views. We refer to our modified classifier as a \"MultiPath\" network. We couple our MultiPath network with DeepMask object proposals, which are well suited for localization and small objects, and adapt our pipeline to predict segmentation masks in addition to bounding boxes. The combined system improves results over the baseline Fast R-CNN detector with Selective Search by 66% overall and by 4x on small objects. It placed second in both the COCO 2015 detection and segmentation challenges.', '저널': 'British Machine Vision Conference', '저자': 'Sergey Zagoruyko, Adam Lerer, Tsung-Yi Lin, Pedro O Pinheiro, Sam Gross, Soumith Chintala, Piotr Dollár', '전체 인용횟수': '281회 인용2015201620172018201920202021202220231950543739392318', '학술 문서': 'A multipath network for object detectionS Zagoruyko, A Lerer, TY Lin, PO Pinheiro, S Gross…\\xa0- arXiv preprint arXiv:1604.02135, 2016281회 인용 관련 학술자료 전체 8개의 버전 '}, title='A multipath network for object detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Revisiting ResNets: Improved Training and Scaling Strategies': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={}, title='Revisiting ResNets: Improved Training and Scaling Strategies', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Cross-view image geolocalization': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={}, title='Cross-view image geolocalization', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Magic3d: High-resolution text-to-3d content creation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={}, title='Magic3d: High-resolution text-to-3d content creation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'inerf: Inverting neural radiance fields for pose estimation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={}, title='inerf: Inverting neural radiance fields for pose estimation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2018/4/1', '게시자': 'IEEE', '권': '40', '설명': \"In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or `atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third\\xa0…\", '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L Yuille', '전체 인용횟수': '17722회 인용2017201820192020202120222023393129322452747340438513617', '페이지': '834-848', '학술 문서': 'Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfsLC Chen, G Papandreou, I Kokkinos, K Murphy…\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 201717722회 인용 관련 학술자료 전체 10개의 버전 ', '호': '4'}, title='Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Feature extraction from faces using deformable templates': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1992/8', '게시자': 'Kluwer Academic Publishers', '권': '8', '설명': ' We propose a method for detecting and describing features of faces using deformable templates. The feature of interest, an eye for example, is described by a parameterized template. An energy function is defined which links edges, peaks, and valleys in the image intensity to corresponding properties of the template. The template then interacts dynamically with the image by altering its parameter values to minimize the energy function, thereby deforming itself to find the best fit. The final parametr values can be used as descriptors for the feature. We illustrate this method by showing deformable templates detecting eyes and mouths in real images. We demonstrate their ability for tracking features.', '저널': 'International journal of computer vision', '저자': 'Alan L Yuille, Peter W Hallinan, David S Cohen', '전체 인용횟수': '3162회 인용1991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220232333286296101110100104162119125148129180127141136137111116136106110986966623928363617', '페이지': '99-111', '학술 문서': 'Feature extraction from faces using deformable templatesAL Yuille, PW Hallinan, DS Cohen\\xa0- International journal of computer vision, 19923162회 인용 관련 학술자료 전체 13개의 버전 '}, title='Feature extraction from faces using deformable templates', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Region competition: Unifying snakes, region growing, and Bayes/MDL for multiband image segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1996/9', '게시자': 'IEEE', '권': '18', '설명': 'We present a novel statistical and variational approach to image segmentation based on a new algorithm, named region competition. This algorithm is derived by minimizing a generalized Bayes/minimum description length (MDL) criterion using the variational principle. The algorithm is guaranteed to converge to a local minimum and combines aspects of snakes/balloons and region growing. The classic snakes/balloons and region growing algorithms can be directly derived from our approach. We provide theoretical analysis of region competition including accuracy of boundary location, criteria for initial conditions, and the relationship to edge detection using filters. It is straightforward to generalize the algorithm to multiband segmentation and we demonstrate it on gray level images, color images and texture images. The novel color model allows us to eliminate intensity gradients and shadows, thereby obtaining\\xa0…', '저널': 'IEEE transactions on pattern analysis and machine intelligence', '저자': 'Song Chun Zhu, Alan Yuille', '전체 인용횟수': '3136회 인용199619971998199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202325455569869812414114018119717817917217216015316313413310784906741423514', '페이지': '884-900', '학술 문서': 'Region competition: Unifying snakes, region growing, and Bayes/MDL for multiband image segmentationSC Zhu, A Yuille\\xa0- IEEE transactions on pattern analysis and machine\\xa0…, 19963136회 인용 관련 학술자료 전체 17개의 버전 ', '호': '9'}, title='Region competition: Unifying snakes, region growing, and Bayes/MDL for multiband image segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Transunet: Transformers make strong encoders for medical image segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2021/2/8', '설명': 'Medical image segmentation is an essential prerequisite for developing healthcare systems, especially for disease diagnosis and treatment planning. On various medical image segmentation tasks, the u-shaped architecture, also known as U-Net, has become the de-facto standard and achieved tremendous success. However, due to the intrinsic locality of convolution operations, U-Net generally demonstrates limitations in explicitly modeling long-range dependency. Transformers, designed for sequence-to-sequence prediction, have emerged as alternative architectures with innate global self-attention mechanisms, but can result in limited localization abilities due to insufficient low-level details. In this paper, we propose TransUNet, which merits both Transformers and U-Net, as a strong alternative for medical image segmentation. On one hand, the Transformer encodes tokenized image patches from a convolution neural network (CNN) feature map as the input sequence for extracting global contexts. On the other hand, the decoder upsamples the encoded features which are then combined with the high-resolution CNN feature maps to enable precise localization. We argue that Transformers can serve as strong encoders for medical image segmentation tasks, with the combination of U-Net to enhance finer details by recovering localized spatial information. TransUNet achieves superior performances to various competing methods on different medical applications including multi-organ segmentation and cardiac segmentation. Code and models are available at https://github.com/Beckschen/TransUNet.', '저널': 'arXiv preprint arXiv:2102.04306', '저자': 'Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille, Yuyin Zhou', '전체 인용횟수': '2262회 인용2021202220231347171378', '학술 문서': 'Transunet: Transformers make strong encoders for medical image segmentationJ Chen, Y Lu, Q Yu, X Luo, E Adeli, Y Wang, L Lu…\\xa0- arXiv preprint arXiv:2102.04306, 20212262회 인용 관련 학술자료 전체 4개의 버전 '}, title='Transunet: Transformers make strong encoders for medical image segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Object perception as Bayesian inference': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2004/2/4', '게시자': 'Annual Reviews', '권': '55', '설명': 'We perceive the shapes and material properties of objects quickly and reliably despite the complexity and objective ambiguities of natural images. Typical images are highly complex because they consist of many objects embedded in background clutter. Moreover, the image features of an object are extremely variable and ambiguous owing to the effects of projection, occlusion, background clutter, and illumination. The very success of everyday vision implies neural mechanisms, yet to be understood, that discount irrelevant information and organize ambiguous or noisy local image features into objects and surfaces. Recent work in Bayesian theories of visual perception has shown how complexity may be managed and ambiguity resolved through the task-dependent, probabilistic integration of prior object knowledge with image features.', '저자': 'Daniel Kersten, Pascal Mamassian, Alan Yuille', '전체 인용횟수': '1545회 인용200420052006200720082009201020112012201320142015201620172018201920202021202220231332375162527564908285831031251149691918396', '출처': 'Annu. Rev. Psychol.', '페이지': '271-304', '학술 문서': 'Object perception as Bayesian inferenceD Kersten, P Mamassian, A Yuille\\xa0- Annu. Rev. Psychol., 20041545회 인용 관련 학술자료 전체 33개의 버전 '}, title='Object perception as Bayesian inference', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Attention to scale: Scale-aware semantic image segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': 'Incorporating multi-scale features in fully convolutional neural networks (FCNs) has been a key element to achieving state-of-the-art performance on semantic image segmentation. One common way to extract multi-scale features is to feed multiple resized input images to a shared deep network and then merge the resulting features for pixel-wise classification. In this work, we propose an attention mechanism that learns to softly weight the multi-scale features at each pixel location. We adapt a state-of-the-art semantic image segmentation model, which we jointly train with multi-scale input images and the attention model. The proposed attention model not only outperforms average-and max-pooling, but allows us to diagnostically visualize the importance of features at different positions and scales. Moreover, we show that adding extra supervision to the output at each scale is essential to achieving excellent performance when merging multi-scale features. We demonstrate the effectiveness of our model with extensive experiments on three challenging datasets, including PASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO 2014.', '저자': 'Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu, Alan L Yuille', '전체 인용횟수': '1538회 인용201620172018201920202021202220232984182239263275249193', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '3640-3649', '학술 문서': 'Attention to scale: Scale-aware semantic image segmentationLC Chen, Y Yang, J Wang, W Xu, AL Yuille\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20161538회 인용 관련 학술자료 전체 14개의 버전 '}, title='Attention to scale: Scale-aware semantic image segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep captioning with multimodal recurrent neural networks (m-rnn)': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/12/20', '설명': 'In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval. The project page of this work is: www.stat.ucla.edu/~junhua.mao/m-RNN.html .', '저널': 'arXiv preprint arXiv:1412.6632', '저자': 'Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, Alan Yuille', '전체 인용횟수': '1460회 인용20152016201720182019202020212022202355123174207230170203154110', '학술 문서': 'Deep captioning with multimodal recurrent neural networks (m-rnn)J Mao, W Xu, Y Yang, J Wang, Z Huang, A Yuille\\xa0- arXiv preprint arXiv:1412.6632, 20141460회 인용 관련 학술자료 전체 16개의 버전 '}, title='Deep captioning with multimodal recurrent neural networks (m-rnn)', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015', '설명': 'Deep convolutional neural networks (DCNNs) trained on a large number of images with strong pixel-level annotations have recently significantly pushed the state-of-art in semantic image segmentation. We study the more challenging problem of learning DCNNs for semantic image segmentation from either (1) weakly annotated training data such as bounding boxes or image-level labels or (2) a combination of few strongly labeled and many weakly labeled images, sourced from one or multiple datasets. We develop Expectation-Maximization (EM) methods for semantic image segmentation model training under these weakly supervised and semi-supervised settings. Extensive experimental evaluation shows that the proposed techniques can learn models delivering competitive results on the challenging PASCAL VOC 2012 image segmentation benchmark, while requiring significantly less annotation effort. We share source code implementing the proposed system at https://bitbucket. org/deeplab/deeplab-public.', '저자': 'George Papandreou, Liang-Chieh Chen, Kevin P Murphy, Alan L Yuille', '전체 인용횟수': '1456회 인용20152016201720182019202020212022202325106132157211245208200139', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '1742-1750', '학술 문서': 'Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentationG Papandreou, LC Chen, KP Murphy, AL Yuille\\xa0- Proceedings of the IEEE international conference on\\xa0…, 20151456회 인용 관련 학술자료 전체 15개의 버전 '}, title='Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The secrets of salient object segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '설명': 'In this paper we provide an extensive evaluation of fixation prediction and salient object segmentation algorithms as well as statistics of major datasets. Our analysis identifies serious design flaws of existing salient object benchmarks, called the dataset design bias, by over emphasising the stereotypical concepts of saliency. The dataset design bias does not only create the discomforting disconnection between fixations and salient object segmentation, but also misleads the algorithm designing. Based on our analysis, we propose a new high quality dataset that offers both fixation and salient object segmentation ground-truth. With fixations and salient object being presented simultaneously, we are able to bridge the gap between fixations and salient objects, and propose a novel method for salient object segmentation. Finally, we report significant benchmark progress on 3 existing datasets of segmenting salient objects.', '저자': 'Yin Li, Xiaodi Hou, Christof Koch, James M Rehg, Alan L Yuille', '전체 인용횟수': '1451회 인용201420152016201720182019202020212022202355292129158177236219194160', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '280-287', '학술 문서': 'The secrets of salient object segmentationY Li, X Hou, C Koch, JM Rehg, AL Yuille\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20141451회 인용 관련 학술자료 전체 29개의 버전 '}, title='The secrets of salient object segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The role of context for object detection and semantic segmentation in the wild': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '설명': 'In this paper we study the role of context in existing state-of-the-art detection and segmentation approaches. Towards this goal, we label every pixel of PASCAL VOC 2010 detection challenge with a semantic category. We believe this data will provide plenty of challenges to the community, as it contains 520 additional classes for semantic segmentation and object detection. Our analysis shows that nearest neighbor based approaches perform poorly on semantic segmentation of contextual classes, showing the variability of PASCAL imagery. Furthermore, improvements of exist ing contextual models for detection is rather modest. In order to push forward the performance in this difficult scenario, we propose a novel deformable part-based model, which exploits both local context around each candidate detection as well as global context at the level of the scene. We show that this contextual reasoning significantly helps in detecting objects at all scales.', '저자': 'Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, Alan Yuille', '전체 인용횟수': '1447회 인용201420152016201720182019202020212022202355484122132154185240208241', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '891-898', '학술 문서': 'The role of context for object detection and semantic segmentation in the wildR Mottaghi, X Chen, X Liu, NG Cho, SW Lee, S Fidler…\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20141447회 인용 관련 학술자료 전체 30개의 버전 '}, title='The role of context for object detection and semantic segmentation in the wild', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The concave-convex procedure': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2003/4/1', '게시자': 'MIT Press', '권': '15', '설명': \"The concave-convex procedure (CCCP) is a way to construct discrete-time iterative dynamical systems that are guaranteed to decrease global optimization and energy functions monotonically. This procedure can be applied to almost any optimization problem, and many existing algorithms can be interpreted in terms of it. In particular, we prove that all expectation-maximization algorithms and classes of Legendre minimization and variational bounding algorithms can be reexpressed in terms of CCCP. We show that many existing neural network and mean-field theory algorithms are also examples of CCCP. The generalized iterative scaling algorithm and Sinkhorn's algorithm can also be expressed as CCCP by changing variables. CCCP can be used both as a new way to understand, and prove the convergence of, existing optimization algorithms and as a procedure for generating new algorithms.\", '저널': 'Neural computation', '저자': 'Alan L Yuille, Anand Rangarajan', '전체 인용횟수': '1363회 인용200420052006200720082009201020112012201320142015201620172018201920202021202220235877233137629110810810611810510494917810660', '페이지': '915-936', '학술 문서': 'The concave-convex procedureAL Yuille, A Rangarajan\\xa0- Neural computation, 20031363회 인용 관련 학술자료 전체 9개의 버전 ', '호': '4'}, title='The concave-convex procedure', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019', '설명': 'Recently, Neural Architecture Search (NAS) has successfully identified neural network architectures that exceed human designed ones on large-scale image classification. In this paper, we study NAS for semantic image segmentation. Existing works often focus on searching the repeatable cell structure, while hand-designing the outer network structure that controls the spatial resolution changes. This choice simplifies the search space, but becomes increasingly problematic for dense image prediction which exhibits a lot more network level architectural variations. Therefore, we propose to search the network level structure in addition to the cell level structure, which forms a hierarchical architecture search space. We present a network level search space that includes many popular designs, and develop a formulation that allows efficient gradient-based architecture search (3 P100 GPU days on Cityscapes images). We demonstrate the effectiveness of the proposed method on the challenging Cityscapes, PASCAL VOC 2012, and ADE20K datasets. Auto-DeepLab, our architecture searched specifically for semantic image segmentation, attains state-of-the-art performance without any ImageNet pretraining.', '저자': 'Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua, Alan L Yuille, Li Fei-Fei', '전체 인용횟수': '1078회 인용2019202020212022202378253290266182', '컨퍼런스': 'Proceedings of the IEEE/CVF conference on computer vision and pattern recognition', '페이지': '82-92', '학술 문서': 'Auto-deeplab: Hierarchical neural architecture search for semantic image segmentationC Liu, LC Chen, F Schroff, H Adam, W Hua, AL Yuille…\\xa0- Proceedings of the IEEE/CVF conference on computer\\xa0…, 20191078회 인용 관련 학술자료 전체 12개의 버전 '}, title='Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Mitigating adversarial effects through randomization': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017/11/6', '설명': 'Convolutional neural networks have demonstrated high accuracy on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. For example, imperceptible perturbations added to clean images can cause convolutional neural networks to fail. In this paper, we propose to utilize randomization at inference time to mitigate adversarial effects. Specifically, we use two randomization operations: random resizing, which resizes the input images to a random size, and random padding, which pads zeros around the input images in a random manner. Extensive experiments demonstrate that the proposed randomization method is very effective at defending against both single-step and iterative attacks. Our method provides the following advantages: 1) no additional training or fine-tuning, 2) very few additional computations, 3) compatible with other adversarial defense methods. By combining the proposed randomization method with an adversarially trained model, it achieves a normalized score of 0.924 (ranked No.2 among 107 defense teams) in the NIPS 2017 adversarial examples defense challenge, which is far better than using adversarial training alone with a normalized score of 0.773 (ranked No.56). The code is public available at https://github.com/cihangxie/NIPS2017_adv_challenge_defense.', '저널': 'arXiv preprint arXiv:1711.01991', '저자': 'Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, Alan Yuille', '전체 인용횟수': '1050회 인용2017201820192020202120222023364138187218232204', '학술 문서': 'Mitigating adversarial effects through randomizationC Xie, J Wang, Z Zhang, Z Ren, A Yuille\\xa0- arXiv preprint arXiv:1711.01991, 20171050회 인용 관련 학술자료 전체 7개의 버전 '}, title='Mitigating adversarial effects through randomization', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Vision as Bayesian inference: analysis by synthesis?': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2006/7/1', '게시자': 'Elsevier', '권': '10', '설명': \"We argue that the study of human vision should be aimed at determining how humans perform natural tasks with natural images. Attempts to understand the phenomenology of vision from artificial stimuli, although worthwhile as a starting point, can lead to faulty generalizations about visual systems, because of the enormous complexity of natural images. Dealing with this complexity is daunting, but Bayesian inference on structured probability distributions offers the ability to design theories of vision that can deal with the complexity of natural images, and that use ‘analysis by synthesis' strategies with intriguing similarities to the brain. We examine these strategies using recent examples from computer vision, and outline some important imlications for cognitive science.\", '저자': 'Alan Yuille, Daniel Kersten', '전체 인용횟수': '1003회 인용20062007200820092010201120122013201420152016201720182019202020212022202382240714146508544495674755458716880', '출처': 'Trends in cognitive sciences', '페이지': '301-308', '학술 문서': 'Vision as Bayesian inference: analysis by synthesis?A Yuille, D Kersten\\xa0- Trends in cognitive sciences, 20061003회 인용 관련 학술자료 전체 28개의 버전 ', '호': '7'}, title='Vision as Bayesian inference: analysis by synthesis?', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Adversarial examples for semantic segmentation and object detection': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'It has been well demonstrated that adversarial examples, ie, natural images with visually imperceptible perturbations added, cause deep networks to fail on image classification. In this paper, we extend adversarial examples to semantic segmentation and object detection which are much more difficult. Our observation is that both segmentation and detection are based on classifying multiple targets on an image (eg, the target is a pixel or a receptive field in segmentation, and an object proposal in detection). This inspires us to optimize a loss function over a set of targets for generating adversarial perturbations. Based on this, we propose a novel algorithm named Dense Adversary Generation (DAG), which applies to the state-of-the-art networks for segmentation and detection. We find that the adversarial perturbations can be transferred across networks with different training data, based on different architectures, and even for different recognition tasks. In particular, the transfer ability across networks with the same architecture is more significant than in other cases. Besides, we show that summing up heterogeneous perturbations often leads to better transfer performance, which provides an effective method of black-box adversarial attack.', '저자': 'Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, Alan Yuille', '전체 인용횟수': '977회 인용20172018201920202021202220231573123199175196194', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '1369-1378', '학술 문서': 'Adversarial examples for semantic segmentation and object detectionC Xie, J Wang, Z Zhang, Y Zhou, L Xie, A Yuille\\xa0- Proceedings of the IEEE international conference on\\xa0…, 2017977회 인용 관련 학술자료 전체 13개의 버전 '}, title='Adversarial examples for semantic segmentation and object detection', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Scaling theorems for zero crossings': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1986/1', '게시자': 'IEEE', '설명': 'We characterize some properties of the zero crossings of the Laplacian of signals-in particular images-filtered with linear filters, as a function of the scale of the filter (extending recent work by Witkin [16]). We prove that in any dimension the only filter that does not create generic zero crossings as the scale increases is the Gaussian. This result can be generalized to apply to level crossings of any linear differential operator: it applies in particular to ridges and ravines in the image intensity. In the case of the second derivative along the gradient, there is no filter that avoids creation of zero crossings, unless the filtering is performed after the derivative is applied.', '저널': 'IEEE Transactions on Pattern Analysis and Machine Intelligence', '저자': 'Alan L Yuille, Tomaso A Poggio', '전체 인용횟수': '977회 인용198519861987198819891990199119921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023101523443534384637604440363428192619203026183523242726222514231411968449', '페이지': '15-25', '학술 문서': 'Scaling theorems for zero crossingsAL Yuille, TA Poggio\\xa0- IEEE Transactions on Pattern Analysis and Machine\\xa0…, 1986977회 인용 관련 학술자료 전체 23개의 버전 ', '호': '1'}, title='Scaling theorems for zero crossings', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Detecting and reading text in natural scenes': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2004/6/27', '게시자': 'IEEE', '권': '2', '설명': 'This paper gives an algorithm for detecting and reading text in natural images. The algorithm is intended for use by blind and visually impaired subjects walking through city scenes. We first obtain a dataset of city images taken by blind and normally sighted subjects. From this dataset, we manually label and extract the text regions. Next we perform statistical analysis of the text regions to determine which image features are reliable indicators of text and have low entropy (i.e. feature response is similar for all text images). We obtain weak classifiers by using joint probabilities for feature responses on and off text. These weak classifiers are used as input to an AdaBoost machine learning algorithm to train a strong classifier. In practice, we trained a cascade with 4 strong classifiers containing 79 features. An adaptive binarization and extension algorithm is applied to those regions selected by the cascade classifier\\xa0…', '저자': 'Xiangrong Chen, Alan L Yuille', '전체 인용횟수': '972회 인용20042005200620072008200920102011201220132014201520162017201820192020202120222023521282227313259648493929283714936332711', '컨퍼런스': 'Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.', '페이지': 'II-II', '학술 문서': 'Detecting and reading text in natural scenesX Chen, AL Yuille\\xa0- Proceedings of the 2004 IEEE Computer Society\\xa0…, 2004972회 인용 관련 학술자료 전체 15개의 버전 '}, title='Detecting and reading text in natural scenes', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Generation and comprehension of unambiguous object descriptions': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '설명': 'We propose a method that can generate an unambiguous description (known as a referring expression) of a specific object or region in an image, and which can also comprehend or interpret such an expression to infer which object is being described. We show that our method outperforms previous methods that generate descriptions of objects without taking into account other potentially ambiguous objects in the scene. Our model is inspired by recent successes of deep learning methods for image captioning, but while image captioning is difficult to evaluate, our task allows for easy objective evaluation. We also present a new large-scale dataset for referring expressions, based on MS-COCO. We have released the dataset and a toolbox for visualization and evaluation, see https://github. com/mjhucla/Google_Refexp_toolbox.', '저자': 'Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, Kevin Murphy', '전체 인용횟수': '965회 인용2015201620172018201920202021202220235146089103124125176254', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '11-20', '학술 문서': 'Generation and comprehension of unambiguous object descriptionsJ Mao, J Huang, A Toshev, O Camburu, AL Yuille…\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 2016965회 인용 관련 학술자료 전체 16개의 버전 '}, title='Generation and comprehension of unambiguous object descriptions', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Genetic cnn': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'The deep convolutional neural network (CNN) is the state-of-the-art solution for large-scale visual recognition. Following some basic principles such as increasing network depth and constructing highway connections, researchers have manually designed a lot of fixed network architectures and verified their effectiveness. In this paper, we discuss the possibility of learning deep network structures automatically. Note that the number of possible network structures increases exponentially with the number of layers in the network, which motivates us to adopt the genetic algorithm to efficiently explore this large search space. The core idea is to propose an encoding method to represent each network structure in a fixed-length binary string. The genetic algorithm is initialized by generating a set of randomized individuals. In each generation, we define standard genetic operations, eg, selection, mutation and crossover, to generate competitive individuals and eliminate weak ones. The competitiveness of each individual is defined as its recognition accuracy, which is obtained via a standalone training process on a reference dataset. We run the genetic process on CIFAR10, a small-scale dataset, demonstrating its ability to find high-quality structures which are little studied before. The learned powerful structures are also transferrable to the ILSVRC2012 dataset for large-scale visual recognition.', '저자': 'Lingxi Xie, Alan Yuille', '전체 인용횟수': '951회 인용2017201820192020202120222023870137159217177175', '컨퍼런스': 'Proceedings of the IEEE international conference on computer vision', '페이지': '1379-1388', '학술 문서': 'Genetic cnnL Xie, A Yuille\\xa0- Proceedings of the IEEE international conference on\\xa0…, 2017951회 인용 관련 학술자료 전체 15개의 버전 '}, title='Genetic cnn', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The design and use of steerable filters': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1991/9/9', '권': '13', '설명': 'Oriented ﬁlters are useful in many early vision and image processing tasks. One often needs to apply the same ﬁlter, rotated to different angles under adaptive control, or wishes to calculate the ﬁlter response at various orientations. We present an efﬁcient architecture to synthesize ﬁlters of arbitrary orientations from linear combinations of basis ﬁlters, allowing one to adaptively “steer” a ﬁlter to any orientation, and to determine analytically the ﬁlter output as a function of orientation. Steerable ﬁlters may be designed in quadrature pairs to allow adaptive control over phase as well as orientation. We show how to design and steer the ﬁlters and present examples of their use in several tasks: the analysis of orientation and phase, angularly adaptive ﬁltering, edge detection, and shape from shading. One can also build a self-similar steerable pyramid representation. The same concepts can be generalized to the design of 3\\xa0…', '저널': 'IEEE Transactions on Pattern analysis and machine intelligence', '저자': 'William T Freeman, Edward H Adelson', '전체 인용횟수': '4605회 인용19921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023151653666986103768794971131631631912372042522402492422332232082021801771301101118872', '페이지': '891-906', '학술 문서': 'The design and use of steerable filtersWT Freeman, EH Adelson\\xa0- IEEE Transactions on Pattern analysis and machine\\xa0…, 19914605회 인용 관련 학술자료 전체 23개의 버전 ', '호': '9'}, title='The design and use of steerable filters', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'LabelMe: a database and web-based tool for image annotation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2008/5', '게시자': 'Springer US', '권': '77', '설명': '  We seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web. ', '저널': 'International journal of computer vision', '저자': 'Bryan C Russell, Antonio Torralba, Kevin P Murphy, William T Freeman', '전체 인용횟수': '4255회 인용200720082009201020112012201320142015201620172018201920202021202220234995177210228219276242245273260233271302333397341', '페이지': '157-173', '학술 문서': 'LabelMe: a database and web-based tool for image annotationBC Russell, A Torralba, KP Murphy, WT Freeman\\xa0- International journal of computer vision, 20084255회 인용 관련 학술자료 전체 30개의 버전 '}, title='LabelMe: a database and web-based tool for image annotation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Example-based super-resolution': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2002/8/7', '게시자': 'IEEE', '권': '22', '설명': \"We call methods for achieving high-resolution enlargements of pixel-based images super-resolution algorithms. Many applications in graphics or image processing could benefit from such resolution independence, including image-based rendering (IBR), texture mapping, enlarging consumer photographs, and converting NTSC video content to high-definition television. We built on another training-based super-resolution algorithm and developed a faster and simpler algorithm for one-pass super-resolution. Our algorithm requires only a nearest-neighbor search in the training set for a vector derived from each patch of local image data. This one-pass super-resolution algorithm is a step toward achieving resolution independence in image-based representations. We don't expect perfect resolution independence-even the polygon representation doesn't have that-but increasing the resolution independence of pixel\\xa0…\", '저널': 'IEEE Computer graphics and Applications', '저자': 'William T Freeman, Thouis R Jones, Egon C Pasztor', '전체 인용횟수': '3497회 인용200320042005200620072008200920102011201220132014201520162017201820192020202120222023365143629010113716918024425528630828322724219619614712978', '페이지': '56-65', '학술 문서': 'Example-based super-resolutionWT Freeman, TR Jones, EC Pasztor\\xa0- IEEE Computer graphics and Applications, 20023497회 인용 관련 학술자료 전체 28개의 버전 ', '호': '2'}, title='Example-based super-resolution', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'First M87 event horizon telescope results. IV. Imaging the central supermassive black hole': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2019/4/10', '게시자': 'IOP Publishing', '권': '875', '설명': 'First M87 Event Horizon Telescope Results. IV. Imaging the Central Supermassive Black Hole \\n- IOPscience This site uses cookies. By continuing to use this site you agree to our use of \\ncookies. To find out more, see our Privacy and Cookies policy. Close this notification IOP \\nScience home Skip to content Accessibility Help Search Journals Journals list Browse more \\nthan 100 science journal titles Subject collections Read the very best research published in \\nIOP journals Publishing partners Partner organisations and publications Open access IOP \\nPublishing open access policy guide IOP Conference Series Read open access proceedings \\nfrom science conferences worldwide Books Publishing Support Login IOPscience login / Sign \\nUp Click here to close this panel. Primary search Search all IOPscience content Article Lookup \\nSelect journal (required) Volume number: Issue number (if known): Article or page number: …', '저널': 'The Astrophysical Journal Letters', '저자': 'Kazunori Akiyama, Antxon Alberdi, Walter Alef, Keiichi Asada, Rebecca Azulay, Anne-Kathrin Baczko, David Ball, Mislav Baloković, John Barrett, Dan Bintley, Lindy Blackburn, Wilfred Boland, Katherine L Bouman, Geoffrey C Bower, Michael Bremer, Christiaan D Brinkerink, Roger Brissenden, Silke Britzen, Avery E Broderick, Dominique Broguiere, Thomas Bronzwaer, Do-Young Byun, John E Carlstrom, Andrew Chael, Chi-kwan Chan, Shami Chatterjee, Koushik Chatterjee, Ming-Tang Chen, Yongjun Chen, Ilje Cho, Pierre Christian, John E Conway, James M Cordes, Geoffrey B Crew, Yuzhu Cui, Jordy Davelaar, Mariafelicia De Laurentis, Roger Deane, Jessica Dempsey, Gregory Desvignes, Jason Dexter, Sheperd S Doeleman, Ralph P Eatough, Heino Falcke, Vincent L Fish, Ed Fomalont, Raquel Fraga-Encinas, William T Freeman, Per Friberg, Christian M Fromm, José L Gómez, Peter Galison, Charles F Gammie, Roberto García, Olivier Gentaz, Boris Georgiev, Ciriaco Goddi, Roman Gold, Minfeng Gu, Mark Gurwell, Kazuhiro Hada, Michael H Hecht, Ronald Hesper, Luis C Ho, Paul Ho, Mareki Honma, Chih-Wei L Huang, Lei Huang, David H Hughes, Shiro Ikeda, Makoto Inoue, Sara Issaoun, David J James, Buell T Jannuzi, Michael Janssen, Britton Jeter, Wu Jiang, Michael D Johnson, Svetlana Jorstad, Taehyun Jung, Mansour Karami, Ramesh Karuppusamy, Tomohisa Kawashima, Garrett K Keating, Mark Kettenis, Jae-Young Kim, Junhan Kim, Jongsoo Kim, Motoki Kino, Jun Yi Koay, Patrick M Koch, Shoko Koyama, Michael Kramer, Carsten Kramer, Thomas P Krichbaum, Cheng-Yu Kuo, Tod R Lauer, Sang-Sung Lee, Yan-Rong Li, Zhiyuan Li, Michael Lindqvist, Kuo Liu, Elisabetta Liuzzo, Wen-Ping Lo, Andrei P Lobanov, Laurent Loinard, Colin Lonsdale, Ru-Sen Lu, Nicholas R MacDonald, Jirong Mao, Sera Markoff, Daniel P Marrone, Alan P Marscher, Iván Martí-Vidal, Satoki Matsushita, Lynn D Matthews, Lia Medeiros, Karl M Menten, Yosuke Mizuno, Izumi Mizuno, James M Moran, Kotaro Moriyama, Monika Moscibrodzka, Cornelia Müller, Hiroshi Nagai, Neil M Nagar, Masanori Nakamura, Ramesh Narayan, Gopal Narayanan, Iniyan Natarajan, Roberto Neri, Chunchong Ni, Aristeidis Noutsos, Hiroki Okino, Héctor Olivares, Tomoaki Oyama, Feryal Özel, Daniel CM Palumbo, Nimesh Patel, Ue-Li Pen, Dominic W Pesce, Vincent Piétu, Richard Plambeck, Aleksandar PopStefanija, Oliver Porth, Ben Prather, Jorge A Preciado-López, Dimitrios Psaltis, Hung-Yi Pu, Venkatessh Ramakrishnan', '전체 인용횟수': '3445회 인용20192020202120222023343660754772889', '페이지': 'L4', '학술 문서': 'First M87 event horizon telescope results. IV. Imaging the central supermassive black holeK Akiyama, A Alberdi, W Alef, K Asada, R Azulay…\\xa0- The Astrophysical Journal Letters, 20192764회 인용 관련 학술자료 전체 71개의 버전 First M87 event horizon telescope results. IV. Imaging the central supermassive black holeEvent Horizon Telescope Collaboration\\xa0- arXiv preprint arXiv:1906.11241, 2019873회 인용 관련 학술자료 전체 3개의 버전 col.«First M87 Event Horizon Telescope Results. IV. Imaging the Central Supermassive Black Hole»K Akiyama\\xa0- Astrophys. J, 20192회 인용 관련 학술자료 First M87 Event Horizon Telescope Results. IV. Imaging the Central Supermassive Black HoleK Asada, P Ho, CWL Huang, PM Koch, S Koyama…\\xa0- The Astrophysical Journal Letters 875 (1), L4, 2019', '호': '1'}, title='First M87 event horizon telescope results. IV. Imaging the central supermassive black hole', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning low-level vision': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2000/10', '게시자': 'Kluwer Academic Publishers', '권': '40', '설명': ' We describe a learning-based method for low-level vision problems—estimating scenes from images. We generate a synthetic world of scenes and their corresponding rendered images, modeling their relationships with a Markov network. Bayesian belief propagation allows us to efficiently find a local maximum of the posterior probability for the scene, given an image. We call this approach VISTA—Vision by Image/Scene TrAining. We apply VISTA to the “super-resolution” problem (estimating high frequency details from a low-resolution image), showing good results. To illustrate the potential breadth of the technique, we also apply it in two other problem domains, both simplified. We learn to distinguish shading from reflectance variations in a single image under particular lighting conditions. For the motion estimation problem in a “blobs world”, we show figure/ground discrimination, solution of the\\xa0…', '저널': 'International journal of computer vision', '저자': 'William T Freeman, Egon C Pasztor, Owen T Carmichael', '전체 인용횟수': '2290회 인용200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023152330647888103108123125119142162141141121147921048970695951', '페이지': '25-47', '학술 문서': 'Learning low-level visionWT Freeman, EC Pasztor, OT Carmichael\\xa0- International journal of computer vision, 20002290회 인용 관련 학술자료 전체 38개의 버전 '}, title='Learning low-level vision', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Shiftable multiscale transforms': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1992/3', '게시자': 'IEEE', '권': '38', '설명': 'One of the major drawbacks of orthogonal wavelet transforms is their lack of translation invariance: the content of wavelet subbands is unstable under translations of the input signal. Wavelet transforms are also unstable with respect to dilations of the input signal and, in two dimensions, rotations of the input signal. The authors formalize these problems by defining a type of translation invariance called shiftability. In the spatial domain, shiftability corresponds to a lack of aliasing; thus, the conditions under which the property holds are specified by the sampling theorem. Shiftability may also be applied in the context of other domains, particularly orientation and scale. Jointly shiftable transforms that are simultaneously shiftable in more than one domain are explored. Two examples of jointly shiftable transforms are designed and implemented: a 1-D transform that is jointly shiftable in position and scale, and a 2-D\\xa0…', '저널': 'IEEE transactions on Information Theory', '저자': 'Eero P Simoncelli, William T Freeman, Edward H Adelson, David J Heeger', '전체 인용횟수': '2058회 인용19921993199419951996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023914253445496661676477779292104115951118963728178795865654044453821', '페이지': '587-607', '학술 문서': 'Shiftable multiscale transformsEP Simoncelli, WT Freeman, EH Adelson, DJ Heeger\\xa0- IEEE transactions on Information Theory, 19922054회 인용 관련 학술자료 전체 27개의 버전 Shiftable multiscale transformEP Siomoncelli, WT Freeman, EH Adelson, D Heege\\xa0- IEEE Transactions on Information Theory, 19924회 인용 관련 학술자료 ', '호': '2'}, title='Shiftable multiscale transforms', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016', '권': '29', '설명': 'We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.', '저널': 'Advances in neural information processing systems', '저자': 'Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, Josh Tenenbaum', '전체 인용횟수': '2018회 인용201620172018201920202021202220239107230339340348297319', '학술 문서': 'Learning a probabilistic latent space of object shapes via 3d generative-adversarial modelingJ Wu, C Zhang, T Xue, B Freeman, J Tenenbaum\\xa0- Advances in neural information processing systems, 20162018회 인용 관련 학술자료 전체 13개의 버전 '}, title='Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Understanding belief propagation and its generalizations': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2003/1/8', '권': '8', '설명': '“Inference” problems arise in statistical physics, computer vision, error-correcting coding theory, and AI. We explain the principles behind the belief propagation (BP) algorithm, which is an efficient way to solve inference problems based on passing local messages. We develop a unified approach, with examples, notation, and graphical models borrowed from the relevant disciplines. We explain the close connection between the BP algorithm and the Bethe approximation of statistical physics. In particular, we show that BP can only converge to a fixed point that is also a stationary point of the Bethe approximation to the free energy. This result helps expain the successes of the BP algorithm, and enables connections to be made with variational approaches to approximate inference. The connection of BP with the Bethe approximation also suggests a way to construct new message passing algorithms based on improvements to Bethe’s approximation introduced by Kikuchi and others. The new generalized belief propagation (GBP) algorithms are significantly more accurate than ordinary BP for some problems. We illustrate how to construct GBP algorithms with a detailed example.', '저널': 'Exploring artificial intelligence in the new millennium', '저자': 'Jonathan S Yedidia, William T Freeman, Yair Weiss', '전체 인용횟수': '2012회 인용2002200320042005200620072008200920102011201220132014201520162017201820192020202120222023123253761038992126111138149115109981131057191102687756', '페이지': '0018-9448', '학술 문서': 'Understanding belief propagation and its generalizationsJS Yedidia, WT Freeman, Y Weiss\\xa0- Exploring artificial intelligence in the new millennium, 20032012회 인용 관련 학술자료 전체 17개의 버전 ', '호': '236-239'}, title='Understanding belief propagation and its generalizations', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Constructing free-energy approximations and generalized belief propagation algorithms': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2005/6/27', '게시자': 'IEEE', '권': '51', '설명': 'Important inference problems in statistical physics, computer vision, error-correcting coding theory, and artificial intelligence can all be reformulated as the computation of marginal probabilities on factor graphs. The belief propagation (BP) algorithm is an efficient way to solve these problems that is exact when the factor graph is a tree, but only approximate when the factor graph has cycles. We show that BP fixed points correspond to the stationary points of the Bethe approximation of the free energy for a factor graph. We explain how to obtain region-based free energy approximations that improve the Bethe approximation, and corresponding generalized belief propagation (GBP) algorithms. We emphasize the conditions a free energy approximation must satisfy in order to be a \"valid\" or \"maxent-normal\" approximation. We describe the relationship between four different methods that can be used to generate valid\\xa0…', '저널': 'IEEE Transactions on information theory', '저자': 'Jonathan S Yedidia, William T Freeman, Yair Weiss', '전체 인용횟수': '1880회 인용200420052006200720082009201020112012201320142015201620172018201920202021202220233056831011061171019914713010712893102777884836547', '페이지': '2282-2312', '학술 문서': 'Constructing free-energy approximations and generalized belief propagation algorithmsJS Yedidia, WT Freeman, Y Weiss\\xa0- IEEE Transactions on information theory, 20051880회 인용 관련 학술자료 전체 40개의 버전 ', '호': '7'}, title='Constructing free-energy approximations and generalized belief propagation algorithms', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Eulerian video magnification for revealing subtle changes in the world': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2012/7/1', '게시자': 'ACM', '권': '31', '설명': 'Our goal is to reveal temporal variations in videos that are difficult or impossible to see with the naked eye and display them in an indicative manner. Our method, which we call Eulerian Video Magnification, takes a standard video sequence as input, and applies spatial decomposition, followed by temporal filtering to the frames. The resulting signal is then amplified to reveal hidden information. Using our method, we are able to visualize the flow of blood as it fills the face and also to amplify and reveal small motions. Our technique can run in real time to show phenomena occurring at the temporal frequencies selected by the user.', '저널': 'ACM transactions on graphics (TOG)', '저자': 'Hao-Yu Wu, Michael Rubinstein, Eugene Shih, John Guttag, Frédo Durand, William Freeman', '전체 인용횟수': '1692회 인용201320142015201620172018201920202021202220234086138157183195185190153196139', '페이지': '1-8', '학술 문서': 'Eulerian video magnification for revealing subtle changes in the worldHY Wu, M Rubinstein, E Shih, J Guttag, F Durand…\\xa0- ACM transactions on graphics (TOG), 20121692회 인용 관련 학술자료 전체 13개의 버전 ', '호': '4'}, title='Eulerian video magnification for revealing subtle changes in the world', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The steerable pyramid: A flexible architecture for multi-scale derivative computation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1995/10/23', '게시자': 'IEEE', '권': '3', '설명': 'We describe an architecture for efficient and accurate linear decomposition of an image into scale and orientation subbands. The basis functions of this decomposition are directional derivative operators of any desired order. We describe the construction and implementation of the transform.', '저자': 'Eero P Simoncelli, William T Freeman', '전체 인용횟수': '1574회 인용1996199719981999200020012002200320042005200620072008200920102011201220132014201520162017201820192020202120222023119131818282933335556576573596373807785931161047261676646', '컨퍼런스': 'Proceedings., International Conference on Image Processing', '페이지': '444-447', '학술 문서': 'The steerable pyramid: A flexible architecture for multi-scale derivative computationEP Simoncelli, WT Freeman\\xa0- Proceedings., International Conference on Image\\xa0…, 19951574회 인용 관련 학술자료 전체 16개의 버전 '}, title='The steerable pyramid: A flexible architecture for multi-scale derivative computation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Understanding and evaluating blind deconvolution algorithms': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2009/6/20', '게시자': 'IEEE', '설명': 'Blind deconvolution is the recovery of a sharp version of a blurred image when the blur kernel is unknown. Recent algorithms have afforded dramatic progress, yet many aspects of the problem remain challenging and hard to understand. The goal of this paper is to analyze and evaluate recent blind deconvolution algorithms both theoretically and experimentally. We explain the previously reported failure of the naive MAP approach by demonstrating that it mostly favors no-blur explanations. On the other hand we show that since the kernel size is often smaller than the image size a MAP estimation of the kernel alone can be well constrained and accurately recover the true blur. The plethora of recent deconvolution techniques makes an experimental evaluation on ground-truth data important. We have collected blur data with ground truth and compared recent algorithms under equal settings. Additionally, our data\\xa0…', '저자': 'Anat Levin, Yair Weiss, Fredo Durand, William T Freeman', '전체 인용횟수': '1543회 인용20092010201120122013201420152016201720182019202020212022202318396478105102134130116119106119126149115', '컨퍼런스': '2009 IEEE conference on computer vision and pattern recognition', '페이지': '1964-1971', '학술 문서': 'Understanding and evaluating blind deconvolution algorithmsA Levin, Y Weiss, F Durand, WT Freeman\\xa0- 2009 IEEE conference on computer vision and pattern\\xa0…, 20091543회 인용 관련 학술자료 전체 19개의 버전 '}, title='Understanding and evaluating blind deconvolution algorithms', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Generalized belief propagation': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2000', '권': '13', '설명': \"Belief propagation (BP) was only supposed to work for tree-like networks but works surprisingly well in many applications involving networks with loops, including turbo codes. However, there has been little understanding of the algorithm or the nature of the solutions it finds for general graphs. We show that BP can only converge to a stationary point of an approximate free energy, known as the Bethe free energy in statis (cid: 173) tical physics. This result characterizes BP fixed-points and makes connections with variational approaches to approximate inference. More importantly, our analysis lets us build on the progress made in statistical physics since Bethe's approximation was introduced in 1935. Kikuchi and others have shown how to construct more ac (cid: 173) curate free energy approximations, of which Bethe's approximation is the simplest. Exploiting the insights from our analysis, we de (cid: 173) rive generalized belief propagation (GBP) versions ofthese Kikuchi approximations. These new message passing algorithms can be significantly more accurate than ordinary BP, at an adjustable in (cid: 173) crease in complexity. We illustrate such a new GBP algorithm on a grid Markov network and show that it gives much more accurate marginal probabilities than those found using ordinary BP.\", '저널': 'Advances in neural information processing systems', '저자': 'Jonathan S Yedidia, William Freeman, Yair Weiss', '전체 인용횟수': '1469회 인용20002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202341939716283696898868983846873485740494851704847', '학술 문서': 'Generalized belief propagationJS Yedidia, W Freeman, Y Weiss\\xa0- Advances in neural information processing systems, 20001469회 인용 관련 학술자료 전체 16개의 버전 '}, title='Generalized belief propagation', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Context-based vision system for place and object recognition': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2003/10/13', '게시자': 'IEEE', '설명': 'While navigating in an environment, a vision system has to be able to recognize where it is and what the main objects in the scene are. We present a context-based vision system for place and object recognition. The goal is to identify familiar locations (e.g., office 610, conference room 941, main street), to categorize new environments (office, corridor, street) and to use that information to provide contextual priors for object recognition (e.g., tables are more likely in an office than a street). We present a low-dimensional global image representation that provides relevant information for place recognition and categorization, and show how such contextual information introduces strong priors that simplify object recognition. We have trained the system to recognize over 60 locations (indoors and outdoors) and to suggest the presence and locations of more than 20 different object types. The algorithm has been integrated\\xa0…', '저널': 'Proceedings Ninth IEEE International Conference on Computer Vision', '저자': 'Torralba', '전체 인용횟수': '1276회 인용2004200520062007200820092010201120122013201420152016201720182019202020212022202328355965697296757610481756670646344553424', '페이지': '273-280 vol. 1', '학술 문서': 'Context-based vision system for place and object recognitionTorralba\\xa0- Proceedings Ninth IEEE International Conference on\\xa0…, 20031276회 인용 관련 학술자료 전체 22개의 버전 '}, title='Context-based vision system for place and object recognition', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Hand gesture machine control system': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '1997/1/14', '발명자': 'William T Freeman, Craig D Weissman', '설명': 'A system for the control from a distance of machines having displays incls hand gesture detection in which the hand gesture causes movement of an on-screen hand icon over an on-screen machine control icon, with the hand icon moving the machine control icon in accordance with sensed hand movements to effectuate machine control. In one embodiment, TV control led by hand signals includes detecting a single hand gesture and providing a hand icon on the screen along with the provision of icons representing TV controls such as volume, channel, color, density, etc., in which a television camera detects the hand in a noisy background through correlation techniques based on values of local image orientation. In order to trigger the system into operation, a trigger gesture such as the\" how\" sign is distinguished from the background through the utilization of orientation angle differences. From correlation values\\xa0…', '전체 인용횟수': '1181회 인용199920002001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202338710467111420294240781521591271309769595723198', '출원번호': '08391955', '특허 번호': '5594469', '특허청': 'US', '학술 문서': 'Hand gesture machine control systemWT Freeman, CD Weissman\\xa0- US Patent 5,594,469, 19971181회 인용 관련 학술자료 전체 2개의 버전 '}, title='Hand gesture machine control system', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Machine learning, a probabilistic perspective': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014/4/3', '게시자': 'Taylor & Francis', '권': '27', '설명': 'The book also introduces the notion of a Bayesian likelihood function (p. 228), which “differs slightly from that in classical statistics.” The only difference I can spot is in the interpretation: Both functions of (θ, x) are numerically the same. Overall, the chapter on Bayesian inference does not spend much time on prior specification. There is a section on conjugate priors that does not mention picking the hyperparameters. While improper priors are introduced as limits of proper priors and as conveying “the least amount of information about [the parameters]”(p. 236), the difficulty in using improper priors for hypothesis testing is not mentioned. Both Chib’s method and the Savage-Dickey density ratio are suggested for the approximation of marginal likelihoods.', '저자': 'Christian Robert', '전체 인용횟수': '15089회 인용201320142015201620172018201920202021202220231854587179691177149518012016219421141778', '출처': 'CHANCE', '페이지': '62-63', '학술 문서': 'Machine learning: a probabilistic perspective*KP Murphy - 201214700회 인용 관련 학술자료 전체 12개의 버전 Machine learning, a probabilistic perspectiveC Robert - 2014411회 인용 관련 학술자료 ', '호': '2'}, title='Machine learning, a probabilistic perspective', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Dynamic bayesian networks: representation, inference and learning': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2002', '기관': 'University of California, Berkeley', '설명': 'Modelling sequential data is important in many areas of science and engineering. Hidden Markov models (HMMs) and Kalman filter models (KFMs) are popular for this because they are simple and flexible. For example, HMMs have been used for speech recognition and bio-sequence analysis, and KFMs have been used for problems ranging from tracking planes and missiles to predicting the economy. However, HMMs and KFMs are limited in their “expressive power”. Dynamic Bayesian Networks (DBNs) generalize HMMs by allowing the state space to be represented in factored form, instead of as a single discrete random variable. DBNs generalize KFMs by allowing arbitrary probability distributions, not just (unimodal) linear-Gaussian. In this thesis, I will discuss how to represent many different kinds of models as DBNs, how to perform exact and approximate inference in DBNs, and how to learn DBN models from\\xa0…', '저자': 'Kevin Patrick Murphy', '전체 인용횟수': '3862회 인용2003200420052006200720082009201020112012201320142015201620172018201920202021202220236294164161170221258263237246228236242200179176159141150127105', '학술 문서': 'Dynamic bayesian networks: representation, inference and learningKP Murphy - 20023862회 인용 관련 학술자료 전체 16개의 버전 '}, title='Dynamic bayesian networks: representation, inference and learning', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Speed/accuracy trade-offs for modern convolutional object detectors': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2017', '설명': 'The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-to-apples comparisons are difficult due to different base feature extractors (eg, VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN (Ren et al., 2015), R-FCN (Dai et al., 2016) and SSD (Liu et al., 2016) systems, which we view as\" meta-architectures\" and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task.', '저자': 'Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, Kevin Murphy', '전체 인용횟수': '3210회 인용201720182019202020212022202398458631667609417283', '컨퍼런스': 'Proceedings of the IEEE conference on computer vision and pattern recognition', '페이지': '7310-7311', '학술 문서': 'Speed/accuracy trade-offs for modern convolutional object detectorsJ Huang, V Rathod, C Sun, M Zhu, A Korattikara…\\xa0- Proceedings of the IEEE conference on computer\\xa0…, 20173204회 인용 관련 학술자료 전체 13개의 버전 Speed and accuracy trade-offs for modern convolutional object detectors*A Fathi, A Korattikara, C Sun, I Fischer, J Huang… - 20177회 인용 관련 학술자료 '}, title='Speed/accuracy trade-offs for modern convolutional object detectors', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Loopy belief propagation for approximate inference: An empirical study': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2013/1/23', '설명': 'Recently, researchers have demonstrated that loopy belief propagation - the use of Pearls polytree algorithm IN a Bayesian network WITH loops OF error- correcting codes.The most dramatic instance OF this IS the near Shannon - limit performance OF Turbo Codes codes whose decoding algorithm IS equivalent TO loopy belief propagation IN a chain - structured Bayesian network. IN this paper we ask : IS there something special about the error - correcting code context, OR does loopy propagation WORK AS an approximate inference schemeIN a more general setting? We compare the marginals computed using loopy propagation TO the exact ones IN four Bayesian network architectures, including two real - world networks : ALARM AND QMR.We find that the loopy beliefs often converge AND WHEN they do, they give a good approximation TO the correct marginals.However,ON the QMR network, the loopy beliefs oscillated AND had no obvious relationship TO the correct posteriors. We present SOME initial investigations INTO the cause OF these oscillations, AND show that SOME simple methods OF preventing them lead TO the wrong results.', '저널': 'arXiv preprint arXiv:1301.6725', '저자': 'Kevin Murphy, Yair Weiss, Michael I Jordan', '전체 인용횟수': '2306회 인용2000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220237252350689693102120981039716914513614511612412210197949160', '학술 문서': 'Loopy belief propagation for approximate inference: An empirical studyK Murphy, Y Weiss, MI Jordan\\xa0- arXiv preprint arXiv:1301.6725, 20132306회 인용 관련 학술자료 전체 10개의 버전 '}, title='Loopy belief propagation for approximate inference: An empirical study', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Knowledge Vault: A Web-scale approach to probabilistic knowledge fusion': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2014', '설명': \"Recent years have witnessed a proliferation of large-scale knowledge bases, including Wikipedia, Freebase, YAGO, Microsoft's Satori, and Google's Knowledge Graph. To increase the scale even further, we need to explore automatic methods for constructing knowledge bases. Previous approaches have primarily focused on text-based extraction, which can be very noisy. Here we introduce Knowledge Vault, a Web-scale probabilistic knowledge base that combines extractions from Web content (obtained via analysis of text, tabular data, page structure, and human annotations) with prior knowledge derived from existing knowledge repositories. We employ supervised machine learning methods for fusing these distinct information sources. The Knowledge Vault is substantially bigger than any previously published structured knowledge repository, and features a probabilistic inference system that computes\\xa0…\", '저자': 'Xin Luna Dong, K Murphy, E Gabrilovich, G Heitz, W Horn, N Lao, Thomas Strohmann, Shaohua Sun, Wei Zhang', '전체 인용횟수': '2129회 인용201420152016201720182019202020212022202325123201240255269262301241188', '컨퍼런스': 'KDD', '학술 문서': 'Knowledge vault: A web-scale approach to probabilistic knowledge fusionX Dong, E Gabrilovich, G Heitz, W Horn, N Lao…\\xa0- Proceedings of the 20th ACM SIGKDD international\\xa0…, 20142129회 인용 관련 학술자료 전체 23개의 버전 '}, title='Knowledge Vault: A Web-scale approach to probabilistic knowledge fusion', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Rao-Blackwellised particle filtering for dynamic Bayesian networks': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2001/1', '게시자': 'Springer New York', '도서': 'Sequential Monte Carlo methods in practice', '설명': ' Particle filtering in high dimensional state-spaces can be inefficient because a large number of samples is needed to represent the posterior. A standard technique to increase the efficiency of sampling techniques is to reduce the size of the state space by marginalizing out some of the variables analytically; this is called Rao-Blackwellisation (Casella and Robert 1996). Combining these two techniques results in Rao-Blackwellised particle filtering (RBPF) (Doucet 1998, Doucet, de Freitas, Murphy and Russell 2000). In this chapter, we explain RBPF, discuss when it can be used, and give a detailed example of its application to the problem of map learning for a mobile robot, which has a very large (~ 2100) discrete state space.', '저자': 'Kevin Murphy, Stuart Russell', '전체 인용횟수': '1991회 인용2001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202362343507086112107107941031021041111071261091229289776964', '페이지': '499-515', '학술 문서': 'Rao-Blackwellised particle filtering for dynamic Bayesian networksK Murphy, S Russell\\xa0- Sequential Monte Carlo methods in practice, 20011991회 인용 관련 학술자료 전체 29개의 버전 Rao-blackwellised particle ﬁltering for dynamic bayesian networks*A Doucet, N de Freitas, K Murphy, S Russell\\xa0- UAl-20003회 인용 관련 학술자료 '}, title='Rao-Blackwellised particle filtering for dynamic Bayesian networks', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'A review of relational machine learning for knowledge graphs': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2015/12/17', '게시자': 'IEEE', '권': '104', '설명': 'Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be “trained” on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive data sets. The first is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing\\xa0…', '저자': 'Maximilian Nickel, Kevin Murphy, Volker Tresp, Evgeniy Gabrilovich', '전체 인용횟수': '1770회 인용2015201620172018201920202021202220232777149197234301311262195', '출처': 'Proceedings of the IEEE', '페이지': '11-33', '학술 문서': 'A review of relational machine learning for knowledge graphsM Nickel, K Murphy, V Tresp, E Gabrilovich\\xa0- Proceedings of the IEEE, 20151770회 인용 관련 학술자료 전체 19개의 버전 ', '호': '1'}, title='A review of relational machine learning for knowledge graphs', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'The bayes net toolbox for matlab': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2001', '권': '33', '저널': 'Computing science and statistics', '저자': 'Kevin Murphy', '전체 인용횟수': '1727회 인용2001200220032004200520062007200820092010201120122013201420152016201720182019202020212022202351843557692113861071199497102981129685716862443629', '학술 문서': 'The bayes net toolbox for matlabK Murphy\\xa0- Computing science and statistics, 20011727회 인용 관련 학술자료 전체 2개의 버전 '}, title='The bayes net toolbox for matlab', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " 'Deep variational information bottleneck': Paper(DOI=None, crossref_json=None, google_schorlar_metadata={'게시 날짜': '2016/12/1', '설명': 'We present a variational approximation to the information bottleneck of Tishby et al. (1999). This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training. We call this method \"Deep Variational Information Bottleneck\", or Deep VIB. We show that models trained with the VIB objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack.', '저널': 'arXiv preprint arXiv:1612.00410', '저자': 'Alexander A Alemi, Ian Fischer, Joshua V Dillon, Kevin Murphy', '전체 인용횟수': '1466회 인용20172018201920202021202220231881146248290339337', '학술 문서': 'Deep variational information bottleneckAA Alemi, I Fischer, JV Dillon, K Murphy\\xa0- arXiv preprint arXiv:1612.00410, 20161466회 인용 관련 학술자료 전체 4개의 버전 '}, title='Deep variational information bottleneck', authors=None, abstract=None, conference=None, journal=None, year=None, reference_list=None, referenced_list=None, cite_bibtex=None),\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "def get_doi_from_title(title):\n",
    "    encoded_title = urllib.parse.quote_plus(title)\n",
    "    api_url = f\"https://api.crossref.org/works?query.title={encoded_title}\"\n",
    "    response = requests.get(api_url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        items = data['message'].get('items', [])\n",
    "        if items:\n",
    "            # Assuming the first result is the correct one\n",
    "            return items[0].get('DOI')\n",
    "    return None\n",
    "\n",
    "def get_references_from_doi(doi):\n",
    "    api_url = f\"https://api.crossref.org/works/{doi}\"\n",
    "    response = requests.get(api_url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        references = data['message'].get('reference', [])\n",
    "        return references\n",
    "    return []\n",
    "\n",
    "\n",
    "'''\n",
    "# Example usage:\n",
    "paper_title = \"Example Title of Your Academic Paper\"\n",
    "paper_title = \"3D Gaussian Splatting for Real-Time Radiance Field Rendering\"\n",
    "paper_title = \"Sound-Guided Semantic Image Manipulation\"\n",
    "\n",
    "paper_doi = get_doi_from_title(paper_title)\n",
    "\n",
    "if paper_doi:\n",
    "    print(f\"DOI for '{paper_title}': {paper_doi}\")\n",
    "    references = get_references_from_doi(paper_doi)\n",
    "    if references:\n",
    "        print(\"References:\")\n",
    "        for ref in references:\n",
    "            print(ref.get('DOI', 'No DOI provided'))\n",
    "else:\n",
    "    print(f\"No DOI found for the paper titled '{paper_title}'\")\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def getMetaData(title) :\n",
    "\n",
    "    # URL encode the title to ensure it's in the correct format for a URL\n",
    "    encoded_title = urllib.parse.quote_plus(title)\n",
    "\n",
    "    # The Crossref API endpoint for works\n",
    "    api_url = f\"https://api.crossref.org/works?query.title={encoded_title}\"\n",
    "\n",
    "    # Make the GET request to the Crossref API\n",
    "    response = requests.get(api_url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the response to JSON\n",
    "        data = response.json()\n",
    "\n",
    "        # Check if there are items in the message\n",
    "        if 'items' in data['message']:\n",
    "            # Loop through the items to find the first one with a DOI\n",
    "            for item in data['message']['items']:\n",
    "                # Print the DOI\n",
    "                #print(\"Title:\", item.get('title')[0])\n",
    "                #print(\"DOI:\", item.get('DOI'))\n",
    "                break\n",
    "        else:\n",
    "            print(\"No results found for this title.\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "class CrossRefHandler :\n",
    "    def __init__(self) :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Time series causal analyses of the logistics industry market size and its growth in Korea\n",
      "DOI: 10.15735/kls.2008.16.1.005\n",
      "\n",
      "Title: Effects of low intensity pulsed ultrasound stimulation on metabolic lipolysis of adipocytes\n",
      "DOI: 10.1101/2022.03.29.486238\n",
      "\n",
      "Title: Low-intensity pulsed ultrasound induces metabolic lipolysis of adipocytes\n",
      "DOI: 10.1121/10.0007709\n",
      "\n",
      "Title: Effects of Facebook Fan Page Users’ Social Capital Needs, Innovativeness and Self-Presentation Needs on Interactive Behavior: Brand Identification as a Moderator Variable\n",
      "DOI: 10.17485/ijst/2016/v9i26/97277\n",
      "\n",
      "Title: The effect of youtube Pre-Roll advertising on VTR (View through rate) and CTR (Click through rate)\n",
      "DOI: 10.5958/0976-5506.2018.01036.7\n",
      "\n",
      "Title: An Analysis on the Degree of Importance of Multifactorial Value of Agriculture-Rural Village and Policies' Public Relations\n",
      "DOI: 10.21331/jprapr.2014.7.3.001\n",
      "\n",
      "Title: Which Company is the Best for Green Advertising?: The Effects of Green Advertising on Cosumer Response Focused on Advertiser Characteristics\n",
      "DOI: 10.17485/ijst/2015/v8is7/70913\n",
      "\n",
      "Title: A Content Analysis of Green Advertising Claims in Korea\n",
      "DOI: 10.17485/ijst/2016/v9i29/94763\n",
      "\n",
      "Title: Multiplexed Ultrasound Imaging Using Spectral Analysis on Gas Vesicles\n",
      "DOI: 10.1002/adhm.202200568\n",
      "\n",
      "Title: Z-section and 3D rendering imaging using 130 and 50 MHz ultrasound and bacteria generated ultrasound contrast agents\n",
      "DOI: 10.1121/10.0007721\n",
      "\n",
      "Title: Multiplexed Ultrasound Imaging Using Spectral Analysis on Gas Vesicles (Adv. Healthcare Mater. 17/2022)\n",
      "DOI: 10.1002/adhm.202270103\n",
      "\n",
      "Title: The Role and Design of Intelligence Oversight System in the Digital Age\n",
      "DOI: 10.38176/publiclaw.2023.2.51.3.677\n",
      "\n",
      "Title: Learning Hand Articulations by Hallucinating Heat Distribution\n",
      "DOI: 10.1109/iccv.2017.337\n",
      "\n",
      "Title: A Co-orientation Model Based Comparison of the Evaluation of PR Firms' Professionalism between the In-house PR Officers and the Account Executives of PR Firm\n",
      "DOI: 10.15814/jpr.2009.13.2.38\n",
      "\n",
      "Title: What is the Role of TV Commercials in the Trans-Media Era?\n",
      "DOI: 10.17485/ijst/2016/v9i41/103846\n",
      "\n",
      "Title: Multiplexed ultrasound imaging using clustered gas vesicles and spectral imaging\n",
      "DOI: 10.1101/2021.10.15.464596\n",
      "\n",
      "Title: Dual Policy Learning for Aggregation Optimization in Graph Neural Network-based Recommender Systems\n",
      "DOI: 10.1145/3543507.3583241\n",
      "\n",
      "Title: Relationship between Spatio-Temporal Travel Patterns Derived from Smart-Card Data and Local Environmental Characteristics of Seoul, Korea\n",
      "DOI: 10.3390/su10030787\n",
      "\n",
      "Title: Analysis of Fire Accident Factors on Construction Sites Using Web Crawling and Deep Learning Approach\n",
      "DOI: 10.3390/su132111694\n",
      "\n",
      "Title: The Portrayal of Older People in Television Advertisements: A Cross-Cultural Content Analysis of the United States and South Korea\n",
      "DOI: 10.2190/ellg-jely-uccy-4l8m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "# Replace 'author_name' with the name of the author you're searching for\n",
    "author_name = \"Sangpil Kim\"\n",
    "encoded_author_name = urllib.parse.quote_plus(author_name)\n",
    "\n",
    "# The Crossref API endpoint for works\n",
    "api_url = f\"https://api.crossref.org/works?query.author={encoded_author_name}\"\n",
    "\n",
    "# Make the GET request to the Crossref API\n",
    "response = requests.get(api_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the response to JSON\n",
    "    data = response.json()\n",
    "\n",
    "    # Loop through the items and print the titles and DOIs\n",
    "    for item in data['message']['items']:\n",
    "        title = item.get('title')[0] if item.get('title') else 'No title provided'\n",
    "        doi = item.get('DOI', 'No DOI provided')\n",
    "        print(f\"Title: {title}\")\n",
    "        print(f\"DOI: {doi}\")\n",
    "        print()\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 2, '2': 2, '3': 3, '4': 4, 'a': 3, 'b': 4}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic1 = {\"1\": 1, \"2\":2, \"3\":3, \"4\":4}\n",
    "\n",
    "dic2 = {\"1\": 2, \"a\": 3, \"b\": 4}\n",
    "\n",
    "dic1.update(dic2)\n",
    "\n",
    "dic1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112/112 [00:11<00:00,  9.50it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for title in tqdm(title_list) :\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Darwin'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "first",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
